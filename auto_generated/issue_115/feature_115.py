#!/usr/bin/env python3
"""
Perfect Knowledge Injection 2.0 - 3æ®µéšçŸ¥è­˜æ³¨å…¥ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£… (Issue #115)

Advanced Knowledge Injection System with:
- æ®µéšçš„çŸ¥è­˜æ³¨å…¥: Basic â†’ Contextual â†’ Expert ãƒ¬ãƒ™ãƒ«
- å‹•çš„çŸ¥è­˜é©å¿œ: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŸ¥è­˜æ›´æ–°ãƒ»æœ€é©åŒ–
- å“è³ªä¿è¨¼çµ±åˆ: æ³¨å…¥çŸ¥è­˜ã®æ¤œè¨¼ãƒ»ç›£æŸ»ã‚·ã‚¹ãƒ†ãƒ 
- ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢: ãƒ™ã‚¯ãƒˆãƒ«åŒ–çŸ¥è­˜æ¤œç´¢ãƒ»é–¢é€£æ€§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

Author: ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼ï¼ˆClaude Elderï¼‰
Created: 2025/07/22 - Issue #115å®Œå…¨å®Ÿè£…ç‰ˆ
"""

import asyncio
import hashlib
import json
import logging
import pickle
import sqlite3
import time
from datetime import datetime, timezone, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union
import threading
from collections import defaultdict, deque
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss

logger = logging.getLogger(__name__)

class KnowledgeLevel(Enum):
    """çŸ¥è­˜ãƒ¬ãƒ™ãƒ«"""
    BASIC = "basic"
    CONTEXTUAL = "contextual" 
    EXPERT = "expert"

class InjectionStrategy(Enum):
    """æ³¨å…¥æˆ¦ç•¥"""
    SEQUENTIAL = "sequential"      # æ®µéšçš„æ³¨å…¥
    PARALLEL = "parallel"         # ä¸¦åˆ—æ³¨å…¥
    ADAPTIVE = "adaptive"         # é©å¿œå‹æ³¨å…¥
    SMART_MERGE = "smart_merge"   # ã‚¹ãƒãƒ¼ãƒˆãƒãƒ¼ã‚¸

class KnowledgeQuality(Enum):
    """çŸ¥è­˜å“è³ªãƒ¬ãƒ™ãƒ«"""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    EXPERT = 4
    ELITE = 5

@dataclass
class KnowledgeChunk:
    """çŸ¥è­˜ãƒãƒ£ãƒ³ã‚¯"""
    id: str
    content: str
    level: KnowledgeLevel
    quality: KnowledgeQuality
    tags: List[str]
    embeddings: Optional[np.ndarray]
    metadata: Dict[str, Any]
    created_at: datetime
    updated_at: datetime
    access_count: int = 0
    relevance_score: float = 0.0

@dataclass
class InjectionResult:
    """æ³¨å…¥çµæœ"""
    success: bool
    chunks_injected: int
    injection_time: float
    quality_score: float
    strategy_used: InjectionStrategy
    level_distribution: Dict[KnowledgeLevel, int]
    errors: List[str]
    warnings: List[str]
    metadata: Dict[str, Any]

@dataclass
class KnowledgeContext:
    """çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""
    task_type: str
    domain: str
    complexity_level: int  # 1-10
    required_expertise: List[str]
    user_preferences: Dict[str, Any]
    session_history: List[str]
    performance_constraints: Dict[str, Any]

class SemanticSearchEngine:
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)
        self.index = None
        self.dimension = 384  # all-MiniLM-L6-v2ã®æ¬¡å…ƒæ•°
        self.chunk_map: Dict[int, str] = {}
        self._build_index()
    
    def _build_index(self):
        """FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        self.index = faiss.IndexFlatIP(self.dimension)  # Inner Product for cosine similarity
    
    def add_chunks(self, chunks: List[KnowledgeChunk]):
        """ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ """
        if not chunks:
            return
        
        embeddings = []
        for i, chunk in enumerate(chunks):
            if chunk.embeddings is None:
                # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
                chunk.embeddings = self.model.encode([chunk.content])[0]
                chunk.embeddings = chunk.embeddings / np.linalg.norm(chunk.embeddings)  # normalize
            
            embeddings.append(chunk.embeddings)
            self.chunk_map[self.index.ntotal + i] = chunk.id
        
        if embeddings:
            embeddings_array = np.array(embeddings).astype('float32')
            self.index.add(embeddings_array)
    
    def search(self, query: str, k: int = 10) -> List[Tuple[str, float]]:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Ÿè¡Œ"""
        if self.index.ntotal == 0:
            return []
        
        query_embedding = self.model.encode([query])[0]
        query_embedding = query_embedding / np.linalg.norm(query_embedding)
        query_embedding = query_embedding.astype('float32').reshape(1, -1)
        
        scores, indices = self.index.search(query_embedding, min(k, self.index.ntotal))
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx != -1 and idx in self.chunk_map:
                results.append((self.chunk_map[idx], float(score)))
        
        return results

class KnowledgeQualityAssurance:
    """çŸ¥è­˜å“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.quality_rules = self._load_quality_rules()
        self.validation_history = []
    
    def validate_chunk(self, chunk: KnowledgeChunk) -> Dict[str, Any]:
        """ãƒãƒ£ãƒ³ã‚¯å“è³ªæ¤œè¨¼"""
        issues = []
        warnings = []
        score = 100.0
        
        # å†…å®¹å“è³ªãƒã‚§ãƒƒã‚¯
        if len(chunk.content) < 50:
            issues.append("Content too short")
            score -= 20
        
        if not chunk.content.strip():
            issues.append("Empty content")
            score = 0
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        if not chunk.tags:
            warnings.append("No tags specified")
            score -= 5
        
        if not chunk.metadata:
            warnings.append("No metadata provided")
            score -= 5
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        word_count = len(chunk.content.split())
        if word_count < 10:
            issues.append("Content lacks semantic richness")
            score -= 15
        elif word_count > 1000:
            warnings.append("Content might be too verbose")
            score -= 5
        
        # å“è³ªã‚¹ã‚³ã‚¢ã«åŸºã¥ãå“è³ªãƒ¬ãƒ™ãƒ«æ±ºå®š
        if score >= 90:
            suggested_quality = KnowledgeQuality.ELITE
        elif score >= 80:
            suggested_quality = KnowledgeQuality.EXPERT
        elif score >= 70:
            suggested_quality = KnowledgeQuality.HIGH
        elif score >= 60:
            suggested_quality = KnowledgeQuality.MEDIUM
        else:
            suggested_quality = KnowledgeQuality.LOW
        
        return {
            "quality_score": score,
            "suggested_quality": suggested_quality,
            "issues": issues,
            "warnings": warnings,
            "is_valid": len(issues) == 0
        }
    
    def _load_quality_rules(self) -> Dict[str, Any]:
        """å“è³ªãƒ«ãƒ¼ãƒ«èª­ã¿è¾¼ã¿"""
        return {
            "min_content_length": 50,
            "max_content_length": 10000,
            "required_tags": ["domain", "difficulty"],
            "semantic_coherence_threshold": 0.7
        }

class PerfectKnowledgeInjector:
    """Perfect Knowledge Injection 2.0 ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, 
                 db_path: str = "/home/aicompany/ai_co/data/knowledge_injection_v2.db",
                 enable_semantic_search: bool = True):
        self.db_path = db_path
        self.enable_semantic_search = enable_semantic_search
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.search_engine = SemanticSearchEngine() if enable_semantic_search else None
        self.quality_assurance = KnowledgeQualityAssurance()
        
        # çŸ¥è­˜ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.knowledge_store: Dict[str, KnowledgeChunk] = {}
        self.injection_history: List[InjectionResult] = []
        
        # çµ±è¨ˆãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.access_stats = defaultdict(int)
        self.performance_cache = {}
        self.cache_ttl = 3600  # 1æ™‚é–“
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self._init_database()
        self._load_knowledge_base()
    
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        try:
            Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
            
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS knowledge_chunks (
                        id TEXT PRIMARY KEY,
                        content TEXT NOT NULL,
                        level TEXT NOT NULL,
                        quality INTEGER NOT NULL,
                        tags TEXT NOT NULL,
                        embeddings BLOB,
                        metadata TEXT NOT NULL,
                        created_at TEXT NOT NULL,
                        updated_at TEXT NOT NULL,
                        access_count INTEGER DEFAULT 0,
                        relevance_score REAL DEFAULT 0.0
                    )
                """)
                
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS injection_results (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp TEXT NOT NULL,
                        success BOOLEAN NOT NULL,
                        chunks_injected INTEGER NOT NULL,
                        injection_time REAL NOT NULL,
                        quality_score REAL NOT NULL,
                        strategy_used TEXT NOT NULL,
                        level_distribution TEXT NOT NULL,
                        errors TEXT NOT NULL,
                        warnings TEXT NOT NULL,
                        metadata TEXT NOT NULL
                    )
                """)
                
                conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_chunks_level ON knowledge_chunks(level)
                """)
                
                conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_chunks_quality ON knowledge_chunks(quality)
                """)
                
                conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_results_timestamp ON injection_results(timestamp)
                """)
                
                conn.commit()
                
        except Exception as e:
            logger.error(f"Database initialization error: {e}")
    
    def _load_knowledge_base(self):
        """çŸ¥è­˜ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT * FROM knowledge_chunks")
                rows = cursor.fetchall()
                
                chunks = []
                for row in rows:
                    chunk = KnowledgeChunk(
                        id=row[0],
                        content=row[1],
                        level=KnowledgeLevel(row[2]),
                        quality=KnowledgeQuality(row[3]),
                        tags=json.loads(row[4]),
                        embeddings=pickle.loads(row[5]) if row[5] else None,
                        metadata=json.loads(row[6]),
                        created_at=datetime.fromisoformat(row[7]),
                        updated_at=datetime.fromisoformat(row[8]),
                        access_count=row[9],
                        relevance_score=row[10]
                    )
                    self.knowledge_store[chunk.id] = chunk
                    chunks.append(chunk)
                
                # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
                if self.search_engine and chunks:
                    self.search_engine.add_chunks(chunks)
                
                logger.info(f"Loaded {len(chunks)} knowledge chunks")
                
        except Exception as e:
            logger.error(f"Knowledge base loading error: {e}")
    
    def add_knowledge(self, 
                     content: str, 
                     level: KnowledgeLevel, 
                     tags: List[str], 
                     metadata: Optional[Dict[str, Any]] = None) -> str:
        """çŸ¥è­˜è¿½åŠ """
        
        # çŸ¥è­˜ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
        chunk_id = hashlib.md5(content.encode()).hexdigest()
        chunk = KnowledgeChunk(
            id=chunk_id,
            content=content,
            level=level,
            quality=KnowledgeQuality.MEDIUM,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            tags=tags,
            embeddings=None,  # å¾Œã§ç”Ÿæˆ
            metadata=metadata or {},
            created_at=datetime.now(timezone.utc),
            updated_at=datetime.now(timezone.utc)
        )
        
        # å“è³ªæ¤œè¨¼
        validation_result = self.quality_assurance.validate_chunk(chunk)
        if not validation_result["is_valid"]:
            raise ValueError(f"Knowledge quality validation failed: {validation_result['issues']}")
        
        chunk.quality = validation_result["suggested_quality"]
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ç”¨ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
        if self.search_engine:
            self.search_engine.add_chunks([chunk])
        
        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«è¿½åŠ 
        self.knowledge_store[chunk_id] = chunk
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
        self._save_chunk_to_db(chunk)
        
        logger.info(f"Added knowledge chunk {chunk_id} with quality {chunk.quality.name}")
        return chunk_id
    
    def _save_chunk_to_db(self, chunk: KnowledgeChunk):
        """ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO knowledge_chunks
                    (id, content, level, quality, tags, embeddings, metadata, 
                     created_at, updated_at, access_count, relevance_score)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    chunk.id,
                    chunk.content,
                    chunk.level.value,
                    chunk.quality.value,
                    json.dumps(chunk.tags),
                    pickle.dumps(chunk.embeddings) if chunk.embeddings is not None else None,
                    json.dumps(chunk.metadata),
                    chunk.created_at.isoformat(),
                    chunk.updated_at.isoformat(),
                    chunk.access_count,
                    chunk.relevance_score
                ))
                conn.commit()
        except Exception as e:
            logger.error(f"Database save error: {e}")
    
    async def inject_knowledge(self, 
                              context: KnowledgeContext, 
                              strategy: InjectionStrategy = InjectionStrategy.ADAPTIVE) -> InjectionResult:
        """çŸ¥è­˜æ³¨å…¥å®Ÿè¡Œ"""
        
        start_time = time.time()
        errors = []
        warnings = []
        injected_chunks = []
        
        try:
            # 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã«ã‚ˆã‚‹çŸ¥è­˜é¸æŠ
            relevant_chunks = await self._select_relevant_knowledge(context)
            if not relevant_chunks:
                warnings.append("No relevant knowledge found for context")
                return self._create_injection_result(
                    success=False, chunks_injected=0, 
                    injection_time=time.time() - start_time,
                    errors=["No relevant knowledge available"],
                    warnings=warnings, strategy_used=strategy
                )
            
            # 2. æ³¨å…¥æˆ¦ç•¥ã«åŸºã¥ãå‡¦ç†
            if strategy == InjectionStrategy.SEQUENTIAL:
                injected_chunks = await self._sequential_injection(relevant_chunks, context)
            elif strategy == InjectionStrategy.PARALLEL:
                injected_chunks = await self._parallel_injection(relevant_chunks, context)
            elif strategy == InjectionStrategy.ADAPTIVE:
                injected_chunks = await self._adaptive_injection(relevant_chunks, context)
            elif strategy == InjectionStrategy.SMART_MERGE:
                injected_chunks = await self._smart_merge_injection(relevant_chunks, context)
            
            # 3. å“è³ªæ¤œè¨¼
            quality_score = await self._calculate_injection_quality(injected_chunks, context)
            
            # 4. çµ±è¨ˆæ›´æ–°
            for chunk in injected_chunks:
                chunk.access_count += 1
                self.access_stats[chunk.id] += 1
                self._save_chunk_to_db(chunk)
            
            # 5. çµæœä½œæˆ
            injection_time = time.time() - start_time
            result = self._create_injection_result(
                success=True,
                chunks_injected=len(injected_chunks),
                injection_time=injection_time,
                quality_score=quality_score,
                strategy_used=strategy,
                level_distribution=self._calculate_level_distribution(injected_chunks),
                errors=errors,
                warnings=warnings
            )
            
            # 6. çµæœä¿å­˜
            self._save_injection_result(result)
            self.injection_history.append(result)
            
            logger.info(f"Knowledge injection completed: {len(injected_chunks)} chunks, quality={quality_score:.2f}")
            return result
            
        except Exception as e:
            errors.append(str(e))
            logger.error(f"Knowledge injection failed: {e}")
            return self._create_injection_result(
                success=False, chunks_injected=0,
                injection_time=time.time() - start_time,
                errors=errors, warnings=warnings,
                strategy_used=strategy
            )
    
    async def _select_relevant_knowledge(self, context: KnowledgeContext) -> List[KnowledgeChunk]:
        """é–¢é€£çŸ¥è­˜é¸æŠ"""
        relevant_chunks = []
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã«ã‚ˆã‚‹é–¢é€£æ€§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        if self.search_engine:
            query = f"{context.task_type} {context.domain} {' '.join(context.required_expertise)}"
            search_results = self.search_engine.search(query, k=50)
            
            for chunk_id, relevance_score in search_results:
                if chunk_id in self.knowledge_store:
                    chunk = self.knowledge_store[chunk_id]
                    chunk.relevance_score = relevance_score
                    
                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé©åˆæ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if self._is_context_compatible(chunk, context):
                        relevant_chunks.append(chunk)
        
        # å“è³ªãƒ»ãƒ¬ãƒ™ãƒ«åˆ¥ã‚½ãƒ¼ãƒˆ
        relevant_chunks.sort(key=lambda c: (c.relevance_score, c.quality.value), reverse=True)
        
        # è¤‡é›‘åº¦ã«å¿œã˜ãŸé¸æŠæ•°èª¿æ•´
        max_chunks = min(20, max(5, context.complexity_level * 2))
        return relevant_chunks[:max_chunks]
    
    def _is_context_compatible(self, chunk: KnowledgeChunk, context: KnowledgeContext) -> bool:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé©åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        if context.domain and context.domain not in chunk.tags:
            return False
        
        # è¤‡é›‘åº¦ãƒ¬ãƒ™ãƒ«é©åˆãƒã‚§ãƒƒã‚¯
        level_complexity = {
            KnowledgeLevel.BASIC: 3,
            KnowledgeLevel.CONTEXTUAL: 6,
            KnowledgeLevel.EXPERT: 10
        }
        
        if level_complexity[chunk.level] > context.complexity_level + 2:
            return False
        
        return True
    
    async def _sequential_injection(self, chunks: List[KnowledgeChunk], context: KnowledgeContext) -> List[KnowledgeChunk]:
        """æ®µéšçš„æ³¨å…¥"""
        injected = []
        
        # ãƒ¬ãƒ™ãƒ«é †ã§æ®µéšçš„æ³¨å…¥
        for level in [KnowledgeLevel.BASIC, KnowledgeLevel.CONTEXTUAL, KnowledgeLevel.EXPERT]:
            level_chunks = [c for c in chunks if c.level == level]
            for chunk in level_chunks[:5]:  # å„ãƒ¬ãƒ™ãƒ«æœ€å¤§5ãƒãƒ£ãƒ³ã‚¯
                injected.append(chunk)
                await asyncio.sleep(0.001)  # éåŒæœŸå‡¦ç†ã®ãŸã‚
        
        return injected
    
    async def _parallel_injection(self, chunks: List[KnowledgeChunk], context: KnowledgeContext) -> List[KnowledgeChunk]:
        """ä¸¦åˆ—æ³¨å…¥"""
        # ä¸¦åˆ—å‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        tasks = []
        for chunk in chunks[:15]:  # æœ€å¤§15ãƒãƒ£ãƒ³ã‚¯
            tasks.append(self._process_chunk_async(chunk, context))
        
        results = await asyncio.gather(*tasks)
        return [chunk for chunk, success in results if success]
    
    async def _adaptive_injection(self, chunks: List[KnowledgeChunk], context: KnowledgeContext) -> List[KnowledgeChunk]:
        """é©å¿œå‹æ³¨å…¥"""
        injected = []
        
        # å‹•çš„å“è³ªé–¾å€¤èª¿æ•´
        quality_threshold = max(1, context.complexity_level // 2)
        
        for chunk in chunks:
            if chunk.quality.value >= quality_threshold:
                injected.append(chunk)
                
                # å‹•çš„é–¾å€¤èª¿æ•´
                if len(injected) > 10:
                    quality_threshold += 1
                    
            if len(injected) >= 20:
                break
        
        return injected
    
    async def _smart_merge_injection(self, chunks: List[KnowledgeChunk], context: KnowledgeContext) -> List[KnowledgeChunk]:
        """ã‚¹ãƒãƒ¼ãƒˆãƒãƒ¼ã‚¸æ³¨å…¥"""
        # é‡è¤‡å†…å®¹ã®ãƒãƒ¼ã‚¸ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        merged_chunks = []
        processed_content = set()
        
        for chunk in chunks:
            content_hash = hashlib.md5(chunk.content[:100].encode()).hexdigest()
            
            if content_hash not in processed_content:
                processed_content.add(content_hash)
                merged_chunks.append(chunk)
                
            if len(merged_chunks) >= 15:
                break
        
        return merged_chunks
    
    async def _process_chunk_async(self, chunk: KnowledgeChunk, context: KnowledgeContext) -> Tuple[KnowledgeChunk, bool]:
        """ãƒãƒ£ãƒ³ã‚¯éåŒæœŸå‡¦ç†"""
        await asyncio.sleep(0.001)  # éåŒæœŸå‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        return chunk, True
    
    async def _calculate_injection_quality(self, chunks: List[KnowledgeChunk], context: KnowledgeContext) -> float:
        """æ³¨å…¥å“è³ªè¨ˆç®—"""
        if not chunks:
            return 0.0
        
        # åŸºæœ¬å“è³ªã‚¹ã‚³ã‚¢
        quality_scores = [chunk.quality.value for chunk in chunks]
        base_score = sum(quality_scores) / len(quality_scores) * 20  # 0-100ã‚¹ã‚±ãƒ¼ãƒ«
        
        # é–¢é€£æ€§ã‚¹ã‚³ã‚¢
        relevance_scores = [chunk.relevance_score for chunk in chunks]
        relevance_score = sum(relevance_scores) / len(relevance_scores) * 100
        
        # å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢
        level_diversity = len(set(chunk.level for chunk in chunks)) / 3 * 100
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        overall_score = (base_score * 0.4 + relevance_score * 0.4 + level_diversity * 0.2)
        return min(100.0, overall_score)
    
    def _calculate_level_distribution(self, chunks: List[KnowledgeChunk]) -> Dict[KnowledgeLevel, int]:
        """ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒè¨ˆç®—"""
        distribution = {level: 0 for level in KnowledgeLevel}
        for chunk in chunks:
            distribution[chunk.level] += 1
        return distribution
    
    def _create_injection_result(self, **kwargs) -> InjectionResult:
        """æ³¨å…¥çµæœä½œæˆ"""
        return InjectionResult(
            success=kwargs.get('success', False),
            chunks_injected=kwargs.get('chunks_injected', 0),
            injection_time=kwargs.get('injection_time', 0.0),
            quality_score=kwargs.get('quality_score', 0.0),
            strategy_used=kwargs.get('strategy_used', InjectionStrategy.ADAPTIVE),
            level_distribution=kwargs.get('level_distribution', {}),
            errors=kwargs.get('errors', []),
            warnings=kwargs.get('warnings', []),
            metadata=kwargs.get('metadata', {})
        )
    
    def _save_injection_result(self, result: InjectionResult):
        """æ³¨å…¥çµæœä¿å­˜"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT INTO injection_results
                    (timestamp, success, chunks_injected, injection_time, quality_score,
                     strategy_used, level_distribution, errors, warnings, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    datetime.now(timezone.utc).isoformat(),
                    result.success,
                    result.chunks_injected,
                    result.injection_time,
                    result.quality_score,
                    result.strategy_used.value,
                    json.dumps({k.value: v for k, v in result.level_distribution.items()}),
                    json.dumps(result.errors),
                    json.dumps(result.warnings),
                    json.dumps(result.metadata)
                ))
                conn.commit()
        except Exception as e:
            logger.error(f"Result save error: {e}")
    
    def get_injection_statistics(self) -> Dict[str, Any]:
        """æ³¨å…¥çµ±è¨ˆå–å¾—"""
        if not self.injection_history:
            return {"message": "No injection history available"}
        
        successful_injections = [r for r in self.injection_history if r.success]
        
        return {
            "total_injections": len(self.injection_history),
            "successful_injections": len(successful_injections),
            "success_rate": len(successful_injections) / len(self.injection_history) * 100,
            "average_quality_score": sum(r.quality_score for r in successful_injections) / len(successful_injections) if successful_injections else 0,
            "average_injection_time": sum(r.injection_time for r in successful_injections) / len(successful_injections) if successful_injections else 0,
            "total_chunks_in_store": len(self.knowledge_store),
            "most_accessed_chunks": sorted(self.access_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        }
    
    def export_knowledge_base(self, format: str = "json") -> str:
        """çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if format == "json":
            export_data = {}
            for chunk_id, chunk in self.knowledge_store.items():
                export_data[chunk_id] = {
                    "content": chunk.content,
                    "level": chunk.level.value,
                    "quality": chunk.quality.value,
                    "tags": chunk.tags,
                    "metadata": chunk.metadata,
                    "access_count": chunk.access_count,
                    "relevance_score": chunk.relevance_score
                }
            return json.dumps(export_data, indent=2, ensure_ascii=False)
        else:
            raise ValueError(f"Unsupported export format: {format}")


# ä½¿ç”¨ä¾‹ã¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
async def initialize_sample_knowledge_base(injector: PerfectKnowledgeInjector):
    """ã‚µãƒ³ãƒ—ãƒ«çŸ¥è­˜ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
    
    sample_knowledge = [
        {
            "content": "Pythonã®åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿å‹ã«ã¯ã€int, float, str, bool, list, dict, tupleãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã¯æœ€ã‚‚é »ç¹ã«ä½¿ç”¨ã•ã‚Œã‚‹å‹ã§ã™ã€‚",
            "level": KnowledgeLevel.BASIC,
            "tags": ["python", "basics", "data-types"],
            "metadata": {"domain": "programming", "difficulty": 1}
        },
        {
            "content": "éåŒæœŸãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã§ã¯ã€async/awaitã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ«ãƒ¼ãƒãƒ³ã‚’å®šç¾©ã—ã€I/Oå¾…æ©Ÿæ™‚é–“ã‚’åŠ¹ç‡çš„ã«æ´»ç”¨ã§ãã¾ã™ã€‚asyncio.gather()ã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œã§ãã¾ã™ã€‚",
            "level": KnowledgeLevel.CONTEXTUAL,
            "tags": ["python", "async", "concurrency"],
            "metadata": {"domain": "programming", "difficulty": 5}
        },
        {
            "content": "é«˜æ€§èƒ½ãªã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã§ã¯ã€CAPå®šç†ï¼ˆConsistency, Availability, Partition toleranceï¼‰ã‚’ç†è§£ã—ã€ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã«å¿œã˜ã¦é©åˆ‡ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’é¸æŠã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã§ã¯Partition toleranceã¯å¿…é ˆã§ã‚ã‚Šã€Consistencyã¨Availabilityã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚",
            "level": KnowledgeLevel.EXPERT,
            "tags": ["system-design", "distributed-systems", "cap-theorem"],
            "metadata": {"domain": "architecture", "difficulty": 9}
        }
    ]
    
    for knowledge in sample_knowledge:
        try:
            chunk_id = injector.add_knowledge(
                content=knowledge["content"],
                level=knowledge["level"],
                tags=knowledge["tags"],
                metadata=knowledge["metadata"]
            )
            print(f"Added knowledge chunk: {chunk_id}")
        except Exception as e:
            print(f"Failed to add knowledge: {e}")

async def demonstrate_perfect_knowledge_injection():
    """Perfect Knowledge Injection 2.0 ãƒ‡ãƒ¢"""
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    injector = PerfectKnowledgeInjector(
        db_path="/tmp/demo_knowledge.db",
        enable_semantic_search=True
    )
    
    print("ğŸš€ Perfect Knowledge Injection 2.0 Demo")
    print("=" * 50)
    
    # ã‚µãƒ³ãƒ—ãƒ«çŸ¥è­˜ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
    await initialize_sample_knowledge_base(injector)
    
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
    context = KnowledgeContext(
        task_type="code_development",
        domain="python",
        complexity_level=6,
        required_expertise=["async_programming", "performance"],
        user_preferences={"detail_level": "high"},
        session_history=["async", "performance"],
        performance_constraints={"max_injection_time": 2.0}
    )
    
    print(f"\nğŸ“‹ Context: {context.task_type} in {context.domain} domain")
    print(f"    Complexity: {context.complexity_level}/10")
    print(f"    Required expertise: {context.required_expertise}")
    
    # å„æˆ¦ç•¥ã§ã®çŸ¥è­˜æ³¨å…¥ãƒ†ã‚¹ãƒˆ
    strategies = [
        InjectionStrategy.SEQUENTIAL,
        InjectionStrategy.PARALLEL,
        InjectionStrategy.ADAPTIVE,
        InjectionStrategy.SMART_MERGE
    ]
    
    for strategy in strategies:
        print(f"\nğŸ”„ Testing {strategy.value} injection strategy...")
        
        result = await injector.inject_knowledge(context, strategy)
        
        print(f"    Success: {result.success}")
        print(f"    Chunks injected: {result.chunks_injected}")
        print(f"    Quality score: {result.quality_score:.2f}")
        print(f"    Injection time: {result.injection_time:.3f}s")
        
        if result.errors:
            print(f"    Errors: {result.errors}")
        if result.warnings:
            print(f"    Warnings: {result.warnings}")
    
    # çµ±è¨ˆè¡¨ç¤º
    print(f"\nğŸ“Š Knowledge Injection Statistics:")
    stats = injector.get_injection_statistics()
    for key, value in stats.items():
        if key != "most_accessed_chunks":
            print(f"    {key}: {value}")
    
    print("\nâœ… Perfect Knowledge Injection 2.0 Demo completed!")


if __name__ == "__main__":
    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    asyncio.run(demonstrate_perfect_knowledge_injection())