#!/usr/bin/env python3
"""
Complete Knowledge Base Migration to PostgreSQL Magic Grimoire System
å®Œå…¨ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ  - æ¼ã‚Œã‚¼ãƒ­ä¿è¨¼
"""

import asyncio
import hashlib
import json
import logging
import os
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import asyncpg

# Add project root to path
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from libs.grimoire_database import MagicSchool, SpellType
from libs.rag_grimoire_integration import RagGrimoireConfig, RagGrimoireIntegration

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class MigrationStats:
    """ç§»è¡Œçµ±è¨ˆ"""

    total_files: int = 0
    processed_files: int = 0
    successful_migrations: int = 0
    failed_migrations: int = 0
    skipped_files: int = 0
    total_size: int = 0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    errors: List[str] = None

    def __post_init__(self):
        if self.errors is None:
            self.errors = []


class ComprehensiveKnowledgeMigrator:
    """å®Œå…¨ãƒŠãƒ¬ãƒƒã‚¸ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.stats = MigrationStats()
        self.knowledge_base_path = PROJECT_ROOT / "knowledge_base"
        self.grimoire_integration = None
        self.batch_size = 10  # ãƒãƒƒãƒå‡¦ç†ã‚µã‚¤ã‚º

        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ†é¡
        self.file_categories = {
            "master_knowledge": [],
            "elder_council": [],
            "incident_management": [],
            "ai_learning": [],
            "archives": [],
            "special_files": [],
            "executable_scripts": [],
        }

        logger.info("ğŸš€ Complete Knowledge Migration System initialized")

    async def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # Grimoireçµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            config = RagGrimoireConfig()
            self.grimoire_integration = RagGrimoireIntegration(config)
            await self.grimoire_integration.initialize()

            logger.info("âœ… Grimoire integration system ready")
            return True

        except Exception as e:
            logger.error(f"âŒ Initialization failed: {e}")
            return False

    def analyze_knowledge_base(self) -> Dict[str, Any]:
        """å®Œå…¨ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹åˆ†æ"""
        logger.info("ğŸ” Starting comprehensive knowledge base analysis...")

        analysis = {
            "directories": [],
            "files_by_category": {},
            "special_files": [],
            "external_dependencies": [],
            "total_stats": {},
        }

        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
        all_files = []
        for root, dirs, files in os.walk(self.knowledge_base_path):
            for file in files:
                file_path = Path(root) / file
                all_files.append(file_path)

        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ†é¡
        for file_path in all_files:
            category = self._categorize_file(file_path)
            if category not in analysis["files_by_category"]:
                analysis["files_by_category"][category] = []
            analysis["files_by_category"][category].append(str(file_path))

            # ç‰¹æ®Šãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡º
            if self._is_special_file(file_path):
                analysis["special_files"].append(
                    {
                        "path": str(file_path),
                        "type": self._get_special_type(file_path),
                        "size": file_path.stat().st_size if file_path.exists() else 0,
                    }
                )

        # çµ±è¨ˆ
        analysis["total_stats"] = {
            "total_files": len(all_files),
            "total_size": sum(f.stat().st_size for f in all_files if f.exists()),
            "categories": len(analysis["files_by_category"]),
            "special_files": len(analysis["special_files"]),
        }

        self.stats.total_files = analysis["total_stats"]["total_files"]
        self.stats.total_size = analysis["total_stats"]["total_size"]

        logger.info(
            f"ğŸ“Š Analysis complete: {self.stats.total_files} files, {self.stats.total_size/1024/1024:.2f}MB"
        )
        return analysis

    def _categorize_file(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        path_str = str(file_path).lower()

        if "master" in path_str or "core_knowledge" in path_str:
            return "master_knowledge"
        elif "elder_council" in path_str or "council" in path_str:
            return "elder_council"
        elif "incident" in path_str or "auto_fix" in path_str:
            return "incident_management"
        elif (
            "learning" in path_str or "feedback" in path_str or "evolution" in path_str
        ):
            return "ai_learning"
        elif "archive" in path_str or "daily_log" in path_str:
            return "archives"
        elif file_path.suffix in [".py", ".sh"]:
            return "executable_scripts"
        elif file_path.suffix == ".md":
            return "documentation"
        elif file_path.suffix == ".json":
            return "structured_data"
        else:
            return "other"

    def _is_special_file(self, file_path: Path) -> bool:
        """ç‰¹æ®Šãƒ•ã‚¡ã‚¤ãƒ«åˆ¤å®š"""
        return (
            file_path.is_symlink()
            or file_path.name.startswith(".")
            or file_path.suffix in [".py", ".sh"]
            or file_path.stat().st_size > 100000
            or "postgresql" in file_path.name.lower()  # 100KBä»¥ä¸Š
            or any(lang in file_path.name for lang in ["japanese", "æ—¥æœ¬èª"])
        )

    def _get_special_type(self, file_path: Path) -> str:
        """ç‰¹æ®Šãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—å–å¾—"""
        if file_path.is_symlink():
            return "symbolic_link"
        elif file_path.name.startswith("."):
            return "hidden_file"
        elif file_path.suffix in [".py", ".sh"]:
            return "executable_script"
        elif file_path.stat().st_size > 100000:
            return "large_file"
        else:
            return "special_content"

    async def migrate_all_knowledge(self, dry_run: bool = False) -> Dict[str, Any]:
        """å…¨ãƒŠãƒ¬ãƒƒã‚¸ã®å®Œå…¨ç§»è¡Œ"""
        logger.info(
            f"ğŸš€ Starting {'DRY RUN' if dry_run else 'LIVE'} complete knowledge migration..."
        )

        self.stats.start_time = datetime.now()

        # åˆ†æ
        analysis = self.analyze_knowledge_base()

        # ç§»è¡Œå®Ÿè¡Œ
        migration_results = {}

        for category, files in analysis["files_by_category"].items():
            logger.info(f"ğŸ“ Migrating category: {category} ({len(files)} files)")

            category_results = await self._migrate_category(category, files, dry_run)
            migration_results[category] = category_results

            self.stats.processed_files += len(files)

        self.stats.end_time = datetime.now()

        # çµæœã‚µãƒãƒªãƒ¼
        summary = self._generate_migration_summary(analysis, migration_results)

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        if not dry_run:
            await self._save_migration_report(summary)

        logger.info(
            f"âœ… Migration complete: {self.stats.successful_migrations}/{self.stats.total_files} files"
        )
        return summary

    async def _migrate_category(
        self, category: str, files: List[str], dry_run: bool
    ) -> Dict[str, Any]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ç§»è¡Œ"""
        results = {
            "category": category,
            "total_files": len(files),
            "successful": 0,
            "failed": 0,
            "skipped": 0,
            "files": [],
        }

        for file_path_str in files:
            file_path = Path(file_path_str)

            try:
                if not file_path.exists():
                    results["skipped"] += 1
                    continue

                # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                content = self._read_file_content(file_path)
                if not content:
                    results["skipped"] += 1
                    continue

                # ç§»è¡Œå®Ÿè¡Œ
                if not dry_run:
                    spell_id = await self._migrate_single_file(
                        file_path, content, category
                    )
                    results["files"].append(
                        {
                            "path": str(file_path),
                            "spell_id": spell_id,
                            "size": file_path.stat().st_size,
                            "status": "success",
                        }
                    )
                    self.stats.successful_migrations += 1
                else:
                    results["files"].append(
                        {
                            "path": str(file_path),
                            "size": file_path.stat().st_size,
                            "status": "dry_run",
                        }
                    )

                results["successful"] += 1

            except Exception as e:
                logger.error(f"âŒ Failed to migrate {file_path}: {e}")
                results["failed"] += 1
                self.stats.failed_migrations += 1
                self.stats.errors.append(f"{file_path}: {str(e)}")

                results["files"].append(
                    {"path": str(file_path), "status": "failed", "error": str(e)}
                )

        return results

    def _read_file_content(self, file_path: Path) -> Optional[str]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„èª­ã¿è¾¼ã¿"""
        try:
            if file_path.suffix == ".json":
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                return json.dumps(data, ensure_ascii=False, indent=2)
            else:
                with open(file_path, "r", encoding="utf-8") as f:
                    return f.read()
        except Exception as e:
            logger.warning(f"âš ï¸ Could not read {file_path}: {e}")
            return None

    async def _migrate_single_file(
        self, file_path: Path, content: str, category: str
    ) -> str:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ç§»è¡Œ"""
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æº–å‚™
        metadata = {
            "original_path": str(file_path),
            "category": category,
            "file_size": len(content),
            "file_type": file_path.suffix,
            "migration_date": datetime.now().isoformat(),
            "relative_path": str(file_path.relative_to(self.knowledge_base_path)),
        }

        # é­”æ³•å­¦æ´¾ã®æ±ºå®š
        magic_school = self._determine_magic_school(file_path, content)

        # å‘ªæ–‡åã®ç”Ÿæˆ
        spell_name = self._generate_spell_name(file_path, category)

        # Grimoireã‚·ã‚¹ãƒ†ãƒ ã«è¿½åŠ 
        spell_id = await self.grimoire_integration.add_knowledge_unified(
            spell_name=spell_name,
            content=content,
            metadata=metadata,
            category=category,
            tags=self._generate_tags(file_path, content),
        )

        return spell_id

    def _determine_magic_school(self, file_path: Path, content: str) -> MagicSchool:
        """é­”æ³•å­¦æ´¾æ±ºå®š"""
        path_str = str(file_path).lower()
        content_lower = content.lower()

        if (
            "incident" in path_str
            or "error" in content_lower
            or "crisis" in content_lower
        ):
            return MagicSchool.CRISIS_SAGE
        elif (
            "task" in path_str
            or "project" in content_lower
            or "workflow" in content_lower
        ):
            return MagicSchool.TASK_ORACLE
        elif "search" in path_str or "rag" in content_lower or "query" in content_lower:
            return MagicSchool.SEARCH_MYSTIC
        else:
            return MagicSchool.KNOWLEDGE_SAGE

    def _generate_spell_name(self, file_path: Path, category: str) -> str:
        """å‘ªæ–‡åç”Ÿæˆ"""
        base_name = file_path.stem
        return f"{category}_{base_name}".replace(" ", "_").replace("-", "_")

    def _generate_tags(self, file_path: Path, content: str) -> List[str]:
        """ã‚¿ã‚°ç”Ÿæˆ"""
        tags = []

        # ãƒ‘ã‚¹ãƒ™ãƒ¼ã‚¹ã‚¿ã‚°
        path_parts = file_path.parts
        tags.extend([part for part in path_parts if part != "knowledge_base"])

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã‚¿ã‚°
        content_lower = content.lower()
        tag_keywords = [
            "claude",
            "elder",
            "council",
            "sage",
            "incident",
            "task",
            "rag",
            "postgresql",
            "migration",
            "test",
            "error",
            "system",
            "ai",
        ]

        for keyword in tag_keywords:
            if keyword in content_lower:
                tags.append(keyword)

        return list(set(tags))  # é‡è¤‡é™¤å»

    def _generate_migration_summary(
        self, analysis: Dict[str, Any], results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç§»è¡Œã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        duration = (self.stats.end_time - self.stats.start_time).total_seconds()

        return {
            "migration_id": hashlib.md5(
                f"{self.stats.start_time}".encode()
            ).hexdigest()[:16],
            "timestamp": self.stats.start_time.isoformat(),
            "duration_seconds": duration,
            "source_analysis": analysis,
            "migration_results": results,
            "statistics": {
                "total_files": self.stats.total_files,
                "processed_files": self.stats.processed_files,
                "successful_migrations": self.stats.successful_migrations,
                "failed_migrations": self.stats.failed_migrations,
                "skipped_files": self.stats.skipped_files,
                "success_rate": (
                    self.stats.successful_migrations / self.stats.total_files * 100
                )
                if self.stats.total_files > 0
                else 0,
                "total_size_mb": self.stats.total_size / 1024 / 1024,
                "errors": self.stats.errors,
            },
        }

    async def _save_migration_report(self, summary: Dict[str, Any]):
        """ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        report_file = PROJECT_ROOT / f"migration_report_{summary['migration_id']}.json"

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“„ Migration report saved: {report_file}")

    async def verify_migration(self) -> Dict[str, Any]:
        """ç§»è¡Œæ¤œè¨¼"""
        logger.info("ğŸ” Verifying migration completeness...")

        # PostgreSQLã‹ã‚‰å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—
        grimoire_spells = await self._get_all_grimoire_spells()

        # å…ƒãƒ‡ãƒ¼ã‚¿ã¨æ¯”è¼ƒ
        original_files = list(self.knowledge_base_path.rglob("*"))
        original_md_files = [
            f for f in original_files if f.suffix in [".md", ".json"] and f.is_file()
        ]

        verification = {
            "original_files_count": len(original_md_files),
            "migrated_spells_count": len(grimoire_spells),
            "coverage_percentage": (len(grimoire_spells) / len(original_md_files) * 100)
            if original_md_files
            else 0,
            "missing_files": [],
            "verification_passed": False,
        }

        # ä¸è¶³ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
        migrated_paths = {
            spell.get("metadata", {}).get("original_path") for spell in grimoire_spells
        }

        for original_file in original_md_files:
            if str(original_file) not in migrated_paths:
                verification["missing_files"].append(str(original_file))

        verification["verification_passed"] = len(verification["missing_files"]) == 0

        logger.info(
            f"âœ… Verification complete: {verification['coverage_percentage']:.1f}% coverage"
        )
        return verification

    async def _get_all_grimoire_spells(self) -> List[Dict[str, Any]]:
        """å…¨Grimoireå‘ªæ–‡å–å¾—"""
        try:
            # PostgreSQLã‹ã‚‰ç›´æ¥å–å¾—
            conn = await asyncpg.connect(os.getenv("GRIMOIRE_DATABASE_URL"))

            rows = await conn.fetch(
                """
                SELECT id, spell_name, content, created_at
                FROM knowledge_grimoire
                ORDER BY created_at DESC
            """
            )

            await conn.close()

            return [dict(row) for row in rows]

        except Exception as e:
            logger.error(f"âŒ Failed to get grimoire spells: {e}")
            return []

    async def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.grimoire_integration:
            await self.grimoire_integration.cleanup()


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    migrator = ComprehensiveKnowledgeMigrator()

    try:
        # åˆæœŸåŒ–
        if not await migrator.initialize():
            logger.error("âŒ Failed to initialize migrator")
            return

        # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã§ç¢ºèª
        logger.info("ğŸ§ª Running dry run analysis...")
        dry_run_results = await migrator.migrate_all_knowledge(dry_run=True)

        print("\n" + "=" * 60)
        print("ğŸ“Š DRY RUN ANALYSIS RESULTS")
        print("=" * 60)
        print(f"Total files found: {dry_run_results['statistics']['total_files']}")
        print(f"Total size: {dry_run_results['statistics']['total_size_mb']:.2f} MB")
        print(f"Categories: {len(dry_run_results['migration_results'])}")

        for category, result in dry_run_results["migration_results"].items():
            print(f"  - {category}: {result['total_files']} files")

        # å®Ÿéš›ã®ç§»è¡Œå®Ÿè¡Œç¢ºèª
        print("\n" + "=" * 60)
        print("ğŸš€ PROCEEDING WITH LIVE MIGRATION...")
        print("=" * 60)

        logger.info("ğŸš€ Starting LIVE migration...")
        live_results = await migrator.migrate_all_knowledge(dry_run=False)

        print("\n" + "=" * 60)
        print("âœ… LIVE MIGRATION COMPLETED")
        print("=" * 60)
        print(f"Successful: {live_results['statistics']['successful_migrations']}")
        print(f"Failed: {live_results['statistics']['failed_migrations']}")
        print(f"Success rate: {live_results['statistics']['success_rate']:.1f}%")

        # æ¤œè¨¼
        verification = await migrator.verify_migration()
        print(f"\nğŸ” VERIFICATION: {verification['coverage_percentage']:.1f}% coverage")

        if verification["verification_passed"]:
            print("ğŸ‰ Migration verification PASSED - No data loss detected!")
        else:
            print(f"âš ï¸ Missing {len(verification['missing_files'])} files")

    except Exception as e:
        logger.error(f"âŒ Migration failed: {e}")
        raise
    finally:
        await migrator.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
