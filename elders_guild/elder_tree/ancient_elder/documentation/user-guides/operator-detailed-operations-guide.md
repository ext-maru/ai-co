# âš™ï¸ Auto Issue Processor A2A é‹ç”¨è€…å‘ã‘è©³ç´°é‹ç”¨ã‚¬ã‚¤ãƒ‰

## ğŸ¯ æ¦‚è¦

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Auto Issue Processor A2Aã®æ—¥å¸¸é‹ç”¨ã‚’æ‹…å½“ã™ã‚‹é‹ç”¨è€…å‘ã‘ã«ã€è©³ç´°ãªé‹ç”¨æ‰‹é †ã¨æœ€é©åŒ–æ‰‹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ“Š é‹ç”¨ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

### ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã‚³ãƒãƒ³ãƒ‰

```bash
# ãƒ¡ã‚¤ãƒ³ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
./scripts/monitor_auto_issue_processor.sh

# åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§è©³ç´°ç›£è¦–
watch -n 5 'echo "=== System Status ===" && \
  ps aux | grep auto_issue_processor | head -3 && \
  echo "=== Processing Queue ===" && \
  curl -s http://localhost:8080/api/metrics | jq ".processing" && \
  echo "=== Recent Errors ===" && \
  tail -5 logs/auto_issue_processor.log | grep ERROR'
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
# performance_monitor.py
import psutil
import json
from datetime import datetime

def collect_performance_metrics():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
    metrics = {
        'timestamp': datetime.utcnow().isoformat(),
        'system': {
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'load_average': psutil.getloadavg()[0]
        },
        'process': {},
        'application': {}
    }
    
    # Auto Issue Processor ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–
    for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):
        if 'auto_issue_processor' in proc.info['name']:
            metrics['process'] = {
                'pid': proc.info['pid'],
                'cpu_percent': proc.info['cpu_percent'],
                'memory_percent': proc.info['memory_percent']
            }
            break
    
    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    try:
        with open('logs/auto_issue_processing.json', 'r') as f:
            app_data = json.load(f)
            metrics['application'] = {
                'total_processed': len(app_data.get('recent_issues', [])),
                'success_rate': app_data.get('success_rate', 0),
                'avg_processing_time': app_data.get('avg_processing_time', 0)
            }
    except Exception as e:
        metrics['application'] = {'error': str(e)}
    
    return metrics

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    metrics = collect_performance_metrics()
    print(json.dumps(metrics, indent=2))
```

## ğŸ”„ å‡¦ç†ãƒ•ãƒ­ãƒ¼ç®¡ç†

### ã‚­ãƒ¥ãƒ¼ç®¡ç†

```bash
# å‡¦ç†å¾…ã¡Issueç¢ºèª
gh issue list --label "auto-processable" --state open --json number,title,createdAt

# å‡¦ç†ä¸­ã‚¿ã‚¹ã‚¯ç¢ºèª
curl -s http://localhost:8080/api/status | jq '.active_tasks'

# å‡¦ç†å®Œäº†PRç¢ºèª
gh pr list --search "Auto-fix" --state open --json number,title,createdAt
```

### å‡¦ç†å„ªå…ˆåº¦èª¿æ•´

```python
# priority_manager.py
from libs.integrations.github.auto_issue_processor import AutoIssueProcessor

class PriorityManager:
    """å‡¦ç†å„ªå…ˆåº¦ç®¡ç†"""
    
    def __init__(self):
        self.processor = AutoIssueProcessor()
    
    async def adjust_processing_capacity(self, load_level: str):
        """è² è·ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸå‡¦ç†èƒ½åŠ›èª¿æ•´"""
        capacity_settings = {
            'low': {
                'max_parallel': 5,
                'target_priorities': ['critical', 'high', 'medium'],
                'processing_interval': 10
            },
            'medium': {
                'max_parallel': 3,
                'target_priorities': ['critical', 'high'],
                'processing_interval': 15
            },
            'high': {
                'max_parallel': 1,
                'target_priorities': ['critical'],
                'processing_interval': 30
            }
        }
        
        settings = capacity_settings.get(load_level, capacity_settings['medium'])
        
        # ç’°å¢ƒå¤‰æ•°æ›´æ–°
        os.environ['AUTO_ISSUE_MAX_PARALLEL'] = str(settings['max_parallel'])
        self.processor.target_priorities = settings['target_priorities']
        
        print(f"Processing capacity adjusted for {load_level} load")
        return settings

# ä½¿ç”¨ä¾‹
async def main():
    manager = PriorityManager()
    # ã‚·ã‚¹ãƒ†ãƒ è² è·ãŒé«˜ã„æ™‚
    await manager.adjust_processing_capacity('high')
```

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### å‡¦ç†æ™‚é–“åˆ†æ

```bash
# processing_analysis.sh
#!/bin/bash

echo "=== Processing Time Analysis ==="

# å¹³å‡å‡¦ç†æ™‚é–“
grep "PROCESSING_COMPLETED" logs/auto_issue_processor.log | \
  awk '{print $NF}' | awk '{sum+=$1; count++} END {printf "Average: %.2f seconds\n", sum/count}'

# æœ€é•·å‡¦ç†æ™‚é–“
grep "PROCESSING_COMPLETED" logs/auto_issue_processor.log | \
  awk '{print $NF}' | sort -n | tail -1 | \
  xargs printf "Longest: %.2f seconds\n"

# å‡¦ç†æ™‚é–“åˆ†å¸ƒ
echo "=== Processing Time Distribution ==="
grep "PROCESSING_COMPLETED" logs/auto_issue_processor.log | \
  awk '{print $NF}' | \
  awk '{
    if ($1 < 60) fast++
    else if ($1 < 300) medium++
    else slow++
  } END {
    printf "Fast (<60s): %d\n", fast
    printf "Medium (60-300s): %d\n", medium  
    printf "Slow (>300s): %d\n", slow
  }'
```

### ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–

```python
# resource_optimizer.py
import gc
import os
from libs.performance_optimizer import get_performance_optimizer

class ResourceOptimizer:
    """ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–ç®¡ç†"""
    
    def __init__(self):
        self.optimizer = get_performance_optimizer()
    
    def optimize_memory(self):
        """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–"""
        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å¼·åˆ¶å®Ÿè¡Œ
        collected = gc.collect()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        self.optimizer.clear_cache()
        
        # å¤§ããªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è§£æ”¾
        self.optimizer.cleanup_large_objects()
        
        return f"Memory optimized: {collected} objects collected"
    
    def optimize_disk_usage(self):
        """ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡æœ€é©åŒ–"""
        # å¤ã„ãƒ­ã‚°ã®åœ§ç¸®
        os.system("find logs/ -name '*.log' -mtime +7 -exec gzip {} \;")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        os.system("find /tmp -name 'auto_issue_*' -mtime +1 -delete")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        os.system("find cache/ -mtime +3 -delete")
        
        return "Disk usage optimized"
    
    def optimize_network(self):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æœ€é©åŒ–"""
        # æ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ–
        self.optimizer.optimize_connection_pools()
        
        # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™èª¿æ•´
        self.optimizer.adjust_rate_limits()
        
        return "Network optimized"

# ä½¿ç”¨ä¾‹
optimizer = ResourceOptimizer()
print(optimizer.optimize_memory())
print(optimizer.optimize_disk_usage())
print(optimizer.optimize_network())
```

## ğŸ”§ è¨­å®šç®¡ç†

### å‹•çš„è¨­å®šå¤‰æ›´

```python
# config_manager.py
import yaml
import json
from typing import Dict, Any

class ConfigManager:
    """è¨­å®šç®¡ç†"""
    
    def __init__(self):
        self.config_files = {
            'main': 'configs/auto_issue_processor.yaml',
            'quality': 'configs/quality_gate.yaml',
            'security': 'configs/security_settings.yaml'
        }
    
    def load_config(self, config_type: str) -> Dict[str, Any]:
        """è¨­å®šèª­ã¿è¾¼ã¿"""
        with open(self.config_files[config_type], 'r') as f:
            return yaml.safe_load(f)
    
    def update_config(self, config_type: str, updates: Dict[str, Any]):
        """è¨­å®šæ›´æ–°"""
        config = self.load_config(config_type)
        config.update(updates)
        
        with open(self.config_files[config_type], 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
    
    def get_runtime_config(self) -> Dict[str, Any]:
        """å®Ÿè¡Œæ™‚è¨­å®šå–å¾—"""
        return {
            'max_parallel': os.getenv('AUTO_ISSUE_MAX_PARALLEL', '3'),
            'timeout': os.getenv('AUTO_ISSUE_TIMEOUT', '300'),
            'log_level': os.getenv('AUTO_ISSUE_LOG_LEVEL', 'INFO'),
            'debug_mode': os.getenv('AUTO_ISSUE_DEBUG', 'false').lower() == 'true'
        }

# ä½¿ç”¨ä¾‹
config_mgr = ConfigManager()

# å“è³ªã‚²ãƒ¼ãƒˆåŸºæº–ã‚’ä¸€æ™‚çš„ã«ç·©å’Œ
config_mgr.update_config('quality', {
    'minimum_score': 60,  # é€šå¸¸ã¯70
    'security_threshold': 8  # é€šå¸¸ã¯5
})
```

### ç’°å¢ƒåˆ¥è¨­å®š

```yaml
# configs/environments.yaml
development:
  debug: true
  log_level: DEBUG
  max_parallel: 1
  timeout: 600
  use_cache: false

staging:
  debug: false
  log_level: INFO
  max_parallel: 2
  timeout: 300
  use_cache: true

production:
  debug: false
  log_level: WARNING
  max_parallel: 5
  timeout: 180
  use_cache: true
  enable_monitoring: true
```

## ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

### æ—¥æ¬¡é‹ç”¨ãƒ¬ãƒãƒ¼ãƒˆ

```python
# daily_report_generator.py
from datetime import datetime, timedelta
import json

def generate_daily_report():
    """æ—¥æ¬¡é‹ç”¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    
    # ãƒ­ã‚°åˆ†æ
    processed_issues = analyze_processed_issues(yesterday)
    error_analysis = analyze_errors(yesterday)
    performance_stats = analyze_performance(yesterday)
    
    report = {
        'date': yesterday.isoformat(),
        'summary': {
            'total_issues_processed': processed_issues['total'],
            'success_rate': processed_issues['success_rate'],
            'avg_processing_time': performance_stats['avg_time'],
            'errors_count': error_analysis['total_errors']
        },
        'issues_breakdown': processed_issues['breakdown'],
        'performance': performance_stats,
        'errors': error_analysis['error_types'],
        'recommendations': generate_recommendations(processed_issues, error_analysis, performance_stats)
    }
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_file = f'reports/daily_report_{yesterday.strftime("%Y%m%d")}.json'
    with open(report_file, 'w') as f:
        json.dump(report, f, indent=2)
    
    return report

def generate_recommendations(issues, errors, performance):
    """æ”¹å–„ææ¡ˆç”Ÿæˆ"""
    recommendations = []
    
    # æˆåŠŸç‡ãŒä½ã„å ´åˆ
    if issues['success_rate'] < 80:
        recommendations.append({
            'type': 'alert',
            'message': 'Success rate below threshold (80%)',
            'action': 'Review error logs and adjust quality gate settings'
        })
    
    # å‡¦ç†æ™‚é–“ãŒé•·ã„å ´åˆ
    if performance['avg_time'] > 300:
        recommendations.append({
            'type': 'optimization',
            'message': 'Average processing time exceeds 5 minutes',
            'action': 'Consider reducing parallel processing or optimizing code generation'
        })
    
    # ã‚¨ãƒ©ãƒ¼ãŒå¤šã„å ´åˆ
    if errors['total_errors'] > 10:
        recommendations.append({
            'type': 'maintenance',
            'message': 'High error count detected',
            'action': 'Investigate most common error types and apply fixes'
        })
    
    return recommendations
```

### é€±æ¬¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ

```bash
# weekly_performance_report.sh
#!/bin/bash

WEEK_START=$(date -d '7 days ago' '+%Y-%m-%d')
WEEK_END=$(date '+%Y-%m-%d')

echo "=== Weekly Performance Report ($WEEK_START to $WEEK_END) ==="

# å‡¦ç†çµ±è¨ˆ
echo "## Processing Statistics"
grep "ISSUE_PROCESSED" logs/auto_issue_processor.log | \
  grep -E "$WEEK_START|$WEEK_END" | wc -l | \
  xargs printf "Total Issues Processed: %d\n"

# æˆåŠŸç‡
SUCCESS_COUNT=$(grep "PR_CREATED_SUCCESS" logs/auto_issue_processor.log | \
  grep -E "$WEEK_START|$WEEK_END" | wc -l)
TOTAL_COUNT=$(grep "ISSUE_PROCESSED" logs/auto_issue_processor.log | \
  grep -E "$WEEK_START|$WEEK_END" | wc -l)

if [ $TOTAL_COUNT -gt 0 ]; then
  awk "BEGIN {printf \"Success Rate: %.1f%%\n\", ($SUCCESS_COUNT/$TOTAL_COUNT)*100}"
fi

# ã‚¨ãƒ©ãƒ¼åˆ†æ
echo "## Error Analysis"
grep "ERROR" logs/auto_issue_processor.log | \
  grep -E "$WEEK_START|$WEEK_END" | \
  awk '{print $5}' | sort | uniq -c | sort -nr | head -5
```

## ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†

### ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

```python
# alert_manager.py
import smtplib
import json
from email.mime.text import MIMEText
from datetime import datetime

class AlertManager:
    """ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†"""
    
    def __init__(self):
        self.alert_rules = self.load_alert_rules()
        self.notification_channels = {
            'email': self.send_email_alert,
            'slack': self.send_slack_alert,
            'webhook': self.send_webhook_alert
        }
    
    def check_alert_conditions(self, metrics: dict):
        """ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ãƒã‚§ãƒƒã‚¯"""
        for rule in self.alert_rules:
            if self.evaluate_condition(rule['condition'], metrics):
                self.trigger_alert(rule, metrics)
    
    def evaluate_condition(self, condition: str, metrics: dict) -> bool:
        """æ¡ä»¶è©•ä¾¡"""
        # å®‰å…¨ãªæ¡ä»¶è©•ä¾¡
        allowed_keys = metrics.keys()
        for key in allowed_keys:
            condition = condition.replace(key, f"metrics['{key}']")
        
        try:
            return eval(condition)
        except:
            return False
    
    def trigger_alert(self, rule: dict, metrics: dict):
        """ã‚¢ãƒ©ãƒ¼ãƒˆç™ºå‹•"""
        alert_data = {
            'timestamp': datetime.utcnow().isoformat(),
            'rule_name': rule['name'],
            'severity': rule['severity'],
            'message': rule['message'],
            'metrics': metrics
        }
        
        # é€šçŸ¥é€ä¿¡
        for channel in rule['channels']:
            if channel in self.notification_channels:
                self.notification_channels[channel](alert_data)
    
    def send_email_alert(self, alert_data: dict):
        """ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        msg = MIMEText(json.dumps(alert_data, indent=2))
        msg['Subject'] = f"[AUTO-ISSUE] {alert_data['severity']}: {alert_data['rule_name']}"
        msg['From'] = 'auto-issue-processor@example.com'
        msg['To'] = 'ops-team@example.com'
        
        # SMTPé€ä¿¡ï¼ˆè¨­å®šã«å¿œã˜ã¦ï¼‰
        # smtp = smtplib.SMTP('localhost')
        # smtp.send_message(msg)
        # smtp.quit()

# alert_rules.yaml
rules:
  - name: high_error_rate
    condition: "error_rate > 0.1"
    severity: HIGH
    message: "Error rate exceeds 10%"
    channels: ["email", "slack"]
  
  - name: processing_delay
    condition: "avg_processing_time > 300"
    severity: MEDIUM
    message: "Processing time exceeds 5 minutes"
    channels: ["slack"]
  
  - name: memory_usage_high
    condition: "memory_percent > 80"
    severity: MEDIUM
    message: "Memory usage exceeds 80%"
    channels: ["email"]
```

## ğŸ“‹ é‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### æ—¥æ¬¡é‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```markdown
## æ—¥æ¬¡é‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ (æ¯æœ9:00)
- [ ] ã‚·ã‚¹ãƒ†ãƒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
- [ ] ãƒ—ãƒ­ã‚»ã‚¹ç”Ÿå­˜ç¢ºèª
- [ ] ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ç¢ºèªï¼ˆCPU < 70%, Memory < 80%, Disk < 80%ï¼‰
- [ ] ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼ç¢ºèªï¼ˆå‰æ—¥åˆ†ï¼‰

### å‡¦ç†çŠ¶æ³ç¢ºèª (æ¯æœ9:15)
- [ ] å‰æ—¥å‡¦ç†Issueæ•°ç¢ºèª
- [ ] æˆåŠŸç‡ç¢ºèªï¼ˆç›®æ¨™: >85%ï¼‰
- [ ] å‡¦ç†å¾…ã¡ã‚­ãƒ¥ãƒ¼ç¢ºèª
- [ ] ç•°å¸¸ãªå‡¦ç†æ™‚é–“Issueç¢ºèª

### é‹ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (æ¯æœ9:30)
- [ ] APIä½¿ç”¨é‡ç¢ºèª
- [ ] ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡ç¢ºèª
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çŠ¶æ³ç¢ºèª
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒ©ãƒ¼ãƒˆç¢ºèª

### å“è³ªç›£è¦– (æ¯æœ9:45)
- [ ] ç”Ÿæˆã•ã‚ŒãŸPRã®å“è³ªç¢ºèª
- [ ] ãƒ†ã‚¹ãƒˆå¤±æ•—ç‡ç¢ºèª
- [ ] ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼å¾…ã¡PRç¢ºèª
- [ ] å“è³ªã‚²ãƒ¼ãƒˆå¤±æ•—Issueç¢ºèª
```

### é€±æ¬¡é‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```markdown
## é€±æ¬¡é‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ (æ¯é€±æœˆæ›œæ—¥)

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
- [ ] é€±æ¬¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
- [ ] å‡¦ç†æ™‚é–“ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
- [ ] ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨å‚¾å‘ç¢ºèª
- [ ] ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š

### è¨­å®šæœ€é©åŒ–
- [ ] ä¸¦åˆ—å‡¦ç†æ•°èª¿æ•´æ¤œè¨
- [ ] å“è³ªã‚²ãƒ¼ãƒˆåŸºæº–è¦‹ç›´ã—
- [ ] ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šç¢ºèª
- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šæœ€é©åŒ–

### ä¿å®ˆä½œæ¥­
- [ ] ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºèª
- [ ] ä¾å­˜é–¢ä¿‚æ›´æ–°ç¢ºèª
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ãƒƒãƒé©ç”¨
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–

### ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
- [ ] é‹ç”¨ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
- [ ] æ”¹å–„ææ¡ˆæ›¸ä½œæˆ
- [ ] ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæŒ¯ã‚Šè¿”ã‚Š
- [ ] æ¬¡é€±é‹ç”¨è¨ˆç”»ç­–å®š
```

## ğŸ”— é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [æ—¥å¸¸é‹ç”¨ã‚¬ã‚¤ãƒ‰](../runbooks/daily-operations-guide.md)
- [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰](../runbooks/troubleshooting-guide.md)
- [ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œã‚¬ã‚¤ãƒ‰](../runbooks/incident-response-guide.md)
- [ç®¡ç†è€…å‘ã‘ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¬ã‚¤ãƒ‰](administrator-security-guide.md)

---
*æœ€çµ‚æ›´æ–°: 2025å¹´7æœˆ21æ—¥*