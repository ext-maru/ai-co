#!/usr/bin/env python3
"""
ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Issue #93: OSSç§»è¡Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
æ—¢å­˜ã®IntegrationTestRunnerã¨pytestã®æ€§èƒ½æ¯”è¼ƒ

ä½œæˆæ—¥: 2025å¹´7æœˆ19æ—¥
"""
import json
import statistics
import subprocess
import time
from pathlib import Path
from typing import Any, Dict, List

import matplotlib.pyplot as plt
import pandas as pd


class TestPerformanceComparator:
    """ãƒ†ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ„ãƒ¼ãƒ«"""

    def __init__(self):
        self.results = {"original": [], "pytest": []}
        self.report_dir = Path("test_reports")
        self.report_dir.mkdir(exist_ok=True)

    def run_original_tests(self, iterations: int = 5) -> Dict[str, Any]:
        """æ—¢å­˜ã®ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè¡Œ"""
        print("ğŸ”§ æ—¢å­˜ã®IntegrationTestRunnerå®Ÿè¡Œä¸­...")
        durations = []

        for i in range(iterations):
            start_time = time.time()

            # æ—¢å­˜ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼ˆç°¡æ˜“ç‰ˆï¼‰
            try:
                result = subprocess.run(
                    [
                        "python3",
                        "-m",
                        "pytest",
                        "tests/unit/test_integration_test_framework.py",
                        "-v",
                    ],
                    capture_output=True,
                    text=True,
                    timeout=300,
                )
                duration = time.time() - start_time
                durations.append(duration)
                print(f"  å®Ÿè¡Œ {i+1}/{iterations}: {duration:0.2f}ç§’")
            except Exception as e:
                print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        return {
            "framework": "original",
            "durations": durations,
            "avg_duration": statistics.mean(durations) if durations else 0,
            "std_deviation": statistics.stdev(durations) if len(durations) > 1 else 0,
            "min_duration": min(durations) if durations else 0,
            "max_duration": max(durations) if durations else 0,
        }

    def run_pytest_tests(self, iterations: int = 5) -> Dict[str, Any]:
        """pytestç‰ˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        print("ğŸš€ pytestç‰ˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        durations = []

        for i in range(iterations):
            start_time = time.time()

            try:
                result = subprocess.run(
                    [
                        "python3",
                        "-m",
                        "pytest",
                        "tests/poc/test_integration_pytest.py",
                        "-v",
                        "--tb=short",
                        "-n",
                        "auto",
                    ],
                    capture_output=True,
                    text=True,
                    timeout=300,
                )
                duration = time.time() - start_time
                durations.append(duration)
                print(f"  å®Ÿè¡Œ {i+1}/{iterations}: {duration:0.2f}ç§’")
            except Exception as e:
                print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        return {
            "framework": "pytest",
            "durations": durations,
            "avg_duration": statistics.mean(durations) if durations else 0,
            "std_deviation": statistics.stdev(durations) if len(durations) > 1 else 0,
            "min_duration": min(durations) if durations else 0,
            "max_duration": max(durations) if durations else 0,
        }

    def compare_code_metrics(self) -> Dict[str, Any]:
        """ã‚³ãƒ¼ãƒ‰è¡Œæ•°ã¨ã‚³ãƒ³ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£æ¯”è¼ƒ"""
        metrics = {}

        # æ—¢å­˜ã‚³ãƒ¼ãƒ‰
        original_file = Path("libs/integration_test_framework.py")
        if original_file.exists():
            with open(original_file) as f:
                original_lines = len(f.readlines())
            metrics["original"] = {
                "lines_of_code": original_lines,
                "file_size": original_file.stat().st_size,
            }

        # pytestç‰ˆ
        pytest_file = Path("tests/poc/test_integration_pytest.py")
        if pytest_file.exists():
            with open(pytest_file) as f:
                pytest_lines = len(f.readlines())
            metrics["pytest"] = {
                "lines_of_code": pytest_lines,
                "file_size": pytest_file.stat().st_size,
            }

        # å‰Šæ¸›ç‡è¨ˆç®—
        if "original" in metrics and "pytest" in metrics:
            reduction = (
                1
                - metrics["pytest"]["lines_of_code"]
                / metrics["original"]["lines_of_code"]
            ) * 100
            metrics["code_reduction_percentage"] = reduction

        return metrics

    def generate_report(self, original_results: Dict, pytest_results: Dict) -> str:
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        code_metrics = self.compare_code_metrics()

        report = f"""# ğŸš€ pytestç§»è¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ
**Issue #93: OSSç§»è¡Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**
**ç”Ÿæˆæ—¥**: {time.strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š å®Ÿè¡Œæ™‚é–“æ¯”è¼ƒ

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | æ—¢å­˜ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | pytest | æ”¹å–„ç‡ |
|------------|-------------------|---------|--------|
| å¹³å‡å®Ÿè¡Œæ™‚é–“ | {original_results['avg_duration']:0.2f}ç§’ | {pytest_results[ \
    'avg_duration']:0.2f}ç§’ | {((original_results['avg_duration'] - \
        pytest_results['avg_duration']) / original_results['avg_duration'] * 100):0.1f}% |
| æœ€å°å®Ÿè¡Œæ™‚é–“ | {original_results['min_duration']:0.2f}ç§’ | {pytest_results['min_duration']:0.2f}ç§’ | - |
| æœ€å¤§å®Ÿè¡Œæ™‚é–“ | {original_results['max_duration']:0.2f}ç§’ | {pytest_results['max_duration']:0.2f}ç§’ | - |
| æ¨™æº–åå·® | {original_results['std_deviation']:0.2f} | {pytest_results['std_deviation']:0.2f} | - |

## ğŸ“ˆ ã‚³ãƒ¼ãƒ‰å‰Šæ¸›

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | æ—¢å­˜ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | pytest | å‰Šæ¸›ç‡ |
|------------|-------------------|---------|--------|
| ã‚³ãƒ¼ãƒ‰è¡Œæ•° | {
    code_metrics.get('original',
    {}).get('lines_of_code',
    'N/A')} | {code_metrics.get('pytest',
    {}).get('lines_of_code',
    'N/A')} | {code_metrics.get('code_reduction_percentage',
    0):0.1f
}% |
| ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º | {
    code_metrics.get('original',
    {}).get('file_size',
    0) / 1024:0.1f} KB | {code_metrics.get('pytest',
    {}).get('file_size',
    0) / 1024:0.1f
} KB | - |

## ğŸ¯ pytestç§»è¡Œã®ãƒ¡ãƒªãƒƒãƒˆ

1 **ä¸¦åˆ—å®Ÿè¡Œã‚µãƒãƒ¼ãƒˆ**: pytest-xdistã«ã‚ˆã‚‹è‡ªå‹•ä¸¦åˆ—åŒ–
2 **è±Šå¯Œãªãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£**: å†åˆ©ç”¨å¯èƒ½ãªãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
3 **testcontainersçµ±åˆ**: Dockerã‚³ãƒ³ãƒ†ãƒŠã®è‡ªå‹•ç®¡ç†
4 **è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆ**: HTML/XMLå½¢å¼ã®å‡ºåŠ›ã‚µãƒãƒ¼ãƒˆ
5 **ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ **: è±Šå¯Œãªæ‹¡å¼µæ©Ÿèƒ½

## ğŸš§ ç§»è¡Œæ™‚ã®è€ƒæ…®äº‹é …

1 **å­¦ç¿’ã‚³ã‚¹ãƒˆ**: pytestã®æ¦‚å¿µã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ç¿’å¾—
2 **ä¾å­˜é–¢ä¿‚**: è¿½åŠ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¿…è¦
3 **è¨­å®šç§»è¡Œ**: æ—¢å­˜ã®è¨­å®šã‚’pytest.iniã¸ç§»è¡Œ

## ğŸ“ æ¨å¥¨äº‹é …

- æ®µéšçš„ç§»è¡Œ: æ–°è¦ãƒ†ã‚¹ãƒˆã‹ã‚‰pytestæ¡ç”¨
- CI/CDçµ±åˆ: pytest-xdistã§ä¸¦åˆ—å®Ÿè¡Œã‚’æ´»ç”¨
- ã‚«ãƒãƒ¬ãƒƒã‚¸å‘ä¸Š: pytest-covã§ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®šã‚’å¼·åŒ–
"""

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = self.report_dir / "pytest_migration_comparison.md"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report)

        # ã‚°ãƒ©ãƒ•ç”Ÿæˆ
        self._generate_charts(original_results, pytest_results)

        return report

    def _generate_charts(self, original_results: Dict, pytest_results: Dict):
        """æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆ"""
        # å®Ÿè¡Œæ™‚é–“æ¯”è¼ƒã‚°ãƒ©ãƒ•
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

        # å¹³å‡å®Ÿè¡Œæ™‚é–“ã®æ£’ã‚°ãƒ©ãƒ•
        frameworks = ["æ—¢å­˜", "pytest"]
        durations = [original_results["avg_duration"], pytest_results["avg_duration"]]
        colors = ["#3498db", "#2ecc71"]

        ax1.bar(frameworks, durations, color=colors)
        ax1.set_ylabel("å®Ÿè¡Œæ™‚é–“ï¼ˆç§’ï¼‰")
        ax1.set_title("å¹³å‡å®Ÿè¡Œæ™‚é–“æ¯”è¼ƒ")
        ax1.grid(axis="y", alpha=0.3)

        # å®Ÿè¡Œæ™‚é–“ã®åˆ†å¸ƒ
        ax2.boxplot(
            [original_results["durations"], pytest_results["durations"]],
            labels=frameworks,
        )
        ax2.set_ylabel("å®Ÿè¡Œæ™‚é–“ï¼ˆç§’ï¼‰")
        ax2.set_title("å®Ÿè¡Œæ™‚é–“åˆ†å¸ƒ")
        ax2.grid(axis="y", alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.report_dir / "performance_comparison.png", dpi=300)
        plt.close()


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    comparator = TestPerformanceComparator()

    print("ğŸ” ãƒ†ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚’é–‹å§‹ã—ã¾ã™...")

    # æ—¢å­˜ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    original_results = comparator.run_original_tests(iterations=3)

    # pytestç‰ˆå®Ÿè¡Œ
    pytest_results = comparator.run_pytest_tests(iterations=3)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = comparator.generate_report(original_results, pytest_results)

    print("\n" + "=" * 60)
    print(report)
    print("=" * 60)

    print(f"\nâœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å…ˆ: test_reports/pytest_migration_comparison.md")
    print(f"ğŸ“Š ã‚°ãƒ©ãƒ•ä¿å­˜å…ˆ: test_reports/performance_comparison.png")


if __name__ == "__main__":
    main()
