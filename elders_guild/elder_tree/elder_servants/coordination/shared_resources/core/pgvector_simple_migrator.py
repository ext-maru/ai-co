#!/usr/bin/env python3
"""
pgvectorç°¡æ˜“ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚¿ãƒ¼
æ–‡å­—ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å•é¡Œã‚’å›é¿ã—ãŸç°¡æ½”ãªå®Ÿè£…
"""

import sqlite3
import json

import subprocess
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

class SimplePgVectorMigrator:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªpgvectorãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.sqlite_db = '/home/aicompany/ai_co/knowledge_base/integrated_knowledge.db'
        self.postgres_config = {
            'host': 'localhost',
            'port': 8003,
            'database': 'elders_guild_pgvector',
            'user': 'admin'
        }
        
    def migrate_data(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        logger.info("ğŸ”„ ç°¡æ˜“ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")
        
        try:
            # SQLiteã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            conn = sqlite3connect(self.sqlite_db)
            conn.row_factory = sqlite3Row
            cursor = conn.execute("SELECT COUNT(*) FROM knowledge_documents")
            total_count = cursor.fetchone()[0]
            
            logger.info(f"ğŸ“Š ã‚½ãƒ¼ã‚¹ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_count}")
            
            # å°ãƒãƒƒãƒã§å‡¦ç†
            migrated_count = 0
            batch_size = 5
            
            for offset in range(0, total_count, batch_size):  # å…¨ä»¶å‡¦ç†
                batch = self._get_batch(conn, batch_size, offset)
                if not batch:
                    break
                    
                success = self._migrate_batch_simple(batch)
                if success:
                    migrated_count += len(batch)
                    logger.info(f"ğŸ“ˆ ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é€²æ—: {migrated_count}ä»¶")
                else:
                    logger.warning(f"âš ï¸ ãƒãƒƒãƒ{offset}~{offset+batch_size}å¤±æ•—")
                    
            conn.close()
            
            return {
                'status': 'success',
                'total_records': total_count,
                'migrated_records': migrated_count,
                'success_rate': migrated_count / total_count if total_count > 0 else 0,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _get_batch(self, conn, batch_size: int, offset: int) -> List[Dict]:
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿å–å¾—"""
        cursor = conn.execute(
            "SELECT title, content, source_file, category, priority "
            "FROM knowledge_documents ORDER BY priority LIMIT ? OFFSET ?",
            [batch_size, offset]
        )
        return [dict(row) for row in cursor.fetchall()]
    
    def _migrate_batch_simple(self, batch: List[Dict]) -> bool:
        """ç°¡æ½”ãªãƒãƒƒãƒãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã§SQLã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ

                for row in batch:
                    # å®‰å…¨ãªå€¤ã®æº–å‚™
                    title = self._safe_text(row['title'])[:100]
                    content = self._safe_text(row['content'])[:1000]
                    source_file = self._safe_text(row['source_file'])
                    category = self._safe_text(row['category'])
                    priority = int(row.get('priority', 5))
                    
                    # ç°¡æ˜“ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå°‘æ•°ã®æ¬¡å…ƒï¼‰
                    vector = self._generate_simple_vector(content)
                    
                    # ãƒãƒƒã‚·ãƒ¥ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
                    file_hash = f"hash_{hash(content) % 1000000}"
                    file_size = len(content)
                    
                    # å®‰å…¨ãªSQL
                    sql = f"""
INSERT INTO knowledge_documents (
    title, content, source_file, file_hash, category, priority, 
    file_size, embedding, sage_type, created_at
) VALUES (
    '{title}', 
    '{content}', 
    '{source_file}', 
    '{file_hash}',
    '{category}', 
    {priority}, 
    {file_size},
    '{vector}', 
    'knowledge_sage', 
    NOW()
);
"""
                    f.write(sql)
                
                sql_file = f.name
            
            # PostgreSQLã§SQLå®Ÿè¡Œ
            cmd = [
                'sg', 'docker', '-c',
                (
                    f"f'docker exec -i elders-guild-postgres-new psql -U admin -d "
                    f"{self.postgres_config["database"]} < {sql_file}'"
                )
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            Path(sql_file).unlink()
            
            return result.returncode == 0
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _safe_text(self, text: str) -> str:
        """å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›"""
        if not text:
            return ''
        # SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–
        return text.replace("'", "''").replace("\n", " ").replace("\r", "")[:500]
    
    def _generate_simple_vector(self, text: str) -> str:
        """ç°¡æ˜“ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆï¼ˆ384æ¬¡å…ƒï¼‰"""
        if not text:
            return '[' + ','.join(['0'] * 384) + ']'
        
        # æ–‡å­—é »åº¦ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“ãƒ™ã‚¯ãƒˆãƒ«
        vector = [0.0] * 384
        text_clean = text.lower()[:384]
        for i, char in enumerate(text_clean):
            vector[i] = ord(char) / 255.0
            
        # æ­£è¦åŒ–
        norm = sum(x*x for x in vector) ** 0.5
        if norm > 0:
            vector = [x/norm for x in vector]
            
        # JSONå½¢å¼ã§è¿”å´
        return '[' + ','.join(f'{v:0.6f}' for v in vector) + ']'

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logging.basicConfig(level=logging.INFO)
    
    migrator = SimplePgVectorMigrator()
    result = migrator.migrate_data()
    
    print(json.dumps(result, indent=2, ensure_ascii=False))
    
    if result['status'] == 'success':
        print(f"\nâœ… ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆåŠŸ: {result['migrated_records']}ä»¶")
        return 0
    else:
        print(f"\nâŒ ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—: {result.get('error', 'Unknown error')}")
        return 1

if __name__ == "__main__":
    exit(main())