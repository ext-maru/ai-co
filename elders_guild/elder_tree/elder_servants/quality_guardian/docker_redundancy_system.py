#!/usr/bin/env python3
"""
Docker Redundancy System - Dockerå†—é•·åŒ–ã‚·ã‚¹ãƒ†ãƒ 
å››è³¢è€…æ¨å¥¨Phase 3æœ€å„ªå…ˆã‚¿ã‚¹ã‚¯

Docker Composeå†—é•·åŒ–ãƒ»é«˜å¯ç”¨æ€§ã‚·ã‚¹ãƒ†ãƒ 
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãƒ»è‡ªå‹•ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ãƒ»ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
"""

import sys
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import asyncio
import json
import logging
import os
import subprocess

import threading
import time
import uuid
from collections import defaultdict, deque
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Union

import psutil
import yaml

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
try:
    from libs.worker_auto_recovery_system import WorkerHealthMonitor
    from libs.worker_monitoring_dashboard import (
        MetricsCollector,
        WorkerMonitoringDashboard,
    )

    EXISTING_SYSTEMS_AVAILABLE = True
except ImportError:
    EXISTING_SYSTEMS_AVAILABLE = False
    logging.warning("Existing systems not available")

logger = logging.getLogger(__name__)

@dataclass
class RedundancyConfig:
    """å†—é•·åŒ–è¨­å®š"""

    # Docker Composeè¨­å®š
    compose_file_path: str = "docker-compose.redundancy.yml"
    network_name: str = "ai-company-network"

    # ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š
    default_restart_policy: str = "always"
    health_check_interval: int = 30
    health_check_timeout: int = 10
    health_check_retries: int = 3

    # ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼è¨­å®š
    failover_threshold: int = 3  # é€£ç¶šå¤±æ•—å›æ•°
    failover_timeout: int = 30  # ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
    auto_recovery_enabled: bool = True

    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è¨­å®š
    min_replicas: Dict[str, int] = None
    max_replicas: Dict[str, int] = None
    scale_up_threshold: float = 80.0  # CPUä½¿ç”¨ç‡%
    scale_down_threshold: float = 20.0

    # ç›£è¦–è¨­å®š
    monitoring_interval: int = 10  # ç›£è¦–é–“éš”ï¼ˆç§’ï¼‰
    metrics_retention_hours: int = 24
    alert_enabled: bool = True

    def __post_init__(self):
        """__post_init__ç‰¹æ®Šãƒ¡ã‚½ãƒƒãƒ‰"""
        if self.min_replicas is None:
            self.min_replicas = {
                "pm-worker": 1,
                "task-worker": 2,
                "monitoring-dashboard": 1,
            }
        if self.max_replicas is None:
            self.max_replicas = {
                "pm-worker": 4,
                "task-worker": 8,
                "monitoring-dashboard": 2,
            }

@dataclass
class ServiceStatus:
    """ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹"""

    name: str
    container_id: str
    status: str  # running, exited, restarting, etc.
    health: str  # healthy, unhealthy, starting
    created_at: datetime
    started_at: Optional[datetime]
    cpu_percent: float
    memory_mb: float
    restart_count: int
    last_restart: Optional[datetime]
    replicas_running: int
    replicas_target: int
    ports: List[Dict[str, Any]]
    networks: List[str]
    volumes: List[str]

@dataclass
class FailoverEvent:
    """ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆ"""

    id: str
    service_name: str
    trigger_type: str  # health_failure, container_exit, resource_exhaustion
    failure_details: Dict[str, Any]
    failover_strategy: str  # restart, scale_up, switch_backup
    started_at: datetime
    completed_at: Optional[datetime]
    success: bool
    recovery_actions: List[Dict[str, Any]]
    impact_assessment: Dict[str, Any]

class DockerComposeManager:
    """Docker Composeç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: RedundancyConfig):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.config = config
        self.compose_path = PROJECT_ROOT / self.config.compose_file_path

    def generate_compose_config(
        self, services_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Docker Composeè¨­å®šã‚’ç”Ÿæˆ"""
        compose_config = {
            "version": "3.8",
            "services": {},
            "networks": {
                self.config.network_name: {
                    "driver": "bridge",
                    "driver_opts": {"com.docker.network.enable_ipv6": "false"},
                }
            },
            "volumes": {},
        }

        # ã‚µãƒ¼ãƒ“ã‚¹è¨­å®šã®ç”Ÿæˆ
        for service_name, service_config in services_config.items():
            # ãƒ—ãƒ©ã‚¤ãƒãƒªã‚µãƒ¼ãƒ“ã‚¹
            primary_service = self._generate_service_definition(
                service_name, service_config, "primary"
            )
            compose_config["services"][f"{service_name}-primary"] = primary_service

            # ãƒ¬ãƒ—ãƒªã‚«æ•°ãŒ2ä»¥ä¸Šã®å ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒ“ã‚¹ã‚‚ç”Ÿæˆ
            if service_config.get("replicas", 1) >= 2:
                backup_service = self._generate_service_definition(
                    service_name, service_config, "backup"
                )
                compose_config["services"][f"{service_name}-backup"] = backup_service

            # 3ã¤ä»¥ä¸Šã®ãƒ¬ãƒ—ãƒªã‚«ã®å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®š
            if service_config.get("replicas", 1) >= 3:
                cluster_service = self._generate_cluster_service_definition(
                    service_name, service_config
                )
                compose_config["services"][service_name] = cluster_service

        # ãƒœãƒªãƒ¥ãƒ¼ãƒ è¨­å®šã®è¿½åŠ 
        self._add_volume_definitions(compose_config, services_config)

        return compose_config

    def _generate_service_definition(
        self, service_name: str, config: Dict[str, Any], role: str
    ) -> Dict[str, Any]:
        """å€‹åˆ¥ã‚µãƒ¼ãƒ“ã‚¹å®šç¾©ã‚’ç”Ÿæˆ"""
        service_def = {
            "image": config.get("image", f"ai-company/{service_name}:latest"),
            "container_name": f"{service_name}-{role}",
            "restart": config.get("restart_policy", self.config.default_restart_policy),
            "networks": [self.config.network_name],
            "environment": config.get("environment", {}),
            "volumes": config.get("volumes", []),
            "depends_on": config.get("dependencies", []),
        }

        # ãƒãƒ¼ãƒˆè¨­å®šï¼ˆãƒ—ãƒ©ã‚¤ãƒãƒªã®ã¿å…¬é–‹ï¼‰
        if role == "primary" and "ports" in config:
            service_def["ports"] = config["ports"]

        # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯è¨­å®š
        if config.get("health_check", True):
            service_def["healthcheck"] = {
                "test": config.get(
                    "health_check_command",
                    "curl -f http://localhost:8080/health || exit 1",
                ),
                "interval": f"{self.config.health_check_interval}s",
                "timeout": f"{self.config.health_check_timeout}s",
                "retries": self.config.health_check_retries,
                "start_period": "40s",
            }

        # ç’°å¢ƒå›ºæœ‰ã®è¨­å®š
        service_def["environment"].update(
            {
                "SERVICE_ROLE": role,
                "SERVICE_NAME": service_name,
                "REDUNDANCY_ENABLED": "true",
            }
        )

        return service_def

    def _generate_cluster_service_definition(
        self, service_name: str, config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µãƒ¼ãƒ“ã‚¹å®šç¾©ã‚’ç”Ÿæˆ"""
        replicas = config.get("replicas", 3)

        cluster_def = {
            "image": config.get("image", f"ai-company/{service_name}:latest"),
            "deploy": {
                "replicas": replicas,
                "restart_policy": {
                    "condition": "on-failure",
                    "delay": "5s",

                    "window": "120s",
                },
                "update_config": {
                    "parallelism": 1,
                    "delay": "10s",
                    "failure_action": "rollback",
                    "monitor": "60s",
                },
                "rollback_config": {
                    "parallelism": 1,
                    "delay": "0s",
                    "failure_action": "pause",
                    "monitor": "60s",
                },
            },
            "networks": [self.config.network_name],
            "environment": config.get("environment", {}),
            "volumes": config.get("volumes", []),
        }

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç”¨ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        if config.get("health_check", True):
            cluster_def["healthcheck"] = {
                "test": config.get(
                    "health_check_command",
                    "curl -f http://localhost:8080/health || exit 1",
                ),
                "interval": f"{self.config.health_check_interval}s",
                "timeout": f"{self.config.health_check_timeout}s",
                "retries": self.config.health_check_retries,
                "start_period": "40s",
            }

        return cluster_def

    def _add_volume_definitions(
        self, compose_config: Dict[str, Any], services_config: Dict[str, Any]
    ):
        """ãƒœãƒªãƒ¥ãƒ¼ãƒ å®šç¾©ã‚’è¿½åŠ """
        # å…±é€šãƒœãƒªãƒ¥ãƒ¼ãƒ 
        compose_config["volumes"].update(
            {
                "ai_company_data": {
                    "driver": "local",
                    "driver_opts": {
                        "type": "none",
                        "o": "bind",
                        "device": str(PROJECT_ROOT / "data"),
                    },
                },
                "ai_company_logs": {
                    "driver": "local",
                    "driver_opts": {
                        "type": "none",
                        "o": "bind",
                        "device": str(PROJECT_ROOT / "logs"),
                    },
                },
                "ai_company_config": {
                    "driver": "local",
                    "driver_opts": {
                        "type": "none",
                        "o": "bind",
                        "device": str(PROJECT_ROOT / "config"),
                    },
                },
            }
        )

    def generate_compose_yaml(self, services_config: Dict[str, Any]) -> str:
        """Docker Compose YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
        compose_config = self.generate_compose_config(services_config)
        return yaml.dump(compose_config, default_flow_style=False, sort_keys=False)

    def save_compose_file(self, services_config: Dict[str, Any]) -> str:
        """Docker Compose ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        compose_yaml = self.generate_compose_yaml(services_config)

        with open(self.compose_path, "w") as f:
            f.write(compose_yaml)

        logger.info(f"Docker Compose file saved to {self.compose_path}")
        return str(self.compose_path)

    def deploy_services(self) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤"""
        try:
            # Docker Compose up ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
            cmd = [
                "docker-compose",
                "-f",
                str(self.compose_path),
                "up",
                "-d",
                "--remove-orphans",
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)

            if result.returncode == 0:
                # ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸã‚µãƒ¼ãƒ“ã‚¹ãƒªã‚¹ãƒˆã‚’å–å¾—
                deployed_services = self._get_deployed_services()

                return {
                    "success": True,
                    "deployed_services": deployed_services,
                    "output": result.stdout,
                    "deployment_time": datetime.now().isoformat(),
                }
            else:
                return {
                    "success": False,
                    "error": result.stderr,
                    "returncode": result.returncode,
                }

        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "error": "Deployment timeout (5 minutes)",
                "returncode": -1,
            }
        except Exception as e:
            return {"success": False, "error": str(e), "returncode": -1}

    def _get_deployed_services(self) -> List[str]:
        """ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸã‚µãƒ¼ãƒ“ã‚¹ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        try:
            cmd = ["docker-compose", "-f", str(self.compose_path), "ps", "--services"]
            result = subprocess.run(cmd, capture_output=True, text=True)

            if result.returncode == 0:
                return [
                    line.strip() for line in result.stdout.split("\n") if line.strip()
                ]
            else:
                return []
        except Exception as e:
            logger.error(f"Failed to get deployed services: {e}")
            return []

    def rolling_update(self, update_config: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆå®Ÿè¡Œ"""
        service = update_config["service"]
        strategy = update_config.get("strategy", "rolling")

        try:
            if strategy == "rolling":
                return self._execute_rolling_update(update_config)
            elif strategy == "blue_green":
                return self._execute_blue_green_update(update_config)
            else:
                return {
                    "success": False,
                    "error": f"Unknown update strategy: {strategy}",
                }
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _execute_rolling_update(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆå®Ÿè¡Œ"""
        service = config["service"]
        new_image = config.get("new_image")
        max_unavailable = config.get("max_unavailable", 1)
        update_delay = config.get("update_delay", 30)

        updated_replicas = []

        # ãƒ—ãƒ©ã‚¤ãƒãƒªã‚µãƒ¼ãƒ“ã‚¹æ›´æ–°
        primary_result = self._update_single_service(f"{service}-primary", new_image)
        if primary_result["success"]:
            updated_replicas.append(f"{service}-primary")
            time.sleep(update_delay)

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒ“ã‚¹æ›´æ–°
        backup_result = self._update_single_service(f"{service}-backup", new_image)
        if backup_result["success"]:
            updated_replicas.append(f"{service}-backup")

        return {
            "success": len(updated_replicas) > 0,
            "strategy": "rolling",
            "updated_replicas": updated_replicas,
            "update_time": datetime.now().isoformat(),
        }

    def _update_single_service(
        self, service_name: str, new_image: Optional[str]
    ) -> Dict[str, Any]:
        """å˜ä¸€ã‚µãƒ¼ãƒ“ã‚¹ã®æ›´æ–°"""
        try:
            cmd = [
                "docker-compose",
                "-f",
                str(self.compose_path),
                "up",
                "-d",
                service_name,
            ]
            if new_image:
                # æ–°ã—ã„ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æŒ‡å®šã—ã¦ãƒ—ãƒ«
                pull_cmd = ["docker", "pull", new_image]
                subprocess.run(pull_cmd, check=True)

            result = subprocess.run(cmd, capture_output=True, text=True)
            return {
                "success": result.returncode == 0,
                "output": result.stdout,
                "error": result.stderr if result.returncode != 0 else None,
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    def rollback_service(self, rollback_config: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        service = rollback_config["service"]
        target_version = rollback_config["target_version"]
        immediate = rollback_config.get("immediate", False)

        try:
            # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾è±¡ã®ç‰¹å®š
            services_to_rollback = [f"{service}-primary", f"{service}-backup"]

            rollback_results = []
            for service_name in services_to_rollback:
                # å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æŒ‡å®š
                old_image = f"ai-company/{service}:{target_version}"
                result = self._update_single_service(service_name, old_image)
                rollback_results.append(
                    {
                        "service": service_name,
                        "success": result["success"],
                        "error": result.get("error"),
                    }
                )

                if not immediate:
                    time.sleep(10)  # æ®µéšçš„ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

            success_count = sum(1 for r in rollback_results if r["success"])

            return {
                "success": success_count > 0,
                "rolled_back_to": target_version,
                "rollback_reason": rollback_config.get("reason", "Manual rollback"),
                "rollback_results": rollback_results,
                "rollback_time": datetime.now().isoformat(),
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

class ServiceHealthMonitor:
    """ã‚µãƒ¼ãƒ“ã‚¹ãƒ˜ãƒ«ã‚¹ç›£è¦–ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: RedundancyConfig):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.config = config
        self.docker_client = self._get_docker_client()
        self.service_cache = {}
        self.health_history = defaultdict(deque)

    def _get_docker_client(self):
        """Docker ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆãƒ¢ãƒƒã‚¯å¯¾å¿œï¼‰"""
        try:
            import docker

            return docker.from_env()
        except ImportError:
            logger.warning(
                "Docker Python client not available, using subprocess fallback"
            )
            return None

    def check_all_services(self) -> Dict[str, Any]:
        """å…¨ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ˜ãƒ«ã‚¹çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
        services = []
        healthy_count = 0
        unhealthy_count = 0

        try:
            # Docker Compose ã®ã‚µãƒ¼ãƒ“ã‚¹ä¸€è¦§ã‚’å–å¾—
            service_names = self._get_compose_services()

            for service_name in service_names:
                service_status = self._check_single_service(service_name)
                services.append(service_status)

                if service_status["health_status"] == "healthy":
                    healthy_count += 1
                else:
                    unhealthy_count += 1

                # ãƒ˜ãƒ«ã‚¹å±¥æ­´ã«è¿½åŠ 
                self.health_history[service_name].append(
                    {
                        "timestamp": datetime.now(),
                        "status": service_status["health_status"],
                        "container_status": service_status["container_status"],
                    }
                )

                # å±¥æ­´ã¯æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
                if len(self.health_history[service_name]) > 100:
                    self.health_history[service_name].popleft()

        except Exception as e:
            logger.error(f"Failed to check services health: {e}")
            return {"success": False, "error": str(e)}

        return {
            "success": True,
            "services": services,
            "healthy_count": healthy_count,
            "unhealthy_count": unhealthy_count,
            "total_replicas": len(services),
            "check_time": datetime.now().isoformat(),
        }

    def _get_compose_services(self) -> List[str]:
        """Docker Compose ã‚µãƒ¼ãƒ“ã‚¹ä¸€è¦§ã‚’å–å¾—"""
        try:
            compose_path = PROJECT_ROOT / self.config.compose_file_path
            cmd = ["docker-compose", "-f", str(compose_path), "ps", "--services"]
            result = subprocess.run(cmd, capture_output=True, text=True)

            if result.returncode == 0:
                return [
                    line.strip() for line in result.stdout.split("\n") if line.strip()
                ]
            else:
                return []
        except Exception as e:
            logger.error(f"Failed to get compose services: {e}")
            return []

    def _check_single_service(self, service_name: str) -> Dict[str, Any]:
        """å˜ä¸€ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ˜ãƒ«ã‚¹çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            # Docker container inspect ã‚’ä½¿ç”¨ã—ã¦ãƒ˜ãƒ«ã‚¹çŠ¶æ…‹ã‚’å–å¾—
            cmd = ["docker", "inspect", service_name, "--format", "{{json .}}"]
            result = subprocess.run(cmd, capture_output=True, text=True)

            if result.returncode == 0:
                container_info = json.loads(result.stdout)
                state = container_info["State"]

                # ãƒ˜ãƒ«ã‚¹çŠ¶æ…‹ã®åˆ¤å®š
                health_status = "unknown"
                if "Health" in state:
                    health_status = state["Health"]["Status"]
                elif state.get("Running", False):
                    health_status = "healthy"
                elif state.get("Status") == "exited":
                    health_status = "unhealthy"

                return {
                    "name": service_name,
                    "container_id": container_info["Id"][:12],
                    "container_status": state.get("Status", "unknown"),
                    "health_status": health_status,
                    "started_at": state.get("StartedAt"),
                    "restart_count": container_info.get("RestartCount", 0),
                    "cpu_percent": 0.0,  # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯åˆ¥é€”å–å¾—
                    "memory_mb": 0.0,
                    "ports": container_info.get("NetworkSettings", {}).get("Ports", {}),
                    "networks": list(
                        container_info.get("NetworkSettings", {})
                        .get("Networks", {})
                        .keys()
                    ),
                }
            else:
                return {
                    "name": service_name,
                    "container_id": "N/A",
                    "container_status": "not_found",
                    "health_status": "unhealthy",
                    "error": result.stderr,
                }

        except Exception as e:
            return {
                "name": service_name,
                "container_id": "N/A",
                "container_status": "error",
                "health_status": "unhealthy",
                "error": str(e),
            }

    def get_service_metrics(self, service_name: str) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹ã®è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        try:
            # Docker stats ã‚’ä½¿ç”¨ã—ã¦ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ã‚’å–å¾—
            cmd = [
                "docker",
                "stats",
                service_name,
                "--no-stream",
                "--format",
                "table {{.CPUPerc}}\t{{.MemUsage}}",
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)

            if result.returncode == 0:
                lines = result.stdout.strip().split("\n")
                if len(lines) >= 2:
                    stats_line = lines[1]  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    parts = stats_line.split("\t")

                    cpu_percent = float(parts[0].rstrip("%"))
                    memory_usage = parts[1]  # "123.4MiB / 2GiB" å½¢å¼

                    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’MBã«å¤‰æ›
                    memory_mb = self._parse_memory_usage(memory_usage)

                    return {
                        "cpu_percent": cpu_percent,
                        "memory_mb": memory_mb,
                        "timestamp": datetime.now().isoformat(),
                    }

            return {
                "cpu_percent": 0.0,
                "memory_mb": 0.0,
                "error": "Could not retrieve metrics",
            }

        except Exception as e:
            return {"cpu_percent": 0.0, "memory_mb": 0.0, "error": str(e)}

    def _parse_memory_usage(self, memory_str: str) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ–‡å­—åˆ—ã‚’MBã«å¤‰æ›"""
        try:
            # "123.4MiB / 2GiB" ã‹ã‚‰ "123.4MiB" ã‚’æŠ½å‡º
            used_part = memory_str.split(" / ")[0]

            if "MiB" in used_part:
                return float(used_part.replace("MiB", ""))
            elif "GiB" in used_part:
                return float(used_part.replace("GiB", "")) * 1024
            elif "KiB" in used_part:
                return float(used_part.replace("KiB", "")) / 1024
            else:
                return 0.0
        except Exception:
            return 0.0

class FailoverManager:
    """ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: RedundancyConfig):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.config = config
        self.failure_counts = defaultdict(int)
        self.failure_history = defaultdict(deque)
        self.active_failovers = {}

    def should_trigger_failover(self, failure_event: Dict[str, Any]) -> bool:
        """ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã¹ãã‹ã‚’åˆ¤å®š"""
        service = failure_event["service"]
        failure_type = failure_event["type"]

        # é€£ç¶šå¤±æ•—å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        self.failure_counts[service] += 1
        self.failure_history[service].append(
            {
                "timestamp": datetime.now(),
                "type": failure_type,
                "details": failure_event,
            }
        )

        # å±¥æ­´ã¯æœ€æ–°50ä»¶ã®ã¿ä¿æŒ
        if len(self.failure_history[service]) > 50:
            self.failure_history[service].popleft()

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼åˆ¤å®š
        consecutive_failures = failure_event.get(
            "consecutive_failures", self.failure_counts[service]
        )

        # é–¾å€¤ãƒã‚§ãƒƒã‚¯
        if consecutive_failures >= self.config.failover_threshold:
            logger.warning(
                f"Service {service} exceeded failure threshold ({consecutive_failures} >= " \
                    "{self.config.failover_threshold})"
            )
            return True

        # é‡å¤§ãªå¤±æ•—ã‚¿ã‚¤ãƒ—ã®å ´åˆã¯å³åº§ã«ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼
        critical_failure_types = [
            "container_exit",
            "health_check_timeout",
            "resource_exhaustion",
        ]
        if failure_type in critical_failure_types:
            logger.error(
                f"Critical failure detected for service {service}: {failure_type}"
            )
            return True

        return False

    def execute_failover(self, failure_event: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ã‚’å®Ÿè¡Œ"""
        service = failure_event["service"]
        failover_id = str(uuid.uuid4())

        failover_event = FailoverEvent(
            id=failover_id,
            service_name=service,
            trigger_type=failure_event["type"],
            failure_details=failure_event,
            failover_strategy="",
            started_at=datetime.now(),
            completed_at=None,
            success=False,
            recovery_actions=[],
            impact_assessment={},
        )

        self.active_failovers[failover_id] = failover_event

        try:
            # ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼æˆ¦ç•¥ã®æ±ºå®š
            strategy = self._determine_failover_strategy(failure_event)
            failover_event.failover_strategy = strategy["action"]

            # ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼å®Ÿè¡Œ
            if strategy["action"] == "restart_container":
                recovery_result = self._restart_container(service)
            elif strategy["action"] == "scale_up":
                recovery_result = self._scale_up_service(service, strategy)
            elif strategy["action"] == "switch_to_backup":
                recovery_result = self._switch_to_backup(service, strategy)
            else:
                recovery_result = {
                    "success": False,
                    "error": f"Unknown strategy: {strategy['action']}",
                }

            # çµæœã®è¨˜éŒ²
            failover_event.completed_at = datetime.now()
            failover_event.success = recovery_result["success"]
            failover_event.recovery_actions = recovery_result.get("actions", [])

            # å½±éŸ¿è©•ä¾¡
            failover_time = (
                failover_event.completed_at - failover_event.started_at
            ).total_seconds()
            failover_event.impact_assessment = {
                "failover_time_seconds": failover_time,
                "downtime_estimated": recovery_result.get("downtime_seconds", 0),
                "affected_connections": recovery_result.get("affected_connections", 0),
            }

            if recovery_result["success"]:
                # å¤±æ•—ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
                self.failure_counts[service] = 0
                logger.info(f"Failover completed successfully for service {service}")

            return {
                "success": recovery_result["success"],
                "failover_id": failover_id,
                "strategy": strategy["action"],
                "backup_service": strategy.get("target_service"),
                "failover_time": failover_time,
                "recovery_actions": failover_event.recovery_actions,
                "error": recovery_result.get("error"),
            }

        except Exception as e:
            failover_event.completed_at = datetime.now()
            failover_event.success = False
            logger.error(f"Failover failed for service {service}: {e}")

            return {"success": False, "failover_id": failover_id, "error": str(e)}

    def _determine_failover_strategy(
        self, failure_event: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼æˆ¦ç•¥ã‚’æ±ºå®š"""
        service = failure_event["service"]
        failure_type = failure_event["type"]

        # ã‚µãƒ¼ãƒ“ã‚¹åã‹ã‚‰ãƒ™ãƒ¼ã‚¹åã‚’æŠ½å‡º
        base_service = service.replace("-primary", "").replace("-backup", "")

        # å¤±æ•—ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãæˆ¦ç•¥æ±ºå®š
        if failure_type in ["container_exit", "health_check_failed"]:
            # ã¾ãšã¯å†èµ·å‹•ã‚’è©¦è¡Œ
            return {
                "action": "restart_container",
                "target_service": service,
                "estimated_downtime": 30,
            }

        elif failure_type == "resource_exhaustion":
            # ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ã®å ´åˆã¯ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
            return {
                "action": "scale_up",
                "target_service": base_service,
                "scale_factor": 1,
                "estimated_downtime": 60,
            }

        elif failure_type == "network_partition":
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ–­ã®å ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã«åˆ‡ã‚Šæ›¿ãˆ
            backup_service = (
                f"{base_service}-backup"
                if not service.endswith("-backup")
                else f"{base_service}-primary"
            )
            return {
                "action": "switch_to_backup",
                "target_service": backup_service,
                "estimated_downtime": 10,
            }

        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å†èµ·å‹•
            return {
                "action": "restart_container",
                "target_service": service,
                "estimated_downtime": 30,
            }

    def _restart_container(self, service: str) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ãƒŠã‚’å†èµ·å‹•"""
        try:
            cmd = ["docker", "restart", service]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)

            if result.returncode == 0:
                return {
                    "success": True,
                    "actions": [
                        {
                            "type": "restart_container",
                            "service": service,
                            "timestamp": datetime.now().isoformat(),
                        }
                    ],
                    "downtime_seconds": 30,
                }
            else:
                return {"success": False, "error": result.stderr, "actions": []}

        except subprocess.TimeoutExpired:
            return {"success": False, "error": "Restart timeout", "actions": []}
        except Exception as e:
            return {"success": False, "error": str(e), "actions": []}

    def _scale_up_service(
        self, service: str, strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—"""
        try:
            # Docker Compose scale ã‚³ãƒãƒ³ãƒ‰
            scale_factor = strategy.get("scale_factor", 1)
            compose_path = PROJECT_ROOT / self.config.compose_file_path

            cmd = [
                "docker-compose",
                "-f",
                str(compose_path),
                "up",
                "-d",
                "--scale",
                f"{service}={scale_factor}",
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)

            if result.returncode == 0:
                return {
                    "success": True,
                    "actions": [
                        {
                            "type": "scale_up",
                            "service": service,
                            "scale_factor": scale_factor,
                            "timestamp": datetime.now().isoformat(),
                        }
                    ],
                    "downtime_seconds": 0,
                }
            else:
                return {"success": False, "error": result.stderr, "actions": []}

        except Exception as e:
            return {"success": False, "error": str(e), "actions": []}

    def _switch_to_backup(
        self, service: str, strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒ“ã‚¹ã«åˆ‡ã‚Šæ›¿ãˆ"""
        try:
            target_service = strategy["target_service"]

            # å¤±æ•—ã—ãŸã‚µãƒ¼ãƒ“ã‚¹ã‚’åœæ­¢
            stop_cmd = ["docker", "stop", service]
            stop_result = subprocess.run(stop_cmd, capture_output=True, text=True)

            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒ“ã‚¹ã‚’é–‹å§‹ï¼ˆæ—¢ã«å‹•ã„ã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            start_cmd = ["docker", "start", target_service]
            start_result = subprocess.run(start_cmd, capture_output=True, text=True)

            actions = []
            if stop_result.returncode == 0:
                actions.append(
                    {
                        "type": "stop_service",
                        "service": service,
                        "timestamp": datetime.now().isoformat(),
                    }
                )

            if start_result.returncode == 0:
                actions.append(
                    {
                        "type": "start_backup",
                        "service": target_service,
                        "timestamp": datetime.now().isoformat(),
                    }
                )

            return {
                "success": len(actions) > 0,
                "actions": actions,
                "downtime_seconds": 10,
            }

        except Exception as e:
            return {"success": False, "error": str(e), "actions": []}

    def execute_automatic_recovery(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """è‡ªå‹•å¾©æ—§ã‚’å®Ÿè¡Œ"""
        failed_service = scenario["failed_service"]
        backup_service = scenario.get("backup_service")
        recovery_strategy = scenario["recovery_strategy"]

        recovery_actions = []

        try:
            if recovery_strategy == "restart_and_fallback":
                # ã¾ãšå†èµ·å‹•ã‚’è©¦è¡Œ
                restart_result = self._restart_container(failed_service)
                recovery_actions.extend(restart_result.get("actions", []))

                # å†èµ·å‹•å¤±æ•—ã®å ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã«åˆ‡ã‚Šæ›¿ãˆ
                if not restart_result["success"] and backup_service:
                    fallback_result = self._switch_to_backup(
                        failed_service, {"target_service": backup_service}
                    )
                    recovery_actions.extend(fallback_result.get("actions", []))
                    success = fallback_result["success"]
                else:
                    success = restart_result["success"]

            elif recovery_strategy == "scale_up_existing":
                # æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
                scale_result = self._scale_up_service(
                    failed_service, {"scale_factor": 2}
                )
                recovery_actions.extend(scale_result.get("actions", []))
                success = scale_result["success"]

            else:
                return {
                    "success": False,
                    "error": f"Unknown recovery strategy: {recovery_strategy}",
                    "recovery_actions": [],
                }

            return {
                "success": success,
                "recovery_actions": recovery_actions,
                "recovery_time": datetime.now().isoformat(),
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "recovery_actions": recovery_actions,
            }

    def detect_cascading_failure(
        self, failure_events: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰éšœå®³ã‚’æ¤œçŸ¥"""
        if len(failure_events) < 2:
            return {"is_cascading": False}

        # æ™‚é–“çª“å†…ã§ã®å¤±æ•—ã‚’åˆ†æ
        time_window = timedelta(minutes=5)
        recent_failures = []

        for event in failure_events:
            event_time = datetime.fromisoformat(event["timestamp"])
            if datetime.now() - event_time <= time_window:
                recent_failures.append(event)

        # ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰éšœå®³ã®åˆ¤å®š
        if len(recent_failures) >= 3:
            affected_services = len(set(event["service"] for event in recent_failures))
            time_span = max(
                datetime.fromisoformat(e["timestamp"]) for e in recent_failures
            ) - min(datetime.fromisoformat(e["timestamp"]) for e in recent_failures)

            return {
                "is_cascading": True,
                "affected_services": affected_services,
                "failure_count": len(recent_failures),
                "time_window": time_span.total_seconds(),
            }

        return {"is_cascading": False}

    def generate_emergency_response_plan(
        self, cascade_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç·Šæ€¥æ™‚å¯¾å¿œè¨ˆç”»ã‚’ç”Ÿæˆ"""
        emergency_actions = []

        # ã‚·ã‚¹ãƒ†ãƒ åˆ†é›¢
        emergency_actions.append(
            {
                "type": "system_isolation",
                "priority": "immediate",
                "description": "Isolate failed services to prevent further cascade",
                "estimated_time": 60,
            }
        )

        # ç®¡ç†è€…ã‚¢ãƒ©ãƒ¼ãƒˆ
        emergency_actions.append(
            {
                "type": "alert_administrators",
                "priority": "immediate",
                "description": "Send critical alert to system administrators",
                "estimated_time": 5,
            }
        )

        # å¥å…¨ãªã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
        emergency_actions.append(
            {
                "type": "emergency_scale_up",
                "priority": "high",
                "description": "Scale up healthy services to handle increased load",
                "estimated_time": 120,
            }
        )

        # ãƒ‡ãƒ¼ã‚¿ä¿è­·
        emergency_actions.append(
            {
                "type": "data_protection",
                "priority": "high",
                "description": "Enable emergency data backup and protection",
                "estimated_time": 300,
            }
        )

        return {
            "priority": "critical",
            "estimated_total_time": sum(
                action["estimated_time"] for action in emergency_actions
            ),
            "emergency_actions": emergency_actions,
            "cascade_severity": cascade_info.get("affected_services", 0),
            "plan_generated_at": datetime.now().isoformat(),
        }

class DockerRedundancySystem:
    """Dockerå†—é•·åŒ–ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: Optional[RedundancyConfig] = None):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.config = config or RedundancyConfig()
        self.compose_manager = DockerComposeManager(self.config)
        self.health_monitor = ServiceHealthMonitor(self.config)
        self.failover_manager = FailoverManager(self.config)

        # ç›£è¦–ãƒ«ãƒ¼ãƒ—åˆ¶å¾¡
        self.monitoring_active = False
        self.monitoring_thread = None

        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
        self.dashboard_integration = None
        if EXISTING_SYSTEMS_AVAILABLE:
            try:
                from libs.worker_monitoring_dashboard import WorkerMonitoringDashboard

                logger.info("Existing monitoring dashboard available for integration")
            except ImportError:
                pass

    def initialize_redundancy(self, services_config: Dict[str, Any]) -> Dict[str, Any]:
        """å†—é•·åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–"""
        try:
            logger.info("Initializing Docker redundancy system...")

            # Docker Compose ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
            compose_file = self.compose_manager.save_compose_file(services_config)
            logger.info(f"Generated Docker Compose file: {compose_file}")

            # ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ—ãƒ­ã‚¤
            deploy_result = self.compose_manager.deploy_services()
            if not deploy_result["success"]:
                return {
                    "success": False,
                    "error": f"Failed to deploy services: {deploy_result['error']}",
                }

            # ç›£è¦–é–‹å§‹
            self.start_monitoring()

            return {
                "success": True,
                "compose_file": compose_file,
                "deployed_services": deploy_result["deployed_services"],
                "monitoring_active": self.monitoring_active,
                "initialization_time": datetime.now().isoformat(),
            }

        except Exception as e:
            logger.error(f"Failed to initialize redundancy system: {e}")
            return {"success": False, "error": str(e)}

    def start_monitoring(self):
        """ç›£è¦–ã‚’é–‹å§‹"""
        if self.monitoring_active:
            logger.warning("Monitoring is already active")
            return

        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop, daemon=True
        )
        self.monitoring_thread.start()
        logger.info("Started redundancy monitoring")

    def stop_monitoring(self):
        """ç›£è¦–ã‚’åœæ­¢"""
        self.monitoring_active = False
        if self.monitoring_thread and self.monitoring_thread.is_alive():
            self.monitoring_thread.join(timeout=10)
        logger.info("Stopped redundancy monitoring")

    def _monitoring_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                # ãƒ˜ãƒ«ã‚¹çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
                health_status = self.health_monitor.check_all_services()

                if health_status["success"]:
                    # ç•°å¸¸ãªã‚µãƒ¼ãƒ“ã‚¹ã®ãƒã‚§ãƒƒã‚¯
                    for service in health_status["services"]:
                        if service["health_status"] != "healthy":
                            # ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼åˆ¤å®š
                            failure_event = {
                                "service": service["name"],
                                "type": "health_check_failed",
                                "container_status": service["container_status"],
                                "health_status": service["health_status"],
                                "timestamp": datetime.now().isoformat(),
                                "consecutive_failures": 1,
                            }

                            if self.failover_manager.should_trigger_failover(
                                failure_event
                            ):
                                logger.warning(
                                    f"Triggering failover for service {service['name']}"
                                )
                                failover_result = (
                                    self.failover_manager.execute_failover(
                                        failure_event
                                    )
                                )

                                if failover_result["success"]:
                                    logger.info(
                                        f"Failover completed for {service['name']}"
                                    )
                                else:
                                    logger.error(
                                        f"Failover failed for {service['name']}: {failover_result.get('error')}"
                                    )

                # ç›£è¦–é–“éš”å¾…æ©Ÿ
                time.sleep(self.config.monitoring_interval)

            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}")
                time.sleep(self.config.monitoring_interval)

    def get_system_status(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®çŠ¶æ…‹ã‚’å–å¾—"""
        try:
            # ãƒ˜ãƒ«ã‚¹çŠ¶æ…‹å–å¾—
            health_status = self.health_monitor.check_all_services()

            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼æƒ…å ±
            active_failovers = {
                fid: {
                    "service": fo.service_name,
                    "strategy": fo.failover_strategy,
                    "started_at": fo.started_at.isoformat(),
                    "success": fo.success,
                }
                for fid, fo in self.failover_manager.active_failovers.items()
                if fo.completed_at is None
                or (datetime.now() - fo.completed_at).seconds < 300
            }

            # ã‚·ã‚¹ãƒ†ãƒ å¯ç”¨æ€§è¨ˆç®—
            if health_status["success"] and health_status["services"]:
                availability = (
                    health_status["healthy_count"] / len(health_status["services"])
                ) * 100
            else:
                availability = 0.0

            return {
                "success": True,
                "monitoring_active": self.monitoring_active,
                "health_status": health_status,
                "active_failovers": active_failovers,
                "system_availability": availability,
                "config": asdict(self.config),
                "status_time": datetime.now().isoformat(),
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def integrate_with_dashboard(self, dashboard) -> Dict[str, Any]:
        """ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¨ã®çµ±åˆ"""
        try:
            self.dashboard_integration = dashboard
            logger.info("Integrated with monitoring dashboard")

            return {"success": True, "integration_time": datetime.now().isoformat()}
        except Exception as e:
            logger.error(f"Failed to integrate with dashboard: {e}")
            return {"success": False, "error": str(e)}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ã®ã‚µãƒ¼ãƒ“ã‚¹è¨­å®šä¾‹
DEFAULT_SERVICES_CONFIG = {
    "pm-worker": {
        "image": "ai-company/pm-worker:latest",
        "replicas": 2,
        "environment": {
            "RABBITMQ_HOST": "rabbitmq",
            "LOG_LEVEL": "INFO",
            "WORKER_TYPE": "pm",
        },
        "dependencies": ["monitoring-dashboard"],
        "restart_policy": "always",
        "health_check": True,
        "health_check_command": "python3 -c \"import workers.async_pm_worker_simple; print('OK')\" || exit 1",
    },
    "task-worker": {
        "image": "ai-company/task-worker:latest",
        "replicas": 3,
        "environment": {
            "RABBITMQ_HOST": "rabbitmq",
            "LOG_LEVEL": "INFO",
            "WORKER_TYPE": "task",
        },
        "dependencies": ["monitoring-dashboard"],
        "restart_policy": "always",
        "health_check": True,
        "health_check_command": "python3 -c \"import workers.simple_task_worker; print('OK')\" || exit 1",
    },
    "monitoring-dashboard": {
        "image": "ai-company/monitoring:latest",
        "replicas": 1,
        "environment": {"LOG_LEVEL": "INFO", "WEB_PORT": "8000"},
        "ports": ["8000:8000"],
        "volumes": ["./data:/app/data", "./logs:/app/logs"],
        "restart_policy": "always",
        "health_check": True,
        "health_check_command": "curl -f http://localhost:8000/health || exit 1",
    },
}

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    logging.basicConfig(level=logging.INFO)

    # å†—é•·åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    redundancy_system = DockerRedundancySystem()

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    result = redundancy_system.initialize_redundancy(DEFAULT_SERVICES_CONFIG)

    if result["success"]:
        print("âœ… Docker Redundancy System initialized successfully")
        print(f"ğŸ“ Compose file: {result['compose_file']}")
        print(f"ğŸš€ Deployed services: {result['deployed_services']}")
        print(f"ğŸ“Š Monitoring active: {result['monitoring_active']}")
    else:
        print(f"âŒ Failed to initialize: {result['error']}")
