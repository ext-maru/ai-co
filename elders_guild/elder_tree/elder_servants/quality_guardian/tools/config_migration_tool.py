#!/usr/bin/env python3
"""
ğŸš€ Elders Guild è¨­å®šçµ±åˆãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ„ãƒ¼ãƒ«
æ®µéšçš„çµ±åˆå®Ÿè£… - æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®å®Œå…¨äº’æ›æ€§ç¢ºä¿

ã“ã®ãƒ„ãƒ¼ãƒ«ã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆã‚’æ®µéšçš„ã«å®Ÿè¡Œã—ã€
æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œã‚’ä¸­æ–­ã™ã‚‹ã“ã¨ãªãçµ±åˆã‚’å®Œäº†ã—ã¾ã™ã€‚
"""

import argparse
import json
import logging
import os
import shutil
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from libs.env_config import Config as LegacyConfig
from libs.integrated_config_system import IntegratedConfigSystem, get_config

logger = logging.getLogger(__name__)

class ConfigMigrationTool:
    """è¨­å®šãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ„ãƒ¼ãƒ«"""

    def __init__(self, dry_run: bool = False):
        self.dry_run = dry_run
        self.project_root = Path("/home/aicompany/ai_co")
        self.config_dir = self.project_root / "config"
        self.backup_dir = self.config_dir / "backups"
        self.migration_dir = self.config_dir / "migration"

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        self.migration_dir.mkdir(parents=True, exist_ok=True)

        # çµ±åˆè¨­å®šã‚·ã‚¹ãƒ†ãƒ 
        self.integrated_config = IntegratedConfigSystem()

        # ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹
        self.migration_state = {
            "phase": "not_started",
            "completed_steps": [],
            "failed_steps": [],
            "timestamp": datetime.now().isoformat(),
        }

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        self.backup_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    def run_migration(self, phase: str = "all") -> bool:
        """ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"""
        logger.info(
            f"Starting configuration migration (phase: {phase}, dry_run: {self.dry_run})"
        )

        try:
            if phase == "all" or phase == "phase1":
                self.execute_phase1()

            if phase == "all" or phase == "phase2":
                self.execute_phase2()

            if phase == "all" or phase == "phase3":
                self.execute_phase3()

            self.migration_state["phase"] = "completed"
            self.save_migration_state()

            logger.info("Migration completed successfully")
            return True

        except Exception as e:
            logger.error(f"Migration failed: {e}")
            self.migration_state["phase"] = "failed"
            self.migration_state["error"] = str(e)
            self.save_migration_state()
            return False

    def execute_phase1(self):
        """Phase 1: ç·Šæ€¥çµ±åˆ - é‡è¤‡è¨­å®šã®å³åº§è§£æ±º"""
        logger.info("Executing Phase 1: Emergency Integration")

        # ã‚¹ãƒ†ãƒƒãƒ—1: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        self.backup_existing_configs()

        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¢ãƒ‡ãƒ«æŒ‡å®šçµ±ä¸€
        self.unify_model_specifications()

        # ã‚¹ãƒ†ãƒƒãƒ—3: é‡è¤‡Slackè¨­å®šçµ±åˆ
        self.merge_slack_configs()

        # ã‚¹ãƒ†ãƒƒãƒ—4: åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ è¨­å®šçµ±åˆ
        self.merge_system_configs()

        # ã‚¹ãƒ†ãƒƒãƒ—5: çµ±åˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
        self.generate_integrated_configs()

        self.migration_state["completed_steps"].append("phase1")

    def execute_phase2(self):
        """Phase 2: æ§‹é€ æ”¹å–„ - è¨­å®šæ§‹é€ ã®æœ€é©åŒ–"""
        logger.info("Executing Phase 2: Structural Improvements")

        # ã‚¹ãƒ†ãƒƒãƒ—1: éšå±¤å®šç¾©ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«åŒ–
        self.create_hierarchy_config()

        # ã‚¹ãƒ†ãƒƒãƒ—2: ç’°å¢ƒåˆ¥è¨­å®šåˆ†é›¢
        self.separate_environment_configs()

        # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½è¿½åŠ 
        self.add_config_validation()

        # ã‚¹ãƒ†ãƒƒãƒ—4: è¨­å®šäº’æ›æ€§ãƒ¬ã‚¤ãƒ¤ãƒ¼ä½œæˆ
        self.create_compatibility_layer()

        self.migration_state["completed_steps"].append("phase2")

    def execute_phase3(self):
        """Phase 3: é«˜åº¦åŒ– - å‹•çš„æ©Ÿèƒ½ã®å®Ÿè£…"""
        logger.info("Executing Phase 3: Advanced Features")

        # ã‚¹ãƒ†ãƒƒãƒ—1: å‹•çš„è¨­å®šãƒªãƒ­ãƒ¼ãƒ‰
        self.implement_dynamic_reload()

        # ã‚¹ãƒ†ãƒƒãƒ—2: è¨­å®šå¤‰æ›´ç›£æŸ»ãƒ­ã‚°
        self.implement_audit_logging()

        # ã‚¹ãƒ†ãƒƒãƒ—3: è‡ªå‹•è¨­å®šæœ€é©åŒ–
        self.implement_auto_optimization()

        self.migration_state["completed_steps"].append("phase3")

    def backup_existing_configs(self):
        """æ—¢å­˜è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        logger.info("Backing up existing configuration files")

        backup_path = self.backup_dir / f"pre_migration_{self.backup_timestamp}"
        backup_path.mkdir(exist_ok=True)

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
        config_files = [
            "config.json",
            "system.json",
            "system.conf",
            "slack_config.json",
            "slack_pm_config.json",
            "slack_api_config.json",
            "slack_monitor.json",
            "worker.json",
            "worker_config.json",
            "async_workers_config.yaml",
            "worker_recovery.yaml",
            "database.conf",
            "logging.json",
            "storage.json",
            "task_types.json",
        ]

        backed_up_files = []
        for filename in config_files:
            source_path = self.config_dir / filename
            if source_path.exists():
                dest_path = backup_path / filename
                if not self.dry_run:
                    shutil.copy2(source_path, dest_path)
                backed_up_files.append(filename)
                logger.info(f"Backed up: {filename}")

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            "timestamp": self.backup_timestamp,
            "files": backed_up_files,
            "migration_version": "1.0",
        }

        if not self.dry_run:
            with open(backup_path / "metadata.json", "w") as f:
                json.dump(metadata, f, indent=2)

        logger.info(f"Backup completed: {backup_path}")

    def unify_model_specifications(self):
        """ãƒ¢ãƒ‡ãƒ«æŒ‡å®šã®çµ±ä¸€"""
        logger.info("Unifying model specifications")

        unified_model = "claude-sonnet-4-20250514"

        # config.jsonã®æ›´æ–°
        config_path = self.config_dir / "config.json"
        if config_path.exists():
            with open(config_path) as f:
                config = json.load(f)

            if "claude" in config:
                config["claude"]["model"] = unified_model

            if not self.dry_run:
                with open(config_path, "w") as f:
                    json.dump(config, f, indent=2)

        # worker.jsonã®æ›´æ–°
        worker_path = self.config_dir / "worker.json"
        if worker_path.exists():
            with open(worker_path) as f:
                worker_config = json.load(f)

            worker_config["default_model"] = unified_model

            if not self.dry_run:
                with open(worker_path, "w") as f:
                    json.dump(worker_config, f, indent=2)

        # YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°
        async_worker_path = self.config_dir / "async_workers_config.yaml"
        if async_worker_path.exists():
            with open(async_worker_path) as f:
                async_config = yaml.safe_load(f)

            if (
                "enhanced_task_worker" in async_config
                and "claude_model" in async_config["enhanced_task_worker"]
            ):
                async_config["enhanced_task_worker"]["claude_model"] = unified_model

            if not self.dry_run:
                with open(async_worker_path, "w") as f:
                    yaml.dump(async_config, f, default_flow_style=False)

        logger.info(f"Model specification unified to: {unified_model}")

    def merge_slack_configs(self):
        """Slackè¨­å®šã®çµ±åˆ"""
        logger.info("Merging Slack configurations")

        merged_config = {
            "basic": {"enabled": True, "rate_limit": 1},
            "channels": {
                "default": "#ai-notifications",
                "error": "#elders-guild-errors",
                "notification": "#elders-guild-notifications",
            },
            "polling": {"enabled": True, "interval": 20},
        }

        # æ—¢å­˜è¨­å®šã‹ã‚‰çµ±åˆ
        slack_files = [
            "slack_config.json",
            "slack_pm_config.json",
            "slack_api_config.json",
            "slack_monitor.json",
        ]

        # ç¹°ã‚Šè¿”ã—å‡¦ç†
        for filename in slack_files:
            file_path = self.config_dir / filename
            if file_path.exists():
                with open(file_path) as f:
                    config = json.load(f)

                # åŸºæœ¬è¨­å®šã®çµ±åˆ
                if "bot_token" in config:
                    merged_config["auth"] = merged_config.get("auth", {})
                    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¸Šã€å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ç’°å¢ƒå¤‰æ•°ã§ç®¡ç†

                if "polling_interval" in config:
                    merged_config["polling"]["interval"] = config["polling_interval"]

                if "rate_limit" in config:
                    merged_config["basic"]["rate_limit"] = config["rate_limit"]

                # ãã®ä»–ã®è¨­å®šã‚‚çµ±åˆ
                for key, value in config.items():
                    if key not in ["bot_token", "app_token"]:  # ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–æƒ…å ±ã¯é™¤å¤–
                        merged_config[key] = value

        # çµ±åˆè¨­å®šã‚’ä¿å­˜
        integrated_slack_path = self.config_dir / "integrated" / "slack.yaml"
        if not self.dry_run:
            integrated_slack_path.parent.mkdir(exist_ok=True)
            with open(integrated_slack_path, "w") as f:
                yaml.dump(merged_config, f, default_flow_style=False)

        logger.info("Slack configurations merged successfully")

    def merge_system_configs(self):
        """ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã®çµ±åˆ"""
        logger.info("Merging system configurations")

        merged_config = {
            "system": {"name": "Elders Guild", "version": "6.0", "language": "ja"},
            "project": {"name": "ai_co", "root_dir": "/home/aicompany/ai_co"},
        }

        # system.jsonã‹ã‚‰çµ±åˆ
        system_json_path = self.config_dir / "system.json"
        if system_json_path.exists():
            with open(system_json_path) as f:
                system_config = json.load(f)

            if "language" in system_config:
                merged_config["system"]["language"] = system_config["language"]

        # system.confã‹ã‚‰çµ±åˆ
        system_conf_path = self.config_dir / "system.conf"
        if system_conf_path.exists():
            with open(system_conf_path) as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#") and "=" in line:
                        key, value = line.split("=", 1)
                        key = key.strip()
                        value = value.strip()

                        if not (key == "PROJECT_DIR"):
                            continue  # Early return to reduce nesting
                        # Reduced nesting - original condition satisfied
                        if key == "PROJECT_DIR":
                            merged_config["project"]["root_dir"] = value
                        elif key == "PROJECT_NAME":
                            merged_config["project"]["name"] = value

        # çµ±åˆè¨­å®šã‚’ä¿å­˜
        integrated_core_path = self.config_dir / "integrated" / "core.yaml"
        if not self.dry_run:
            integrated_core_path.parent.mkdir(exist_ok=True)
            with open(integrated_core_path, "w") as f:
                yaml.dump(merged_config, f, default_flow_style=False)

        logger.info("System configurations merged successfully")

    def generate_integrated_configs(self):
        """çµ±åˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ"""
        logger.info("Generating integrated configuration files")

        if not self.dry_run:
            self.integrated_config.create_integrated_config_files()

        logger.info("Integrated configuration files generated")

    def create_hierarchy_config(self):
        """éšå±¤è¨­å®šã®ä½œæˆ"""
        logger.info("Creating hierarchy configuration")

        hierarchy_config = {
            "hierarchy": {
                "supreme_authority": {
                    "name": "ã‚°ãƒ©ãƒ³ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼maru",
                    "title": "Grand Elder maru",
                    "role": "æœ€é«˜æ¨©é™è€…ãƒ»æˆ¦ç•¥æ±ºå®šè€…",
                },
                "executive_partner": {
                    "name": "ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼",
                    "title": "Claude Elder",
                    "role": "é–‹ç™ºå®Ÿè¡Œè²¬ä»»è€…ãƒ»4è³¢è€…çµ±æ‹¬",
                },
                "wisdom_council": {
                    "name": "4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ ",
                    "title": "Four Sages System",
                    "enabled": True,
                },
            }
        }

        hierarchy_path = self.config_dir / "integrated" / "hierarchy.yaml"
        if not self.dry_run:
            hierarchy_path.parent.mkdir(exist_ok=True)
            with open(hierarchy_path, "w") as f:
                yaml.dump(hierarchy_config, f, default_flow_style=False)

        logger.info("Hierarchy configuration created")

    def separate_environment_configs(self):
        """ç’°å¢ƒåˆ¥è¨­å®šã®åˆ†é›¢"""
        logger.info("Separating environment-specific configurations")

        environments = ["development", "staging", "production"]

        for env in environments:
            env_config = {
                "environment": env,

                "monitoring": {
                    "enabled": env == "production",

                },
            }

            env_path = self.config_dir / "integrated" / f"{env}.yaml"
            if not self.dry_run:
                env_path.parent.mkdir(exist_ok=True)
                with open(env_path, "w") as f:
                    yaml.dump(env_config, f, default_flow_style=False)

        logger.info("Environment-specific configurations separated")

    def add_config_validation(self):
        """è¨­å®šãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã®è¿½åŠ """
        logger.info("Adding configuration validation")

        validation_config = {
            "validation": {
                "enabled": True,
                "strict_mode": True,
                "required_fields": {
                    "claude": ["model", "api_key"],
                    "slack": ["bot_token"],
                    "database": ["host", "database"],
                },
            }
        }

        validation_path = self.config_dir / "integrated" / "validation.yaml"
        if not self.dry_run:
            validation_path.parent.mkdir(exist_ok=True)
            with open(validation_path, "w") as f:
                yaml.dump(validation_config, f, default_flow_style=False)

        logger.info("Configuration validation added")

    def create_compatibility_layer(self):
        """äº’æ›æ€§ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ä½œæˆ"""
        logger.info("Creating compatibility layer")

        compatibility_mapping = {
            "legacy_paths": {
                "config.json": "integrated/core.yaml",
                "slack_config.json": "integrated/slack.yaml",
                "worker_config.json": "integrated/workers.yaml",
                "database.conf": "integrated/database.yaml",
            },
            "field_mappings": {
                "claude.model": "claude.api.model",
                "workers.timeout": "workers.common.timeout",
                "slack.bot_token": "slack.auth.bot_token",
            },
        }

        compatibility_path = self.config_dir / "integrated" / "compatibility.yaml"
        if not self.dry_run:
            compatibility_path.parent.mkdir(exist_ok=True)
            with open(compatibility_path, "w") as f:
                yaml.dump(compatibility_mapping, f, default_flow_style=False)

        logger.info("Compatibility layer created")

    def implement_dynamic_reload(self):
        """å‹•çš„è¨­å®šãƒªãƒ­ãƒ¼ãƒ‰ã®å®Ÿè£…"""
        logger.info("Implementing dynamic configuration reload")

        # è¨­å®šå¤‰æ›´ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
        reload_script = """#!/usr/bin/env python3
import time
import os
from pathlib import Path
from libs.integrated_config_system import integrated_config

def monitor_config_changes():
    config_dir = Path("/home/aicompany/ai_co/config/integrated")
    last_modified = {}

    while True:
        for config_file in config_dir.glob("*.yaml"):
            current_modified = config_file.stat().st_mtime

            if config_file.name not in last_modified:
                last_modified[config_file.name] = current_modified
                continue

            if current_modified > last_modified[config_file.name]:
                print(f"Configuration changed: {config_file.name}")
                # è¨­å®šãƒªãƒ­ãƒ¼ãƒ‰
                namespace = config_file.stem
                integrated_config.get_config(namespace, force_reload=True)
                last_modified[config_file.name] = current_modified

        time.sleep(5)

if __name__ == "__main__":
    monitor_config_changes()
"""

        reload_script_path = self.project_root / "tools" / "config_reload_monitor.py"
        if not self.dry_run:
            reload_script_path.parent.mkdir(exist_ok=True)
            with open(reload_script_path, "w") as f:
                f.write(reload_script)
            reload_script_path.chmod(0o755)

        logger.info("Dynamic configuration reload implemented")

    def implement_audit_logging(self):
        """è¨­å®šå¤‰æ›´ç›£æŸ»ãƒ­ã‚°ã®å®Ÿè£…"""
        logger.info("Implementing configuration audit logging")

        audit_config = {
            "audit": {
                "enabled": True,
                "log_file": "/home/aicompany/ai_co/logs/config_audit.log",
                "retention_days": 30,
                "log_level": "INFO",
            }
        }

        audit_path = self.config_dir / "integrated" / "audit.yaml"
        if not self.dry_run:
            audit_path.parent.mkdir(exist_ok=True)
            with open(audit_path, "w") as f:
                yaml.dump(audit_config, f, default_flow_style=False)

        logger.info("Configuration audit logging implemented")

    def implement_auto_optimization(self):
        """è‡ªå‹•è¨­å®šæœ€é©åŒ–ã®å®Ÿè£…"""
        logger.info("Implementing automatic configuration optimization")

        optimization_config = {
            "optimization": {
                "enabled": True,
                "auto_tune": True,
                "performance_monitoring": True,
                "adaptive_settings": {
                    "worker_scaling": True,
                    "cache_tuning": True,
                    "rate_limiting": True,
                },
            }
        }

        optimization_path = self.config_dir / "integrated" / "optimization.yaml"
        if not self.dry_run:
            optimization_path.parent.mkdir(exist_ok=True)
            with open(optimization_path, "w") as f:
                yaml.dump(optimization_config, f, default_flow_style=False)

        logger.info("Automatic configuration optimization implemented")

    def save_migration_state(self):
        """ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ä¿å­˜"""
        state_path = self.migration_dir / "migration_state.json"
        if not self.dry_run:
            with open(state_path, "w") as f:
                json.dump(self.migration_state, f, indent=2)

    def rollback_migration(self):
        """ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        logger.info("Rolling back migration")

        backup_path = self.backup_dir / f"pre_migration_{self.backup_timestamp}"
        if not backup_path.exists():
            logger.error("Backup not found, cannot rollback")
            return False

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ
        with open(backup_path / "metadata.json") as f:
            metadata = json.load(f)

        for filename in metadata["files"]:
            source_path = backup_path / filename
            dest_path = self.config_dir / filename

            if source_path.exists():
                if not self.dry_run:
                    shutil.copy2(source_path, dest_path)
                logger.info(f"Restored: {filename}")

        logger.info("Migration rollback completed")
        return True

    def validate_migration(self) -> bool:
        """ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®æ¤œè¨¼"""
        logger.info("Validating migration results")

        try:
            # çµ±åˆè¨­å®šã‚·ã‚¹ãƒ†ãƒ ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
            health_check = self.integrated_config.health_check()

            if health_check["overall_status"] != "healthy":
                logger.error("Health check failed")
                return False

            # å„åå‰ç©ºé–“ã®è¨­å®šãŒæ­£å¸¸ã«èª­ã¿è¾¼ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for namespace in ["core", "claude", "slack", "workers", "database"]:
                config = get_config(namespace)
                if not config:
                    logger.error(f"Failed to load config for namespace: {namespace}")
                    return False

            logger.info("Migration validation successful")
            return True

        except Exception as e:
            logger.error(f"Migration validation failed: {e}")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="Elders Guild Configuration Migration Tool"
    )
    parser.add_argument(
        "--phase",
        choices=["phase1", "phase2", "phase3", "all"],
        default="all",
        help="Migration phase to execute",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be done without making changes",
    )
    parser.add_argument(
        "--rollback", action="store_true", help="Rollback the migration"
    )
    parser.add_argument(
        "--validate", action="store_true", help="Validate migration results"
    )
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")

    args = parser.parse_args()

    # ãƒ­ã‚°è¨­å®š

    logging.basicConfig(
        level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ„ãƒ¼ãƒ«åˆæœŸåŒ–
    migration_tool = ConfigMigrationTool(dry_run=args.dry_run)

    try:
        if args.rollback:
            success = migration_tool.rollback_migration()
        elif args.validate:
            success = migration_tool.validate_migration()
        else:
            success = migration_tool.run_migration(args.phase)

        if success:
            logger.info("Operation completed successfully")
            sys.exit(0)
        else:
            logger.error("Operation failed")
            sys.exit(1)

    except KeyboardInterrupt:
        logger.info("Operation cancelled by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
