"""
Tests for Quality Metrics Monitor - å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ

TDD Cycle: Red â†’ Green â†’ Refactor
Issue #309: è‡ªå‹•åŒ–å“è³ªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£… - Phase 4
ç›®çš„: å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®å®Œå…¨ãƒ†ã‚¹ãƒˆ
"""

import pytest
import asyncio
import time
from datetime import datetime, timedelta
from unittest.mock import MagicMock, patch
from typing import List, Dict, Any

# Test imports
from elders_guild.elder_tree.quality.quality_metrics_monitor import (
    QualityMetricsMonitor, QualityMetric, QualityAlert, MonitoringReport,
    AlertLevel, MetricType
)


class TestQualityMetricsMonitor:
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ å®Œå…¨ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def monitor(self):
        """ãƒ¢ãƒ‹ã‚¿ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ"""
        return QualityMetricsMonitor(monitoring_interval=0.1)  # ãƒ†ã‚¹ãƒˆç”¨çŸ­ã„é–“éš”
    
    @pytest.fixture
    def mock_pipeline_result(self):
        """ãƒ¢ãƒƒã‚¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœä½œæˆ"""
        result = MagicMock()
        result.unified_quality_score = 92.5
        result.overall_status = "ELDER_APPROVED"
        result.pipeline_efficiency = 88.0
        return result
    
    @pytest.fixture
    def mock_poor_pipeline_result(self):
        """ä½å“è³ªãƒ¢ãƒƒã‚¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœä½œæˆ"""
        result = MagicMock()
        result.unified_quality_score = 65.0  # é—¾å€¤ä»¥ä¸‹
        result.overall_status = "ELDER_REJECTED"
        result.pipeline_efficiency = 45.0  # é—¾å€¤ä»¥ä¸‹
        return result
    
    def test_quality_metrics_monitor_initialization(self, monitor):
        """ğŸ”´ Red: å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒ¼åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        assert monitor is not None
        assert monitor.monitor_id == "QMM-ELDER-001"
        assert monitor.monitoring_interval == 0.1
        assert hasattr(monitor, 'thresholds')
        assert hasattr(monitor, 'metrics_history')
        assert hasattr(monitor, 'active_alerts')
        assert hasattr(monitor, 'resolved_alerts')
        
        # é—¾å€¤è¨­å®šç¢ºèª
        assert MetricType.QUALITY_SCORE in monitor.thresholds
        assert monitor.thresholds[MetricType.QUALITY_SCORE]["critical"] == 75.0
        assert monitor.thresholds[MetricType.EXECUTION_TIME]["warning"] == 20.0
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´åˆæœŸåŒ–ç¢ºèª
        assert len(monitor.metrics_history) == len(MetricType)
        for metric_type in MetricType:
            assert metric_type in monitor.metrics_history
    
    def test_pipeline_execution_recording(self, monitor, mock_pipeline_result):
        """ğŸ”´ Red: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œè¨˜éŒ²ãƒ†ã‚¹ãƒˆ"""
        pipeline_id = "TEST-PIPELINE-001"
        execution_time = 15.5
        
        # å®Ÿè¡Œè¨˜éŒ²
        monitor.record_pipeline_execution(
            mock_pipeline_result, execution_time, pipeline_id
        )
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ç¢ºèª
        assert len(monitor.metrics_history[MetricType.QUALITY_SCORE]) == 1
        assert len(monitor.metrics_history[MetricType.EXECUTION_TIME]) == 1
        assert len(monitor.metrics_history[MetricType.ELDER_APPROVAL_RATE]) == 1
        assert len(monitor.metrics_history[MetricType.PIPELINE_EFFICIENCY]) == 1
        assert len(monitor.metrics_history[MetricType.ERROR_RATE]) == 1
        
        # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ç¢ºèª
        quality_metric = monitor.metrics_history[MetricType.QUALITY_SCORE][0]
        assert quality_metric.value == 92.5
        assert quality_metric.pipeline_id == pipeline_id
        
        execution_metric = monitor.metrics_history[MetricType.EXECUTION_TIME][0]
        assert execution_metric.value == 15.5
        
        approval_metric = monitor.metrics_history[MetricType.ELDER_APPROVAL_RATE][0]
        assert approval_metric.value == 1.0  # ELDER_APPROVED = 1.0
        
        error_metric = monitor.metrics_history[MetricType.ERROR_RATE][0]
        assert error_metric.value == 0.0  # æˆåŠŸæ™‚ã¯0.0
    
    def test_threshold_alert_generation(self, monitor, mock_poor_pipeline_result):
        """ğŸ”´ Red: é—¾å€¤ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        pipeline_id = "TEST-PIPELINE-POOR"
        execution_time = 35.0  # é—¾å€¤è¶…é (critical: 30.0)
        
        # ä½å“è³ªçµæœè¨˜éŒ²
        monitor.record_pipeline_execution(
            mock_poor_pipeline_result, execution_time, pipeline_id
        )
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆç¢ºèª
        assert len(monitor.active_alerts) > 0
        
        # å“è³ªã‚¹ã‚³ã‚¢ã‚¢ãƒ©ãƒ¼ãƒˆç¢ºèª (65.0 < 75.0)
        quality_alert_key = f"{MetricType.QUALITY_SCORE.value}-{AlertLevel.CRITICAL.value}"
        assert quality_alert_key in monitor.active_alerts
        
        quality_alert = monitor.active_alerts[quality_alert_key]
        assert quality_alert.alert_level == AlertLevel.CRITICAL
        assert quality_alert.current_value == 65.0
        assert quality_alert.threshold_value == 75.0
        
        # å®Ÿè¡Œæ™‚é–“ã‚¢ãƒ©ãƒ¼ãƒˆç¢ºèª (35.0 > 30.0)
        execution_alert_key = f"{MetricType.EXECUTION_TIME.value}-{AlertLevel.CRITICAL.value}"
        assert execution_alert_key in monitor.active_alerts
        
        execution_alert = monitor.active_alerts[execution_alert_key]
        assert execution_alert.alert_level == AlertLevel.CRITICAL
        assert execution_alert.current_value == 35.0
        assert execution_alert.threshold_value == 30.0
    
    def test_alert_level_classification(self, monitor):
        """ğŸ”´ Red: ã‚¢ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«åˆ†é¡ãƒ†ã‚¹ãƒˆ"""
        pipeline_id = "TEST-ALERT-LEVELS"
        
        # Warningãƒ¬ãƒ™ãƒ«ãƒ†ã‚¹ãƒˆ (å“è³ªã‚¹ã‚³ã‚¢: 80.0 < 85.0)
        warning_result = MagicMock()
        warning_result.unified_quality_score = 80.0
        warning_result.overall_status = "ELDER_CONDITIONAL"
        warning_result.pipeline_efficiency = 75.0
        
        monitor.record_pipeline_execution(warning_result, 15.0, pipeline_id)
        
        warning_alert_key = f"{MetricType.QUALITY_SCORE.value}-{AlertLevel.WARNING.value}"
        assert warning_alert_key in monitor.active_alerts
        assert monitor.active_alerts[warning_alert_key].alert_level == AlertLevel.WARNING
        
        # Emergencyãƒ¬ãƒ™ãƒ«ãƒ†ã‚¹ãƒˆ (å“è³ªã‚¹ã‚³ã‚¢: 55.0 < 60.0)
        monitor.active_alerts.clear()  # ã‚¯ãƒªã‚¢
        
        emergency_result = MagicMock()
        emergency_result.unified_quality_score = 55.0
        emergency_result.overall_status = "ELDER_REJECTED"
        emergency_result.pipeline_efficiency = 40.0
        
        monitor.record_pipeline_execution(emergency_result, 50.0, pipeline_id)
        
        emergency_alert_key = f"{MetricType.QUALITY_SCORE.value}-{AlertLevel.EMERGENCY.value}"
        assert emergency_alert_key in monitor.active_alerts
        assert monitor.active_alerts[emergency_alert_key].alert_level == AlertLevel.EMERGENCY
    
    def test_alert_callback_system(self, monitor):
        """ğŸ”´ Red: ã‚¢ãƒ©ãƒ¼ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°è¨­å®š
        callback_calls = []
        
        def test_callback(alert: QualityAlert):
            callback_calls.append(alert)
        
        monitor.add_alert_callback(test_callback)
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”Ÿã•ã›ã‚‹
        poor_result = MagicMock()
        poor_result.unified_quality_score = 70.0  # Criticalãƒ¬ãƒ™ãƒ«
        poor_result.overall_status = "ELDER_REJECTED"
        poor_result.pipeline_efficiency = 55.0
        
        monitor.record_pipeline_execution(poor_result, 25.0, "TEST-CALLBACK")
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å‘¼ã³å‡ºã—ç¢ºèª
        assert len(callback_calls) > 0
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å‰Šé™¤ãƒ†ã‚¹ãƒˆ
        monitor.remove_alert_callback(test_callback)
        callback_calls.clear()
        
        monitor.record_pipeline_execution(poor_result, 25.0, "TEST-CALLBACK-2")
        assert len(callback_calls) == 0  # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å‰Šé™¤å¾Œã¯å‘¼ã³å‡ºã—ãªã—
    
    def test_elder_council_escalation(self, monitor):
        """ğŸ”´ Red: Elder Councilã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æœ‰åŠ¹åŒ–
        monitor.elder_council_reporting["auto_escalation"] = True
        
        # Emergencyãƒ¬ãƒ™ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”Ÿ
        emergency_result = MagicMock()
        emergency_result.unified_quality_score = 45.0  # Emergencyãƒ¬ãƒ™ãƒ«
        emergency_result.overall_status = "ELDER_REJECTED"
        emergency_result.pipeline_efficiency = 35.0
        
        monitor.record_pipeline_execution(emergency_result, 50.0, "TEST-ESCALATION")
        
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºèª
        emergency_alerts = [
            alert for alert in monitor.active_alerts.values()
            if alert.alert_level == AlertLevel.EMERGENCY and alert.elder_council_notified
        ]
        assert len(emergency_alerts) > 0
    
    def test_alert_resolution(self, monitor):
        """ğŸ”´ Red: ã‚¢ãƒ©ãƒ¼ãƒˆè§£æ±ºãƒ†ã‚¹ãƒˆ"""
        # ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”Ÿ
        poor_result = MagicMock()
        poor_result.unified_quality_score = 70.0
        poor_result.overall_status = "ELDER_REJECTED"
        poor_result.pipeline_efficiency = 55.0
        
        monitor.record_pipeline_execution(poor_result, 25.0, "TEST-RESOLUTION")
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆå­˜åœ¨ç¢ºèª
        assert len(monitor.active_alerts) > 0
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆIDå–å¾—
        alert_id = list(monitor.active_alerts.values())[0].alert_id
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆè§£æ±º
        monitor.resolve_alert(alert_id, "Test resolution")
        
        # è§£æ±ºç¢ºèª
        resolved_alert_found = False
        for resolved_alert in monitor.resolved_alerts:
            if resolved_alert.alert_id == alert_id:
                assert resolved_alert.resolved is True
                assert resolved_alert.resolution_timestamp is not None
                resolved_alert_found = True
                break
        
        assert resolved_alert_found
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆã‹ã‚‰å‰Šé™¤ç¢ºèª
        active_alert_exists = any(
            alert.alert_id == alert_id for alert in monitor.active_alerts.values()
        )
        assert not active_alert_exists
    
    def test_current_metrics_summary(self, monitor, mock_pipeline_result):
        """ğŸ”´ Red: ç¾åœ¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦ãƒ†ã‚¹ãƒˆ"""
        # åˆæœŸçŠ¶æ…‹ï¼ˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰
        summary = monitor.get_current_metrics_summary()
        assert isinstance(summary, dict)
        assert len(summary) == len(MetricType)
        
        for metric_type in MetricType:
            assert metric_type.value in summary
            assert summary[metric_type.value]["current_value"] is None
            assert summary[metric_type.value]["trend"] == "NO_DATA"
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        for i in range(10):
            result = MagicMock()
            result.unified_quality_score = 90.0 + i  # æ¼¸å¢—ãƒˆãƒ¬ãƒ³ãƒ‰
            result.overall_status = "ELDER_APPROVED"
            result.pipeline_efficiency = 80.0 + i
            
            monitor.record_pipeline_execution(result, 15.0, f"TEST-{i}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚ã‚Šæ¦‚è¦
        summary = monitor.get_current_metrics_summary()
        
        quality_summary = summary[MetricType.QUALITY_SCORE.value]
        assert quality_summary["current_value"] == 99.0  # æœ€æ–°å€¤
        assert quality_summary["average_1h"] is not None
        assert quality_summary["trend"] in ["IMPROVING", "STABLE", "DEGRADING"]
    
    def test_monitoring_report_generation(self, monitor, mock_pipeline_result):
        """ğŸ”´ Red: ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        for i in range(5):
            result = MagicMock()
            result.unified_quality_score = 85.0 + i * 2
            result.overall_status = "ELDER_APPROVED" if i < 4 else "ELDER_REJECTED"
            result.pipeline_efficiency = 75.0 + i * 3
            
            monitor.record_pipeline_execution(result, 18.0 + i, f"TEST-REPORT-{i}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ (1æ™‚é–“åˆ†)
        report = monitor.generate_monitoring_report(period_hours=1)
        
        assert isinstance(report, MonitoringReport)
        assert report.report_id.startswith("QMR-")
        assert report.total_executions == 5
        assert report.successful_executions == 4  # 4å›æˆåŠŸ
        assert report.elder_approval_rate == 80.0  # 4/5 * 100
        assert report.average_quality_score > 0.0
        assert report.average_execution_time > 0.0
        assert report.pipeline_efficiency_trend in ["IMPROVING", "STABLE", "DEGRADING"]
        assert isinstance(report.active_alerts, list)
        assert isinstance(report.resolved_alerts, list)
        assert isinstance(report.recommendations, list)
        assert isinstance(report.elder_council_summary, dict)
        assert report.timestamp is not None
        
        # Elder Councilæ¦‚è¦ç¢ºèª
        elder_summary = report.elder_council_summary
        assert "overall_health" in elder_summary
        assert elder_summary["overall_health"] in ["GOOD", "NEEDS_ATTENTION", "CRITICAL"]
        assert "monitoring_authority" in elder_summary
        assert elder_summary["monitoring_authority"] == monitor.monitor_id
    
    def test_monitoring_loop_start_stop(self, monitor):
        """ğŸ”´ Red: ç›£è¦–ãƒ«ãƒ¼ãƒ—é–‹å§‹ãƒ»åœæ­¢ãƒ†ã‚¹ãƒˆ"""
        # åˆæœŸçŠ¶æ…‹
        assert not monitor._monitoring_active
        assert monitor._monitoring_thread is None
        
        # ç›£è¦–é–‹å§‹
        monitor.start_monitoring()
        assert monitor._monitoring_active is True
        assert monitor._monitoring_thread is not None
        
        # çŸ­æ™‚é–“å¾…æ©Ÿ
        time.sleep(0.2)
        
        # ç›£è¦–åœæ­¢
        monitor.stop_monitoring()
        assert monitor._monitoring_active is False
    
    def test_monitoring_statistics(self, monitor, mock_pipeline_result):
        """ğŸ”´ Red: ç›£è¦–çµ±è¨ˆãƒ†ã‚¹ãƒˆ"""
        # åˆæœŸçµ±è¨ˆ
        stats = monitor.get_monitoring_statistics()
        assert isinstance(stats, dict)
        assert stats["monitor_id"] == "QMM-ELDER-001"
        assert stats["monitoring_active"] is False
        assert stats["total_metrics_recorded"] == 0
        assert stats["active_alerts_count"] == 0
        assert stats["resolved_alerts_count"] == 0
        
        # ãƒ‡ãƒ¼ã‚¿è¿½åŠ å¾Œ
        monitor.record_pipeline_execution(mock_pipeline_result, 15.0, "STATS-TEST")
        
        stats = monitor.get_monitoring_statistics()
        assert stats["total_metrics_recorded"] > 0
        assert "elder_council_reporting" in stats
        assert "last_elder_report" in stats
    
    def test_recommended_actions_generation(self, monitor):
        """ğŸ”´ Red: æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        # ã‚¢ãƒ©ãƒ¼ãƒˆä½œæˆ
        quality_alert = QualityAlert(
            alert_id="TEST-ALERT-001",
            alert_level=AlertLevel.CRITICAL,
            metric_type=MetricType.QUALITY_SCORE,
            message="å“è³ªã‚¹ã‚³ã‚¢ä½ä¸‹",
            current_value=70.0,
            threshold_value=75.0,
            pipeline_id="TEST-PIPELINE",
            timestamp=datetime.now()
        )
        
        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å–å¾—
        actions = monitor._get_recommended_actions(quality_alert)
        
        assert isinstance(actions, list)
        assert len(actions) > 0
        
        # å“è³ªã‚¹ã‚³ã‚¢é–¢é€£ã®æ¨å¥¨äº‹é …ç¢ºèª
        assert any("å“è³ªåŸºæº–" in action for action in actions)
        
        # å®Ÿè¡Œæ™‚é–“ã‚¢ãƒ©ãƒ¼ãƒˆã®å ´åˆ
        execution_alert = QualityAlert(
            alert_id="TEST-ALERT-002",
            alert_level=AlertLevel.WARNING,
            metric_type=MetricType.EXECUTION_TIME,
            message="å®Ÿè¡Œæ™‚é–“è¶…é",
            current_value=25.0,
            threshold_value=20.0,
            pipeline_id="TEST-PIPELINE",
            timestamp=datetime.now()
        )
        
        execution_actions = monitor._get_recommended_actions(execution_alert)
        assert any("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹" in action for action in execution_actions)
    
    def test_alert_auto_resolution_logic(self, monitor):
        """ğŸ”´ Red: ã‚¢ãƒ©ãƒ¼ãƒˆè‡ªå‹•è§£æ±ºãƒ­ã‚¸ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
        # ã‚¢ãƒ©ãƒ¼ãƒˆä½œæˆï¼ˆéå»ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰
        old_alert = QualityAlert(
            alert_id="OLD-ALERT",
            alert_level=AlertLevel.WARNING,
            metric_type=MetricType.QUALITY_SCORE,
            message="å¤ã„ã‚¢ãƒ©ãƒ¼ãƒˆ",
            current_value=80.0,
            threshold_value=85.0,
            pipeline_id="TEST",
            timestamp=datetime.now() - timedelta(hours=3)  # 3æ™‚é–“å‰
        )
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆã«è¿½åŠ 
        alert_key = f"{old_alert.metric_type.value}-{old_alert.alert_level.value}"
        monitor.active_alerts[alert_key] = old_alert
        
        # æœ€æ–°ã®æ”¹å–„ã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ 
        for i in range(5):
            improved_metric = QualityMetric(
                metric_type=MetricType.QUALITY_SCORE,
                value=90.0 + i,  # é—¾å€¤ã‚’ä¸Šå›ã‚‹æ”¹å–„å€¤
                timestamp=datetime.now(),
                pipeline_id="TEST"
            )
            monitor.metrics_history[MetricType.QUALITY_SCORE].append(improved_metric)
        
        # è§£æ±ºåˆ¤å®šãƒ†ã‚¹ãƒˆ
        recent_metrics = list(monitor.metrics_history[MetricType.QUALITY_SCORE])[-5:]
        is_resolved = monitor._is_alert_resolved(old_alert, recent_metrics)
        
        assert is_resolved is True  # æ”¹å–„ã•ã‚ŒãŸã®ã§è§£æ±ºã•ã‚Œã‚‹ã¹ã
    
    def test_quality_metric_dataclass(self):
        """ğŸ”´ Red: QualityMetricãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ãƒ†ã‚¹ãƒˆ"""
        timestamp = datetime.now()
        metric = QualityMetric(
            metric_type=MetricType.QUALITY_SCORE,
            value=92.5,
            timestamp=timestamp,
            pipeline_id="TEST-PIPELINE-001",
            additional_data={"status": "ELDER_APPROVED"}
        )
        
        assert metric.metric_type == MetricType.QUALITY_SCORE
        assert metric.value == 92.5
        assert metric.timestamp == timestamp
        assert metric.pipeline_id == "TEST-PIPELINE-001"
        assert metric.additional_data["status"] == "ELDER_APPROVED"
    
    def test_quality_alert_dataclass(self):
        """ğŸ”´ Red: QualityAlertãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ãƒ†ã‚¹ãƒˆ"""
        timestamp = datetime.now()
        alert = QualityAlert(
            alert_id="ALERT-TEST-001",
            alert_level=AlertLevel.WARNING,
            metric_type=MetricType.EXECUTION_TIME,
            message="å®Ÿè¡Œæ™‚é–“ãŒé—¾å€¤ã‚’è¶…éã—ã¾ã—ãŸ",
            current_value=25.0,
            threshold_value=20.0,
            pipeline_id="TEST-PIPELINE",
            timestamp=timestamp
        )
        
        assert alert.alert_id == "ALERT-TEST-001"
        assert alert.alert_level == AlertLevel.WARNING
        assert alert.metric_type == MetricType.EXECUTION_TIME
        assert alert.current_value == 25.0
        assert alert.threshold_value == 20.0
        assert alert.resolved is False  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        assert alert.elder_council_notified is False  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def test_monitoring_report_dataclass(self):
        """ğŸ”´ Red: MonitoringReportãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ãƒ†ã‚¹ãƒˆ"""
        start_time = datetime.now() - timedelta(hours=24)
        end_time = datetime.now()
        timestamp = datetime.now()
        
        report = MonitoringReport(
            report_id="QMR-TEST-001",
            period_start=start_time,
            period_end=end_time,
            total_executions=100,
            successful_executions=95,
            average_quality_score=91.5,
            elder_approval_rate=95.0,
            average_execution_time=18.5,
            pipeline_efficiency_trend="IMPROVING",
            active_alerts=[],
            resolved_alerts=[],
            recommendations=["Continue current quality practices"],
            elder_council_summary={"health": "GOOD"},
            timestamp=timestamp
        )
        
        assert report.report_id == "QMR-TEST-001"
        assert report.total_executions == 100
        assert report.successful_executions == 95
        assert report.elder_approval_rate == 95.0
        assert report.pipeline_efficiency_trend == "IMPROVING"
        assert len(report.recommendations) == 1
        assert report.elder_council_summary["health"] == "GOOD"


# Integration tests
class TestQualityMetricsMonitorIntegration:
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    @pytest.mark.integration
    def test_real_pipeline_monitoring_integration(self):
        """ğŸ”´ Red: å®Ÿéš›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç›£è¦–çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆã‚¹ã‚­ãƒƒãƒ—å¯èƒ½ï¼‰"""
        pytest.skip("å®Ÿè£…å®Œäº†å¾Œã«æœ‰åŠ¹åŒ–")
        
        # Real integration with UnifiedQualityPipeline
        # Will be enabled after implementation
    
    @pytest.mark.performance
    def test_monitoring_performance_benchmarks(self):
        """ğŸ”´ Red: ç›£è¦–æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""
        pytest.skip("å®Ÿè£…å®Œäº†å¾Œã«æœ‰åŠ¹åŒ–")
        
        # Performance benchmarking will be added after implementation


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
