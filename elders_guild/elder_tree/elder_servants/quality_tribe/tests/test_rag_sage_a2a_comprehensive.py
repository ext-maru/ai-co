#!/usr/bin/env python3
"""
"ğŸ”" RAG Sage A2A Agent - åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
========================================

Elder Loop Phase 4: å³å¯†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ä¸¦è¡Œæ€§ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»çµ±åˆãƒ†ã‚¹ãƒˆ

Author: Claude Elder
Created: 2025-07-23
"""

import asyncio
import json
import time
import logging
from datetime import datetime
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor
import threading
import random
import gc
import pytest
import tempfile
import os

# ç’°å¢ƒå¤‰æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ã‚¹ã‚’è¨­å®š
import sys
import os
# shared_libs.configã‹ã‚‰ELDERS_GUILD_HOMEã‚’å–å¾—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from shared_libs.config import config
sys.path.insert(0, config.ELDERS_GUILD_HOME)
from rag_sage.business_logic import RAGProcessor


class TestRAGSageA2AComprehensive:
    """RAG Sage A2A AgentåŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """ãƒ†ã‚¹ãƒˆç”¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.performance_metrics = {}
        self.logger = logging.getLogger("rag_sage_comprehensive_test")
        yield
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    
    @pytest.mark.asyncio
    async def test_performance(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        # å¤§é‡æ–‡æ›¸ã®å‡¦ç†æ€§èƒ½ãƒ†ã‚¹ãƒˆ
        test_documents = []
        for i in range(100):
            document = {
                "title": f"Test Document {i}",
                "content": f"This is test content for document {i}. It contains information about topic {i % 10}.",
                "category": ["engineering", "science", "business"][i % 3],
                "source": f"test_source_{i}",
                "vector_id": f"vec_{i}"
            }
            test_documents.append(document)
        
        # æ–‡æ›¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆæ™‚é–“æ¸¬å®š
        start_time = time.time()
        
        for doc in test_documents:
            result = await processor.process_action("add_document", {"document": doc})
            assert result.get("success"), f"æ–‡æ›¸è¿½åŠ å¤±æ•—: {doc['title']}"
        
        index_time = time.time() - start_time
        
        # æ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        search_queries = [
            "engineering topic",
            "science information", 
            "business document",
            "test content",
            "topic 5"
        ]
        
        search_start = time.time()
        search_results = []
        
        for query in search_queries:
            result = await processor.process_action("search", {"query": query})
            search_results.append(result)
        
        search_time = time.time() - search_start
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        avg_index_time = index_time / len(test_documents)
        avg_search_time = search_time / len(search_queries)
        
        self.performance_metrics["indexing"] = {
            "total_documents": len(test_documents),
            "total_time": index_time,
            "avg_time_per_doc": avg_index_time,
            "docs_per_second": len(test_documents) / index_time
        }
        
        self.performance_metrics["searching"] = {
            "total_queries": len(search_queries),
            "total_time": search_time,
            "avg_time_per_query": avg_search_time,
            "queries_per_second": len(search_queries) / search_time
        }
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–ç¢ºèª
        assert avg_index_time < 0.1, f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ™‚é–“ãŒé•·ã„: {avg_index_time:.3f}s/doc"
        assert avg_search_time < 0.5, f"æ¤œç´¢æ™‚é–“ãŒé•·ã„: {avg_search_time:.3f}s/query"
        
        print(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(test_documents)/index_time:.1f} docs/sec, {len(search_queries)/search_time:.1f} queries/sec")
    
    @pytest.mark.asyncio
    async def test_concurrency(self):
        """ä¸¦è¡Œæ€§ãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        # ä¸¦è¡Œæ¤œç´¢ã‚¿ã‚¹ã‚¯å®šç¾©
        async def concurrent_search(query_id):
            query = f"concurrent test query {query_id % 5}"
            return await processor.process_action("search", {"query": query})
        
        # ä¸¦è¡Œæ–‡æ›¸è¿½åŠ ã‚¿ã‚¹ã‚¯
        async def concurrent_add_document(doc_id):
            document = {
                "title": f"Concurrent Document {doc_id}",
                "content": f"Concurrent content for testing parallel processing {doc_id}",
                "category": "concurrent_test",
                "source": f"concurrent_source_{doc_id}"
            }
            return await processor.process_action("add_document", {"document": document})
        
        # ä¸¦è¡Œå®Ÿè¡Œ
        start_time = time.time()
        
        # æ–‡æ›¸è¿½åŠ ã¨æ¤œç´¢ã‚’åŒæ™‚å®Ÿè¡Œ
        add_tasks = [concurrent_add_document(i) for i in range(20)]
        search_tasks = [concurrent_search(i) for i in range(30)]
        
        all_tasks = add_tasks + search_tasks
        results = await asyncio.gather(*all_tasks, return_exceptions=True)
        
        end_time = time.time()
        
        # çµæœåˆ†æ
        add_results = results[:20]
        search_results = results[20:]
        
        successful_adds = [r for r in add_results if not isinstance(r, Exception) and r.get("success")]
        successful_searches = [r for r in search_results if not isinstance(r, Exception) and r.get("success")]
        
        concurrent_time = end_time - start_time
        
        self.performance_metrics["concurrency"] = {
            "total_operations": len(all_tasks),
            "successful_adds": len(successful_adds),
            "successful_searches": len(successful_searches),
            "execution_time": concurrent_time,
            "operations_per_second": len(all_tasks) / concurrent_time
        }
        
        # ä¸¦è¡Œæ€§åŸºæº–ç¢ºèª
        assert len(successful_adds) >= 18, f"ä¸¦è¡Œæ–‡æ›¸è¿½åŠ å¤±æ•—ãŒå¤šã„: {20 - len(successful_adds)} å¤±æ•—"
        assert len(successful_searches) >= 25, f"ä¸¦è¡Œæ¤œç´¢å¤±æ•—ãŒå¤šã„: {30 - len(successful_searches)} å¤±æ•—"
        assert concurrent_time < 10.0, f"ä¸¦è¡Œå®Ÿè¡Œæ™‚é–“ãŒé•·ã„: {concurrent_time:.3f}s"
        
        print(f"âœ… ä¸¦è¡Œæ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(successful_adds)+len(successful_searches)}/{len(all_tasks)} æˆåŠŸ, {concurrent_time:.3f}s")
    
    @pytest.mark.asyncio
    async def test_error_handling(self):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        # ã‚¨ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        error_test_cases = [
            {
                "name": "ç©ºãƒ‡ãƒ¼ã‚¿æ–‡æ›¸è¿½åŠ ",
                "action": "add_document",
                "data": {"document": {}}
            },
            {
                "name": "ç„¡åŠ¹ãªæ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿",
                "action": "search",
                "data": {"query": "", "max_results": -1}
            },
            {
                "name": "å­˜åœ¨ã—ãªã„æ–‡æ›¸æ›´æ–°",
                "action": "update_document",
                "data": {"document_id": "non_existent_id", "updates": {"title": "New Title"}}
            },
            {
                "name": "ç„¡åŠ¹ãªãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢",
                "action": "vector_search",
                "data": {"vector": "not_a_vector", "k": "not_a_number"}
            },
            {
                "name": "å·¨å¤§ã‚¯ã‚¨ãƒª",
                "action": "search",
                "data": {"query": "x" * 10000}
            },
            {
                "name": "ç„¡åŠ¹ãªãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶",
                "action": "search_with_filters",
                "data": {"query": "test", "filters": {"invalid_field": None}}
            },
            {
                "name": "é‡è¤‡æ–‡æ›¸è¿½åŠ ",
                "action": "add_document",
                "data": {"document": {"document_id": "duplicate_id", "title": "Duplicate", "content": "Test"}}
            }
        ]
        
        # é‡è¤‡ãƒ†ã‚¹ãƒˆç”¨ã«æ–‡æ›¸ã‚’äº‹å‰è¿½åŠ 
        await processor.process_action("add_document", {
            "document": {"document_id": "duplicate_id", "title": "Original", "content": "Original content"}
        })
        
        error_handling_results = []
        
        for test_case in error_test_cases:
            try:
                result = await processor.process_action(test_case["action"], test_case["data"])
                
                if result.get("success"):
                    error_handling_results.append({
                        "case": test_case["name"],
                        "status": "unexpected_success",
                        "result": result
                    })
                else:
                    if "error" in result and isinstance(result["error"], str):
                        error_handling_results.append({
                            "case": test_case["name"],
                            "status": "properly_handled",
                            "error": result["error"]
                        })
                    else:
                        error_handling_results.append({
                            "case": test_case["name"],
                            "status": "improper_error_format",
                            "result": result
                        })
            
            except Exception as e:
                error_handling_results.append({
                    "case": test_case["name"],
                    "status": "unhandled_exception",
                    "exception": str(e)
                })
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è©•ä¾¡
        properly_handled = len([r for r in error_handling_results if r["status"] == "properly_handled"])
        total_cases = len(error_test_cases)
        
        assert properly_handled >= total_cases * 0.8, f"ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸ååˆ†: {properly_handled}/{total_cases}"
        
        print(f"âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆåŠŸ: {properly_handled}/{total_cases} é©åˆ‡å‡¦ç†")
    
    @pytest.mark.asyncio
    async def test_data_integrity(self):
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        # ãƒ†ã‚¹ãƒˆç”¨æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿
        original_document = {
            "document_id": "integrity_test_doc",
            "title": "Data Integrity Test Document",
            "content": "This is a test document for verifying data integrity in RAG system.",
            "metadata": {
                "author": "Test Author",
                "created_at": "2025-07-23",
                "tags": ["test", "integrity", "rag"],
                "version": 1.0
            },
            "category": "testing",
            "source": "test_suite"
        }
        
        # 1. æ–‡æ›¸è¿½åŠ 
        add_result = await processor.process_action("add_document", {"document": original_document})
        assert add_result.get("success"), "æ–‡æ›¸è¿½åŠ å¤±æ•—"
        
        doc_id = add_result["data"]["document_id"]
        
        # 2. æ–‡æ›¸å–å¾—ã—ã¦æ•´åˆæ€§ç¢ºèª
        get_result = await processor.process_action("get_document", {"document_id": doc_id})
        assert get_result.get("success"), "æ–‡æ›¸å–å¾—å¤±æ•—"
        
        retrieved_doc = get_result["data"]["document"]
        
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        assert retrieved_doc["title"] == original_document["title"], "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ•´åˆ"
        assert retrieved_doc["content"] == original_document["content"], "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä¸æ•´åˆ"
        assert retrieved_doc["category"] == original_document["category"], "ã‚«ãƒ†ã‚´ãƒªä¸æ•´åˆ"
        assert retrieved_doc["metadata"]["tags"] == original_document["metadata"]["tags"], "ã‚¿ã‚°ä¸æ•´åˆ"
        
        # 3. æ–‡æ›¸æ›´æ–°
        updates = {
            "title": "Updated Title",
            "metadata": {
                "version": 2.0,
                "updated_at": "2025-07-23"
            }
        }
        
        update_result = await processor.process_action("update_document", {
            "document_id": doc_id,
            "updates": updates
        })
        assert update_result.get("success"), "æ–‡æ›¸æ›´æ–°å¤±æ•—"
        
        # 4. æ›´æ–°å¾Œã®æ•´åˆæ€§ç¢ºèª
        get_updated_result = await processor.process_action("get_document", {"document_id": doc_id})
        assert get_updated_result.get("success"), "æ›´æ–°å¾Œæ–‡æ›¸å–å¾—å¤±æ•—"
        
        updated_doc = get_updated_result["data"]["document"]
        
        assert updated_doc["title"] == updates["title"], "æ›´æ–°å¾Œã‚¿ã‚¤ãƒˆãƒ«ä¸æ•´åˆ"
        assert updated_doc["metadata"]["version"] == 2.0, "ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ•´åˆ"
        assert updated_doc["content"] == original_document["content"], "æœªæ›´æ–°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å¤‰æ›´"
        
        # 5. æ¤œç´¢ã§ã®æ•´åˆæ€§ç¢ºèª
        search_result = await processor.process_action("search", {"query": "Updated Title"})
        assert search_result.get("success"), "æ¤œç´¢å¤±æ•—"
        
        search_docs = search_result["data"]["documents"]
        found_doc = next((d for d in search_docs if d["document_id"] == doc_id), None)
        assert found_doc is not None, "æ›´æ–°å¾Œæ–‡æ›¸ãŒæ¤œç´¢çµæœã«å«ã¾ã‚Œãªã„"
        
        print("âœ… ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ: è¿½åŠ ãƒ»å–å¾—ãƒ»æ›´æ–°ãƒ»æ¤œç´¢ã®æ•´åˆæ€§ç¢ºèªå®Œäº†")
    
    @pytest.mark.asyncio
    async def test_complex_queries(self):
        """è¤‡é›‘ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®å¤šæ§˜ãªæ–‡æ›¸ã‚’æº–å‚™
        test_corpus = [
            {
                "title": "Machine Learning Fundamentals",
                "content": "Machine learning is a subset of artificial intelligence that enables systems to learn from data.",
                "category": "technology",
                "tags": ["ml", "ai", "fundamentals"]
            },
            {
                "title": "Deep Learning Advanced Techniques",
                "content": "Deep learning uses neural networks with multiple layers to process complex patterns in data.",
                "category": "technology",
                "tags": ["dl", "neural-networks", "advanced"]
            },
            {
                "title": "Natural Language Processing",
                "content": "NLP enables computers to understand, interpret, and generate human language.",
                "category": "technology",
                "tags": ["nlp", "language", "ai"]
            },
            {
                "title": "Business Applications of AI",
                "content": "AI transforms business operations through automation, prediction, and optimization.",
                "category": "business",
                "tags": ["business", "ai", "applications"]
            },
            {
                "title": "Healthcare AI Solutions",
                "content": "AI in healthcare improves diagnosis, treatment planning, and patient care.",
                "category": "healthcare",
                "tags": ["healthcare", "ai", "medical"]
            }
        ]
        
        # æ–‡æ›¸ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
        for doc in test_corpus:
            await processor.process_action("add_document", {"document": doc})
        
        # è¤‡é›‘ãªã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        complex_queries = [
            {
                "name": "ãƒãƒ«ãƒãƒ¯ãƒ¼ãƒ‰æ¤œç´¢",
                "query": "machine learning neural networks",
                "expected_min_results": 2
            },
            {
                "name": "ãƒ•ãƒ¬ãƒ¼ã‚ºæ¤œç´¢",
                "query": '"artificial intelligence"',
                "expected_min_results": 1
            },
            {
                "name": "ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ä»˜ãæ¤œç´¢",
                "action": "search_with_filters",
                "query": "AI",
                "filters": {"category": "technology"},
                "expected_min_results": 2
            },
            {
                "name": "ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹æ¤œç´¢",
                "action": "search_with_filters",
                "query": "learning",
                "filters": {"tags": ["ai"]},
                "expected_min_results": 1
            },
            {
                "name": "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢",
                "query": "intelligent systems that learn",
                "expected_min_results": 2
            }
        ]
        
        query_results = []
        
        for test_case in complex_queries:
            action = test_case.get("action", "search")
            
            if action == "search":
                result = await processor.process_action(action, {"query": test_case["query"]})
            else:
                result = await processor.process_action(action, {
                    "query": test_case["query"],
                    "filters": test_case.get("filters", {})
                })
            
            if result.get("success"):
                docs_found = len(result["data"]["documents"])
                query_results.append({
                    "name": test_case["name"],
                    "docs_found": docs_found,
                    "meets_expectation": docs_found >= test_case["expected_min_results"]
                })
        
        # çµæœè©•ä¾¡
        successful_queries = [r for r in query_results if r["meets_expectation"]]
        success_rate = len(successful_queries) / len(query_results) * 100
        
        assert success_rate >= 80, f"è¤‡é›‘ã‚¯ã‚¨ãƒªæˆåŠŸç‡ãŒä½ã„: {success_rate:.1f}%"
        
        print(f"âœ… è¤‡é›‘ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(successful_queries)}/{len(query_results)} ã‚¯ã‚¨ãƒªæˆåŠŸ")
    
    @pytest.mark.asyncio
    async def test_memory_efficiency(self):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            import psutil
            
            # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            process = psutil.Process(os.getpid())
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            processor = RAGProcessor()
            
            # å¤§é‡æ–‡æ›¸å‡¦ç†
            batch_size = 100
            num_batches = 5
            
            for batch in range(num_batches):
                batch_docs = []
                for i in range(batch_size):
                    doc_id = batch * batch_size + i
                    document = {
                        "title": f"Memory Test Document {doc_id}",
                        "content": f"This is content for memory efficiency testing. " * 50,  # é•·ã‚ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
                        "metadata": {
                            "batch": batch,
                            "index": i,
                            "timestamp": datetime.now().isoformat()
                        }
                    }
                    batch_docs.append(document)
                
                # ãƒãƒƒãƒå‡¦ç†
                for doc in batch_docs:
                    await processor.process_action("add_document", {"document": doc})
                
                # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¿ƒé€²
                if batch % 2 == 0:
                    gc.collect()
            
            # æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = final_memory - initial_memory
            
            total_docs = batch_size * num_batches
            memory_per_doc = memory_increase / total_docs * 1024  # KB per document
            
            self.performance_metrics["memory_efficiency"] = {
                "initial_memory_mb": initial_memory,
                "final_memory_mb": final_memory,
                "memory_increase_mb": memory_increase,
                "total_documents": total_docs,
                "memory_per_doc_kb": memory_per_doc
            }
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§åŸºæº–
            assert memory_increase < 200, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ ãŒå¤§ãã„: {memory_increase:.1f}MB"
            assert memory_per_doc < 50, f"æ–‡æ›¸ã‚ãŸã‚Šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤§ãã„: {memory_per_doc:.1f}KB/doc"
            
            print(f"âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ: +{memory_increase:.1f}MB, {memory_per_doc:.1f}KB/doc")
            
        except ImportError:
            pytest.skip("psutilæœªåˆ©ç”¨å¯èƒ½ã€ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆã‚¹ã‚­ãƒƒãƒ—")
    
    @pytest.mark.asyncio
    async def test_search_accuracy(self):
        """æ¤œç´¢ç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        # ç²¾åº¦ãƒ†ã‚¹ãƒˆç”¨ã®æ–‡æ›¸ã‚»ãƒƒãƒˆ
        test_documents = [
            {
                "document_id": "doc1",
                "title": "Python Programming Guide",
                "content": "Python is a high-level programming language known for its simplicity and readability.",
                "category": "programming"
            },
            {
                "document_id": "doc2",
                "title": "Java Development Best Practices",
                "content": "Java is an object-oriented programming language widely used for enterprise applications.",
                "category": "programming"
            },
            {
                "document_id": "doc3",
                "title": "Machine Learning with Python",
                "content": "Python provides excellent libraries for machine learning such as scikit-learn and TensorFlow.",
                "category": "ml"
            },
            {
                "document_id": "doc4",
                "title": "Data Science Fundamentals",
                "content": "Data science combines statistics, programming, and domain knowledge to extract insights.",
                "category": "data-science"
            },
            {
                "document_id": "doc5",
                "title": "Web Development with JavaScript",
                "content": "JavaScript is essential for modern web development, both frontend and backend.",
                "category": "web"
            }
        ]
        
        # æ–‡æ›¸ã‚’è¿½åŠ 
        for doc in test_documents:
            await processor.process_action("add_document", {"document": doc})
        
        # æ¤œç´¢ç²¾åº¦ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        accuracy_tests = [
            {
                "query": "Python programming",
                "expected_docs": ["doc1", "doc3"],
                "must_include": ["doc1"]
            },
            {
                "query": "machine learning libraries",
                "expected_docs": ["doc3"],
                "must_include": ["doc3"]
            },
            {
                "query": "programming language",
                "expected_docs": ["doc1", "doc2", "doc5"],
                "must_include": ["doc1", "doc2"]
            },
            {
                "query": "data analysis",
                "expected_docs": ["doc4", "doc3"],
                "must_include": ["doc4"]
            }
        ]
        
        accuracy_results = []
        
        for test in accuracy_tests:
            result = await processor.process_action("search", {
                "query": test["query"],
                "max_results": 5
            })
            
            if result.get("success"):
                found_ids = [doc["document_id"] for doc in result["data"]["documents"]]
                
                # å¿…é ˆæ–‡æ›¸ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                must_include_found = all(doc_id in found_ids for doc_id in test["must_include"])
                
                # æœŸå¾…ã•ã‚Œã‚‹æ–‡æ›¸ã¨ã®é‡è¤‡ç‡
                expected_set = set(test["expected_docs"])
                found_set = set(found_ids[:len(test["expected_docs"])])  # ä¸Šä½Nä»¶ã§è©•ä¾¡
                overlap = len(expected_set & found_set) / len(expected_set) if expected_set else 0
                
                accuracy_results.append({
                    "query": test["query"],
                    "must_include_found": must_include_found,
                    "overlap_ratio": overlap,
                    "found_docs": found_ids
                })
        
        # ç²¾åº¦è©•ä¾¡
        must_include_success = sum(1 for r in accuracy_results if r["must_include_found"])
        avg_overlap = sum(r["overlap_ratio"] for r in accuracy_results) / len(accuracy_results)
        
        self.performance_metrics["search_accuracy"] = {
            "total_tests": len(accuracy_tests),
            "must_include_success": must_include_success,
            "average_overlap_ratio": avg_overlap
        }
        
        assert must_include_success == len(accuracy_tests), f"å¿…é ˆæ–‡æ›¸ã®æ¤œç´¢æ¼ã‚Œ: {must_include_success}/{len(accuracy_tests)}"
        assert avg_overlap >= 0.7, f"æ¤œç´¢ç²¾åº¦ãŒä½ã„: {avg_overlap:.2f}"
        
        print(f"âœ… æ¤œç´¢ç²¾åº¦ãƒ†ã‚¹ãƒˆæˆåŠŸ: å¿…é ˆæ–‡æ›¸ {must_include_success}/{len(accuracy_tests)}, å¹³å‡é‡è¤‡ç‡ {avg_overlap:.2f}")
    
    @pytest.mark.asyncio
    async def test_vector_operations(self):
        """ãƒ™ã‚¯ãƒˆãƒ«æ“ä½œãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç”¨ã®æ–‡æ›¸æº–å‚™
        vector_docs = [
            {
                "document_id": "vec1",
                "title": "Artificial Intelligence Overview",
                "content": "AI encompasses machine learning, deep learning, and other computational intelligence techniques.",
                "vector": [0.1, 0.2, 0.3, 0.4, 0.5]  # ç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«
            },
            {
                "document_id": "vec2",
                "title": "Machine Learning Applications",
                "content": "ML is applied in various domains including computer vision, NLP, and recommendation systems.",
                "vector": [0.2, 0.3, 0.4, 0.5, 0.6]
            },
            {
                "document_id": "vec3",
                "title": "Deep Learning Neural Networks",
                "content": "Deep neural networks have revolutionized pattern recognition and data analysis.",
                "vector": [0.3, 0.4, 0.5, 0.6, 0.7]
            }
        ]
        
        # ãƒ™ã‚¯ãƒˆãƒ«ä»˜ãæ–‡æ›¸ã‚’è¿½åŠ 
        for doc in vector_docs:
            result = await processor.process_action("add_document_with_vector", {"document": doc})
            assert result.get("success"), f"ãƒ™ã‚¯ãƒˆãƒ«æ–‡æ›¸è¿½åŠ å¤±æ•—: {doc['document_id']}"
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        query_vector = [0.25, 0.35, 0.45, 0.55, 0.65]
        
        vector_search_result = await processor.process_action("vector_search", {
            "vector": query_vector,
            "k": 2
        })
        
        assert vector_search_result.get("success"), "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å¤±æ•—"
        
        results = vector_search_result["data"]["documents"]
        assert len(results) == 2, f"æœŸå¾…ã•ã‚Œã‚‹çµæœæ•°ã¨ç•°ãªã‚‹: {len(results)}"
        
        # æœ€ã‚‚è¿‘ã„æ–‡æ›¸ãŒæ­£ã—ãå–å¾—ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        # vec2ã¨vec3ãŒæœ€ã‚‚è¿‘ã„ã¯ãš
        found_ids = [doc["document_id"] for doc in results]
        assert "vec2" in found_ids or "vec3" in found_ids, "æœŸå¾…ã•ã‚Œã‚‹è¿‘å‚æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚‰ãªã„"
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆ + ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
        hybrid_result = await processor.process_action("hybrid_search", {
            "query": "neural networks",
            "vector": query_vector,
            "text_weight": 0.5,
            "vector_weight": 0.5
        })
        
        assert hybrid_result.get("success"), "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å¤±æ•—"
        
        print("âœ… ãƒ™ã‚¯ãƒˆãƒ«æ“ä½œãƒ†ã‚¹ãƒˆæˆåŠŸ: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ»ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢æ­£å¸¸å‹•ä½œ")
    
    @pytest.mark.asyncio
    async def test_batch_operations(self):
        """ãƒãƒƒãƒæ“ä½œãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        # ãƒãƒƒãƒè¿½åŠ ãƒ†ã‚¹ãƒˆ
        batch_documents = []
        for i in range(50):
            batch_documents.append({
                "title": f"Batch Document {i}",
                "content": f"Content for batch processing test document {i}",
                "category": f"batch_{i % 5}"
            })
        
        # ãƒãƒƒãƒè¿½åŠ å®Ÿè¡Œ
        batch_add_result = await processor.process_action("batch_add_documents", {
            "documents": batch_documents
        })
        
        assert batch_add_result.get("success"), "ãƒãƒƒãƒè¿½åŠ å¤±æ•—"
        
        batch_data = batch_add_result["data"]
        assert batch_data["total_processed"] == len(batch_documents), "å‡¦ç†æ•°ãŒä¸€è‡´ã—ãªã„"
        assert batch_data["successful"] >= len(batch_documents) * 0.95, "æˆåŠŸç‡ãŒä½ã„"
        
        # ãƒãƒƒãƒæ›´æ–°ãƒ†ã‚¹ãƒˆ
        batch_updates = []
        for i in range(10):
            if i < len(batch_data.get("document_ids", [])):
                batch_updates.append({
                    "document_id": batch_data["document_ids"][i],
                    "updates": {"title": f"Updated Batch Document {i}"}
                })
        
        if batch_updates:
            batch_update_result = await processor.process_action("batch_update_documents", {
                "updates": batch_updates
            })
            
            assert batch_update_result.get("success"), "ãƒãƒƒãƒæ›´æ–°å¤±æ•—"
        
        # ãƒãƒƒãƒå‰Šé™¤ãƒ†ã‚¹ãƒˆ
        delete_ids = batch_data.get("document_ids", [])[:5] if "document_ids" in batch_data else []
        
        if delete_ids:
            batch_delete_result = await processor.process_action("batch_delete_documents", {
                "document_ids": delete_ids
            })
            
            assert batch_delete_result.get("success"), "ãƒãƒƒãƒå‰Šé™¤å¤±æ•—"
            assert batch_delete_result["data"]["deleted_count"] == len(delete_ids), "å‰Šé™¤æ•°ãŒä¸€è‡´ã—ãªã„"
        
        print("âœ… ãƒãƒƒãƒæ“ä½œãƒ†ã‚¹ãƒˆæˆåŠŸ: è¿½åŠ ãƒ»æ›´æ–°ãƒ»å‰Šé™¤ã®ãƒãƒƒãƒå‡¦ç†ç¢ºèª")
    
    @pytest.mark.asyncio
    async def test_caching_performance(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        # ãƒ†ã‚¹ãƒˆç”¨æ–‡æ›¸ã‚’è¿½åŠ 
        test_doc = {
            "document_id": "cache_test",
            "title": "Caching Test Document",
            "content": "This document is used to test the caching performance of the RAG system."
        }
        
        await processor.process_action("add_document", {"document": test_doc})
        
        # åŒã˜ã‚¯ã‚¨ãƒªã‚’è¤‡æ•°å›å®Ÿè¡Œã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœã‚’æ¸¬å®š
        test_query = "caching performance test"
        
        # åˆå›æ¤œç´¢ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
        start_time = time.time()
        first_result = await processor.process_action("search", {"query": test_query})
        first_search_time = time.time() - start_time
        
        assert first_result.get("success"), "åˆå›æ¤œç´¢å¤±æ•—"
        
        # 2å›ç›®æ¤œç´¢ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰
        start_time = time.time()
        second_result = await processor.process_action("search", {"query": test_query})
        second_search_time = time.time() - start_time
        
        assert second_result.get("success"), "2å›ç›®æ¤œç´¢å¤±æ•—"
        
        # çµæœãŒåŒã˜ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        first_docs = first_result["data"]["documents"]
        second_docs = second_result["data"]["documents"]
        
        assert len(first_docs) == len(second_docs), "ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµæœãŒç•°ãªã‚‹"
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã‚’ç¢ºèª
        speedup = first_search_time / second_search_time if second_search_time > 0 else float('inf')
        
        self.performance_metrics["caching"] = {
            "first_search_time": first_search_time,
            "cached_search_time": second_search_time,
            "speedup_factor": speedup
        }
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœã®åŸºæº–ï¼ˆå°‘ãªãã¨ã‚‚2å€ä»¥ä¸Šé«˜é€ŸåŒ–ï¼‰
        assert speedup >= 2.0, f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœãŒä¸ååˆ†: {speedup:.1f}å€"
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ãƒ†ã‚¹ãƒˆ
        await processor.process_action("clear_cache", {})
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å¾Œã®æ¤œç´¢
        start_time = time.time()
        third_result = await processor.process_action("search", {"query": test_query})
        third_search_time = time.time() - start_time
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å¾Œã¯åˆå›ã¨åŒç¨‹åº¦ã®æ™‚é–“ãŒã‹ã‹ã‚‹ã¯ãš
        assert third_search_time > second_search_time * 1.5, "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ãŒåŠ¹ã„ã¦ã„ãªã„"
        
        print(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ€§èƒ½ãƒ†ã‚¹ãƒˆæˆåŠŸ: {speedup:.1f}å€é«˜é€ŸåŒ–")
    
    @pytest.mark.asyncio
    async def test_advanced_filtering(self):
        """é«˜åº¦ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆç”¨ã®æ–‡æ›¸ã‚»ãƒƒãƒˆ
        filter_test_docs = [
            {
                "title": "Document A",
                "content": "Advanced filtering test document",
                "metadata": {
                    "date": "2025-07-20",
                    "author": "Alice",
                    "priority": 5,
                    "tags": ["important", "technical"]
                }
            },
            {
                "title": "Document B",
                "content": "Another test document for filtering",
                "metadata": {
                    "date": "2025-07-21",
                    "author": "Bob",
                    "priority": 3,
                    "tags": ["review", "draft"]
                }
            },
            {
                "title": "Document C",
                "content": "Third document with different attributes",
                "metadata": {
                    "date": "2025-07-22",
                    "author": "Alice",
                    "priority": 8,
                    "tags": ["important", "urgent"]
                }
            },
            {
                "title": "Document D",
                "content": "Final test document",
                "metadata": {
                    "date": "2025-07-23",
                    "author": "Charlie",
                    "priority": 2,
                    "tags": ["draft"]
                }
            }
        ]
        
        # æ–‡æ›¸ã‚’è¿½åŠ 
        for doc in filter_test_docs:
            await processor.process_action("add_document", {"document": doc})
        
        # è¤‡é›‘ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        filter_tests = [
            {
                "name": "è‘—è€…ãƒ•ã‚£ãƒ«ã‚¿",
                "filters": {"metadata.author": "Alice"},
                "expected_count": 2
            },
            {
                "name": "å„ªå…ˆåº¦ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿",
                "filters": {"metadata.priority": {"$gte": 5}},
                "expected_count": 2
            },
            {
                "name": "ã‚¿ã‚°å«æœ‰ãƒ•ã‚£ãƒ«ã‚¿",
                "filters": {"metadata.tags": {"$contains": "important"}},
                "expected_count": 2
            },
            {
                "name": "æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿",
                "filters": {
                    "metadata.date": {
                        "$gte": "2025-07-21",
                        "$lte": "2025-07-22"
                    }
                },
                "expected_count": 2
            },
            {
                "name": "è¤‡åˆæ¡ä»¶ãƒ•ã‚£ãƒ«ã‚¿",
                "filters": {
                    "$and": [
                        {"metadata.author": "Alice"},
                        {"metadata.priority": {"$gte": 5}}
                    ]
                },
                "expected_count": 2
            }
        ]
        
        filter_results = []
        
        for test in filter_tests:
            result = await processor.process_action("search_with_filters", {
                "query": "document",
                "filters": test["filters"]
            })
            
            if result.get("success"):
                found_count = len(result["data"]["documents"])
                filter_results.append({
                    "name": test["name"],
                    "found": found_count,
                    "expected": test["expected_count"],
                    "correct": found_count == test["expected_count"]
                })
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç²¾åº¦è©•ä¾¡
        correct_filters = sum(1 for r in filter_results if r["correct"])
        filter_accuracy = correct_filters / len(filter_results) * 100
        
        assert filter_accuracy >= 80, f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç²¾åº¦ãŒä½ã„: {filter_accuracy:.1f}%"
        
        print(f"âœ… é«˜åº¦ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆæˆåŠŸ: {correct_filters}/{len(filter_results)} ãƒ•ã‚£ãƒ«ã‚¿æ­£ç¢º")
    
    @pytest.mark.asyncio
    async def test_document_lifecycle(self):
        """æ–‡æ›¸ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        # ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã®ãƒ†ã‚¹ãƒˆ
        lifecycle_doc = {
            "document_id": "lifecycle_test",
            "title": "Document Lifecycle Test",
            "content": "This document will go through the complete lifecycle.",
            "metadata": {
                "version": 1,
                "status": "draft"
            }
        }
        
        # 1. ä½œæˆ
        create_result = await processor.process_action("add_document", {"document": lifecycle_doc})
        assert create_result.get("success"), "æ–‡æ›¸ä½œæˆå¤±æ•—"
        
        doc_id = create_result["data"]["document_id"]
        
        # 2. èª­ã¿å–ã‚Š
        read_result = await processor.process_action("get_document", {"document_id": doc_id})
        assert read_result.get("success"), "æ–‡æ›¸èª­ã¿å–ã‚Šå¤±æ•—"
        assert read_result["data"]["document"]["title"] == lifecycle_doc["title"], "èª­ã¿å–ã‚Šãƒ‡ãƒ¼ã‚¿ä¸ä¸€è‡´"
        
        # 3. æ›´æ–°ï¼ˆè¤‡æ•°å›ï¼‰
        updates_sequence = [
            {"metadata": {"version": 2, "status": "review"}},
            {"title": "Updated Document Lifecycle Test"},
            {"metadata": {"version": 3, "status": "published"}}
        ]
        
        for update in updates_sequence:
            update_result = await processor.process_action("update_document", {
                "document_id": doc_id,
                "updates": update
            })
            assert update_result.get("success"), "æ–‡æ›¸æ›´æ–°å¤±æ•—"
        
        # 4. æ›´æ–°å±¥æ­´ç¢ºèª
        history_result = await processor.process_action("get_document_history", {"document_id": doc_id})
        if history_result.get("success"):
            history = history_result["data"]["history"]
            assert len(history) >= len(updates_sequence), "æ›´æ–°å±¥æ­´ãŒä¸å®Œå…¨"
        
        # 5. æ¤œç´¢ç¢ºèª
        search_result = await processor.process_action("search", {"query": "Updated Document Lifecycle"})
        assert search_result.get("success"), "æ›´æ–°å¾Œæ¤œç´¢å¤±æ•—"
        
        found = any(doc["document_id"] == doc_id for doc in search_result["data"]["documents"])
        assert found, "æ›´æ–°å¾Œæ–‡æ›¸ãŒæ¤œç´¢ã•ã‚Œãªã„"
        
        # 6. å‰Šé™¤
        delete_result = await processor.process_action("delete_document", {"document_id": doc_id})
        assert delete_result.get("success"), "æ–‡æ›¸å‰Šé™¤å¤±æ•—"
        
        # 7. å‰Šé™¤å¾Œç¢ºèª
        get_deleted_result = await processor.process_action("get_document", {"document_id": doc_id})
        assert not get_deleted_result.get("success"), "å‰Šé™¤å¾Œã‚‚æ–‡æ›¸ãŒå–å¾—ã§ãã‚‹"
        
        print("âœ… æ–‡æ›¸ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒ†ã‚¹ãƒˆæˆåŠŸ: ä½œæˆâ†’èª­å–â†’æ›´æ–°â†’å‰Šé™¤ã®å®Œå…¨ã‚µã‚¤ã‚¯ãƒ«ç¢ºèª")
    
    @pytest.mark.asyncio
    async def test_stress_load(self):
        """ã‚¹ãƒˆãƒ¬ã‚¹è² è·ãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆè¨­å®š
        stress_config = {
            "num_documents": 200,
            "num_searches": 300,
            "num_updates": 100,
            "batch_size": 20
        }
        
        start_time = time.time()
        
        # 1. å¤§é‡æ–‡æ›¸è¿½åŠ 
        stress_docs = []
        for i in range(stress_config["num_documents"]):
            stress_docs.append({
                "title": f"Stress Test Document {i}",
                "content": f"Content for stress testing with index {i} and random data {random.random()}",
                "category": f"stress_cat_{i % 10}",
                "metadata": {
                    "index": i,
                    "batch": i // stress_config["batch_size"]
                }
            })
        
        # ãƒãƒƒãƒã§è¿½åŠ 
        for batch_start in range(0, len(stress_docs), stress_config["batch_size"]):
            batch_end = min(batch_start + stress_config["batch_size"], len(stress_docs))
            batch = stress_docs[batch_start:batch_end]
            
            result = await processor.process_action("batch_add_documents", {"documents": batch})
            assert result.get("success"), f"ãƒãƒƒãƒè¿½åŠ å¤±æ•—: ãƒãƒƒãƒ {batch_start // stress_config['batch_size']}"
        
        add_time = time.time() - start_time
        
        # 2. å¤§é‡æ¤œç´¢
        search_start = time.time()
        search_tasks = []
        
        for i in range(stress_config["num_searches"]):
            query = f"stress test {i % 50}"
            task = processor.process_action("search", {"query": query})
            search_tasks.append(task)
            
            # ä¸¦è¡Œå®Ÿè¡Œæ•°ã‚’åˆ¶é™
            if len(search_tasks) >= 10:
                await asyncio.gather(*search_tasks)
                search_tasks = []
        
        if search_tasks:
            await asyncio.gather(*search_tasks)
        
        search_time = time.time() - search_start
        
        # 3. ãƒ©ãƒ³ãƒ€ãƒ æ›´æ–°
        update_start = time.time()
        
        for i in range(stress_config["num_updates"]):
            # ãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡æ›¸ã‚’æ›´æ–°
            random_index = random.randint(0, stress_config["num_documents"] - 1)
            update_data = {
                "document_id": f"stress_doc_{random_index}",  # ä»®å®šã®IDå½¢å¼
                "updates": {
                    "metadata": {
                        "updated": True,
                        "update_count": i
                    }
                }
            }
            
            # ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦ç¶šè¡Œ
            await processor.process_action("update_document", update_data)
        
        update_time = time.time() - update_start
        
        total_time = time.time() - start_time
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.performance_metrics["stress_test"] = {
            "total_operations": stress_config["num_documents"] + stress_config["num_searches"] + stress_config["num_updates"],
            "total_time": total_time,
            "add_time": add_time,
            "search_time": search_time,
            "update_time": update_time,
            "docs_per_second": stress_config["num_documents"] / add_time,
            "searches_per_second": stress_config["num_searches"] / search_time,
            "updates_per_second": stress_config["num_updates"] / update_time
        }
        
        # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆåŸºæº–
        assert total_time < 60, f"ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆæ™‚é–“ãŒé•·ã™ãã‚‹: {total_time:.1f}ç§’"
        assert self.performance_metrics["stress_test"]["docs_per_second"] > 50, "æ–‡æ›¸è¿½åŠ é€Ÿåº¦ãŒé…ã„"
        assert self.performance_metrics["stress_test"]["searches_per_second"] > 100, "æ¤œç´¢é€Ÿåº¦ãŒé…ã„"
        
        print(f"âœ… ã‚¹ãƒˆãƒ¬ã‚¹è² è·ãƒ†ã‚¹ãƒˆæˆåŠŸ: {total_time:.1f}ç§’ã§{self.performance_metrics['stress_test']['total_operations']}æ“ä½œå®Œäº†")
    
    @pytest.mark.asyncio
    async def test_edge_cases(self):
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ"""
        processor = RAGProcessor()
        
        edge_cases = [
            {
                "name": "ç©ºæ–‡å­—åˆ—æ¤œç´¢",
                "action": "search",
                "data": {"query": ""},
                "should_succeed": True
            },
            {
                "name": "è¶…é•·æ–‡æ›¸",
                "action": "add_document",
                "data": {
                    "document": {
                        "title": "Very Long Document",
                        "content": "x" * 10000  # 10,000æ–‡å­—
                    }
                },
                "should_succeed": True
            },
            {
                "name": "ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€æ¤œç´¢",
                "action": "search",
                "data": {"query": "test @#$%^&*() <script>alert('xss')</script>"},
                "should_succeed": True
            },
            {
                "name": "Unicodeæ–‡å­—",
                "action": "add_document",
                "data": {
                    "document": {
                        "title": "Unicode Test æ—¥æœ¬èª í•œêµ­ì–´ ä¸­æ–‡",
                        "content": "å¤šè¨€èªå¯¾å¿œãƒ†ã‚¹ãƒˆ ğŸŒğŸ”ğŸ“š"
                    }
                },
                "should_succeed": True
            },
            {
                "name": "é‡è¤‡ID",
                "action": "add_document",
                "data": {
                    "document": {
                        "document_id": "duplicate_edge_test",
                        "title": "First Document",
                        "content": "Original"
                    }
                },
                "should_succeed": True
            },
            {
                "name": "é‡è¤‡IDï¼ˆ2å›ç›®ï¼‰",
                "action": "add_document",
                "data": {
                    "document": {
                        "document_id": "duplicate_edge_test",
                        "title": "Second Document",
                        "content": "Duplicate"
                    }
                },
                "should_succeed": False
            },
            {
                "name": "ã‚¼ãƒ­çµæœã‚¯ã‚¨ãƒª",
                "action": "search",
                "data": {"query": "completely_nonexistent_term_xyz123"},
                "should_succeed": True
            },
            {
                "name": "ç„¡åŠ¹ãªãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ",
                "action": "vector_search",
                "data": {
                    "vector": [0.1, 0.2],  # æ¬¡å…ƒãŒå°ã•ã™ãã‚‹
                    "k": 5
                },
                "should_succeed": False
            }
        ]
        
        edge_results = []
        
        for test_case in edge_cases:
            try:
                result = await processor.process_action(test_case["action"], test_case["data"])
                
                success = result.get("success", False)
                expected = test_case["should_succeed"]
                
                edge_results.append({
                    "case": test_case["name"],
                    "expected_success": expected,
                    "actual_success": success,
                    "correct": success == expected
                })
                
            except Exception as e:
                edge_results.append({
                    "case": test_case["name"],
                    "expected_success": test_case["should_succeed"],
                    "actual_success": False,
                    "correct": not test_case["should_succeed"],
                    "exception": str(e)
                })
        
        # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹è©•ä¾¡
        correct_cases = [r for r in edge_results if r["correct"]]
        edge_success_rate = len(correct_cases) / len(edge_results) * 100
        
        assert edge_success_rate >= 80, f"ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å‡¦ç†ç‡ãŒä½ã„: {edge_success_rate:.1f}%"
        
        # å¤±æ•—ã‚±ãƒ¼ã‚¹ã®è©³ç´°
        for result in edge_results:
            if not result["correct"]:
                print(f"  âš ï¸ {result['case']}: æœŸå¾…={result['expected_success']}, å®Ÿéš›={result['actual_success']}")
        
        print(f"âœ… ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(correct_cases)}/{len(edge_results)} ã‚±ãƒ¼ã‚¹æ­£å¸¸å‡¦ç†")