#!/usr/bin/env python3
"""
Phase 3 Integration Benchmark for Auto Issue Processor A2A
Tests memory and network optimization components
"""
import asyncio
import time
import psutil
import statistics
import json
from datetime import datetime
import sys
import os

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from libs.memory_stream_optimizer import MemoryStreamOptimizer, CompressionType
from libs.connection_pool_optimizer import ConnectionPoolOptimizer
from libs.adaptive_concurrency_controller import AdaptiveConcurrencyController
from libs.advanced_process_pool import AdvancedProcessPool, ProcessPoolConfig
from libs.distributed_queue_manager import DistributedQueueManager, QueueItem, QueuePriority

class Phase3BenchmarkSuite:
    """Comprehensive Phase 3 benchmark suite"""
    
    def __init__(self):
        self.results = {
            'phase3_components': {},
            'integration_performance': {},
            'memory_efficiency': {},
            'network_optimization': {},
            'comparison_with_phase2': {},
            'timestamp': datetime.now().isoformat()
        }
    
    async def run_all_benchmarks(self)print("üöÄ Starting Phase 3 Integration Benchmark")
    """Run all Phase 3 benchmarks"""
        print("=" * 60)
        
        # Component benchmarks
        await self.benchmark_memory_stream_optimizer()
        await self.benchmark_connection_pool_optimizer()
        
        # Integration benchmark
        await self.benchmark_phase3_integration()
        
        # Memory efficiency tests
        await self.benchmark_memory_efficiency()
        
        # Network optimization tests
        await self.benchmark_network_optimization()
        
        # Comparison with previous phases
        await self.compare_with_previous_phases()
        
        # Generate report
        self.generate_report()
    
    async def benchmark_memory_stream_optimizer(self)print("\nüìä Testing Memory Stream Optimizer...")
    """Benchmark memory stream optimizer"""
        
        optimizer = MemoryStreamOptimizer(
            chunk_size=4096,
            buffer_size=16384,
            enable_compression=True,
            memory_limit_mb=100
        )
        
        results = {
            'compression_ratios': {},
            'processing_times': [],
            'memory_savings': 0,
            'throughput': 0
        }
        
        # Test compression efficiency
        test_data = "This is test data that should compress well. " * 1000
        
        for comp_type in [CompressionType.GZIP]:  # Only test available ones
            start_time = time.time()
            compressed = await optimizer.compress_data(test_data, comp_type)
            compression_time = time.time() - start_time
            
            ratio = len(compressed) / len(test_data.encode('utf-8'))
            results['compression_ratios'][comp_type.value] = {
                'ratio': ratio,
                'time_ms': compression_time * 1000,
                'savings_percent': (1 - ratio) * 100
            }
        
        # Test streaming performance
        print("  - Testing streaming performance...")
        
        async def data_generator():
            for i in range(1000):
                yield f"stream-item-{i}-{'x' * 100}"
        
        start_time = time.time()
        count = 0
        
        async for item in optimizer.process_stream(data_generator()):
            count += 1
            results['processing_times'].append(time.time() - start_time)
            start_time = time.time()
        
        results['throughput'] = count / sum(results['processing_times'])
        results['avg_processing_time_ms'] = statistics.mean(results['processing_times']) * 1000
        
        # Test memory efficiency
        initial_memory = psutil.virtual_memory().used
        
        # Process large dataset
        large_data = ["x" * 10000 for _ in range(100)]
        async for _ in optimizer.process_stream(async_generator(large_data)):
            pass
        
        final_memory = psutil.virtual_memory().used
        memory_increase = (final_memory - initial_memory) / (1024 * 1024)  # MB
        results['memory_increase_mb'] = memory_increase
        
        self.results['phase3_components']['memory_stream_optimizer'] = results
        
        best_compression = min(results['compression_ratios'].values(), key=lambda x: x['ratio'])
        print(f"‚úÖ Memory Optimizer: {best_compression['savings_percent']:0.1f}% compression, "
              f"{results['throughput']:0.1f} items/sec")
    
    async def benchmark_connection_pool_optimizer(self)print("\nüìä Testing Connection Pool Optimizer...")
    """Benchmark connection pool optimizer"""
        
        optimizer = ConnectionPoolOptimizer(
            max_connections=20,
            rate_limit_per_hour=1000,  # Lower for testing

        )
        
        results = {
            'connection_efficiency': {},
            'rate_limiting': {},
            'retry_performance': {},
            'concurrent_performance': {}
        }
        
        # Test connection pooling
        print("  - Testing connection pooling...")
        
        urls = [f"https://api.github.com/test/{i}" for i in range(50)]
        
        start_time = time.time()
        pool_results = await optimizer.execute_pooled_requests(urls)
        pool_time = time.time() - start_time
        
        results['connection_efficiency'] = {
            'requests': len(pool_results),
            'total_time': pool_time,
            'requests_per_second': len(pool_results) / pool_time,
            'pool_reuse_count': optimizer.metrics.pool_reuse_count
        }
        
        # Test rate limiting
        print("  - Testing rate limiting...")
        
        rate_limit_start = time.time()
        for i in range(10):
            allowed = await optimizer.rate_limiter.acquire()
            if not allowed:
                break
        
        results['rate_limiting'] = {
            'requests_allowed': i + 1 if allowed else i,
            'remaining_quota': optimizer.get_remaining_quota(),
            'rate_limit_time': time.time() - rate_limit_start
        }
        
        # Test concurrent requests
        print("  - Testing concurrent requests...")
        
        concurrent_urls = [f"https://api.github.com/concurrent/{i}" for i in range(20)]
        
        concurrent_start = time.time()
        concurrent_results = await optimizer.execute_concurrent_requests(concurrent_urls)
        concurrent_time = time.time() - concurrent_start
        
        results['concurrent_performance'] = {
            'requests': len(concurrent_results),
            'total_time': concurrent_time,
            'concurrent_rps': len(concurrent_results) / concurrent_time,
            'speedup_factor': (len(concurrent_results) * 0.1) / concurrent_time  # vs sequential
        }
        
        self.results['phase3_components']['connection_pool_optimizer'] = results
        
        print(f"‚úÖ Connection Optimizer: {results['connection_efficiency']['requests_per_second']:0.1f} req/sec, "
              f"{results['concurrent_performance']['speedup_factor']:0.1f}x speedup")
    
    async def benchmark_phase3_integration(self)print("\nüìä Testing Phase 3 Integration...")
    """Benchmark integrated Phase 3 system"""
        
        # Initialize all components
        memory_optimizer = MemoryStreamOptimizer(chunk_size=8192, enable_compression=True)
        connection_optimizer = ConnectionPoolOptimizer(max_connections=30)
        concurrency_controller = AdaptiveConcurrencyController(min_workers=4, max_workers=20)
        
        # Create integrated processor
        class Phase3IntegratedProcessor:
            def __init__(self, memory_opt, conn_opt, concurrency_ctrl):
            """Phase3IntegratedProcessorÂá¶ÁêÜ„ÇØ„É©„Çπ"""
                self.memory_opt = memory_opt
                self.conn_opt = conn_opt
                self.concurrency_ctrl = concurrency_ctrl
                self.processed_count = 0
            
            async def process_issues_optimized(self, issue_count: int)start_time = time.time()
    """Process issues with all Phase 3 optimizations"""
                
                # Generate test issues
                async def issue_generator()for i in range(issue_count):
    """issue_generator„É°„ÇΩ„ÉÉ„Éâ"""
                        issue_data = {
                            'id': i,
                            'title': f'Issue {i}',
                            'body': 'x' * 1000,  # 1KB body

                        }
                        yield issue_data
                
                # Process with memory streaming
                processed_issues = []
                
                async for issue in self.memory_opt.process_stream(issue_generator()):
                    # Compress issue data
                    compressed = await self.memory_opt.auto_compress(str(issue))
                    
                    # Simulate API call with connection pooling
                    url = f"https://api.github.com/repos/test/test/issues/{issue['id']}"
                    
                    # Use rate-limited connection
                    if await self.conn_opt.rate_limiter.acquire():
                        # Simulate processing
                        await asyncio.sleep(0.001)  # 1ms processing time
                        self.processed_count += 1
                    
                    processed_issues.append({
                        'original_size': len(str(issue)),
                        'compressed_size': compressed.compressed_size,
                        'compression_ratio': compressed.compressed_size / compressed.original_size
                    })
                
                total_time = time.time() - start_time
                return total_time, processed_issues
        
        processor = Phase3IntegratedProcessor(
            memory_optimizer, connection_optimizer, concurrency_controller
        )
        
        # Test different scales
        test_scales = [100, 500, 1000]
        results = {}
        
        for scale in test_scales:
            print(f"  - Processing {scale} issues...")
            
            elapsed_time, processed = await processor.process_issues_optimized(scale)
            
            # Calculate metrics
            avg_compression_ratio = statistics.mean([p['compression_ratio'] for p in processed])
            total_original_size = sum([p['original_size'] for p in processed])
            total_compressed_size = sum([p['compressed_size'] for p in processed])
            memory_savings = (1 - total_compressed_size / total_original_size) * 100
            
            results[f'scale_{scale}'] = {
                'total_time': elapsed_time,
                'throughput': scale / elapsed_time,
                'avg_compression_ratio': avg_compression_ratio,
                'memory_savings_percent': memory_savings,
                'processed_count': processor.processed_count
            }
            
            # Reset for next test
            processor.processed_count = 0
        
        self.results['integration_performance'] = results
        
        # Print summary
        for scale, data in results.items():
            print(f"‚úÖ {scale}: {data['throughput']:0.1f} issues/sec, "
                  f"{data['memory_savings_percent']:0.1f}% memory saved")
    
    async def benchmark_memory_efficiency(self)print("\nüìä Testing Memory Efficiency...")
    """Benchmark memory efficiency improvements"""
        
        optimizer = MemoryStreamOptimizer(memory_limit_mb=50)
        
        # Test memory pressure handling
        initial_memory = psutil.virtual_memory().percent
        
        # Process increasingly large datasets
        data_sizes = [1000, 5000, 10000]
        memory_results = {}
        
        for size in data_sizes:
            # Create large dataset
            large_items = [f"large-item-{i}-" + "x" * 1000 for i in range(size)]
            
            start_memory = psutil.virtual_memory().percent
            
            # Process with streaming
            count = 0
            async for item in optimizer.process_stream(async_generator(large_items)):
                count += 1
                if count % 1000 == 0:
                    await optimizer.check_memory_pressure()
            
            end_memory = psutil.virtual_memory().percent
            
            memory_results[f'size_{size}'] = {
                'start_memory_percent': start_memory,
                'end_memory_percent': end_memory,
                'memory_increase': end_memory - start_memory,
                'items_processed': count,
                'gc_optimizations': optimizer.metrics.gc_optimizations
            }
        
        self.results['memory_efficiency'] = memory_results
        
        avg_memory_increase = statistics.mean([r['memory_increase'] for r in memory_results.values()])
        print(f"‚úÖ Memory Efficiency: {avg_memory_increase:0.1f}% avg memory increase")
    
    async def benchmark_network_optimization(self)print("\nüìä Testing Network Optimization...")
    """Benchmark network optimization features"""
        
        optimizer = ConnectionPoolOptimizer(max_connections=15)
        
        # Test connection warming
        print("  - Testing connection warming...")
        
        warm_start = time.time()
        await optimizer.warm_connections(10)
        warm_time = time.time() - warm_start
        
        # Test cold vs warm performance
        cold_times = []
        warm_times = []
        
        # Cold requests
        for i in range(5):
            start = time.time()
            await optimizer._make_request(f"https://api.github.com/cold/{i}")
            cold_times.append(time.time() - start)
        
        # Warm requests (pool should be ready)
        for i in range(5):
            start = time.time()
            await optimizer._make_request(f"https://api.github.com/warm/{i}")
            warm_times.append(time.time() - start)
        
        # Test failover
        optimizer.configure_failover([
            "https://api.github.com",
            "https://api-backup.github.com"
        ])
        
        failover_start = time.time()
        try:
            result = await optimizer.make_request_with_failover("/test")
            failover_time = time.time() - failover_start
            failover_success = True
        except:
            failover_time = time.time() - failover_start
            failover_success = False
        
        network_results = {
            'connection_warming': {
                'warm_time': warm_time,
                'warmed_connections': optimizer.metrics.warmed_connections,
                'cold_avg_time': statistics.mean(cold_times),
                'warm_avg_time': statistics.mean(warm_times),
                'improvement_percent': (1 - statistics.mean(warm_times) / statistics.mean(cold_times)) * 100
            },
            'failover': {
                'failover_time': failover_time,
                'success': failover_success,
                'failover_count': optimizer.metrics.failover_count
            }
        }
        
        self.results['network_optimization'] = network_results
        
        improvement = network_results['connection_warming']['improvement_percent']
        print(f"‚úÖ Network Optimization: {improvement:0.1f}% faster with warm connections")
    
    async def compare_with_previous_phases(self)print("\nüìä Comparing with Previous Phases...")
    """Compare Phase 3 with previous phases"""
        
        # Baseline performance (simulated from previous phases)
        baseline_performance = {
            'phase1_baseline': 96.84,  # issues/sec
            'phase2_estimated': 120,   # with optimizations
        }
        
        # Get Phase 3 performance from integration test
        if 'scale_1000' in self.results['integration_performance']:
            phase3_performance = self.results['integration_performance']['scale_1000']['throughput']
        else:
            phase3_performance = 150  # Estimated
        
        comparison = {
            'phase1_baseline': baseline_performance['phase1_baseline'],
            'phase2_optimized': baseline_performance['phase2_estimated'],
            'phase3_integrated': phase3_performance,
            'improvement_over_phase1': ((phase3_performance - baseline_performance['phase1_baseline']) / 
                                      baseline_performance['phase1_baseline'] * 100),
            'improvement_over_phase2': ((phase3_performance - baseline_performance['phase2_estimated']) / 
                                      baseline_performance['phase2_estimated'] * 100),
            'target_achievement': phase3_performance / 200 * 100  # 200 was Phase 3 target
        }
        
        self.results['comparison_with_phase2'] = comparison
        
        print(f"‚úÖ Phase 1: {comparison['phase1_baseline']:0.1f} issues/sec")
        print(f"‚úÖ Phase 2: {comparison['phase2_optimized']:0.1f} issues/sec")
        print(f"‚úÖ Phase 3: {comparison['phase3_integrated']:0.1f} issues/sec")
        print(f"‚úÖ Total Improvement: {comparison['improvement_over_phase1']:0.1f}%")
    
    def generate_report(self):
        """Generate comprehensive benchmark report"""
        report_path = "phase3_benchmark_results.json"
        
        # Add system info
        self.results['system_info'] = {
            'cpu_count': psutil.cpu_count(),
            'memory_gb': psutil.virtual_memory().total / (1024**3),
            'python_version': sys.version,
            'available_compression': {
                'gzip': True,
                'lz4': False,  # Not installed
                'zstd': False  # Not installed
            }
        }
        
        # Save results
        with open(report_path, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        print(f"\nüìÑ Detailed results saved to: {report_path}")
        
        # Print summary
        print("\n" + "=" * 60)
        print("üìä PHASE 3 BENCHMARK SUMMARY")
        print("=" * 60)
        
        if 'memory_stream_optimizer' in self.results['phase3_components']:
            mso = self.results['phase3_components']['memory_stream_optimizer']
            if 'compression_ratios' in mso and mso['compression_ratios']:
                best_comp = min(mso['compression_ratios'].values(), key=lambda x: x['ratio'])
                print(f"Memory Optimizer: {best_comp['savings_percent']:0.1f}% compression savings")
        
        if 'connection_pool_optimizer' in self.results['phase3_components']:
            cpo = self.results['phase3_components']['connection_pool_optimizer']
            if 'concurrent_performance' in cpo:
                speedup = cpo['concurrent_performance']['speedup_factor']
                print(f"Connection Optimizer: {speedup:0.1f}x concurrent speedup")
        
        if 'comparison_with_phase2' in self.results:
            comp = self.results['comparison_with_phase2']
            print(f"\nTotal Performance Gain: {comp['improvement_over_phase1']:0.1f}% over Phase 1")
            print(f"Phase 3 Target Achievement: {comp['target_achievement']:0.1f}%")

async def async_generator(items):
    """Helper async generator"""
    for item in items:
        yield item

async def main()benchmark = Phase3BenchmarkSuite()
"""Run Phase 3 benchmarks"""
    await benchmark.run_all_benchmarks()

if __name__ == "__main__":
    asyncio.run(main())