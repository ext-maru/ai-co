#!/usr/bin/env python3
"""
Comprehensive Grimoire Migration System
å…¨ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºé­”æ³•æ›¸çµ±åˆç§»è¡Œã‚·ã‚¹ãƒ†ãƒ 

514å€‹ã®MDãƒ•ã‚¡ã‚¤ãƒ«ã‚’PostgreSQL + pgvectorã‚·ã‚¹ãƒ†ãƒ ã«ç§»è¡Œã—ã€
4è³¢è€…ã®é­”æ³•æ›¸ã‚’å®Œå…¨çµ±åˆã™ã‚‹åŒ…æ‹¬çš„ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ 
"""

import sys
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import asyncio
import hashlib
import json
import logging
import os
import re
import uuid
from dataclasses import asdict, dataclass, field
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple, Union

try:
    import aiofiles
except ImportError:
    aiofiles = None

from libs.grimoire_database import (
    GrimoireDatabase,
    MagicSchool,
    SpellMetadata,
    SpellType,
)
from libs.grimoire_spell_evolution import EvolutionEngine, EvolutionType
from libs.grimoire_vector_search import GrimoireVectorSearch, SearchQuery
from libs.rag_grimoire_integration import RagGrimoireConfig, RagGrimoireIntegration
from libs.task_sage_grimoire_vectorization import TaskSageGrimoireVectorization

logger = logging.getLogger(__name__)

@dataclass
class FileAnalysis:
    """ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æçµæœ"""

    file_path: str
    relative_path: str
    file_name: str
    content: str
    size: int
    checksum: str

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ
    spell_name: str
    suggested_spell_type: SpellType
    suggested_magic_school: MagicSchool
    suggested_power_level: int
    extracted_tags: List[str]

    # é‡è¤‡åˆ†æ
    similarity_candidates: List[str] = field(default_factory=list)
    is_duplicate: bool = False
    merge_target: Optional[str] = None

    # å“è³ªåˆ†æ
    content_quality_score: float = 0
    technical_complexity: float = 0
    importance_score: float = 0

@dataclass
class MigrationBatch:
    """ç§»è¡Œãƒãƒƒãƒ"""

    batch_id: str
    files: List[FileAnalysis]
    priority: str  # HIGH, MEDIUM, LOW
    estimated_time: float  # æ¨å®šç§»è¡Œæ™‚é–“ï¼ˆåˆ†ï¼‰
    dependencies: List[str] = field(default_factory=list)

@dataclass
class MigrationResult:
    """ç§»è¡Œçµæœ"""

    batch_id: str
    total_files: int
    successful: int
    failed: int
    skipped: int

    successful_spells: List[str] = field(default_factory=list)
    failed_files: List[Dict[str, str]] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)

    start_time: datetime = None
    end_time: datetime = None
    duration_seconds: float = 0

class ContentAnalyzer:
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æãƒ»åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.logger = logging.getLogger(__name__)

        # 4è³¢è€…åˆ†é¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.magic_school_keywords = {
            MagicSchool.KNOWLEDGE_SAGE: [
                "knowledge",
                "learn",
                "pattern",
                "insight",
                "wisdom",
                "analysis",
                "study",
                "research",
                "understanding",
                "concept",
                "theory",
            ],
            MagicSchool.TASK_ORACLE: [
                "task",
                "workflow",
                "process",
                "procedure",
                "step",
                "execution",
                "schedule",
                "priority",
                "planning",
                "project",
                "management",
            ],
            MagicSchool.CRISIS_SAGE: [
                "error",
                "incident",
                "problem",
                "issue",

                "failure",
                "emergency",
                "crisis",
                "recovery",
                "fix",
                "troubleshoot",

            ],
            MagicSchool.SEARCH_MYSTIC: [
                "search",
                "query",
                "retrieval",
                "find",
                "lookup",
                "index",
                "semantic",
                "context",
                "similarity",
                "match",
                "relevance",
            ],
        }

        # ã‚¹ãƒšãƒ«ã‚¿ã‚¤ãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.spell_type_keywords = {
            SpellType.KNOWLEDGE: [
                "guide",
                "concept",
                "theory",
                "principle",
                "overview",
                "introduction",
            ],
            SpellType.PROCEDURE: [
                "how to",
                "steps",
                "procedure",
                "process",
                "workflow",
                "method",
            ],
            SpellType.CONFIGURATION: [
                "config",
                "setting",
                "parameter",
                "option",
                "environment",
                "setup",
            ],

                "example",
                "sample",
                "boilerplate",
                "scaffold",
            ],
            SpellType.REFERENCE: [
                "reference",
                "api",
                "documentation",
                "spec",
                "manual",
                "list",
            ],
        }

    def analyze_file(self, file_path: str) -> FileAnalysis:
        """ãƒ•ã‚¡ã‚¤ãƒ«å®Œå…¨åˆ†æ"""
        try:
            path_obj = Path(file_path)

            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # åŸºæœ¬æƒ…å ±
            checksum = hashlib.sha256(content.encode()).hexdigest()
            size = len(content.encode())

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            spell_name = self._extract_spell_name(path_obj, content)
            spell_type = self._detect_spell_type(content, path_obj.name)
            magic_school = self._infer_magic_school(content, path_obj)
            power_level = self._calculate_power_level(content, path_obj)
            tags = self._extract_tags(content, path_obj)

            # å“è³ªåˆ†æ
            quality_score = self._assess_content_quality(content)
            complexity = self._assess_technical_complexity(content)
            importance = self._assess_importance(content, path_obj)

            return FileAnalysis(
                file_path=str(file_path),
                relative_path=str(path_obj.relative_to(Path.cwd())),
                file_name=path_obj.name,
                content=content,
                size=size,
                checksum=checksum,
                spell_name=spell_name,
                suggested_spell_type=spell_type,
                suggested_magic_school=magic_school,
                suggested_power_level=power_level,
                extracted_tags=tags,
                content_quality_score=quality_score,
                technical_complexity=complexity,
                importance_score=importance,
            )

        except Exception as e:
            self.logger.error(f"File analysis failed: {file_path} - {e}")
            raise

    def _extract_spell_name(self, path_obj: Path, content: str) -> str:
        """ã‚¹ãƒšãƒ«åæŠ½å‡º"""
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€åˆã®H1è¦‹å‡ºã—ã‚’æ¢ã™
        h1_match = re.search(r"^#\s+(.+)$", content, re.MULTILINE)
        if h1_match:
            return h1_match.group(1).strip()

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç”Ÿæˆ
        name = path_obj.stem
        name = re.sub(r"[_-]", " ", name)
        return name.title()

    def _detect_spell_type(self, content: str, filename: str) -> SpellType:
        """ã‚¹ãƒšãƒ«ã‚¿ã‚¤ãƒ—æ¤œå‡º"""
        content_lower = content.lower()
        filename_lower = filename.lower()

        scores = {}
        for spell_type, keywords in self.spell_type_keywords.items():
            score = 0
            for keyword in keywords:
                score += content_lower.count(keyword)
                if keyword in filename_lower:
                    score += 5  # ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã¾ã‚Œã‚‹å ´åˆã¯é‡ã¿ä»˜ã‘
            scores[spell_type] = score

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¿ã‚¤ãƒ—ã‚’è¿”ã™
        return max(scores, key=scores.get) if scores else SpellType.KNOWLEDGE

    def _infer_magic_school(self, content: str, path_obj: Path) -> MagicSchool:
        """é­”æ³•å­¦æ´¾æ¨å®š"""
        content_lower = content.lower()
        path_lower = str(path_obj).lower()

        scores = {}
        for school, keywords in self.magic_school_keywords.items():
            score = 0
            for keyword in keywords:
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…å‡ºç¾å›æ•°
                score += content_lower.count(keyword)
                # ãƒ‘ã‚¹å†…å‡ºç¾ã®é‡ã¿ä»˜ã‘
                if keyword in path_lower:
                    score += 10
            scores[school] = score

        # ãƒ‘ã‚¹è§£æã«ã‚ˆã‚‹è¿½åŠ åˆ¤å®š
        if "task" in path_lower or "workflow" in path_lower:
            scores[MagicSchool.TASK_ORACLE] += 20

            scores[MagicSchool.CRISIS_SAGE] += 20
        elif "search" in path_lower or "rag" in path_lower:
            scores[MagicSchool.SEARCH_MYSTIC] += 20
        elif "knowledge" in path_lower or "guide" in path_lower:
            scores[MagicSchool.KNOWLEDGE_SAGE] += 20

        return max(scores, key=scores.get) if scores else MagicSchool.KNOWLEDGE_SAGE

    def _calculate_power_level(self, content: str, path_obj: Path) -> int:
        """ãƒ‘ãƒ¯ãƒ¼ãƒ¬ãƒ™ãƒ«è¨ˆç®— (1-10)"""
        score = 5  # ãƒ™ãƒ¼ã‚¹å€¤

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
        size = len(content)
        if size > 10000:
            score += 2
        elif size > 5000:
            score += 1
        elif size < 1000:
            score -= 1

        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        important_keywords = [
            "important",
            "critical",
            "essential",
            "main",
            "core",
            "primary",
            "master",
            "guide",
            "overview",
            "architecture",
            "design",
        ]
        content_lower = content.lower()
        for keyword in important_keywords:
            if keyword in content_lower:
                score += 1

        # ãƒ‘ã‚¹è§£æ
        path_lower = str(path_obj).lower()
        if "readme" in path_lower or "main" in path_lower:
            score += 2
        elif "claude" in path_lower:
            score += 3  # Claudeã«é–¢é€£ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¯é‡è¦

            score -= 2

        # è¦‹å‡ºã—ã®æ•°ï¼ˆæ§‹é€ åŒ–åº¦ï¼‰
        h1_count = len(re.findall(r"^#\s+", content, re.MULTILINE))
        h2_count = len(re.findall(r"^##\s+", content, re.MULTILINE))
        if h1_count > 0 and h2_count > 3:
            score += 1

        return max(1, min(10, score))

    def _extract_tags(self, content: str, path_obj: Path) -> List[str]:
        """ã‚¿ã‚°æŠ½å‡º"""
        tags = set()

        # ãƒ‘ã‚¹ã‹ã‚‰ã‚¿ã‚°æŠ½å‡º
        path_parts = path_obj.parts
        for part in path_parts:
            if part not in [".", "..", "home", "aicompany", "ai_co"]:
                tags.add(part.lower())

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—
        if path_obj.suffix == ".md":
            tags.add("markdown")

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        content_lower = content.lower()
        keyword_patterns = [
            r"(?:^|\s)(tdd|test[\s-]?driven[\s-]?development)(?:\s|$)",
            r"(?:^|\s)(api|rest|graphql)(?:\s|$)",
            r"(?:^|\s)(docker|container)(?:\s|$)",
            r"(?:^|\s)(postgres|postgresql|database)(?:\s|$)",
            r"(?:^|\s)(python|javascript|typescript)(?:\s|$)",
            r"(?:^|\s)(claude|ai|llm)(?:\s|$)",
        ]

        for pattern in keyword_patterns:
            matches = re.findall(pattern, content_lower, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    tags.add(match[0])
                else:
                    tags.add(match)

        return sorted(list(tags))

    def _assess_content_quality(self, content: str) -> float:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªè©•ä¾¡ (0-1)"""
        score = 0

        # åŸºæœ¬æ§‹é€ 
        if re.search(r"^#\s+", content, re.MULTILINE):  # H1è¦‹å‡ºã—ã‚ã‚Š
            score += 0.2
        if re.search(r"^##\s+", content, re.MULTILINE):  # H2è¦‹å‡ºã—ã‚ã‚Š
            score += 0.1

        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
        if "```" in content:
            score += 0.2

        # ãƒªã‚¹ãƒˆæ§‹é€ 
        if re.search(r"^\s*[-*+]\s+", content, re.MULTILINE):
            score += 0.1

        # é©åˆ‡ãªé•·ã•
        length = len(content)
        if 500 <= length <= 10000:
            score += 0.2
        elif length > 100:
            score += 0.1

        # æ”¹è¡Œãƒ»æ®µè½æ§‹é€ 
        paragraphs = content.split("\n\n")
        if len(paragraphs) > 2:
            score += 0.1

        # ãƒªãƒ³ã‚¯
        if "[" in content and "]" in content:
            score += 0.1

        return min(1, score)

    def _assess_technical_complexity(self, content: str) -> float:
        """æŠ€è¡“çš„è¤‡é›‘æ€§è©•ä¾¡ (0-1)"""
        score = 0
        content_lower = content.lower()

        # æŠ€è¡“ç”¨èªã®å¯†åº¦
        technical_terms = [
            "algorithm",
            "architecture",
            "async",
            "await",
            "class",
            "function",
            "implementation",
            "interface",
            "method",
            "module",
            "object",
            "parameter",
            "protocol",
            "schema",
            "system",
            "vector",
            "database",
        ]

        term_count = sum(content_lower.count(term) for term in technical_terms)
        term_density = term_count / max(len(content.split()), 1)
        score += min(0.4, term_density * 10)

        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®è¤‡é›‘ã•
        code_blocks = re.findall(r"```[\s\S]*?```", content)
        if code_blocks:
            avg_code_length = sum(len(block) for block in code_blocks) / len(
                code_blocks
            )
            score += min(0.3, avg_code_length / 1000)

        # JSONã‚„YAMLã®å­˜åœ¨
        if "{" in content and "}" in content:
            score += 0.1
        if ":" in content and re.search(r"^\s*\w+:", content, re.MULTILINE):
            score += 0.1

        # è¤‡é›‘ãªæ§‹é€ 
        nesting_level = max(line.count("  ") for line in content.split("\n"))
        score += min(0.1, nesting_level / 20)

        return min(1, score)

    def _assess_importance(self, content: str, path_obj: Path) -> float:
        """é‡è¦åº¦è©•ä¾¡ (0-1)"""
        score = 0

        # ãƒ‘ã‚¹è§£æ
        path_lower = str(path_obj).lower()

        # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«
        important_files = ["readme", "claude", "main", "index", "getting_started"]
        for important in important_files:
            if important in path_lower:
                score += 0.3
                break

        # é‡è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        if "knowledge_base" in path_lower:
            score += 0.2
        elif "docs" in path_lower:
            score += 0.1

            score -= 0.2

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æ
        content_lower = content.lower()
        importance_indicators = [
            "important",
            "critical",
            "essential",
            "required",
            "mandatory",
            "architecture",
            "design",
            "specification",
            "standard",
        ]

        for indicator in importance_indicators:
            if indicator in content_lower:
                score += 0.1

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºï¼ˆé©åº¦ãªæƒ…å ±é‡ï¼‰
        size = len(content)
        if 1000 <= size <= 20000:
            score += 0.1

        return max(0, min(1, score))

class DuplicateDetector:
    """é‡è¤‡ãƒ»é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.logger = logging.getLogger(__name__)
        self.similarity_threshold = 0.8

    def detect_duplicates(self, analyses: List[FileAnalysis]) -> List[FileAnalysis]:
        """é‡è¤‡æ¤œå‡ºã¨çµ±åˆå€™è£œç‰¹å®š"""
        self.logger.info(f"Analyzing {len(analyses)} files for duplicates...")

        # ç°¡å˜ãªé‡è¤‡æ¤œå‡ºï¼ˆãƒã‚§ãƒƒã‚¯ã‚µãƒ ï¼‰
        checksum_groups = {}
        for analysis in analyses:
            if analysis.checksum in checksum_groups:
                checksum_groups[analysis.checksum].append(analysis)
            else:
                checksum_groups[analysis.checksum] = [analysis]

        # é‡è¤‡ã‚’ãƒãƒ¼ã‚¯
        for checksum, group in checksum_groups.items():
            if len(group) > 1:
                # æœ€ã‚‚é‡è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ®‹ã—ã€ä»–ã‚’é‡è¤‡ãƒãƒ¼ã‚¯
                group.sort(key=lambda x: x.importance_score, reverse=True)
                primary = group[0]

                for duplicate in group[1:]:
                    duplicate.is_duplicate = True
                    duplicate.merge_target = primary.file_path
                    primary.similarity_candidates.append(duplicate.file_path)

        # å†…å®¹é¡ä¼¼æ€§æ¤œå‡ºï¼ˆç°¡ç•¥ç‰ˆï¼‰
        self._detect_content_similarity(analyses)

        duplicates_count = sum(1 for a in analyses if a.is_duplicate)
        self.logger.info(f"Found {duplicates_count} duplicates")

        return analyses

    def _detect_content_similarity(self, analyses: List[FileAnalysis]):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é¡ä¼¼æ€§æ¤œå‡º"""
        # ç°¡å˜ãªé¡ä¼¼æ€§æ¤œå‡ºï¼ˆå˜èªé‡è¤‡ç‡ï¼‰
        for i, analysis1 in enumerate(analyses):
            if analysis1is_duplicate:
                continue

            words1 = set(analysis1content.lower().split())

            for analysis2 in analyses[i + 1 :]:
                if analysis2is_duplicate:
                    continue

                words2 = set(analysis2content.lower().split())

                if not words1 or not words2:
                    continue

                # Jaccardä¿‚æ•°è¨ˆç®—
                intersection = words1.intersection(words2)
                union = words1.union(words2)
                similarity = len(intersection) / len(union) if union else 0

                if similarity > self.similarity_threshold:
                    # ã‚ˆã‚Šé‡è¦åº¦ã®ä½ã„æ–¹ã‚’é‡è¤‡ã¨ãƒãƒ¼ã‚¯
                    if analysis1importance_score >= analysis2importance_score:
                        analysis2is_duplicate = True
                        analysis2merge_target = analysis1file_path
                        analysis1similarity_candidates.append(analysis2file_path)
                    else:
                        analysis1is_duplicate = True
                        analysis1merge_target = analysis2file_path
                        analysis2similarity_candidates.append(analysis1file_path)

class ComprehensiveGrimoireMigration:
    """åŒ…æ‹¬çš„é­”æ³•æ›¸ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, database_url: Optional[str] = None):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.database_url = database_url or os.getenv(
            "GRIMOIRE_DATABASE_URL",
            "postgresql://aicompany@localhost:5432/ai_company_grimoire",
        )
        self.logger = logging.getLogger(__name__)

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.grimoire_db = None
        self.vector_search = None
        self.evolution_engine = None
        self.task_sage = None
        self.rag_integration = None

        # åˆ†æã‚·ã‚¹ãƒ†ãƒ 
        self.content_analyzer = ContentAnalyzer()
        self.duplicate_detector = DuplicateDetector()

        # ç§»è¡Œçµ±è¨ˆ
        self.migration_stats = {
            "total_files_found": 0,
            "files_analyzed": 0,
            "duplicates_detected": 0,
            "batches_created": 0,
            "files_migrated": 0,
            "migration_failures": 0,
            "start_time": None,
            "end_time": None,
        }

        # è¨­å®š
        self.batch_size = 50
        self.max_concurrent_migrations = 5

    async def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            self.logger.info("Initializing Comprehensive Grimoire Migration System...")

            # é­”æ³•æ›¸ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
            self.grimoire_db = GrimoireDatabase(self.database_url)
            await self.grimoire_db.initialize()

            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
            self.vector_search = GrimoireVectorSearch(database=self.grimoire_db)
            await self.vector_search.initialize()

            # é€²åŒ–ã‚¨ãƒ³ã‚¸ãƒ³
            self.evolution_engine = EvolutionEngine(database=self.grimoire_db)
            await self.evolution_engine.initialize()

            # ã‚¿ã‚¹ã‚¯è³¢è€…ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚¹ãƒ†ãƒ 
            self.task_sage = TaskSageGrimoireVectorization(self.database_url)
            await self.task_sage.initialize()

            # RAGçµ±åˆã‚·ã‚¹ãƒ†ãƒ 
            config = RagGrimoireConfig(
                database_url=self.database_url, migration_mode=True
            )
            self.rag_integration = RagGrimoireIntegration(config)
            await self.rag_integration.initialize()

            self.logger.info("âœ… Comprehensive Grimoire Migration System initialized")

        except Exception as e:
            self.logger.error(f"Initialization failed: {e}")
            raise

    async def discover_files(self, root_path: str = ".") -> List[str]:
        """MDãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹"""
        self.logger.info(f"Discovering MD files in {root_path}...")

        md_files = []
        root_path_obj = Path(root_path)

        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            "**/.*",  # éš ã—ãƒ•ã‚¡ã‚¤ãƒ«
            "**/venv/**",
            "**/node_modules/**",
            "**/__pycache__/**",

            "**/tmp/**",
        ]

        for md_file in root_path_obj.rglob("*.md"):
            # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            should_exclude = False
            for pattern in exclude_patterns:
                if md_file.match(pattern):
                    should_exclude = True
                    break

            if not should_exclude:
                md_files.append(str(md_file.absolute()))

        self.migration_stats["total_files_found"] = len(md_files)
        self.logger.info(f"Found {len(md_files)} MD files")

        return md_files

    async def analyze_files(self, file_paths: List[str]) -> List[FileAnalysis]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ä¸€æ‹¬åˆ†æ"""
        self.logger.info(f"Analyzing {len(file_paths)} files...")

        analyses = []

        # ä¸¦è¡Œåˆ†æ
        semaphore = asyncio.Semaphore(10)  # åŒæ™‚åˆ†ææ•°åˆ¶é™

        async def analyze_single_file(file_path: str) -> Optional[FileAnalysis]:
            """analyze_single_fileåˆ†æãƒ¡ã‚½ãƒƒãƒ‰"""
            async with semaphore:
                try:
                    # CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯ãªã®ã§åŒæœŸå®Ÿè¡Œ
                    analysis = await asyncio.get_event_loop().run_in_executor(
                        None, self.content_analyzer.analyze_file, file_path
                    )
                    return analysis
                except Exception as e:
                    self.logger.error(f"Analysis failed for {file_path}: {e}")
                    return None

        # ä¸¦è¡Œå®Ÿè¡Œ
        tasks = [analyze_single_file(path) for path in file_paths]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # çµæœåé›†
        for result in results:
            if isinstance(result, FileAnalysis):
                analyses.append(result)
            elif isinstance(result, Exception):
                self.logger.error(f"Analysis exception: {result}")

        self.migration_stats["files_analyzed"] = len(analyses)

        # é‡è¤‡æ¤œå‡º
        analyses = await asyncio.get_event_loop().run_in_executor(
            None, self.duplicate_detector.detect_duplicates, analyses
        )

        duplicates = sum(1 for a in analyses if a.is_duplicate)
        self.migration_stats["duplicates_detected"] = duplicates

        self.logger.info(
            f"Analysis complete: {len(analyses)} files, {duplicates} duplicates"
        )

        return analyses

    def create_migration_batches(
        self, analyses: List[FileAnalysis]
    ) -> List[MigrationBatch]:
        """ç§»è¡Œãƒãƒƒãƒä½œæˆ"""
        self.logger.info("Creating migration batches...")

        # é‡è¤‡ã‚’é™¤å¤–
        non_duplicates = [a for a in analyses if not a.is_duplicate]

        # å„ªå…ˆåº¦åˆ¥åˆ†é¡
        high_priority = []
        medium_priority = []
        low_priority = []

        for analysis in non_duplicates:
            if analysis.importance_score >= 0.7:
                high_priority.append(analysis)
            elif analysis.importance_score >= 0.4:
                medium_priority.append(analysis)
            else:
                low_priority.append(analysis)

        # ãƒãƒƒãƒä½œæˆ
        batches = []

        # é«˜å„ªå…ˆåº¦ãƒãƒƒãƒ
        for i in range(0, len(high_priority), self.batch_size):
            batch_files = high_priority[i : i + self.batch_size]
            batch = MigrationBatch(
                batch_id=f"high_priority_{i // self.batch_size + 1}",
                files=batch_files,
                priority="HIGH",
                estimated_time=len(batch_files) * 0.5,  # 30ç§’/ãƒ•ã‚¡ã‚¤ãƒ«
            )
            batches.append(batch)

        # ä¸­å„ªå…ˆåº¦ãƒãƒƒãƒ
        for i in range(0, len(medium_priority), self.batch_size):
            batch_files = medium_priority[i : i + self.batch_size]
            batch = MigrationBatch(
                batch_id=f"medium_priority_{i // self.batch_size + 1}",
                files=batch_files,
                priority="MEDIUM",
                estimated_time=len(batch_files) * 0.3,
            )
            batches.append(batch)

        # ä½å„ªå…ˆåº¦ãƒãƒƒãƒ
        for i in range(0, len(low_priority), self.batch_size):
            batch_files = low_priority[i : i + self.batch_size]
            batch = MigrationBatch(
                batch_id=f"low_priority_{i // self.batch_size + 1}",
                files=batch_files,
                priority="LOW",
                estimated_time=len(batch_files) * 0.2,
            )
            batches.append(batch)

        self.migration_stats["batches_created"] = len(batches)

        total_files = sum(len(batch.files) for batch in batches)
        total_time = sum(batch.estimated_time for batch in batches)

        self.logger.info(f"Created {len(batches)} batches for {total_files} files")
        self.logger.info(f"Estimated migration time: {total_time:0.1f} minutes")

        return batches

    async def migrate_batch(self, batch: MigrationBatch) -> MigrationResult:
        """ãƒãƒƒãƒç§»è¡Œå®Ÿè¡Œ"""
        result = MigrationResult(
            batch_id=batch.batch_id,
            total_files=len(batch.files),
            successful=0,
            failed=0,
            skipped=0,
            start_time=datetime.now(timezone.utc),
        )

        self.logger.info(
            f"Starting migration of batch {batch.batch_id} ({len(batch.files)} files)"
        )

        try:
            for analysis in batch.files:
                try:
                    # SpellMetadataä½œæˆ
                    spell_metadata = SpellMetadata(
                        id=str(uuid.uuid4()),
                        spell_name=analysis.spell_name,
                        content=analysis.content,
                        spell_type=analysis.suggested_spell_type,
                        magic_school=analysis.suggested_magic_school,
                        tags=analysis.extracted_tags,
                        power_level=analysis.suggested_power_level,
                        casting_frequency=0,
                        last_cast_at=None,
                        is_eternal=analysis.importance_score >= 0.8,  # é«˜é‡è¦åº¦ã¯æ°¸ç¶šåŒ–
                        evolution_history=[],
                        created_at=datetime.now(timezone.utc),
                        updated_at=datetime.now(timezone.utc),
                        version=1,
                    )

                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                    spell_id = await self.grimoire_db.create_spell(spell_metadata)

                    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ 
                    await self.vector_search.index_spell(spell_id, spell_metadata)

                    result.successful += 1
                    result.successful_spells.append(spell_id)

                        f"âœ… Migrated: {analysis.file_name} -> {spell_id}"
                    )

                except Exception as e:
                    result.failed += 1
                    result.failed_files.append(
                        {"file": analysis.file_path, "error": str(e)}
                    )
                    self.logger.error(
                        f"âŒ Migration failed: {analysis.file_name} - {e}"
                    )

            result.end_time = datetime.now(timezone.utc)
            result.duration_seconds = (
                result.end_time - result.start_time
            ).total_seconds()

            self.migration_stats["files_migrated"] += result.successful
            self.migration_stats["migration_failures"] += result.failed

            self.logger.info(
                f"Batch {batch.batch_id} complete: "
                f"{result.successful} successful, {result.failed} failed, "
                f"{result.duration_seconds:0.1f}s"
            )

        except Exception as e:
            self.logger.error(f"Batch migration failed: {batch.batch_id} - {e}")
            result.failed = len(batch.files)
            result.end_time = datetime.now(timezone.utc)
            result.duration_seconds = (
                result.end_time - result.start_time
            ).total_seconds()

        return result

    async def execute_full_migration(self, root_path: str = ".") -> Dict[str, Any]:
        """å®Œå…¨ç§»è¡Œå®Ÿè¡Œ"""
        self.migration_stats["start_time"] = datetime.now(timezone.utc)

        try:
            self.logger.info("ğŸ›ï¸ Starting Comprehensive Grimoire Migration...")

            # 1 ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹
            file_paths = await self.discover_files(root_path)

            if not file_paths:
                self.logger.warning("No MD files found")
                return {"error": "No MD files found"}

            # 2 ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
            analyses = await self.analyze_files(file_paths)

            # 3 ãƒãƒƒãƒä½œæˆ
            batches = self.create_migration_batches(analyses)

            if not batches:
                self.logger.warning("No migration batches created")
                return {"error": "No migration batches created"}

            # 4 ãƒãƒƒãƒç§»è¡Œå®Ÿè¡Œ
            migration_results = []

            # ã‚»ãƒãƒ•ã‚©ã‚¢ã§åŒæ™‚å®Ÿè¡Œæ•°åˆ¶å¾¡
            semaphore = asyncio.Semaphore(self.max_concurrent_migrations)

            async def migrate_single_batch(batch: MigrationBatch) -> MigrationResult:
                """migrate_single_batchãƒ¡ã‚½ãƒƒãƒ‰"""
                async with semaphore:
                    return await self.migrate_batch(batch)

            # å„ªå…ˆåº¦é †ã«å®Ÿè¡Œ
            high_priority_batches = [b for b in batches if b.priority == "HIGH"]
            medium_priority_batches = [b for b in batches if b.priority == "MEDIUM"]
            low_priority_batches = [b for b in batches if b.priority == "LOW"]

            # é«˜å„ªå…ˆåº¦ã‚’å…ˆã«å®Ÿè¡Œ
            if high_priority_batches:
                self.logger.info(
                    f"Migrating {len(high_priority_batches)} high priority batches..."
                )
                high_results = await asyncio.gather(
                    *[migrate_single_batch(batch) for batch in high_priority_batches]
                )
                migration_results.extend(high_results)

            # ä¸­å„ªå…ˆåº¦
            if medium_priority_batches:
                self.logger.info(
                    f"Migrating {len(medium_priority_batches)} medium priority batches..."
                )
                medium_results = await asyncio.gather(
                    *[migrate_single_batch(batch) for batch in medium_priority_batches]
                )
                migration_results.extend(medium_results)

            # ä½å„ªå…ˆåº¦
            if low_priority_batches:
                self.logger.info(
                    f"Migrating {len(low_priority_batches)} low priority batches..."
                )
                low_results = await asyncio.gather(
                    *[migrate_single_batch(batch) for batch in low_priority_batches]
                )
                migration_results.extend(low_results)

            # çµæœé›†è¨ˆ
            self.migration_stats["end_time"] = datetime.now(timezone.utc)

            total_successful = sum(r.successful for r in migration_results)
            total_failed = sum(r.failed for r in migration_results)
            total_duration = (
                self.migration_stats["end_time"] - self.migration_stats["start_time"]
            ).total_seconds()

            # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
            final_report = {
                "migration_completed": True,
                "summary": {
                    "total_files_discovered": self.migration_stats["total_files_found"],
                    "files_analyzed": self.migration_stats["files_analyzed"],
                    "duplicates_detected": self.migration_stats["duplicates_detected"],
                    "batches_processed": len(migration_results),
                    "files_migrated_successfully": total_successful,
                    "migration_failures": total_failed,
                    "total_duration_seconds": total_duration,
                    "average_files_per_second": (
                        total_successful / total_duration if total_duration > 0 else 0
                    ),
                },
                "migration_stats": self.migration_stats,
                "batch_results": [asdict(result) for result in migration_results],
                "database_url": self.database_url,
            }

            # æˆåŠŸãƒ­ã‚°
            self.logger.info(
                f"ğŸ‰ Migration Complete! "
                f"{total_successful} files migrated successfully, "
                f"{total_failed} failures, "
                f"{total_duration:0.1f}s total"
            )

            return final_report

        except Exception as e:
            self.migration_stats["end_time"] = datetime.now(timezone.utc)
            self.logger.error(f"Full migration failed: {e}")

            return {
                "migration_completed": False,
                "error": str(e),
                "migration_stats": self.migration_stats,
            }

    async def generate_migration_report(self, report_data: Dict[str, Any]) -> str:
        """ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_path = f"/home/aicompany/ai_co/migration_reports/grimoire_migration_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(os.path.dirname(report_path), exist_ok=True)

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        if aiofiles:
            async with aiofiles.open(report_path, "w") as f:
                await f.write(json.dumps(report_data, indent=2, default=str))
        else:
            with open(report_path, "w") as f:
                f.write(json.dumps(report_data, indent=2, default=str))

        self.logger.info(f"Migration report saved: {report_path}")
        return report_path

    async def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if self.grimoire_db:
                await self.grimoire_db.close()

            if self.vector_search and hasattr(self.vector_search, "cleanup"):
                await self.vector_search.cleanup()

            if self.evolution_engine and hasattr(self.evolution_engine, "cleanup"):
                await self.evolution_engine.cleanup()

            if self.task_sage and hasattr(self.task_sage, "cleanup"):
                await self.task_sage.cleanup()

            if self.rag_integration and hasattr(self.rag_integration, "cleanup"):
                await self.rag_integration.cleanup()

            self.logger.info("Comprehensive Grimoire Migration System cleaned up")

        except Exception as e:
            self.logger.error(f"Cleanup failed: {e}")

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
async def execute_comprehensive_migration():
    """åŒ…æ‹¬çš„ç§»è¡Œã®å®Ÿè¡Œ"""
    migration_system = ComprehensiveGrimoireMigration()

    try:
        # åˆæœŸåŒ–
        await migration_system.initialize()

        # å®Œå…¨ç§»è¡Œå®Ÿè¡Œ
        result = await migration_system.execute_full_migration()

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_path = await migration_system.generate_migration_report(result)

        print("=" * 80)
        print("ğŸ›ï¸ Elders Guild Grimoire Migration Complete!")
        print("=" * 80)
        print(f"ğŸ“Š Summary:")
        if result.get("migration_completed"):
            summary = result["summary"]
            print(f"  â€¢ Files discovered: {summary['total_files_discovered']}")
            print(f"  â€¢ Files analyzed: {summary['files_analyzed']}")
            print(f"  â€¢ Duplicates detected: {summary['duplicates_detected']}")
            print(f"  â€¢ Files migrated: {summary['files_migrated_successfully']}")
            print(f"  â€¢ Failures: {summary['migration_failures']}")
            print(f"  â€¢ Duration: {summary['total_duration_seconds']:0.1f} seconds")
            print(f"  â€¢ Speed: {summary['average_files_per_second']:0.2f} files/sec")
        else:
            print(f"  âŒ Migration failed: {result.get('error')}")

        print(f"ğŸ“„ Report: {report_path}")
        print("=" * 80)

        return result

    finally:
        await migration_system.cleanup()

if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s [%(name)s] %(levelname)s: %(message)s"
    )

    # å®Ÿè¡Œ
    asyncio.run(execute_comprehensive_migration())
