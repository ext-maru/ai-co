#!/usr/bin/env python3
"""
Migration Engine for Magic Grimoire System
é­”æ³•æ›¸ã‚·ã‚¹ãƒ†ãƒ ç§»è¡Œã‚¨ãƒ³ã‚¸ãƒ³ - 466å€‹MDãƒ•ã‚¡ã‚¤ãƒ«â†’PostgreSQL+pgvector
"""

import asyncio
import hashlib
import json
import logging
import os
import re
import sys
import uuid
from dataclasses import asdict, dataclass
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

logger = logging.getLogger(__name__)

try:
    from libs.grand_elder_approval_system import (
        FourSagesReviewer,
        GrandElderApprovalSystem,
    )
    from libs.grimoire_database import GrimoireDatabase, MagicSchool, SpellType
    from libs.grimoire_vector_search import GrimoireVectorSearch
except ImportError as e:
    # For testing purposes, create mock classes
    logger.warning(f"Import warning: {e}")

    class MockSpellType:
        """MockSpellTypeã‚¯ãƒ©ã‚¹"""
        KNOWLEDGE = "knowledge"
        PROCEDURE = "procedure"
        CONFIGURATION = "configuration"

        REFERENCE = "reference"

    class MockMagicSchool:
        """MockMagicSchoolã‚¯ãƒ©ã‚¹"""
        KNOWLEDGE_SAGE = "knowledge_sage"
        TASK_ORACLE = "task_oracle"
        CRISIS_SAGE = "crisis_sage"
        SEARCH_MYSTIC = "search_mystic"

    SpellType = MockSpellType
    MagicSchool = MockMagicSchool

    class MockGrimoireDatabase:
        """MockGrimoireDatabaseã‚¯ãƒ©ã‚¹"""
        async def initialize(self):
            """initializeãƒ¡ã‚½ãƒƒãƒ‰"""
            return True

        async def close(self):
            """closeãƒ¡ã‚½ãƒƒãƒ‰"""
            pass

    class MockGrimoireVectorSearch:
        """MockGrimoireVectorSearchã‚¯ãƒ©ã‚¹"""
        async def initialize(self):
            """initializeãƒ¡ã‚½ãƒƒãƒ‰"""
            return True

        async def index_spell(self, spell_id, spell_data):
            """index_spellãƒ¡ã‚½ãƒƒãƒ‰"""
            return True

    class MockFourSagesReviewer:
        """MockFourSagesReviewer - 4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ é–¢é€£ã‚¯ãƒ©ã‚¹"""
        pass

    GrimoireDatabase = MockGrimoireDatabase
    GrimoireVectorSearch = MockGrimoireVectorSearch
    FourSagesReviewer = MockFourSagesReviewer

class MigrationStatus(Enum):
    """ç§»è¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""

    PENDING = "pending"  # ç§»è¡Œå¾…ã¡
    ANALYZING = "analyzing"  # åˆ†æä¸­
    CLASSIFYING = "classifying"  # åˆ†é¡ä¸­
    DEDUPLICATING = "deduplicating"  # é‡è¤‡é™¤å»ä¸­
    INDEXING = "indexing"  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ä¸­
    COMPLETED = "completed"  # å®Œäº†
    FAILED = "failed"  # å¤±æ•—
    SKIPPED = "skipped"  # ã‚¹ã‚­ãƒƒãƒ—

class ContentType(Enum):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—"""

    GUIDE = "guide"  # ã‚¬ã‚¤ãƒ‰ãƒ»æ‰‹é †æ›¸
    REFERENCE = "reference"  # ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

    CONFIG = "config"  # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    KNOWLEDGE = "knowledge"  # ä¸€èˆ¬çŸ¥è­˜
    PROCEDURE = "procedure"  # æ‰‹é †æ›¸

@dataclass
class MigrationResult:
    """ç§»è¡Œçµæœ"""

    file_path: str
    status: MigrationStatus
    spell_id: Optional[str] = None
    original_hash: Optional[str] = None
    migration_timestamp: Optional[datetime] = None
    error_message: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    duplicate_of: Optional[str] = None
    sage_classification: Optional[Dict[str, Any]] = None

@dataclass
class MigrationBatch:
    """ç§»è¡Œãƒãƒƒãƒ"""

    batch_id: str
    files: List[str]
    batch_size: int
    started_at: datetime
    completed_at: Optional[datetime] = None
    total_files: int = 0
    successful_migrations: int = 0
    failed_migrations: int = 0
    skipped_files: int = 0
    duplicate_files: int = 0

class MDFileAnalyzer:
    """MDãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.content_patterns = {
            "code_blocks": r"```[\s\S]*?```",
            "headers": r"^#{1,6}\s+(.+)$",
            "lists": r"^[\s]*[-*+]\s+(.+)$",
            "links": r"\[([^\]]+)\]\(([^)]+)\)",
            "images": r"!\[([^\]]*)\]\(([^)]+)\)",
            "tables": r"\|.*\|",
            "commands": r"`([^`]+)`",
        }

        # Elders Guildç‰¹æœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.aicompany_patterns = {
            "tdd_content": r"(TDD|test.*driven|pytest|ãƒ†ã‚¹ãƒˆé§†å‹•)",
            "sage_references": r"(è³¢è€…|sage|ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…|ã‚¿ã‚¹ã‚¯è³¢è€…|ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…|RAGè³¢è€…)",
            "ai_tools": r"(claude|gpt|ai-|assistant)",
            "technical_commands": r"(npm|pip|git|docker|pytest)",
            "incident_content": r"(ã‚¨ãƒ©ãƒ¼|éšœå®³|ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ|å•é¡Œ|ä¿®æ­£|è§£æ±º)",
            "workflow_content": r"(ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼|æ‰‹é †|ãƒ—ãƒ­ã‚»ã‚¹|ãƒ•ãƒ­ãƒ¼)",
        }

    async def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æå®Ÿè¡Œ"""
        try:
            content = await self._read_file(file_path)
            if not content.strip():
                return {"error": "Empty file"}

            # åŸºæœ¬æƒ…å ±
            basic_info = await self._extract_basic_info(file_path, content)

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ
            content_analysis = await self._analyze_content(content)

            # Elders Guildå›ºæœ‰åˆ†æ
            aicompany_analysis = await self._analyze_aicompany_content(content)

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            metadata = await self._extract_metadata(content)

            # åˆ†é¡ææ¡ˆ
            classification = await self._suggest_classification(content, basic_info)

            return {
                "basic_info": basic_info,
                "content_analysis": content_analysis,
                "aicompany_analysis": aicompany_analysis,
                "metadata": metadata,
                "classification": classification,
                "content_hash": hashlib.sha256(content.encode()).hexdigest(),
            }

        except Exception as e:
            logger.error(f"File analysis failed for {file_path}: {e}")
            return {"error": str(e)}

    async def _read_file(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            return file_path.read_text(encoding="utf-8")
        except UnicodeDecodeError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return file_path.read_text(encoding="utf-8", errors="ignore")

    async def _extract_basic_info(
        self, file_path: Path, content: str
    ) -> Dict[str, Any]:
        """åŸºæœ¬æƒ…å ±æŠ½å‡º"""
        stat = file_path.stat()

        # Try to get relative path, fallback to absolute path if not under PROJECT_ROOT
        try:
            relative_path = str(file_path.relative_to(PROJECT_ROOT))
        except ValueError:
            relative_path = str(file_path)

        return {
            "file_name": file_path.name,
            "file_size": stat.st_size,
            "created_at": datetime.fromtimestamp(stat.st_ctime, timezone.utc),
            "modified_at": datetime.fromtimestamp(stat.st_mtime, timezone.utc),
            "line_count": len(content.splitlines()),
            "char_count": len(content),
            "word_count": len(content.split()),
            "relative_path": relative_path,
        }

    async def _analyze_content(self, content: str) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ"""
        analysis = {}

        for pattern_name, pattern in self.content_patterns.items():
            matches = re.findall(pattern, content, re.MULTILINE | re.IGNORECASE)
            analysis[pattern_name] = len(matches)

            if pattern_name == "headers" and matches:
                analysis["main_title"] = matches[0] if matches else None
                analysis["section_titles"] = matches[1:] if len(matches) > 1 else []

        # è¨€èªåˆ†æ
        analysis["language"] = (
            "ja" if re.search(r"[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]", content) else "en"
        )

        # æŠ€è¡“å†…å®¹ã®åˆ†æ
        analysis["technical_density"] = self._calculate_technical_density(content)

        return analysis

    async def _analyze_aicompany_content(self, content: str) -> Dict[str, Any]:
        """Elders Guildå›ºæœ‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ"""
        analysis = {}

        for category, pattern in self.aicompany_patterns.items():
            matches = re.findall(pattern, content, re.IGNORECASE)
            analysis[category] = len(matches)

        # 4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ é–¢é€£åº¦
        sage_score = (
            analysis.get("sage_references", 0)
            + analysis.get("tdd_content", 0)
            + analysis.get("ai_tools", 0)
        )
        analysis["sage_relevance_score"] = min(sage_score / 10, 1.0)

        return analysis

    async def _extract_metadata(self, content: str) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        metadata = {}

        # YAML frontmatteræŠ½å‡º
        yaml_match = re.match(r"^---\n(.*?)\n---", content, re.DOTALL)
        if yaml_match:
            try:
                import yaml

                metadata["frontmatter"] = yaml.safe_load(yaml_match.group(1))
            except ImportError:
                metadata["frontmatter"] = {"raw": yaml_match.group(1)}

        # ã‚¿ã‚°æŠ½å‡º
        tags = re.findall(r"#(\w+)", content)
        metadata["hashtags"] = list(set(tags))

        # æ—¥ä»˜æŠ½å‡º
        date_patterns = [
            r"(\d{4}-\d{2}-\d{2})",
            r"(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)",
            r"(20\d{2}å¹´\d{1,2}æœˆ)",
        ]

        dates = []
        for pattern in date_patterns:
            dates.extend(re.findall(pattern, content))
        metadata["mentioned_dates"] = list(set(dates))

        return metadata

    async def _suggest_classification(
        self, content: str, basic_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åˆ†é¡ææ¡ˆ"""
        content_lower = content.lower()
        file_name_lower = basic_info["file_name"].lower()

        # é­”æ³•å­¦æ´¾ã®åˆ¤å®š
        magic_school = MagicSchool.KNOWLEDGE_SAGE  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        if any(
            keyword in content_lower

        ):
            magic_school = MagicSchool.TASK_ORACLE
        elif any(
            keyword in content_lower
            for keyword in ["error", "incident", "ã‚¨ãƒ©ãƒ¼", "éšœå®³", "ç·Šæ€¥", "fix"]
        ):
            magic_school = MagicSchool.CRISIS_SAGE
        elif any(
            keyword in content_lower
            for keyword in ["search", "find", "rag", "æ¤œç´¢", "æ¢ç´¢"]
        ):
            magic_school = MagicSchool.SEARCH_MYSTIC

        # å‘ªæ–‡ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
        spell_type = SpellType.KNOWLEDGE  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        if any(
            keyword in content_lower
            for keyword in ["guide", "how to", "æ‰‹é †", "step", "procedure"]
        ):
            spell_type = SpellType.PROCEDURE
        elif any(
            keyword in content_lower
            for keyword in ["config", "setting", "è¨­å®š", "configuration"]
        ):
            spell_type = SpellType.CONFIGURATION
        elif any(
            keyword in content_lower

        ):

        elif any(
            keyword in content_lower
            for keyword in ["reference", "api", "doc", "ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹"]
        ):
            spell_type = SpellType.REFERENCE

        # å¨åŠ›ãƒ¬ãƒ™ãƒ«ã®æ¨å®š
        power_level = self._estimate_power_level(content, basic_info)

        # æ°¸ç¶šåŒ–åˆ¤å®š
        is_eternal = self._should_be_eternal(content, basic_info)

        # ã‚¿ã‚°ææ¡ˆ
        suggested_tags = self._suggest_tags(content, basic_info)

        return {
            "magic_school": magic_school,
            "spell_type": spell_type,
            "power_level": power_level,
            "is_eternal": is_eternal,
            "suggested_tags": suggested_tags,
            "confidence_score": self._calculate_confidence(content, basic_info),
        }

    def _calculate_technical_density(self, content: str) -> float:
        """æŠ€è¡“å¯†åº¦è¨ˆç®—"""
        technical_indicators = [
            r"```",  # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
            r"`[^`]+`",  # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰
            r"npm|pip|git|docker|pytest|curl|wget",  # ã‚³ãƒãƒ³ãƒ‰
            r"https?://",  # URL
            r"\.py|\.js|\.md|\.json|\.yaml|\.yml",  # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­
        ]

        total_matches = 0
        for pattern in technical_indicators:
            total_matches += len(re.findall(pattern, content, re.IGNORECASE))

        words = len(content.split())
        return min(total_matches / max(words / 100, 1), 1.0)

    def _estimate_power_level(self, content: str, basic_info: Dict[str, Any]) -> int:
        """å¨åŠ›ãƒ¬ãƒ™ãƒ«æ¨å®š"""
        score = 1

        # ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹åŠ ç‚¹
        if basic_info["char_count"] > 5000:
            score += 2
        elif basic_info["char_count"] > 2000:
            score += 1

        # æŠ€è¡“å¯†åº¦ã«ã‚ˆã‚‹åŠ ç‚¹
        technical_density = self._calculate_technical_density(content)
        if technical_density > 0.3:
            score += 2
        elif technical_density > 0.1:
            score += 1

        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹åŠ ç‚¹
        important_keywords = [
            "critical",
            "important",
            "é‡è¦",
            "å¿…é ˆ",
            "required",
            "essential",
        ]
        if any(keyword in content.lower() for keyword in important_keywords):
            score += 1

        # ã‚³ãƒ¼ãƒ‰ä¾‹ã®å¤šã•
        code_blocks = len(re.findall(r"```", content))
        if code_blocks > 5:
            score += 2
        elif code_blocks > 2:
            score += 1

        return min(score, 10)

    def _should_be_eternal(self, content: str, basic_info: Dict[str, Any]) -> bool:
        """æ°¸ç¶šåŒ–åˆ¤å®š"""
        eternal_indicators = [
            "claude.md",
            "readme.md",
            "tdd",
            "test",
            "è³¢è€…",
            "sage",
            "core",
            "foundation",
            "base",
            "åŸºæœ¬",
            "åŸºç›¤",
        ]

        content_lower = content.lower()
        file_name_lower = basic_info["file_name"].lower()

        return any(
            indicator in content_lower or indicator in file_name_lower
            for indicator in eternal_indicators
        )

    def _suggest_tags(self, content: str, basic_info: Dict[str, Any]) -> List[str]tags = set():
    """ã‚°ææ¡ˆ"""

        # ãƒ•ã‚¡ã‚¤ãƒ«åãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚°
        file_stem = Path(basic_info["file_name"]).stem.lower():
        if "guide" in file_stem:
            tags.add("guide")
        if "tdd" in file_stem:
            tags.add("tdd")
        if "claude" in file_stem:
            tags.add("claude")

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚°
        content_lower = content.lower()

        # æŠ€è¡“ã‚¿ã‚°
        tech_tags = {
            "python": r"python|\.py|pip|pytest",
            "javascript": r"javascript|\.js|npm|node",
            "docker": r"docker|dockerfile|container",
            "git": r"git|github|repository",
            "ai": r"ai|claude|gpt|assistant|äººå·¥çŸ¥èƒ½",
            "database": r"database|sql|postgresql|mysql",
            "web": r"web|http|api|rest|html|css",
            "test": r"test|ãƒ†ã‚¹ãƒˆ|pytest|unittest",
        }

        for tag, pattern in tech_tags.items():
            if re.search(pattern, content_lower):
                tags.add(tag)

        # Elders Guildå›ºæœ‰ã‚¿ã‚°
        if re.search(r"è³¢è€…|sage", content_lower):
            tags.add("four-sages")
        if re.search(r"tdd|ãƒ†ã‚¹ãƒˆé§†å‹•", content_lower):
            tags.add("tdd")
        if re.search(r"incident|ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ|éšœå®³", content_lower):
            tags.add("incident")

        return list(tags)[:10]  # æœ€å¤§10å€‹

    def _calculate_confidence(self, content: str, basic_info: Dict[str, Any]) -> float:
        """ä¿¡é ¼åº¦è¨ˆç®—"""
        confidence = 0.5  # ãƒ™ãƒ¼ã‚¹

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹ä¿¡é ¼åº¦
        if basic_info["char_count"] > 1000:
            confidence += 0.2

        # æ§‹é€ åŒ–åº¦
        headers = len(re.findall(r"^#{1,6}\s+", content, re.MULTILINE))
        if headers > 2:
            confidence += 0.1

        # ã‚³ãƒ¼ãƒ‰ä¾‹ã®å­˜åœ¨
        if re.search(r"```", content):
            confidence += 0.1

        # Elders Guildå›ºæœ‰æ€§
        aicompany_keywords = ["è³¢è€…", "claude", "tdd", "elders guild", "ã‚¨ãƒ«ãƒ€ãƒ¼"]
        matches = sum(1 for keyword in aicompany_keywords if keyword in content.lower())
        confidence += min(matches * 0.05, 0.1)

        return min(confidence, 1.0)

class DuplicateDetector:
    """é‡è¤‡æ¤œå‡ºã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.similarity_threshold = 0.8
        self.content_hashes = {}
        self.similar_groups = []

    async def detect_duplicates(
        self, analyzed_files: List[Dict[str, Any]]
    ) -> List[List[str]]:
        """é‡è¤‡æ¤œå‡ºå®Ÿè¡Œ"""
        try:
            # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®å®Œå…¨ä¸€è‡´æ¤œå‡º
            exact_duplicates = await self._find_exact_duplicates(analyzed_files)

            # é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼æ¤œå‡º
            similar_groups = await self._find_similar_content(analyzed_files)

            # çµæœçµ±åˆ
            all_duplicate_groups = exact_duplicates + similar_groups

            logger.info(
                f"ğŸ” é‡è¤‡æ¤œå‡ºå®Œäº†: {len(exact_duplicates)}å€‹ã®å®Œå…¨ä¸€è‡´, {len(similar_groups)}å€‹ã®é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—"
            )
            return all_duplicate_groups

        except Exception as e:
            logger.error(f"âŒ é‡è¤‡æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []

    async def _find_exact_duplicates(
        self, analyzed_files: List[Dict[str, Any]]
    ) -> List[List[str]]:
        """å®Œå…¨ä¸€è‡´æ¤œå‡º"""
        hash_groups = {}

        for file_analysis in analyzed_files:
            if "error" in file_analysis:
                continue

            content_hash = file_analysis.get("content_hash")
            file_path = file_analysis["basic_info"]["relative_path"]

            if content_hash:
                if content_hash not in hash_groups:
                    hash_groups[content_hash] = []
                hash_groups[content_hash].append(file_path)

        # 2ã¤ä»¥ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒã¤ã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿è¿”ã™
        return [group for group in hash_groups.values() if len(group) > 1]

    async def _find_similar_content(
        self, analyzed_files: List[Dict[str, Any]]
    ) -> List[List[str]]:
        """é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œå‡º"""
        # ç°¡ç•¥åŒ–å®Ÿè£…ï¼šã‚¿ã‚¤ãƒˆãƒ«ã¨æ§‹é€ ã®é¡ä¼¼æ€§ã§åˆ¤å®š
        title_groups = {}

        for file_analysis in analyzed_files:
            if "error" in file_analysis:
                continue

            content_analysis = file_analysis.get("content_analysis", {})
            main_title = content_analysis.get("main_title", "")
            file_path = file_analysis["basic_info"]["relative_path"]

            if main_title:
                # ã‚¿ã‚¤ãƒˆãƒ«ã®æ­£è¦åŒ–
                normalized_title = re.sub(r"[^\w\s]", "", main_title.lower()).strip()

                if normalized_title:
                    if normalized_title not in title_groups:
                        title_groups[normalized_title] = []
                    title_groups[normalized_title].append(file_path)

        # 2ã¤ä»¥ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒã¤ã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿è¿”ã™
        return [group for group in title_groups.values() if len(group) > 1]

class MigrationEngine:
    """ãƒ¡ã‚¤ãƒ³ç§»è¡Œã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, source_directory: Optional[Path] = None):
        """åˆæœŸåŒ–"""
        self.source_directory = source_directory or PROJECT_ROOT / "knowledge_base"
        self.database = GrimoireDatabase()
        self.search_engine = GrimoireVectorSearch()
        self.four_sages = FourSagesReviewer()
        self.analyzer = MDFileAnalyzer()
        self.duplicate_detector = DuplicateDetector()

        # è¨­å®š
        self.batch_size = 50
        self.concurrent_limit = 10

        # çµ±è¨ˆ
        self.total_files_found = 0
        self.total_files_processed = 0
        self.successful_migrations = 0
        self.failed_migrations = 0
        self.skipped_files = 0
        self.duplicate_files = 0

        logger.info("ğŸš€ Migration Engine initialized")

    async def initialize(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            await self.database.initialize()
            await self.search_engine.initialize()

            logger.info("âœ… Migration Engine ready")
            return True

        except Exception as e:
            logger.error(f"âŒ Migration Engine initialization failed: {e}")
            return False

    async def run_full_migration(self) -> Dict[str, Any]migration_start = datetime.now(timezone.utc)migration_id = f"migration_{migration_start.strftime('%Y%m%d_%H%M%S')}"
    """å…¨ç§»è¡Œå®Ÿè¡Œ"""
:
        logger.info(f"ğŸ›ï¸ å®Œå…¨ç§»è¡Œé–‹å§‹: {migration_id}")

        try:
            # Phase 1: ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹
            md_files = await self._discover_md_files()
            self.total_files_found = len(md_files)
            logger.info(f"ğŸ“‹ ç™ºè¦‹ã•ã‚ŒãŸMDãƒ•ã‚¡ã‚¤ãƒ«: {self.total_files_found}å€‹")

            # Phase 2: ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
            logger.info("ğŸ” Phase 2: ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æé–‹å§‹")
            analyzed_files = await self._analyze_files_batch(md_files)

            # Phase 3: é‡è¤‡æ¤œå‡º
            logger.info("ğŸ” Phase 3: é‡è¤‡æ¤œå‡ºé–‹å§‹")
            duplicate_groups = await self.duplicate_detector.detect_duplicates(
                analyzed_files
            )

            # Phase 4: 4è³¢è€…åˆ†é¡
            logger.info("ğŸ§™â€â™‚ï¸ Phase 4: 4è³¢è€…åˆ†é¡é–‹å§‹")
            classified_files = await self._classify_with_four_sages(analyzed_files)

            # Phase 5: é‡è¤‡å‡¦ç†æˆ¦ç•¥æ±ºå®š
            deduplication_plan = await self._create_deduplication_plan(
                duplicate_groups, classified_files
            )

            # Phase 6: ãƒãƒƒãƒç§»è¡Œå®Ÿè¡Œ
            logger.info("ğŸš€ Phase 6: ãƒãƒƒãƒç§»è¡Œå®Ÿè¡Œé–‹å§‹")
            migration_results = await self._execute_migration_batches(
                classified_files, deduplication_plan
            )

            # Phase 7: çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            migration_end = datetime.now(timezone.utc)
            final_report = await self._generate_migration_report(
                migration_id,
                migration_start,
                migration_end,
                migration_results,
                duplicate_groups,
            )

            logger.info(f"âœ… å®Œå…¨ç§»è¡Œå®Œäº†: {migration_id}")
            return final_report

        except Exception as e:
            logger.error(f"âŒ å®Œå…¨ç§»è¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "migration_id": migration_id,
                "status": "failed",
                "error": str(e),
                "completed_at": datetime.now(timezone.utc),
            }

    async def _discover_md_files(self) -> List[Path]:
        """MDãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹"""
        md_files = []

        try:
            # knowledge_baseé…ä¸‹ã®ã™ã¹ã¦ã®.mdãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            for md_file in self.source_directory.rglob("*.md"):
                if md_file.is_file():
                    md_files.append(md_file)

            # ã‚½ãƒ¼ãƒˆï¼ˆä¸€è²«ã—ãŸé †åºã§å‡¦ç†ï¼‰
            md_files.sort(key=lambda x: str(x))

            logger.info(f"ğŸ“‚ MDãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {len(md_files)}å€‹")
            return md_files

        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    async def _analyze_files_batch(self, md_files: List[Path]) -> List[Dict[str, Any]]:
        """ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æãƒãƒƒãƒå‡¦ç†"""
        analyzed_files = []

        # ã‚»ãƒãƒ•ã‚©ã§åŒæ™‚å®Ÿè¡Œæ•°åˆ¶é™
        semaphore = asyncio.Semaphore(self.concurrent_limit)

        async def analyze_single_file(file_pathPath) -> Dict[str, Any]:
    """analyze_single_fileåˆ†æãƒ¡ã‚½ãƒƒãƒ‰"""
            async with semaphore:
                analysis = await self.analyzer.analyze_file(file_path)
                analysis["file_path"] = str(file_path)
                return analysis

        # ä¸¦åˆ—åˆ†æå®Ÿè¡Œ
        tasks = [analyze_single_file(md_file) for md_file in md_files]
        analyzed_files = await asyncio.gather(*tasks, return_exceptions=True)

        # ä¾‹å¤–å‡¦ç†
        valid_analyses = []
        for i, result in enumerate(analyzed_files):
            if isinstance(result, Exception):
                logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æå¤±æ•— {md_files[i]}: {result}")
            else:
                valid_analyses.append(result)

        logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æå®Œäº†: {len(valid_analyses)}/{len(md_files)}å€‹")
        return valid_analyses

    async def _classify_with_four_sages(
        self, analyzed_files: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """4è³¢è€…ã«ã‚ˆã‚‹åˆ†é¡"""
        classified_files = []

        for file_analysis in analyzed_files:
            if "error" in file_analysis:
                classified_files.append(file_analysis)
                continue

            try:
                # 4è³¢è€…ã«ã‚ˆã‚‹è¿½åŠ åˆ†æ
                sage_classification = await self._get_sage_classification(file_analysis)
                file_analysis["sage_classification"] = sage_classification
                classified_files.append(file_analysis)

            except Exception as e:
                logger.error(
                    f"âŒ 4è³¢è€…åˆ†é¡ã‚¨ãƒ©ãƒ¼ {file_analysis.get('file_path', 'unknown')}: {e}"
                )
                file_analysis["sage_classification_error"] = str(e)
                classified_files.append(file_analysis)

        logger.info(f"ğŸ§™â€â™‚ï¸ 4è³¢è€…åˆ†é¡å®Œäº†: {len(classified_files)}å€‹")
        return classified_files

    async def _get_sage_classification(
        self, file_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """4è³¢è€…åˆ†é¡å–å¾—"""
        # ç°¡ç•¥åŒ–å®Ÿè£…ï¼šåˆ†æçµæœãƒ™ãƒ¼ã‚¹ã§åˆ†é¡
        classification = file_analysis.get("classification", {})
        aicompany_analysis = file_analysis.get("aicompany_analysis", {})

        # å„è³¢è€…ã®è¦³ç‚¹ã§ã®è©•ä¾¡
        sage_scores = {
            "knowledge_sage": self._calculate_knowledge_score(file_analysis),
            "task_oracle": self._calculate_task_score(file_analysis),
            "crisis_sage": self._calculate_crisis_score(file_analysis),
            "search_mystic": self._calculate_search_score(file_analysis),
        }

        # ä¸»æ‹…å½“è³¢è€…æ±ºå®š
        primary_sage = max(sage_scores.items(), key=lambda x: x[1])[0]

        return {
            "primary_sage": primary_sage,
            "sage_scores": sage_scores,
            "final_classification": classification,
            "ai_company_relevance": aicompany_analysis.get("sage_relevance_score", 0.0),
        }

    def _calculate_knowledge_score(self, file_analysis: Dict[str, Any]) -> float:
        """ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0.5  # ãƒ™ãƒ¼ã‚¹

        content_analysis = file_analysis.get("content_analysis", {})
        aicompany_analysis = file_analysis.get("aicompany_analysis", {})

        # çŸ¥è­˜çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        if content_analysis.get("headers", 0) > 3:
            score += 0.1
        if content_analysis.get("lists", 0) > 5:
            score += 0.1
        if content_analysis.get("links", 0) > 3:
            score += 0.1

        # Elders GuildçŸ¥è­˜
        if aicompany_analysis.get("sage_references", 0) > 0:
            score += 0.2

        return min(score, 1.0)

    def _calculate_task_score(self, file_analysis: Dict[str, Any]) -> float:
        """ã‚¿ã‚¹ã‚¯è³¢è€…ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0.1  # ãƒ™ãƒ¼ã‚¹

        aicompany_analysis = file_analysis.get("aicompany_analysis", {})
        basic_info = file_analysis.get("basic_info", {})

        # ã‚¿ã‚¹ã‚¯é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        if aicompany_analysis.get("workflow_content", 0) > 0:
            score += 0.3
        if "task" in basic_info.get("file_name", "").lower():
            score += 0.2
        if aicompany_analysis.get("technical_commands", 0) > 2:
            score += 0.2

        return min(score, 1.0)

    def _calculate_crisis_score(self, file_analysis: Dict[str, Any]) -> float:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0.1  # ãƒ™ãƒ¼ã‚¹

        aicompany_analysis = file_analysis.get("aicompany_analysis", {})
        basic_info = file_analysis.get("basic_info", {})

        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆé–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        if aicompany_analysis.get("incident_content", 0) > 0:
            score += 0.4
        if any(
            keyword in basic_info.get("file_name", "").lower()
            for keyword in ["error", "fix", "trouble", "incident"]
        ):
            score += 0.3

        return min(score, 1.0)

    def _calculate_search_score(self, file_analysis: Dict[str, Any]) -> float:
        """RAGè³¢è€…ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0.2  # ãƒ™ãƒ¼ã‚¹

        content_analysis = file_analysis.get("content_analysis", {})
        basic_info = file_analysis.get("basic_info", {})

        # æ¤œç´¢ãƒ»ç™ºè¦‹å¯èƒ½æ€§
        if content_analysis.get("links", 0) > 5:
            score += 0.2
        if basic_info.get("word_count", 0) > 1000:
            score += 0.1
        if content_analysis.get("technical_density", 0) > 0.3:
            score += 0.2

        return min(score, 1.0)

    async def _create_deduplication_plan(
        self, duplicate_groups: List[List[str]], classified_files: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """é‡è¤‡é™¤å»è¨ˆç”»ä½œæˆ"""
        plan = {
            "duplicate_groups": duplicate_groups,
            "merge_candidates": [],
            "skip_duplicates": [],
            "manual_review_required": [],
        }

        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ãƒãƒƒãƒ—ä½œæˆ
        file_info_map = {}
        for file_analysis in classified_files:
            if "error" not in file_analysis:
                file_path = file_analysis.get("file_path", "")
                basic_info = file_analysis.get("basic_info", {})
                file_info_map[basic_info.get("relative_path", file_path)] = (
                    file_analysis
                )

        # é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®å‡¦ç†è¨ˆç”»
        for group in duplicate_groups:
            if len(group) < 2:
                continue

            # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã®ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«æ±ºå®š
            master_candidate = await self._select_master_file(group, file_info_map)

            group_plan = {
                "files": group,
                "master_file": master_candidate,
                "action": "merge" if len(group) <= 3 else "manual_review",
            }

            if group_plan["action"] == "merge":
                plan["merge_candidates"].append(group_plan)
            else:
                plan["manual_review_required"].append(group_plan)

        logger.info(
            f"ğŸ“‹ é‡è¤‡é™¤å»è¨ˆç”»ä½œæˆå®Œäº†: {len(plan['merge_candidates'])}å€‹ã®è‡ªå‹•çµ±åˆ, "
            f"{len(plan['manual_review_required'])}å€‹ã®æ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼"
        )

        return plan

    async def _select_master_file(
        self, duplicate_group: List[str], file_info_map: Dict[str, Any]
    ) -> str:
        """ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«é¸å®š"""
        scored_files = []

        for file_path in duplicate_group:
            file_info = file_info_map.get(file_path)
            if not file_info:
                continue

            score = 0
            basic_info = file_info.get("basic_info", {})
            classification = file_info.get("classification", {})

            # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°åŸºæº–
            score += basic_info.get("char_count", 0) / 1000  # ã‚µã‚¤ã‚º
            score += classification.get("power_level", 1)  # å¨åŠ›ãƒ¬ãƒ™ãƒ«
            score += classification.get("confidence_score", 0) * 10  # ä¿¡é ¼åº¦

            # ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚ˆã‚‹å„ªå…ˆåº¦
            file_name = basic_info.get("file_name", "").lower()
            if "claude" in file_name:
                score += 5
            if "guide" in file_name:
                score += 3
            if "readme" in file_name:
                score += 4

            scored_files.append((file_path, score))

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
        if scored_files:
            return max(scored_files, key=lambda x: x[1])[0]
        else:
            return duplicate_group[0]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    async def _execute_migration_batches(
        self, classified_files: List[Dict[str, Any]], deduplication_plan: Dict[str, Any]
    ) -> List[MigrationResult]:
        """ãƒãƒƒãƒç§»è¡Œå®Ÿè¡Œ"""
        migration_results = []

        # ã‚¹ã‚­ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
        skip_files = set()
        for group_plan in deduplication_plan.get("merge_candidates", []):
            master_file = group_plan["master_file"]
            for file_path in group_plan["files"]:
                if file_path != master_file:
                    skip_files.add(file_path)

        # ãƒãƒƒãƒå‡¦ç†
        for i in range(0, len(classified_files), self.batch_size):
            batch = classified_files[i : i + self.batch_size]
            batch_id = f"batch_{i//self.batch_size + 1}"

            logger.info(f"ğŸš€ ãƒãƒƒãƒ{batch_id}å‡¦ç†é–‹å§‹: {len(batch)}ãƒ•ã‚¡ã‚¤ãƒ«")

            batch_results = await self._process_migration_batch(
                batch, skip_files, batch_id
            )
            migration_results.extend(batch_results)

            # ãƒãƒƒãƒé–“ã®ä¼‘æ†©
            await asyncio.sleep(0.1)

        return migration_results

    async def _process_migration_batch(
        self, batch: List[Dict[str, Any]], skip_files: Set[str], batch_id: str
    ) -> List[MigrationResult]:
        """å˜ä¸€ãƒãƒƒãƒå‡¦ç†"""
        batch_results = []

        for file_analysis in batch:
            result = await self._migrate_single_file(file_analysis, skip_files)
            batch_results.append(result)

            # çµ±è¨ˆæ›´æ–°
            self.total_files_processed += 1
            if result.status == MigrationStatus.COMPLETED:
                self.successful_migrations += 1
            elif result.status == MigrationStatus.FAILED:
                self.failed_migrations += 1
            elif result.status == MigrationStatus.SKIPPED:
                self.skipped_files += 1

        logger.info(f"âœ… ãƒãƒƒãƒ{batch_id}å®Œäº†: {len(batch_results)}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†")
        return batch_results

    async def _migrate_single_file(
        self, file_analysis: Dict[str, Any], skip_files: Set[str]
    ) -> MigrationResult:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ç§»è¡Œ"""
        file_path = file_analysis.get("file_path", "")

        try:
            # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¹ã‚­ãƒƒãƒ—
            if "error" in file_analysis:
                return MigrationResult(
                    file_path=file_path,
                    status=MigrationStatus.FAILED,
                    error_message=file_analysis["error"],
                )

            basic_info = file_analysis.get("basic_info", {})
            relative_path = basic_info.get("relative_path", file_path)

            # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¹ã‚­ãƒƒãƒ—
            if relative_path in skip_files:
                return MigrationResult(
                    file_path=file_path,
                    status=MigrationStatus.SKIPPED,
                    error_message="Duplicate file - skipped in favor of master",
                )

            # å‘ªæ–‡ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
            spell_data = await self._build_spell_data(file_analysis)

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
            spell_id = str(uuid.uuid4())
            await self.search_engine.index_spell(spell_id, spell_data)

            return MigrationResult(
                file_path=file_path,
                status=MigrationStatus.COMPLETED,
                spell_id=spell_id,
                original_hash=file_analysis.get("content_hash"),
                migration_timestamp=datetime.now(timezone.utc),
                metadata=spell_data,
                sage_classification=file_analysis.get("sage_classification"),
            )

        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ç§»è¡Œã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return MigrationResult(
                file_path=file_path, status=MigrationStatus.FAILED, error_message=str(e)
            )

    async def _build_spell_data(self, file_analysis: Dict[str, Any]) -> Dict[str, Any]basic_info = file_analysis.get("basic_info", {})classification = file_analysis.get("classification", {})
    """æ–‡ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰"""
        sage_classification = file_analysis.get("sage_classification", {})

        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹èª­ã¿è¾¼ã¿
        file_path = Path(file_analysis["file_path"])
        content = file_path.read_text(encoding="utf-8")

        # ã‚¹ãƒšãƒ«ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰ - Enumå‡¦ç†ã‚’å®‰å…¨ã«
        spell_type = classification.get("spell_type", SpellType.KNOWLEDGE)
        spell_type_value = (
            spell_type.value if hasattr(spell_type, "value") else str(spell_type)
        )

        spell_data = {:
            "spell_name": self._generate_spell_name(basic_info, classification),
            "content": content,
            "spell_type": spell_type_value,
            "magic_school": sage_classification.get("primary_sage", "knowledge_sage"),
            "tags": classification.get("suggested_tags", []),
            "power_level": classification.get("power_level", 1),
            "is_eternal": classification.get("is_eternal", False),
            "casting_frequency": 0,  # åˆæœŸå€¤
            "original_file_path": basic_info.get("relative_path", ""),
            "migration_metadata": {
                "migrated_at": datetime.now(timezone.utc).isoformat(),
                "original_size": basic_info.get("file_size", 0),
                "confidence_score": classification.get("confidence_score", 0.5),
                "sage_scores": sage_classification.get("sage_scores", {}),
                "ai_company_relevance": sage_classification.get(
                    "ai_company_relevance", 0.0
                ),
            },
        }

        return spell_data

    def _generate_spell_name(
        self, basic_info: Dict[str, Any], classification: Dict[str, Any]
    ) -> str:
        """å‘ªæ–‡åç”Ÿæˆ"""
        file_name = basic_info.get("file_name", "unknown.md")

        # æ‹¡å¼µå­é™¤å»
        spell_name = Path(file_name).stem

        # èª­ã¿ã‚„ã™ãæ•´å½¢
        spell_name = spell_name.replace("_", " ").replace("-", " ")
        spell_name = " ".join(word.capitalize() for word in spell_name.split())

        # Elders Guildå›ºæœ‰ã®èª¿æ•´
        if "claude" in spell_name.lower():
            spell_name = spell_name.replace("Claude", "ğŸ¤– Claude")
        if "tdd" in spell_name.lower():
            spell_name = spell_name.replace("TDD", "ğŸ§ª TDD")
        if "guide" in spell_name.lower():
            spell_name = f"ğŸ“– {spell_name}"

        return spell_name

    async def _generate_migration_report(
        self,
        migration_id: str,
        start_time: datetime,
        end_time: datetime,
        migration_results: List[MigrationResult],
        duplicate_groups: List[List[str]],
    ) -> Dict[str, Any]:
        """ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        duration = end_time - start_time

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥çµ±è¨ˆ
        status_counts = {}
        for status in MigrationStatus:
            status_counts[status.value] = sum(
                1 for r in migration_results if r.status == status
            )

        # æˆåŠŸç‡è¨ˆç®—
        success_rate = (
            status_counts.get("completed", 0) / max(len(migration_results), 1)
        ) * 100

        # è³¢è€…åˆ¥çµ±è¨ˆ
        sage_distribution = {}
        for result in migration_results:
            if result.sage_classification:
                primary_sage = result.sage_classification.get("primary_sage", "unknown")
                sage_distribution[primary_sage] = (
                    sage_distribution.get(primary_sage, 0) + 1
                )

        report = {
            "migration_id": migration_id,
            "status": "completed",
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration_seconds": duration.total_seconds(),
            "summary": {
                "total_files_discovered": self.total_files_found,
                "total_files_processed": len(migration_results),
                "successful_migrations": status_counts.get("completed", 0),
                "failed_migrations": status_counts.get("failed", 0),
                "skipped_files": status_counts.get("skipped", 0),
                "success_rate_percent": round(success_rate, 2),
            },
            "status_breakdown": status_counts,
            "sage_distribution": sage_distribution,
            "duplicate_detection": {
                "total_duplicate_groups": len(duplicate_groups),
                "total_duplicate_files": sum(len(group) for group in duplicate_groups),
            },
            "migration_results": [asdict(result) for result in migration_results],
        }

        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_path = PROJECT_ROOT / f"migration_reports/{migration_id}_report.json"
        report_path.parent.mkdir(exist_ok=True)

        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)

        logger.info(f"ğŸ“Š ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        logger.info(
            f"âœ… ç§»è¡Œå®Œäº†: {status_counts.get('completed', 0)}/{len(migration_results)}ãƒ•ã‚¡ã‚¤ãƒ« "
            f"(æˆåŠŸç‡: {success_rate:0.1f}%)"
        )

        return report

    async def close(self)await self.database.close()
    """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒ­ãƒ¼ã‚º"""
        logger.info("ğŸ›ï¸ Migration Engine closed")

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
async def test_migration_engine()migration_engine = MigrationEngine()
"""ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    try:
        await migration_engine.initialize()

        # ã‚µãƒ³ãƒ—ãƒ«ç§»è¡Œå®Ÿè¡Œ
        report = await migration_engine.run_full_migration()

        print(f"âœ… ç§»è¡Œãƒ†ã‚¹ãƒˆå®Œäº†")
        print(f"ğŸ“Š æˆåŠŸ: {report['summary']['successful_migrations']}ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"âŒ å¤±æ•—: {report['summary']['failed_migrations']}ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {report['summary']['skipped_files']}ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {report['summary']['success_rate_percent']}%")

    finally:
        await migration_engine.close()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(test_migration_engine())
