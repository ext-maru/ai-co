#!/usr/bin/env python3
"""
pgvector å¾¹åº•æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æœ¬å½“ã«å‹•ã„ã¦ã„ã‚‹ã‹å®Œå…¨ã«ç¢ºèªã™ã‚‹
"""

import os
import sys
import asyncio
import asyncpg
import numpy as np
from datetime import datetime
import json
import time

# OpenAIè¨­å®š
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print("âŒ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    sys.exit(1)

from openai import OpenAI

client = OpenAI(api_key=OPENAI_API_KEY)


async def thorough_test():
    """å¾¹åº•çš„ãªå‹•ä½œæ¤œè¨¼"""

    print("ğŸ”¬ pgvector å¾¹åº•æ¤œè¨¼é–‹å§‹")
    print("=" * 80)

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    conn = await asyncpg.connect(
        host="localhost",
        port=5432,
        database="elders_knowledge",
        user="elders_guild",
        password="elders_2025",
    )

    try:
        # 1.0 æ‹¡å¼µæ©Ÿèƒ½ã®ç¢ºèª
        print("1ï¸âƒ£ pgvectoræ‹¡å¼µæ©Ÿèƒ½ã®ç¢ºèª")
        extensions = await conn.fetch(
            """
            SELECT extname, extversion
            FROM pg_extension
            WHERE extname = 'vector'
        """
        )

        if extensions:
            print(
                f"âœ… pgvector v{extensions[0]['extversion']} ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™"
            )
        else:
            print("âŒ pgvectorãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False

        # 2.0 ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã®ç¢ºèª
        print("\n2ï¸âƒ£ ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã®ç¢ºèª")
        columns = await conn.fetch(
            """
            SELECT column_name, data_type, udt_name
            FROM information_schema.columns
            WHERE table_schema = 'knowledge_base'
            AND table_name = 'vector_documents'
            ORDER BY ordinal_position
        """
        )

        print("ã‚«ãƒ©ãƒ æƒ…å ±:")
        for col in columns:
            print(f"  - {col['column_name']}: {col['data_type']} ({col['udt_name']})")

        # vectorã‚«ãƒ©ãƒ ã®ç¢ºèª
        vector_col = [c for c in columns if c["udt_name"] == "vector"]
        if vector_col:
            print("âœ… vectorã‚«ãƒ©ãƒ ãŒæ­£ã—ãå®šç¾©ã•ã‚Œã¦ã„ã¾ã™")
        else:
            print("âŒ vectorã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # 3.0 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¢ºèª
        print("\n3ï¸âƒ£ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¢ºèª")
        indexes = await conn.fetch(
            """
            SELECT
                indexname,
                indexdef
            FROM pg_indexes
            WHERE schemaname = 'knowledge_base'
            AND tablename = 'vector_documents'
        """
        )

        for idx in indexes:
            print(f"  - {idx['indexname']}")
            if "ivfflat" in idx["indexdef"]:
                print("    âœ… IVFFlat ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹")
            elif "hnsw" in idx["indexdef"]:
                print("    âœ… HNSW ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹")

        # 4.0 æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        print("\n4ï¸âƒ£ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª")
        count = await conn.fetchval(
            "SELECT COUNT(*) FROM knowledge_base.vector_documents"
        )
        print(f"ç¾åœ¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {count}")

        # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢
        await conn.execute("TRUNCATE knowledge_base.vector_documents")
        print("æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")

        # 5.0 å¤šæ§˜ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æŠ•å…¥
        print("\n5ï¸âƒ£ å¤šæ§˜ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æŠ•å…¥")
        test_data = [
            # ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰é–¢é€£
            "ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã¯4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ ã§æ§‹æˆã•ã‚Œã‚‹éšå±¤çš„ãªé–‹ç™ºçµ„ç¹”",
            "ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ã¯éå»ã®çŸ¥è­˜ã‚’è“„ç©ã—ã€æœªæ¥ã®é–‹ç™ºã«æ´»ã‹ã™",
            "ã‚¿ã‚¹ã‚¯è³¢è€…ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²æ—ç®¡ç†ã¨å„ªå…ˆé †ä½ä»˜ã‘ã‚’è¡Œã†",
            "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ã¯å±æ©Ÿå¯¾å¿œã¨å•é¡Œè§£æ±ºã®ã‚¹ãƒšã‚·ãƒ£ãƒªã‚¹ãƒˆ",
            "RAGè³¢è€…ã¯æƒ…å ±æ¤œç´¢ã¨çŸ¥è­˜çµ±åˆã‚’æ‹…å½“ã™ã‚‹",
            # æŠ€è¡“é–¢é€£
            "TDDï¼ˆãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™ºï¼‰ã¯Red-Green-Refactorã®ã‚µã‚¤ã‚¯ãƒ«ã§å“è³ªã‚’ä¿è¨¼",
            "pgvectorã¯PostgreSQLã§é«˜é€Ÿãªãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿç¾ã™ã‚‹æ‹¡å¼µæ©Ÿèƒ½",
            "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¯æ„å‘³çš„ãªé¡ä¼¼æ€§ã«åŸºã¥ãæ¤œç´¢æ‰‹æ³•",
            # å…¨ãé–¢ä¿‚ãªã„å†…å®¹
            "ä»Šæ—¥ã®å¤©æ°—ã¯æ™´ã‚Œã§ã™",
            "çŒ«ã¯ã‹ã‚ã„ã„å‹•ç‰©ã§ã™",
        ]

        print("Embeddingç”Ÿæˆä¸­...")
        start_time = time.time()

        for i, text in enumerate(test_data):
            # Embeddingç”Ÿæˆ
            response = client.embeddings.create(
                model="text-embedding-ada-002", input=text
            )
            embedding = response.data[0].embedding

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            await conn.execute(
                """
                INSERT INTO knowledge_base.vector_documents
                (title, content, embedding, metadata)
                VALUES ($1, $2, $3::vector, $4)
            """,
                f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{i+1}",
                text,
                str(embedding),
                json.dumps({"category": "test", "index": i}),
            )
            print(f"  âœ… {i+1}/{len(test_data)} å®Œäº†")

        elapsed = time.time() - start_time
        print(f"æŠ•å…¥æ™‚é–“: {elapsed:0.2f}ç§’")

        # 6.0 æ§˜ã€…ãªã‚¯ã‚¨ãƒªã§ã®æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        print("\n6ï¸âƒ£ æ§˜ã€…ãªã‚¯ã‚¨ãƒªã§ã®æ¤œç´¢ãƒ†ã‚¹ãƒˆ")

        test_queries = [
            "4è³¢è€…ã«ã¤ã„ã¦æ•™ãˆã¦",
            "ãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™ºã®æ–¹æ³•",
            "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã¯",
            "å±æ©Ÿç®¡ç†ã«ã¤ã„ã¦",
            "å‹•ç‰©ã«ã¤ã„ã¦",
        ]

        # ç¹°ã‚Šè¿”ã—å‡¦ç†
        for query in test_queries:
            print(f"\nğŸ” ã‚¯ã‚¨ãƒª: '{query}'")

            # ã‚¯ã‚¨ãƒªã®embeddingç”Ÿæˆ
            start_time = time.time()
            response = client.embeddings.create(
                model="text-embedding-ada-002", input=query
            )
            query_embedding = response.data[0].embedding
            embedding_time = time.time() - start_time

            # æ¤œç´¢å®Ÿè¡Œ
            start_time = time.time()
            results = await conn.fetch(
                """
                SELECT
                    title,
                    content,
                    1 - (embedding <=> $1::vector) as similarity,
                    metadata
                FROM knowledge_base.vector_documents
                ORDER BY embedding <=> $1::vector
                LIMIT 3
            """,
                str(query_embedding),
            )
            search_time = time.time() - start_time

            print(f"  Embeddingç”Ÿæˆ: {embedding_time:0.3f}ç§’")
            print(f"  æ¤œç´¢å®Ÿè¡Œ: {search_time:0.3f}ç§’")
            print("  çµæœ:")
            for j, row in enumerate(results):
                print(f"    {j+1}. {row['title']} (é¡ä¼¼åº¦: {row['similarity']:0.4f})")
                print(f"       {row['content'][:50]}...")

        # 7.0 ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ã®ç¢ºèª
        print("\n7ï¸âƒ£ ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ã®ç¢ºèª")

        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®æ‰‹å‹•è¨ˆç®—ã¨æ¯”è¼ƒ
        first_doc = await conn.fetchrow(
            """
            SELECT embedding::text
            FROM knowledge_base.vector_documents
            LIMIT 1
        """
        )

        if first_doc:
            # æ–‡å­—åˆ—ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’å¾©å…ƒ
            vec_str = first_doc["embedding"]
            vec_list = [float(x) for x in vec_str.strip("[]").split(",")]
            vec_array = np.array(vec_list)

            # è‡ªå·±é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆ1.0ã«ãªã‚‹ã¯ãšï¼‰
            self_similarity = await conn.fetchval(
                """
                SELECT 1 - (embedding <=> embedding)
                FROM knowledge_base.vector_documents
                LIMIT 1
            """
            )

            print(f"è‡ªå·±é¡ä¼¼åº¦: {self_similarity:0.6f} (æœŸå¾…å€¤: 1.0)")
            if abs(self_similarity - 1.0) < 0.0001:
                print("âœ… ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ãŒæ­£ã—ãå‹•ä½œã—ã¦ã„ã¾ã™")
            else:
                print("âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")

        # 8.0 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        print("\n8ï¸âƒ£ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ")

        # 100å›ã®æ¤œç´¢ã‚’å®Ÿè¡Œ
        total_time = 0
        iterations = 100

        test_embedding = str([0.1] * 1536)  # ãƒ€ãƒŸãƒ¼ã®embedding

        for _ in range(iterations):
            start_time = time.time()
            await conn.fetch(
                """
                SELECT title
                FROM knowledge_base.vector_documents
                ORDER BY embedding <=> $1::vector
                LIMIT 1
            """,
                test_embedding,
            )
            total_time += time.time() - start_time

        avg_time = total_time / iterations
        print(f"å¹³å‡æ¤œç´¢æ™‚é–“: {avg_time*1000:0.2f}ms ({iterations}å›ã®å¹³å‡)")

        if avg_time < 0.01:  # 10msä»¥ä¸‹
            print("âœ… å„ªç§€ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
        elif avg_time < 0.1:  # 100msä»¥ä¸‹
            print("âš ï¸ è¨±å®¹ç¯„å›²ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
        else:
            print("âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")

        # 9.0 ç·åˆåˆ¤å®š
        print("\n=" * 80)
        print("ğŸ æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 80)

        all_checks = {
            "pgvectoræ‹¡å¼µæ©Ÿèƒ½": True,
            "ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ": True,
            "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹": len(indexes) > 0,
            "Embeddingç”Ÿæˆ": True,
            "ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜": True,
            "é¡ä¼¼æ¤œç´¢": True,
            "ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—": (
                abs(self_similarity - 1.0) < 0.0001
                if "self_similarity" in locals()
                else False
            ),
            "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹": avg_time < 0.1,
        }

        for check, result in all_checks.items():
            status = "âœ…" if result else "âŒ"
            print(f"{status} {check}")

        if all(all_checks.values()):
            print("\nğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã«åˆæ ¼ï¼pgvectorã¯å®Œå…¨ã«å‹•ä½œã—ã¦ã„ã¾ã™ï¼")
            return True
        else:
            print("\nâš ï¸ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
            return False

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback

        traceback.print_exc()
        return False

    finally:
        await conn.close()


if __name__ == "__main__":
    success = asyncio.run(thorough_test())
    sys.exit(0 if success else 1)
