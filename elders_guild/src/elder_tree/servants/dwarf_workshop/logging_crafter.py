#!/usr/bin/env python3
"""
ğŸ“ Logging Crafter Servant (D14)
================================

ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰å°‚é–€ã®ãƒ‰ãƒ¯ãƒ¼ãƒ•ã‚µãƒ¼ãƒãƒ³ãƒˆã€‚
ãƒ­ã‚°è¨­å®šã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã€åˆ†æãƒ„ãƒ¼ãƒ«ã®å®Ÿè£…ã‚’æ‹…å½“ã€‚

Author: Claude Elder
Created: 2025-07-23
"""

import asyncio
import json
import re
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from collections import Counter, defaultdict
from pathlib import Path
import textwrap

from ..base import DwarfServant, ServantCapability


class LoggingCrafterServant(DwarfServant):
    """
    Logging Crafter - ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ å°‚é–€å®¶
    
    ä¸»ãªè²¬å‹™ï¼š
    - ãƒ­ã‚°è¨­å®šã®ç”Ÿæˆã¨æœ€é©åŒ–
    - ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã®å®Ÿè£…
    - ãƒ­ã‚°åˆ†æãƒ„ãƒ¼ãƒ«ã®æä¾›
    - ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ»ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆ
    """
    
    def __init__(self):
        super().__init__(
            servant_id="D14",
            name="Logging Crafter",
            specialization="ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ãƒ»è¨­å®šãƒ»åˆ†æ"
        )
        
        # èƒ½åŠ›å®šç¾©
        self.capabilities = [
            ServantCapability.CODE_GENERATION,
            ServantCapability.MONITORING,
            ServantCapability.PERFORMANCE_TUNING,
            ServantCapability.SAGE_INTEGRATION
        ]
        
        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°
        self.log_levels = {
            "debug": "DEBUG",
            "info": "INFO",
            "warning": "WARNING",
            "error": "ERROR",
            "critical": "CRITICAL"
        }
        
    async def generate_config(self, requirements: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ­ã‚°è¨­å®šã‚’ç”Ÿæˆ"""
        try:
            environment = requirements.get("environment", "development")
            output = requirements.get("output", "console")
            level = requirements.get("level", "info")
            
            # åŸºæœ¬è¨­å®š
            config = {
                "version": 1,
                "disable_existing_loggers": False,
                "level": self.log_levels.get(level.lower(), "INFO")
            }
            
            # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
            handlers = []
            if isinstance(output, str):
                output = [output]
            elif not isinstance(output, list):
                output = ["console"]
                
            config["handlers"] = {}
            
            # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            if "console" in output:
                config["handlers"]["console"] = {
                    "class": "logging.StreamHandler",
                    "level": config["level"],
                    "formatter": "standard",
                    "stream": "ext://sys.stdout"
                }
                handlers.append("console")
                
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            if "file" in output:
                config["handlers"]["file"] = {
                    "class": "logging.FileHandler",
                    "level": config["level"],
                    "formatter": "standard",
                    "filename": requirements.get("file_path", "app.log"),
                    "mode": "a",
                    "encoding": "utf-8"
                }
                handlers.append("file")
                
            # syslogãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆæœ¬ç•ªç’°å¢ƒï¼‰
            if "syslog" in output:
                config["handlers"]["syslog"] = {
                    "class": "logging.handlers.SysLogHandler",
                    "level": config["level"],
                    "formatter": "syslog",
                    "address": "/dev/log"
                }
                handlers.append("syslog")
                
            # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
            if requirements.get("rotation") == "daily":
                retention_days = 30  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                if requirements.get("retention"):
                    # "30 days" ã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
                    retention_match = re.search(r'(\d+)', requirements["retention"])
                    if retention_match:
                        retention_days = int(retention_match.group(1))
                        
                config["rotation"] = {
                    "when": "midnight",
                    "interval": 1,
                    "backup_count": retention_days
                }
                
                # ãƒ­ãƒ¼ãƒ†ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«å¤‰æ›´
                if "file" in config["handlers"]:
                    config["handlers"]["file"]["class"] = "logging.handlers.TimedRotatingFileHandler"
                    config["handlers"]["file"]["when"] = "midnight"
                    config["handlers"]["file"]["backupCount"] = retention_days
                    
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼è¨­å®š
            config["formatters"] = {
                "standard": {
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
                },
                "syslog": {
                    "format": "%(name)s[%(process)d]: %(levelname)s %(message)s"
                }
            }
            
            # JSONå½¢å¼ã®å ´åˆ
            if requirements.get("format") == "json":
                config["format_type"] = "json"
                config["fields"] = requirements.get("fields", [
                    "timestamp", "level", "logger", "message"
                ])
                config["enrichment"] = {
                    "enabled": requirements.get("enrichment", False)
                }
                
            # ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼è¨­å®š
            config["root"] = {
                "level": config["level"],
                "handlers": handlers
            }
            
            return {
                "success": True,
                "config": config,
                "handlers": handlers
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to generate config: {str(e)}"
            }
            
    async def implement_handler(self, handler_config: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å®Ÿè£…"""
        try:
            handler_type = handler_config.get("type", "file")
            
            if handler_type == "file":
                implementation = self._implement_file_handler(handler_config)
            elif handler_type == "rotating":
                implementation = self._implement_rotating_handler(handler_config)
            elif handler_type == "custom":
                implementation = self._implement_custom_handler(handler_config)
            else:
                return {
                    "success": False,
                    "error": f"Unknown handler type: {handler_type}"
                }
                
            return {
                "success": True,
                "implementation": implementation
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to implement handler: {str(e)}"
            }
            
    def _implement_file_handler(self, config: Dict[str, Any]) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
        filename = config.get("filename", "app.log")
        mode = config.get("mode", "a")
        encoding = config.get("encoding", "utf-8")
        
        return f"""
import logging

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¨­å®š
file_handler = logging.FileHandler(
    filename='{filename}',
    mode='{mode}',
    encoding='{encoding}'
)

# ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’è¨­å®š
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
file_handler.setFormatter(formatter)

# ãƒ­ã‚¬ãƒ¼ã«è¿½åŠ 
logger = logging.getLogger()
logger.addHandler(file_handler)
"""
        
    def _implement_rotating_handler(self, config: Dict[str, Any]) -> str:
        """ãƒ­ãƒ¼ãƒ†ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
        filename = config.get("filename", "app.log")
        max_bytes = config.get("max_bytes", 10485760)
        backup_count = config.get("backup_count", 5)
        
        return f"""
import logging
from logging.handlers import RotatingFileHandler

# ãƒ­ãƒ¼ãƒ†ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¨­å®š
rotating_handler = RotatingFileHandler(
    filename='{filename}',
    maxBytes={max_bytes},
    backupCount={backup_count},
    encoding='utf-8'
)

# ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’è¨­å®š
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
rotating_handler.setFormatter(formatter)

# ãƒ­ã‚¬ãƒ¼ã«è¿½åŠ 
logger = logging.getLogger()
logger.addHandler(rotating_handler)
"""
        
    def _implement_custom_handler(self, config: Dict[str, Any]) -> str:
        """ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
        class_name = config.get("class", "CustomHandler")
        params = config.get("params", {})
        
        # ElasticSearchãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ä¾‹
        if class_name == "ElasticSearchHandler":
            hosts = params.get("hosts", ["localhost:9200"])
            index = params.get("index", "application-logs")
            
            return f"""
import logging
import json
from datetime import datetime
from elasticsearch import Elasticsearch

class ElasticSearchHandler(logging.Handler):
    \"\"\"ElasticSearchã¸ãƒ­ã‚°ã‚’é€ä¿¡ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼\"\"\"
    
    def __init__(self, hosts, index_name):
        super().__init__()
        self.es = Elasticsearch(hosts)
        self.index_name = index_name
        
    def emit(self, record):
        try:
            # ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ElasticSearchç”¨ã«å¤‰æ›
            doc = {{
                'timestamp': datetime.utcnow(),
                'level': record.levelname,
                'logger': record.name,
                'message': record.getMessage(),
                'module': record.module,
                'function': record.funcName,
                'line': record.lineno
            }}
            
            # extra ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
            if hasattr(record, 'extra'):
                doc.update(record.extra)
                
            # ElasticSearchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            self.es.index(
                index=self.index_name,
                body=doc
            )
        except Exception as e:
            self.handleError(record)

# ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã¨è¨­å®š
es_handler = ElasticSearchHandler(
    hosts={hosts},
    index_name='{index}'
)

# ãƒ­ã‚¬ãƒ¼ã«è¿½åŠ 
logger = logging.getLogger()
logger.addHandler(es_handler)
"""
        
        # æ±ç”¨ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        return f"""
class {class_name}(logging.Handler):
    \"\"\"ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼\"\"\"
    
    def __init__(self, **kwargs):
        super().__init__()
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        {self._format_params(params)}
        
    def emit(self, record):
        \"\"\"ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‡¦ç†\"\"\"
        try:
            # ã‚«ã‚¹ã‚¿ãƒ å‡¦ç†ã‚’ã“ã“ã«å®Ÿè£…
            msg = self.format(record)
            # TODO: å®Ÿéš›ã®é€ä¿¡å‡¦ç†ã‚’å®Ÿè£…
            pass
        except Exception:
            self.handleError(record)
"""
        
    def _format_params(self, params: Dict[str, Any]) -> str:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›"""
        lines = []
        for key, value in params.items():
            if isinstance(value, str):
                lines.append(f"self.{key} = '{value}'")
            else:
                lines.append(f"self.{key} = {value}")
        return "\n        ".join(lines)
        
    async def create_formatter(self, format_config: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’ä½œæˆ"""
        try:
            format_type = format_config.get("type", "basic")
            
            if format_type == "basic":
                formatter = self._create_basic_formatter(format_config)
            elif format_type == "json":
                formatter = self._create_json_formatter(format_config)
            else:
                return {
                    "success": False,
                    "error": f"Unknown formatter type: {format_type}"
                }
                
            return {
                "success": True,
                "formatter": formatter
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to create formatter: {str(e)}"
            }
            
    def _create_basic_formatter(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’ä½œæˆ"""
        pattern = config.get("pattern", "%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        
        return {
            "type": "basic",
            "pattern": pattern,
            "implementation": f"""
formatter = logging.Formatter('{pattern}')
"""
        }
        
    def _create_json_formatter(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """JSONãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’ä½œæˆ"""
        fields = config.get("fields", {
            "timestamp": "%(asctime)s",
            "level": "%(levelname)s",
            "logger": "%(name)s",
            "message": "%(message)s"
        })
        
        implementation = """
import json
import logging

class JSONFormatter(logging.Formatter):
    \"\"\"JSONå½¢å¼ã§ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼\"\"\"
    
    def format(self, record):
        log_data = {
"""
        
        for field, pattern in fields.items():
            implementation += f"            '{field}': self._format_field(record, '{pattern}'),\n"
            
        implementation += """        }
        
        # extra ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
        if hasattr(record, 'extra'):
            log_data.update(record.extra)
            
        return json.dumps(log_data, ensure_ascii=False)
        
    def _format_field(self, record, pattern):
        \"\"\"å€‹åˆ¥ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\"\"\"
        if pattern.startswith('%(') and pattern.endswith(')s'):
            attr = pattern[2:-2]
            return getattr(record, attr, '')
        return pattern

# ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
json_formatter = JSONFormatter()
"""
        
        return {
            "type": "json",
            "fields": fields,
            "implementation": implementation
        }
        
    async def analyze_patterns(self, log_samples: List[str]) -> Dict[str, Any]:
        """ãƒ­ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        try:
            patterns = []
            
            # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
            error_pattern = re.compile(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\s+(\w+)\s+(.*)')
            
            # ãƒ­ã‚°ã‚’ãƒ‘ãƒ¼ã‚¹
            parsed_logs = []
            for log in log_samples:
                match = error_pattern.match(log)
                if match:
                    parsed_logs.append({
                        "timestamp": match.group(1),
                        "level": match.group(2),
                        "message": match.group(3)
                    })
                    
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            message_groups = defaultdict(list)
            for log in parsed_logs:
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä¸»è¦éƒ¨åˆ†ã‚’æŠ½å‡º
                msg_parts = log["message"].split()
                if len(msg_parts) >= 3:
                    key = " ".join(msg_parts[:3])
                    message_groups[key].append(log)
                    
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ
            for key, logs in message_groups.items():
                if len(logs) >= 2:  # 2å›ä»¥ä¸Šå‡ºç¾
                    patterns.append({
                        "pattern": key,
                        "count": len(logs),
                        "severity": logs[0]["level"],
                        "first_occurrence": logs[0]["timestamp"],
                        "last_occurrence": logs[-1]["timestamp"]
                    })
                    
            # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
            patterns.sort(key=lambda p: (p["severity"] == "ERROR", p["count"]), reverse=True)
            
            return {
                "success": True,
                "patterns": patterns
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to analyze patterns: {str(e)}"
            }
            
    async def suggest_improvements(self, current_config: Dict[str, Any], issues: List[str]) -> Dict[str, Any]:
        """ãƒ­ã‚°æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ"""
        try:
            suggestions = []
            
            for issue in issues:
                issue_lower = issue.lower()
                
                if "debug" in issue_lower and "production" in issue_lower:
                    suggestions.append({
                        "issue": issue,
                        "improvement": "level",
                        "suggestion": "æœ¬ç•ªç’°å¢ƒã§ã¯ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’INFOä»¥ä¸Šã«è¨­å®š",
                        "config_change": {"level": "INFO"}
                    })
                    
                if "timestamp" in issue_lower:
                    suggestions.append({
                        "issue": issue,
                        "improvement": "timestamp",
                        "suggestion": "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«è¿½åŠ ",
                        "config_change": {
                            "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
                        }
                    })
                    
                if "structured" in issue_lower:
                    suggestions.append({
                        "issue": issue,
                        "improvement": "json",
                        "suggestion": "æ§‹é€ åŒ–ãƒ­ã‚°ï¼ˆJSONå½¢å¼ï¼‰ã®æ¡ç”¨",
                        "config_change": {
                            "formatter": "json",
                            "fields": ["timestamp", "level", "logger", "message", "extra"]
                        }
                    })
                    
            # ä¸€èˆ¬çš„ãªæ”¹å–„ææ¡ˆã‚’è¿½åŠ 
            if current_config.get("handlers") == ["console"]:
                suggestions.append({
                    "issue": "No persistent logging",
                    "improvement": "persistence",
                    "suggestion": "ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ ã—ã¦æ°¸ç¶šåŒ–",
                    "config_change": {
                        "handlers": ["console", "file"]
                    }
                })
                
            return {
                "success": True,
                "suggestions": suggestions
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to suggest improvements: {str(e)}"
            }
            
    async def integrate_framework(self, framework_config: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ã®çµ±åˆã‚’å®Ÿè£…"""
        try:
            framework = framework_config.get("framework", "fastapi")
            
            if framework == "fastapi":
                integration = self._integrate_fastapi(framework_config)
            elif framework == "django":
                integration = self._integrate_django(framework_config)
            elif framework == "flask":
                integration = self._integrate_flask(framework_config)
            else:
                return {
                    "success": False,
                    "error": f"Unknown framework: {framework}"
                }
                
            return {
                "success": True,
                "integration": integration
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to integrate framework: {str(e)}"
            }
            
    def _integrate_fastapi(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """FastAPIçµ±åˆã‚’å®Ÿè£…"""
        middleware_code = """
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
import logging
import time
import uuid
from contextvars import ContextVar

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ•°for request ID
request_id_var: ContextVar[str] = ContextVar('request_id', default='')

class LoggingMiddleware:
    \"\"\"ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ç”¨ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢\"\"\"
    
    def __init__(self, app):
        self.app = app
        self.logger = logging.getLogger("fastapi.access")
        
    async def __call__(self, request: Request, call_next):
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆIDã‚’ç”Ÿæˆ
        request_id = str(uuid.uuid4())
        request_id_var.set(request_id)
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹æ™‚åˆ»
        start_time = time.time()
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°
        self.logger.info(
            f"Request started",
            extra={
                "request_id": request_id,
                "method": request.method,
                "path": request.url.path,
                "client": request.client.host if request.client else None
            }
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†
        response = await call_next(request)
        
        # å‡¦ç†æ™‚é–“è¨ˆç®—
        process_time = time.time() - start_time
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ­ã‚°
        self.logger.info(
            f"Request completed",
            extra={
                "request_id": request_id,
                "status_code": response.status_code,
                "process_time": f"{process_time:.3f}s"
            }
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼ã«request_idã‚’è¿½åŠ 
        response.headers["X-Request-ID"] = request_id
        
        return response
"""
        
        setup_code = """
# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
app = FastAPI()

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.config.dictConfig(logging_config)

# ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã‚’è¿½åŠ 
app.add_middleware(LoggingMiddleware)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆIDãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
class RequestIdFilter(logging.Filter):
    def filter(self, record):
        record.request_id = request_id_var.get()
        return True

# ã™ã¹ã¦ã®ãƒ­ã‚¬ãƒ¼ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¿½åŠ 
for handler in logging.getLogger().handlers:
    handler.addFilter(RequestIdFilter())
"""
        
        return {
            "middleware": middleware_code,
            "setup_code": setup_code,
            "request_id": True,
            "features": ["request_logging", "response_logging", "correlation_id"]
        }
        
    def _integrate_django(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Djangoçµ±åˆã‚’å®Ÿè£…ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        return {
            "middleware": "django.middleware.logging.LoggingMiddleware",
            "settings": {
                "LOGGING": {
                    "version": 1,
                    "disable_existing_loggers": False,
                    "handlers": {
                        "file": {
                            "level": "INFO",
                            "class": "logging.FileHandler",
                            "filename": "django.log",
                        }
                    }
                }
            }
        }
        
    def _integrate_flask(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Flaskçµ±åˆã‚’å®Ÿè£…ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        return {
            "setup_code": """
from flask import Flask, g, request
import logging
import uuid

app = Flask(__name__)

@app.before_request
def before_request():
    g.request_id = str(uuid.uuid4())
    app.logger.info(f"Request: {request.method} {request.path}", 
                   extra={"request_id": g.request_id})

@app.after_request
def after_request(response):
    response.headers["X-Request-ID"] = g.request_id
    return response
"""
        }
        
    async def integrate_monitoring(self, monitoring_config: Dict[str, Any]) -> Dict[str, Any]:
        """ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆã‚’å®Ÿè£…"""
        try:
            system = monitoring_config.get("system", "prometheus")
            metrics = monitoring_config.get("metrics", [])
            
            if system == "prometheus":
                exporter_code = self._create_prometheus_exporter(metrics)
            else:
                return {
                    "success": False,
                    "error": f"Unknown monitoring system: {system}"
                }
                
            return {
                "success": True,
                "integration": {
                    "system": system,
                    "exporter": exporter_code,
                    "metrics": metrics,
                    "export_interval": monitoring_config.get("export_interval", 60)
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to integrate monitoring: {str(e)}"
            }
            
    def _create_prometheus_exporter(self, metrics: List[str]) -> str:
        """Prometheusã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã‚’ä½œæˆ"""
        return f"""
from prometheus_client import Counter, Histogram, generate_latest
import logging

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
log_counter = Counter('log_entries_total', 'Total log entries', ['level'])
error_counter = Counter('log_errors_total', 'Total error logs')
warning_counter = Counter('log_warnings_total', 'Total warning logs')
log_volume = Histogram('log_volume_bytes', 'Log message size in bytes')

class PrometheusLogHandler(logging.Handler):
    \"\"\"Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†ã™ã‚‹ãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼\"\"\"
    
    def emit(self, record):
        # ãƒ¬ãƒ™ãƒ«åˆ¥ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        log_counter.labels(level=record.levelname).inc()
        
        # ã‚¨ãƒ©ãƒ¼ãƒ»è­¦å‘Šã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        if record.levelname == 'ERROR':
            error_counter.inc()
        elif record.levelname == 'WARNING':
            warning_counter.inc()
            
        # ãƒ­ã‚°ã‚µã‚¤ã‚º
        message_size = len(record.getMessage().encode('utf-8'))
        log_volume.observe(message_size)

# ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
prometheus_handler = PrometheusLogHandler()
logging.getLogger().addHandler(prometheus_handler)

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
def metrics_endpoint():
    return generate_latest()
"""
        
    async def implement_correlation(self, correlation_config: Dict[str, Any]) -> Dict[str, Any]:
        """ç›¸é–¢IDå®Ÿè£…ã‚’ç”Ÿæˆ"""
        try:
            header_name = correlation_config.get("header_name", "X-Correlation-ID")
            
            implementation = {
                "filter": f"""
import logging
from contextvars import ContextVar
import uuid

# ç›¸é–¢IDã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ•°
correlation_id_var: ContextVar[str] = ContextVar('correlation_id', default='')

class CorrelationIdFilter(logging.Filter):
    \"\"\"ç›¸é–¢IDã‚’ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ã«è¿½åŠ ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼\"\"\"
    
    def filter(self, record):
        record.correlation_id = correlation_id_var.get() or str(uuid.uuid4())
        return True
""",
                "context_manager": """
from contextlib import contextmanager
from contextvars import ContextVar
import uuid

@contextmanager
def correlation_context(correlation_id=None):
    \"\"\"ç›¸é–¢IDã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼\"\"\"
    if correlation_id is None:
        correlation_id = str(uuid.uuid4())
    
    token = correlation_id_var.set(correlation_id)
    try:
        yield correlation_id
    finally:
        correlation_id_var.reset(token)
""",
                "middleware": f"""
async def correlation_middleware(request, call_next):
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ç›¸é–¢IDã‚’å–å¾—ã¾ãŸã¯ç”Ÿæˆ
    correlation_id = request.headers.get('{header_name}')
    if not correlation_id:
        correlation_id = str(uuid.uuid4())
    
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¨­å®š
    correlation_id_var.set(correlation_id)
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†
    response = await call_next(request)
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼ã«è¿½åŠ 
    response.headers['{header_name}'] = correlation_id
    
    return response
"""
            }
            
            return {
                "success": True,
                "implementation": implementation
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to implement correlation: {str(e)}"
            }
            
    async def implement_sampling(self, sampling_config: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ­ã‚°ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè£…"""
        try:
            strategy = sampling_config.get("strategy", "probabilistic")
            rate = sampling_config.get("rate", 0.1)
            always_sample = sampling_config.get("always_sample", [])
            
            implementation = f"""
import logging
import random

class SamplingFilter(logging.Filter):
    \"\"\"ç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼\"\"\"
    
    def __init__(self, sample_rate={rate}, always_sample={always_sample}):
        self.sample_rate = sample_rate
        self.always_sample = always_sample
        
    def filter(self, record):
        # ç‰¹å®šãƒ¬ãƒ™ãƒ«ã¯å¸¸ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if record.levelname in self.always_sample:
            return True
            
        # ç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        return random.random() < self.sample_rate

# ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¿½åŠ 
sampling_filter = SamplingFilter()
for handler in logging.getLogger().handlers:
    handler.addFilter(sampling_filter)
"""
            
            return {
                "success": True,
                "implementation": implementation
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to implement sampling: {str(e)}"
            }
            
    async def implement_sanitization(self, sanitization_config: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ­ã‚°ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè£…"""
        try:
            patterns = sanitization_config.get("patterns", [])
            replacement = sanitization_config.get("replacement", "[REDACTED]")
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
            pattern_code = "patterns = [\n"
            for pattern in patterns:
                pattern_code += f"    {{'name': '{pattern['name']}', 'regex': re.compile(r'{pattern['regex']}')}},\n"
            pattern_code += "]"
            
            implementation = {
                "filter": f"""
import logging
import re

class SanitizationFilter(logging.Filter):
    \"\"\"æ©Ÿå¯†æƒ…å ±ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼\"\"\"
    
    def __init__(self):
        {pattern_code}
        self.patterns = patterns
        self.replacement = '{replacement}'
        
    def filter(self, record):
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
        message = record.getMessage()
        
        for pattern in self.patterns:
            message = re.sub(pattern['regex'], self.replacement, message)
            
        record.msg = message
        record.args = ()  # å¼•æ•°ã‚’ã‚¯ãƒªã‚¢
        
        return True
""",
                "code": f"""
# ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¿½åŠ 
# re.subã§æ©Ÿå¯†æƒ…å ±ã‚’é™¤å»
sanitization_filter = SanitizationFilter()
for handler in logging.getLogger().handlers:
    handler.addFilter(sanitization_filter)
"""
            }
            
            return {
                "success": True,
                "implementation": implementation
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to implement sanitization: {str(e)}"
            }
            
    async def optimize_for_performance(self, requirements: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚’å®Ÿè£…"""
        try:
            throughput = requirements.get("throughput", "1000 logs/second")
            use_async = requirements.get("async", True)
            use_buffering = requirements.get("buffering", True)
            
            # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ•°å€¤ã‚’æŠ½å‡º
            throughput_match = re.search(r'(\d+)', throughput)
            target_throughput = int(throughput_match.group(1)) if throughput_match else 1000
            
            # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
            buffer_size = max(1000, target_throughput // 10)
            
            optimization = {
                "async_handler": use_async,
                "buffer_size": buffer_size,
                "estimated_throughput": target_throughput * 1.2,  # 20%ãƒãƒ¼ã‚¸ãƒ³
                "implementation": ""
            }
            
            if use_async:
                optimization["implementation"] = f"""
import logging
from logging.handlers import QueueHandler, QueueListener
import queue

# éåŒæœŸãƒ­ã‚°å‡¦ç†ç”¨ã®ã‚­ãƒ¥ãƒ¼
log_queue = queue.Queue(maxsize={buffer_size})

# æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚­ãƒ¥ãƒ¼ãƒªã‚¹ãƒŠãƒ¼ã«ç§»å‹•
existing_handlers = logging.getLogger().handlers[:]
for handler in existing_handlers:
    logging.getLogger().removeHandler(handler)

# ã‚­ãƒ¥ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
queue_handler = QueueHandler(log_queue)
logging.getLogger().addHandler(queue_handler)

# ã‚­ãƒ¥ãƒ¼ãƒªã‚¹ãƒŠãƒ¼ã‚’é–‹å§‹
queue_listener = QueueListener(log_queue, *existing_handlers)
queue_listener.start()

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«ãƒªã‚¹ãƒŠãƒ¼ã‚’åœæ­¢
import atexit
atexit.register(queue_listener.stop)
"""
            
            return {
                "success": True,
                "optimization": optimization
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to optimize for performance: {str(e)}"
            }
            
    async def implement_complete_system(self, requirements: Dict[str, Any]) -> Dict[str, Any]:
        """å®Œå…¨ãªãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…"""
        try:
            system = {
                "configuration": {},
                "handlers": {},
                "formatters": {},
                "filters": {},
                "integration_code": []
            }
            
            # åŸºæœ¬è¨­å®šã‚’ç”Ÿæˆ
            config_result = await self.generate_config({
                "environment": requirements.get("environment", "production"),
                "output": requirements.get("outputs", ["file"]),
                "level": "INFO"
            })
            
            if config_result["success"]:
                system["configuration"] = config_result["config"]
                
            # æ§‹é€ åŒ–ãƒ­ã‚°ãŒå¿…è¦ãªå ´åˆ
            if "structured_logging" in requirements.get("features", []):
                formatter_result = await self.create_formatter({
                    "type": "json",
                    "fields": {
                        "timestamp": "%(asctime)s",
                        "level": "%(levelname)s",
                        "logger": "%(name)s",
                        "message": "%(message)s"
                    }
                })
                if formatter_result["success"]:
                    system["formatters"]["json"] = formatter_result["formatter"]
                    
            # ç›¸é–¢IDãŒå¿…è¦ãªå ´åˆ
            if "correlation_id" in requirements.get("features", []):
                correlation_result = await self.implement_correlation({
                    "header_name": "X-Correlation-ID",
                    "generate_if_missing": True
                })
                if correlation_result["success"]:
                    system["filters"]["correlation_id"] = correlation_result["implementation"]
                    
            # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å®Ÿè£…
            for output in requirements.get("outputs", ["file"]):
                if output == "file":
                    handler_result = await self.implement_handler({
                        "type": "rotating",
                        "filename": str(Path(requirements.get("log_dir", ".")) / "app.log"),
                        "max_bytes": 10485760,
                        "backup_count": 5
                    })
                    if handler_result["success"]:
                        system["handlers"]["file"] = handler_result["implementation"]
                        
                elif output == "elasticsearch":
                    handler_result = await self.implement_handler({
                        "type": "custom",
                        "class": "ElasticSearchHandler",
                        "params": {
                            "hosts": ["localhost:9200"],
                            "index": "application-logs"
                        }
                    })
                    if handler_result["success"]:
                        system["handlers"]["elasticsearch"] = handler_result["implementation"]
                        
            # çµ±åˆã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
            system["integration_code"] = self._generate_integration_code(system)
            
            return {
                "success": True,
                "system": system
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to implement complete system: {str(e)}"
            }
            
    def _generate_integration_code(self, system: Dict[str, Any]) -> List[str]:
        """çµ±åˆã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
        code_parts = []
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        code_parts.append("""
import logging
import logging.config
from pathlib import Path
""")
        
        # è¨­å®šé©ç”¨
        code_parts.append("""
# ãƒ­ã‚°è¨­å®šã‚’é©ç”¨
logging.config.dictConfig(logging_config)
""")
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
        if system["filters"]:
            for name, filter_impl in system["filters"].items():
                if isinstance(filter_impl, dict) and "filter" in filter_impl:
                    code_parts.append(filter_impl["filter"])
                    
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é©ç”¨
        for name, handler_code in system["handlers"].items():
            if isinstance(handler_code, str):
                code_parts.append(handler_code)
                
        return code_parts
        
    async def perform_craft(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ã®å…·ä½“çš„ãªä½œæ¥­ã‚’å®Ÿè¡Œ"""
        action = task_data.get("action")
        data = task_data.get("data", {})
        
        if action == "generate_config":
            return await self.generate_config(data)
        elif action == "implement_handler":
            return await self.implement_handler(data)
        elif action == "create_formatter":
            return await self.create_formatter(data)
        elif action == "analyze_patterns":
            return await self.analyze_patterns(data.get("log_samples", []))
        elif action == "suggest_improvements":
            return await self.suggest_improvements(
                data.get("current_config", {}),
                data.get("issues", [])
            )
        elif action == "integrate_framework":
            return await self.integrate_framework(data)
        elif action == "integrate_monitoring":
            return await self.integrate_monitoring(data)
        elif action == "implement_correlation":
            return await self.implement_correlation(data)
        elif action == "implement_sampling":
            return await self.implement_sampling(data)
        elif action == "implement_sanitization":
            return await self.implement_sanitization(data)
        elif action == "optimize_performance":
            return await self.optimize_for_performance(data)
        elif action == "implement_complete_system":
            return await self.implement_complete_system(data)
        else:
            return {
                "success": False,
                "error": f"Unknown action: {action}"
            }
            
    async def process_elder_message(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """Elder Treeã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†"""
        try:
            action = message.get("action")
            data = message.get("data", {})
            
            if action == "setup_logging":
                project = data.get("project", {})
                requirements = data.get("requirements", {})
                
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦è¨­å®š
                system_requirements = {
                    "application": project.get("type", "fastapi"),
                    "environment": project.get("environment", "production"),
                    "features": [],
                    "outputs": ["file", "console"]
                }
                
                # è¦ä»¶ã‹ã‚‰æ©Ÿèƒ½ã‚’æŠ½å‡º
                if requirements.get("structured_logging"):
                    system_requirements["features"].append("structured_logging")
                if requirements.get("distributed_tracing"):
                    system_requirements["features"].append("correlation_id")
                    
                # å®Œå…¨ãªã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…
                result = await self.implement_complete_system(system_requirements)
                
                if not result["success"]:
                    return result
                    
                # Task Sageã«é€šçŸ¥
                await self.report_to_sage("task", {
                    "task": "logging_system_implemented",
                    "project": project["name"],
                    "features": system_requirements["features"]
                })
                
                return {
                    "success": True,
                    "data": {
                        "logging_config": result["system"]["configuration"],
                        "implementation_files": {
                            "logging_config.py": "\n".join(result["system"]["integration_code"]),
                            "handlers.py": "\n".join(result["system"]["handlers"].values()),
                            "formatters.py": "\n".join(str(f) for f in result["system"]["formatters"].values())
                        },
                        "sage_notified": True
                    }
                }
                
            return {
                "success": False,
                "error": f"Unknown action: {action}"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to process Elder message: {str(e)}"
            }


# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
__all__ = ["LoggingCrafterServant"]