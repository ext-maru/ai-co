#!/usr/bin/env python3
"""
ğŸš¨ Incident Sage A2A Agent - åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
====================================

Elder Loop Phase 4: å³å¯†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—å¯¾å¿œ
Knowledge Sageãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨ã—ãŸåŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ä¸¦è¡Œæ€§ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»çµ±åˆãƒ†ã‚¹ãƒˆ

Author: Claude Elder
Created: 2025-07-23
"""

import asyncio
import json
import time
import logging
from datetime import datetime
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor
import threading
import pytest
import tempfile
import os
from statistics import mean

# ç’°å¢ƒå¤‰æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ã‚¹ã‚’è¨­å®š
import sys
import os
# shared_libs.configã‹ã‚‰ELDERS_GUILD_HOMEã‚’å–å¾—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from shared_libs.config import config
sys.path.insert(0, config.ELDERS_GUILD_HOME)
from incident_sage.business_logic import IncidentProcessor


class TestIncidentSageA2AComprehensive:
    """Incident Sage A2A AgentåŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """ãƒ†ã‚¹ãƒˆç”¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.performance_metrics = {}
        self.logger = logging.getLogger("incident_sage_comprehensive_test")
        yield
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    
    @pytest.mark.asyncio
    async def test_performance(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor()
        
        # å¤§é‡æ“ä½œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        operations = [
            ("detect_incident", {"anomaly_data": {"component": f"service_{i}", "severity": "medium"}})
            for i in range(100)
        ]
        
        start_time = time.time()
        
        # ãƒãƒƒãƒå®Ÿè¡Œ
        results = []
        for operation, data in operations:
            result = await processor.process_action(operation, data)
            results.append(result)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è§£æ
        successful_operations = sum(1 for r in results if r.get("success"))
        throughput = successful_operations / total_time
        avg_time_per_operation = total_time / len(operations)
        
        self.performance_metrics["batch_processing"] = {
            "total_operations": len(operations),
            "successful_operations": successful_operations,
            "total_time": total_time,
            "throughput_ops_per_sec": throughput,
            "avg_time_per_operation": avg_time_per_operation
        }
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–ç¢ºèª
        assert throughput >= 50, f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä½ã„: {throughput:0.1f} ops/sec"
        assert avg_time_per_operation <= 0.1, f"å¹³å‡å®Ÿè¡Œæ™‚é–“é•·ã„: {avg_time_per_operation:0.3f}s"
        
        print(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆæˆåŠŸ: {throughput:0.1f} ops/sec, {avg_time_per_operation:0.3f}s/op")
    
    @pytest.mark.asyncio
    async def test_concurrency(self):
        """ä¸¦è¡Œæ€§ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor()
        
        # ä¸¦è¡Œã‚¿ã‚¹ã‚¯å®šç¾©
        async def concurrent_incident_detection(task_id):
            data = {
                "anomaly_data": {
                    "component": f"concurrent_service_{task_id}",
                    "metric": "error_rate",
                    "severity": "high",
                    "task_id": task_id
                }
            }
            return await processor.process_action("detect_incident", data)
        
        # ä¸¦è¡Œå®Ÿè¡Œ
        start_time = time.time()
        concurrent_tasks = [concurrent_incident_detection(i) for i in range(20)]
        results = await asyncio.gather(*concurrent_tasks, return_exceptions=True)
        end_time = time.time()
        
        # çµæœè§£æ
        successful_results = [r for r in results if not isinstance(r, Exception) and r.get("success")]
        failed_results = [r for r in results if isinstance(r, Exception) or not r.get("success")]
        
        concurrent_time = end_time - start_time
        
        self.performance_metrics["concurrency"] = {
            "total_tasks": len(concurrent_tasks),
            "successful_tasks": len(successful_results),
            "failed_tasks": len(failed_results),
            "execution_time": concurrent_time,
            "concurrent_throughput": len(successful_results) / concurrent_time
        }
        
        # ä¸¦è¡Œæ€§åŸºæº–ç¢ºèª
        assert len(failed_results) <= 2, f"ä¸¦è¡Œå‡¦ç†å¤±æ•—å¤šæ•°: {len(failed_results)}"
        assert concurrent_time <= 5.0, f"ä¸¦è¡Œå®Ÿè¡Œæ™‚é–“é•·ã„: {concurrent_time:0.3f}s"
        
        print(f"âœ… ä¸¦è¡Œæ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(successful_results)}/{len(concurrent_tasks)} æˆåŠŸ, {concurrent_time:0.3f}s")
    
    @pytest.mark.asyncio
    async def test_error_handling(self):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor()
        
        # æ‚ªæ„ã®ã‚ã‚‹/ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        error_test_cases = [
            {
                "name": "NULL ãƒ‡ãƒ¼ã‚¿",
                "action": "detect_incident",
                "data": {}
            },
            {
                "name": "ç©ºãƒ‡ãƒ¼ã‚¿",
                "action": "assess_quality", 
                "data": {}
            },
            {
                "name": "ç„¡åŠ¹JSONæ§‹é€ ",
                "action": "create_alert_rule",
                "data": {"alert_rule": {"invalid": "structure"}}
            },
            {
                "name": "å­˜åœ¨ã—ãªã„ãƒªã‚½ãƒ¼ã‚¹",
                "action": "respond_to_incident",
                "data": {"incident_id": "non_existent_12345"}
            },
            {
                "name": "ç„¡åŠ¹ãƒ‡ãƒ¼ã‚¿å‹", 
                "action": "register_monitoring_target",
                "data": {"target": "not_a_dict"}
            },
            {
                "name": "å·¨å¤§ãƒ‡ãƒ¼ã‚¿",
                "action": "search_similar_incidents",
                "data": {"query": "x" * 10000}
            },
            {
                "name": "è² ã®å€¤",
                "action": "assess_quality",
                "data": {
                    "standard_id": "test",
                    "component": "test",
                    "metrics": {"test_coverage": -50}
                }
            }
        ]
        
        error_handling_results = []
        
        for test_case in error_test_cases:
            try:
                result = await processor.process_action(test_case["action"], test_case["data"])
                
                # ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if result.get("success"):
                    error_handling_results.append({
                        "case": test_case["name"],
                        "status": "unexpected_success",
                        "result": result
                    })
                else:
                    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé©åˆ‡ã‹ç¢ºèª
                    if "error" in result and isinstance(result["error"], str):
                        error_handling_results.append({
                            "case": test_case["name"],
                            "status": "properly_handled",
                            "error": result["error"]
                        })
                    else:
                        error_handling_results.append({
                            "case": test_case["name"],
                            "status": "improper_error_format",
                            "result": result
                        })
            
            except Exception as e:
                # äºˆæœŸã—ãªã„ä¾‹å¤–
                error_handling_results.append({
                    "case": test_case["name"],
                    "status": "unhandled_exception",
                    "exception": str(e)
                })
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è©•ä¾¡
        properly_handled = len([r for r in error_handling_results if r["status"] == "properly_handled"])
        total_cases = len(error_test_cases)
        
        # 80%ä»¥ä¸ŠãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert properly_handled >= total_cases * 0.8, f"ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸ååˆ†: {properly_handled}/{total_cases}"
        
        print(f"âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆåŠŸ: {properly_handled}/{total_cases} é©åˆ‡å‡¦ç†")
    
    @pytest.mark.asyncio
    async def test_data_integrity(self):
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor()
        
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª
        
        # 1.0 ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆã¨å–å¾—ã®æ•´åˆæ€§
        original_incident_data = {
            "anomaly_data": {
                "component": "integrity_test_service",
                "metric": "data_consistency",
                "value": 42.5,
                "threshold": 30.0,
                "severity": "high",
                "confidence": 0.95
            }
        }
        
        create_result = await processor.process_action("detect_incident", original_incident_data)
        assert create_result.get("success"), "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆå¤±æ•—"
        
        incident_id = create_result["data"]["incident_id"]
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰èª­ã¿ç›´ã—
        processor2 = IncidentProcessor()
        assert incident_id in processor2.incidents, "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–å¤±æ•—"
        
        stored_incident = processor2.incidents[incident_id]
        
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç¢ºèª
        assert stored_incident.title == create_result["data"]["title"], "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ•´åˆ"
        assert stored_incident.severity.value == create_result["data"]["severity"], "é‡è¦åº¦ä¸æ•´åˆ"
        
        # 2.0 å“è³ªåŸºæº–ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
        quality_data = {
            "standard": {
                "name": "Integrity Test Standard",
                "description": "ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆç”¨å“è³ªåŸºæº–",
                "category": "testing",
                "metrics": {
                    "data_consistency": {
                        "name": "Data Consistency",
                        "target_value": 99.9,
                        "threshold_min": 95.0,
                        "unit": "%",
                        "description": "ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§"
                    }
                },
                "compliance_threshold": 95.0
            }
        }
        
        register_result = await processor.process_action("register_quality_standard", quality_data)
        assert register_result.get("success"), "å“è³ªåŸºæº–ç™»éŒ²å¤±æ•—"
        
        standard_id = register_result["data"]["standard_id"]
        
        # æ–°ã—ã„ãƒ—ãƒ­ã‚»ãƒƒã‚µã§ç¢ºèª
        processor3 = IncidentProcessor()
        assert standard_id in processor3.quality_standards, "å“è³ªåŸºæº–ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–å¤±æ•—"
        
        stored_standard = processor3.quality_standards[standard_id]
        assert stored_standard.name == quality_data["standard"]["name"], "å“è³ªåŸºæº–åä¸æ•´åˆ"
        
        print("âœ… ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ: ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ»å“è³ªåŸºæº–æ°¸ç¶šåŒ–ç¢ºèª")
    
    @pytest.mark.asyncio
    async def test_complex_workflow(self):
        """è¤‡é›‘ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor()
        
        # è¤‡é›‘ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚·ãƒŠãƒªã‚ª
        # 1.0 è¤‡æ•°ã‚µãƒ¼ãƒ“ã‚¹ã§ã®é€£é–çš„ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ
        services = ["frontend", "api_gateway", "user_service", "payment_service", "database"]
        incidents = []
        
        # æ™‚ç³»åˆ—ã§ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç™ºç”Ÿ
        for i, service in enumerate(services):
            incident_data = {
                "anomaly_data": {
                    "component": service,
                    "metric": "response_time" if i % 2 == 0 else "error_rate",
                    "severity": "critical" if i == 0 else "high",
                    "confidence": 0.9 - (i * 0.1)
                }
            }
            
            result = await processor.process_action("detect_incident", incident_data)
            if result.get("success"):
                incidents.append(result["data"]["incident_id"])
            
            # å°‘ã—å¾…æ©Ÿã—ã¦æ™‚ç³»åˆ—ã‚’ä½œã‚‹
            await asyncio.sleep(0.01)
        
        assert len(incidents) == len(services), f"ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆæ•°ä¸ä¸€è‡´: {len(incidents)} != {len(services)}"
        
        # 2.0 ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’å®Ÿè¡Œ
        pattern_result = await processor.process_action("learn_incident_patterns", {})
        assert pattern_result.get("success"), "ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’å¤±æ•—"
        
        # 3.0 ç›¸é–¢åˆ†æå®Ÿè¡Œ
        correlation_result = await processor.process_action("analyze_correlations", {})
        assert correlation_result.get("success"), "ç›¸é–¢åˆ†æå¤±æ•—"
        
        correlations = correlation_result["data"]["correlations"]
        
        # 4.0 è‡ªå‹•ä¿®å¾©è©¦è¡Œ
        remediation_results = []
        for incident_id in incidents[:2]:  # æœ€åˆã®2ã¤ã®ã¿
            remediation_result = await processor.process_action("attempt_automated_remediation", {
                "incident_id": incident_id
            })
            remediation_results.append(remediation_result)
        
        successful_remediations = [r for r in remediation_results if r.get("success")]
        
        # 5.0 çµ±è¨ˆåˆ†æ
        stats_result = await processor.process_action("get_statistics", {})
        assert stats_result.get("success"), "çµ±è¨ˆå–å¾—å¤±æ•—"
        
        stats = stats_result["data"]
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµæœæ¤œè¨¼
        assert stats["incident_statistics"]["total_incidents"] >= len(services), "çµ±è¨ˆã®ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ•°ä¸æ­£"
        assert len(successful_remediations) > 0, "è‡ªå‹•ä¿®å¾©ãŒå…¨ã¦å¤±æ•—"
        
        print(f"âœ… è¤‡é›‘ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æˆåŠŸ: {len(incidents)}ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ, {len(correlations)}ç›¸é–¢, {len(successful_remediations)}ä¿®å¾©æˆåŠŸ")
    
    @pytest.mark.asyncio
    async def test_memory_efficiency(self):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            import psutil
            
            # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            process = psutil.Process(os.getpid())
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            processor = IncidentProcessor()
            
            # å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            for i in range(500):
                await processor.process_action("detect_incident", {
                    "anomaly_data": {
                        "component": f"memory_test_service_{i}",
                        "metric": "memory_usage",
                        "severity": "low"
                    }
                })
                
                # å“è³ªè©•ä¾¡ã‚‚å®Ÿè¡Œ
                default_standard_id = list(processor.quality_standards.keys())[0]
                await processor.process_action("assess_quality", {
                    "standard_id": default_standard_id,
                    "component": f"component_{i}",
                    "metrics": {"test_coverage": 80.0 + (i % 20)}
                })
            
            # æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = final_memory - initial_memory
            
            self.performance_metrics["memory_usage"] = {
                "initial_memory_mb": initial_memory,
                "final_memory_mb": final_memory,
                "memory_increase_mb": memory_increase,
                "operations_performed": 1000,
                "memory_per_operation_kb": (memory_increase * 1024) / 1000
            }
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§åŸºæº–ç¢ºèª
            assert memory_increase <= 100, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ éå¤§: {memory_increase:0.1f}MB"
            
            memory_per_op = (memory_increase * 1024) / 1000  # KB per operation
            assert memory_per_op <= 10, f"æ“ä½œã‚ãŸã‚Šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡éå¤§: {memory_per_op:0.1f}KB/op"
            
            print(f"âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ: +{memory_increase:0.1f}MB, {memory_per_op:0.1f}KB/op")
            
        except ImportError:
            pytest.skip("psutilæœªåˆ©ç”¨å¯èƒ½ã€ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆã‚¹ã‚­ãƒƒãƒ—")
    
    @pytest.mark.asyncio
    async def test_incident_lifecycle(self):
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor(test_mode=True)
        processor.reset_for_testing()
        
        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ãƒ†ã‚¹ãƒˆ
        
        # 1.0 ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œçŸ¥
        detection_data = {
            "anomaly_data": {
                "component": "lifecycle_test_service",
                "metric": "availability",
                "severity": "critical",
                "confidence": 0.98
            }
        }
        
        detection_result = await processor.process_action("detect_incident", detection_data)
        assert detection_result.get("success"), "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œçŸ¥å¤±æ•—"
        
        incident_id = detection_result["data"]["incident_id"]
        initial_status = detection_result["data"]["status"]
        
        assert initial_status == "open", f"åˆæœŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ä¸æ­£: {initial_status}"
        
        # 2.0 ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œ
        response_result = await processor.process_action("respond_to_incident", {
            "incident_id": incident_id
        })
        
        assert response_result.get("success"), "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œå¤±æ•—"
        
        response_status = response_result["data"]["response_status"]
        incident_status_after_response = response_result["data"]["incident_status"]
        
        # 3.0 è‡ªå‹•ä¿®å¾©è©¦è¡Œ
        remediation_result = await processor.process_action("attempt_automated_remediation", {
            "incident_id": incident_id
        })
        
        assert remediation_result.get("success"), "è‡ªå‹•ä¿®å¾©è©¦è¡Œå¤±æ•—"
        
        remediation_status = remediation_result["data"]["status"]
        
        # 4.0 ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆçŠ¶æ…‹ç¢ºèª
        # test_modeã§ã¯æ°¸ç¶šåŒ–ã•ã‚Œãªã„ãŸã‚ã€åŒã˜ãƒ—ãƒ­ã‚»ãƒƒã‚µã§ç¢ºèª
        assert incident_id in processor.incidents, "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãŒãƒ¡ãƒ¢ãƒªã«å­˜åœ¨ã—ãªã„"
        
        final_incident = processor.incidents[incident_id]
        
        # ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«æ¤œè¨¼
        lifecycle_steps = [
            ("detection", detection_result["data"]["incident_id"]),
            ("response", response_status),
            ("remediation", remediation_status),
            ("final_status", final_incident.status.value)
        ]
        
        print("âœ… ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«æˆåŠŸ:")
        for step_name, step_result in lifecycle_steps:
            print(f"   â€¢ {step_name}: {step_result}")
    
    @pytest.mark.asyncio
    async def test_quality_assessment_comprehensive(self):
        """å“è³ªè©•ä¾¡åŒ…æ‹¬ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor()
        
        # è¤‡æ•°å“è³ªåŸºæº–ã§ã®åŒ…æ‹¬è©•ä¾¡
        
        # 1.0 ã‚«ã‚¹ã‚¿ãƒ å“è³ªåŸºæº–ä½œæˆ
        custom_standards = [
            {
                "name": "Security Standard",
                "category": "security",
                "metrics": {
                    "security_score": {"target_value": 95.0, "threshold_min": 90.0},
                    "vulnerability_count": {"target_value": 0.0, "threshold_min": 2.0}
                },
                "compliance_threshold": 90.0
            },
            {
                "name": "Performance Standard", 
                "category": "performance",
                "metrics": {
                    "response_time": {"target_value": 100.0, "threshold_min": 200.0},
                    "throughput": {"target_value": 1000.0, "threshold_min": 500.0}
                },
                "compliance_threshold": 85.0
            }
        ]
        
        created_standards = []
        for standard_data in custom_standards:
            result = await processor.process_action("register_quality_standard", {
                "standard": standard_data
            })
            if result.get("success"):
                created_standards.append(result["data"]["standard_id"])
        
        assert len(created_standards) == len(custom_standards), "å“è³ªåŸºæº–ä½œæˆæ•°ä¸ä¸€è‡´"
        
        # 2.0 è¤‡æ•°ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã®å“è³ªè©•ä¾¡
        components = ["frontend", "backend", "database"]
        assessment_results = []
        
        for component in components:
            for standard_id in created_standards:
                # ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ
                if "security" in processor.quality_standards[standard_id].name.lower():
                    metrics = {
                        "security_score": 92.5,
                        "vulnerability_count": 1.0
                    }
                else:
                    metrics = {
                        "response_time": 150.0,
                        "throughput": 750.0
                    }
                
                assessment_result = await processor.process_action("assess_quality", {
                    "standard_id": standard_id,
                    "component": component,
                    "metrics": metrics
                })
                
                if assessment_result.get("success"):
                    assessment_results.append(assessment_result["data"])
        
        # 3.0 è©•ä¾¡çµæœåˆ†æ
        total_assessments = len(components) * len(created_standards)
        successful_assessments = len(assessment_results)
        
        assert successful_assessments == total_assessments, f"è©•ä¾¡å®Ÿè¡Œæ•°ä¸ä¸€è‡´: {successful_assessments} != {total_assessments}"
        
        # ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ç‡è¨ˆç®—
        compliant_assessments = [a for a in assessment_results if a["is_compliant"]]
        compliance_rate = len(compliant_assessments) / len(assessment_results) * 100
        
        # å¹³å‡å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
        avg_quality_score = sum(a["overall_score"] for a in assessment_results) / len(assessment_results)
        
        self.performance_metrics["quality_assessment"] = {
            "total_assessments": total_assessments,
            "successful_assessments": successful_assessments,
            "compliance_rate": compliance_rate,
            "average_quality_score": avg_quality_score
        }
        
        print(f"âœ… å“è³ªè©•ä¾¡åŒ…æ‹¬ãƒ†ã‚¹ãƒˆæˆåŠŸ: {compliance_rate:0.1f}%é©åˆ, å¹³å‡{avg_quality_score:0.1f}ç‚¹")
    
    @pytest.mark.asyncio
    async def test_alert_system_integration(self):
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor(test_mode=True)
        processor.reset_for_testing()
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆã‚·ãƒŠãƒªã‚ª
        
        # 1.0 è¤‡æ•°ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ä½œæˆ
        alert_rules = [
            {
                "name": "CPU Alert",
                "condition_expression": "cpu_usage > 80.0",
                "severity": "high",
                "enabled": True
            },
            {
                "name": "Memory Alert",
                "condition_expression": "memory_usage > 90.0", 
                "severity": "critical",
                "enabled": True
            },
            {
                "name": "Disk Alert",
                "condition_expression": "disk_usage > 95.0",
                "severity": "medium",
                "enabled": False  # ç„¡åŠ¹
            }
        ]
        
        created_rules = []
        for rule_data in alert_rules:
            result = await processor.process_action("create_alert_rule", {
                "alert_rule": rule_data
            })
            if result.get("success"):
                created_rules.append(result["data"]["rule_id"])
        
        assert len(created_rules) == len(alert_rules), "ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ä½œæˆæ•°ä¸ä¸€è‡´"
        
        # 2.0 ã‚¢ãƒ©ãƒ¼ãƒˆè©•ä¾¡ã‚·ãƒŠãƒªã‚ª
        test_metrics = [
            {
                "scenario": "normal",
                "metrics": {"cpu_usage": 70.0, "memory_usage": 75.0, "disk_usage": 80.0},
                "expected_alerts": 0
            },
            {
                "scenario": "cpu_high",
                "metrics": {"cpu_usage": 85.0, "memory_usage": 75.0, "disk_usage": 80.0},
                "expected_alerts": 1
            },
            {
                "scenario": "multiple_alerts",
                "metrics": {"cpu_usage": 85.0, "memory_usage": 95.0, "disk_usage": 98.0},
                "expected_alerts": 2  # disk_usageã¯ç„¡åŠ¹ãƒ«ãƒ¼ãƒ«ãªã®ã§é™¤å¤–
            }
        ]
        
        alert_evaluation_results = []
        
        for test_case in test_metrics:
            result = await processor.process_action("evaluate_alert_rules", {
                "metrics": test_case["metrics"],
                "reset_cooldown": True  # ãƒ†ã‚¹ãƒˆç”¨ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ãƒªã‚»ãƒƒãƒˆ
            })
            
            if result.get("success"):
                triggered_count = result["data"]["alert_count"]
                alert_evaluation_results.append({
                    "scenario": test_case["scenario"],
                    "expected": test_case["expected_alerts"],
                    "actual": triggered_count,
                    "match": triggered_count == test_case["expected_alerts"]
                })
        
        # 3.0 ã‚¢ãƒ©ãƒ¼ãƒˆè©•ä¾¡çµæœæ¤œè¨¼
        successful_evaluations = [r for r in alert_evaluation_results if r["match"]]
        
        assert len(successful_evaluations) == len(test_metrics), "ã‚¢ãƒ©ãƒ¼ãƒˆè©•ä¾¡çµæœä¸ä¸€è‡´"
        
        print(f"âœ… ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆæˆåŠŸ: {len(created_rules)}ãƒ«ãƒ¼ãƒ«, {len(successful_evaluations)}ã‚·ãƒŠãƒªã‚ª")
    
    @pytest.mark.asyncio
    async def test_monitoring_comprehensive(self):
        """ç›£è¦–æ©Ÿèƒ½åŒ…æ‹¬ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor()
        
        # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åŒ…æ‹¬ãƒ†ã‚¹ãƒˆ
        
        # 1.0 è¤‡æ•°ç›£è¦–å¯¾è±¡ç™»éŒ²
        monitoring_targets = [
            {
                "name": "Web Server",
                "type": "web_service",
                "endpoint_url": "http://web-server:80",
                "health_check_enabled": True
            },
            {
                "name": "Database Server",
                "type": "database",
                "endpoint_url": "http://db-server:5432",
                "health_check_enabled": True
            },
            {
                "name": "Cache Server",
                "type": "cache",
                "endpoint_url": "http://cache-server:6379",
                "health_check_enabled": False
            }
        ]
        
        created_targets = []
        for target_data in monitoring_targets:
            result = await processor.process_action("register_monitoring_target", {
                "target": target_data
            })
            if result.get("success"):
                created_targets.append(result["data"]["target_id"])
        
        assert len(created_targets) == len(monitoring_targets), "ç›£è¦–å¯¾è±¡ç™»éŒ²æ•°ä¸ä¸€è‡´"
        
        # 2.0 ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        health_check_results = []
        for target_id in created_targets:
            result = await processor.process_action("check_target_health", {
                "target_id": target_id
            })
            if result.get("success"):
                health_check_results.append(result["data"])
        
        assert len(health_check_results) == len(created_targets), "ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œæ•°ä¸ä¸€è‡´"
        
        # 3.0 ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯çµæœåˆ†æ
        healthy_targets = [r for r in health_check_results if r["status"] == "healthy"]
        avg_response_time = sum(r["response_time_ms"] for r in health_check_results) / len(health_check_results)
        avg_uptime = sum(r["uptime_percentage"] for r in health_check_results) / len(health_check_results)
        
        self.performance_metrics["monitoring"] = {
            "total_targets": len(created_targets),
            "healthy_targets": len(healthy_targets),
            "health_rate": len(healthy_targets) / len(created_targets) * 100,
            "average_response_time": avg_response_time,
            "average_uptime": avg_uptime
        }
        
        # åŸºæº–ç¢ºèª
        assert len(healthy_targets) > 0, "å…¨ç›£è¦–å¯¾è±¡ãŒä¸å¥åº·"
        
        print(f"âœ… ç›£è¦–æ©Ÿèƒ½åŒ…æ‹¬ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(healthy_targets)}/{len(created_targets)}å¥åº·, å¹³å‡å¿œç­”{avg_response_time:0.1f}ms")
    
    @pytest.mark.asyncio
    async def test_pattern_learning_advanced(self):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’é«˜åº¦ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor()
        
        # é«˜åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ãƒ†ã‚¹ãƒˆ
        
        # 1.0 è¤‡é›‘ãªã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ä½œæˆ
        incident_patterns = [
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é–¢é€£ãƒ‘ã‚¿ãƒ¼ãƒ³
            *[{
                "component": f"web_server_{i}",
                "metric": "response_time",
                "severity": "high",
                "category": "performance"
            } for i in range(5)],
            
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢é€£ãƒ‘ã‚¿ãƒ¼ãƒ³
            *[{
                "component": f"auth_service_{i}",
                "metric": "failed_logins",
                "severity": "medium",
                "category": "security"
            } for i in range(3)],
            
            # å¯ç”¨æ€§é–¢é€£ãƒ‘ã‚¿ãƒ¼ãƒ³
            *[{
                "component": f"database_{i}",
                "metric": "connection_failure",
                "severity": "critical",
                "category": "availability"
            } for i in range(4)]
        ]
        
        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆ
        created_incidents = []
        for pattern in incident_patterns:
            result = await processor.process_action("detect_incident", {
                "anomaly_data": pattern
            })
            if result.get("success"):
                created_incidents.append(result["data"]["incident_id"])
        
        assert len(created_incidents) == len(incident_patterns), "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆæ•°ä¸ä¸€è‡´"
        
        # 2.0 ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’å®Ÿè¡Œ
        learning_result = await processor.process_action("learn_incident_patterns", {})
        
        assert learning_result.get("success"), "ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’å¤±æ•—"
        
        learning_data = learning_result["data"]
        patterns_learned = learning_data["patterns_learned"]
        patterns = learning_data["patterns"]
        
        # 3.0 å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        expected_categories = ["performance", "security", "availability"]
        learned_categories = [p["category"] for p in patterns]
        
        category_coverage = len(set(learned_categories) & set(expected_categories))
        
        assert category_coverage >= 2, f"ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ã‚«ãƒ†ã‚´ãƒªä¸è¶³: {category_coverage}"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å“è³ªè©•ä¾¡
        pattern_quality_scores = []
        for pattern in patterns:
            quality_score = 0
            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå…±é€šæ€§
            if len(pattern["common_components"]) > 0:
                quality_score += 30
            # ã‚¿ã‚°å…±é€šæ€§
            if len(pattern["common_tags"]) > 0:
                quality_score += 20
            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ•°
            if pattern["incident_count"] >= 3:
                quality_score += 30
            # å¹³å‡é‡è¦åº¦
            if 2.0 <= pattern["average_severity"] <= 4.0:
                quality_score += 20
            
            pattern_quality_scores.append(quality_score)
        
        avg_pattern_quality = sum(pattern_quality_scores) / len(pattern_quality_scores) if pattern_quality_scores else 0
        
        self.performance_metrics["pattern_learning"] = {
            "total_incidents": len(created_incidents),
            "patterns_learned": patterns_learned,
            "category_coverage": category_coverage,
            "average_pattern_quality": avg_pattern_quality
        }
        
        print(f"âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’é«˜åº¦ãƒ†ã‚¹ãƒˆæˆåŠŸ: {patterns_learned}ãƒ‘ã‚¿ãƒ¼ãƒ³, {category_coverage}ã‚«ãƒ†ã‚´ãƒª, å“è³ª{avg_pattern_quality:0.1f}")
    
    @pytest.mark.asyncio
    async def test_correlation_analysis_detailed(self):
        """ç›¸é–¢åˆ†æè©³ç´°ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor(test_mode=True)
        processor.reset_for_testing()
        
        # ç›¸é–¢åˆ†æè©³ç´°ãƒ†ã‚¹ãƒˆ
        
        # 1.0 æ™‚é–“çš„ç›¸é–¢ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆ
        time_correlated_incidents = []
        
        # åŒæ™‚æœŸã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç¾¤1
        for i in range(3):
            result = await processor.process_action("detect_incident", {
                "anomaly_data": {
                    "component": f"frontend_app_{i}",
                    "metric": "error_rate",
                    "severity": "high"
                }
            })
            if result.get("success"):
                time_correlated_incidents.append(result["data"]["incident_id"])
            await asyncio.sleep(0.01)  # çŸ­é–“éš”
        
        # é–“éš”ã‚’é–‹ã‘ã‚‹
        await asyncio.sleep(0.1)
        
        # åŒæ™‚æœŸã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç¾¤2
        for i in range(2):
            result = await processor.process_action("detect_incident", {
                "anomaly_data": {
                    "component": f"backend_service_{i}",
                    "metric": "response_time",
                    "severity": "medium"
                }
            })
            if result.get("success"):
                time_correlated_incidents.append(result["data"]["incident_id"])
            await asyncio.sleep(0.01)
        
        # 2.0 ç©ºé–“çš„ç›¸é–¢ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆ
        space_correlated_incidents = []
        shared_components = ["payment_service", "user_database"]
        
        for component in shared_components:
            for metric in ["availability", "performance"]:
                result = await processor.process_action("detect_incident", {
                    "anomaly_data": {
                        "component": component,
                        "metric": metric,
                        "severity": "high"
                    }
                })
                if result.get("success"):
                    space_correlated_incidents.append(result["data"]["incident_id"])
        
        # 3.0 ç›¸é–¢åˆ†æå®Ÿè¡Œ
        correlation_result = await processor.process_action("analyze_correlations", {})
        
        assert correlation_result.get("success"), "ç›¸é–¢åˆ†æå¤±æ•—"
        
        correlation_data = correlation_result["data"]
        correlations = correlation_data["correlations"]
        analyzed_incidents = correlation_data["analyzed_incidents"]
        
        # 4.0 ç›¸é–¢åˆ†æçµæœè©•ä¾¡
        total_incidents_created = len(time_correlated_incidents) + len(space_correlated_incidents)
        
        assert analyzed_incidents == total_incidents_created, f"åˆ†æã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ•°ä¸ä¸€è‡´: {analyzed_incidents} != {total_incidents_created}"
        
        # ç›¸é–¢å“è³ªè©•ä¾¡
        high_confidence_correlations = [c for c in correlations if c["confidence"] >= 0.7]
        temporal_correlations = [c for c in correlations if c["correlation_type"] == "temporal_spatial"]
        
        self.performance_metrics["correlation_analysis"] = {
            "total_incidents_created": total_incidents_created,
            "total_correlations_found": len(correlations),
            "high_confidence_correlations": len(high_confidence_correlations),
            "temporal_correlations": len(temporal_correlations),
            "correlation_discovery_rate": len(correlations) / max(total_incidents_created, 1)
        }
        
        # æœ€ä½é™ã®ç›¸é–¢æ¤œå‡ºç¢ºèªï¼ˆãƒ‡ãƒ¼ã‚¿ä¾å­˜ãªã®ã§warningãƒ¬ãƒ™ãƒ«ï¼‰
        if len(correlations) == 0:
            print("âš ï¸ ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼ˆæ­£å¸¸ã€ãƒ‡ãƒ¼ã‚¿ä¾å­˜ï¼‰")
        
        print(f"âœ… ç›¸é–¢åˆ†æè©³ç´°ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(correlations)}ç›¸é–¢æ¤œå‡º, {len(high_confidence_correlations)}é«˜ä¿¡é ¼åº¦")
    
    @pytest.mark.asyncio
    async def test_remediation_effectiveness(self):
        """ä¿®å¾©åŠ¹æœãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor()
        
        # ä¿®å¾©åŠ¹æœãƒ†ã‚¹ãƒˆ
        
        # 1.0 ç•°ãªã‚‹ã‚«ãƒ†ã‚´ãƒªã®ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆ
        test_categories = [
            ("performance", "cache_service", "memory_leak"),
            ("availability", "web_server", "service_down"),
            ("quality", "api_gateway", "validation_error"),
            ("security", "auth_service", "unauthorized_access")
        ]
        
        remediation_results = []
        
        for category, component, metric in test_categories:
            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆ
            incident_result = await processor.process_action("detect_incident", {
                "anomaly_data": {
                    "component": component,
                    "metric": metric,
                    "severity": "high",
                    "category": category
                }
            })
            
            if not incident_result.get("success"):
                continue
            
            incident_id = incident_result["data"]["incident_id"]
            
            # ä¿®å¾©è©¦è¡Œ
            remediation_result = await processor.process_action("attempt_automated_remediation", {
                "incident_id": incident_id
            })
            
            if remediation_result.get("success"):
                remediation_data = remediation_result["data"]
                remediation_results.append({
                    "category": category,
                    "component": component,
                    "incident_id": incident_id,
                    "status": remediation_data["status"],
                    "actions_taken": remediation_data.get("actions_taken", [])
                })
        
        # 2.0 ä¿®å¾©åŠ¹æœåˆ†æ
        successful_remediations = [r for r in remediation_results if r["status"] == "success"]
        failed_remediations = [r for r in remediation_results if r["status"] == "failed"]
        no_action_remediations = [r for r in remediation_results if r["status"] == "no_action"]
        
        success_rate = len(successful_remediations) / len(remediation_results) * 100 if remediation_results else 0
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ä¿®å¾©åŠ¹æœ
        category_effectiveness = {}
        for category in [c[0] for c in test_categories]:
            category_results = [r for r in remediation_results if r["category"] == category]
            category_successes = [r for r in category_results if r["status"] == "success"]
            category_effectiveness[category] = len(category_successes) / len(category_results) * 100 if category_results else 0
        
        self.performance_metrics["remediation_effectiveness"] = {
            "total_remediations": len(remediation_results),
            "successful_remediations": len(successful_remediations),
            "failed_remediations": len(failed_remediations),
            "no_action_remediations": len(no_action_remediations),
            "overall_success_rate": success_rate,
            "category_effectiveness": category_effectiveness
        }
        
        # åŠ¹æœåŸºæº–ç¢ºèª
        assert success_rate >= 50, f"ä¿®å¾©æˆåŠŸç‡ä½ã„: {success_rate:0.1f}%"
        
        # å…¨ã‚«ãƒ†ã‚´ãƒªã§ä¿®å¾©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        categories_with_actions = len([r for r in remediation_results if r["actions_taken"]])
        assert categories_with_actions >= len(test_categories) * 0.8, "ä¿®å¾©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®šç¾©ä¸è¶³"
        
        print(f"âœ… ä¿®å¾©åŠ¹æœãƒ†ã‚¹ãƒˆæˆåŠŸ: {success_rate:0.1f}%æˆåŠŸç‡, {len(test_categories)}ã‚«ãƒ†ã‚´ãƒªå¯¾å¿œ")
    
    @pytest.mark.asyncio
    async def test_statistics_accuracy(self):
        """çµ±è¨ˆç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor(test_mode=True)
        processor.reset_for_testing()
        
        # çµ±è¨ˆç²¾åº¦ãƒ†ã‚¹ãƒˆ
        
        # 1.0 æ—¢çŸ¥ãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆä½œæˆ
        known_data = {
            "incidents": [],
            "quality_assessments": [],
            "alert_rules": [],
            "monitoring_targets": []
        }
        
        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆï¼ˆæ—¢çŸ¥æ•°ï¼‰
        incident_severities = ["low", "medium", "high", "critical"]
        incidents_per_severity = 3
        
        for severity in incident_severities:
            for i in range(incidents_per_severity):
                result = await processor.process_action("detect_incident", {
                    "anomaly_data": {
                        "component": f"test_service_{severity}_{i}",
                        "metric": "test_metric",
                        "severity": severity
                    }
                })
                if result.get("success"):
                    known_data["incidents"].append(result["data"]["incident_id"])
        
        # ã„ãã¤ã‹ã®ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚’è§£æ±ºçŠ¶æ…‹ã«ã™ã‚‹
        resolved_count = 0
        for incident_id in known_data["incidents"][:6]:  # åŠåˆ†ã‚’è§£æ±º
            result = await processor.process_action("respond_to_incident", {
                "incident_id": incident_id
            })
            if result.get("success") and result["data"]["incident_status"] == "resolved":
                resolved_count += 1
        
        # å“è³ªè©•ä¾¡å®Ÿè¡Œï¼ˆæ—¢çŸ¥æ•°ï¼‰
        default_standard_id = list(processor.quality_standards.keys())[0]
        quality_assessments = 5
        
        for i in range(quality_assessments):
            result = await processor.process_action("assess_quality", {
                "standard_id": default_standard_id,
                "component": f"test_component_{i}",
                "metrics": {"test_coverage": 80.0 + i}
            })
            if result.get("success"):
                known_data["quality_assessments"].append(result["data"]["assessment_id"])
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ä½œæˆï¼ˆæ—¢çŸ¥æ•°ï¼‰
        alert_rules_count = 4
        for i in range(alert_rules_count):
            result = await processor.process_action("create_alert_rule", {
                "alert_rule": {
                    "name": f"Test Alert {i}",
                    "condition_expression": f"metric_{i} > {i*10}",
                    "severity": "medium",
                    "enabled": i % 2 == 0  # åŠåˆ†ã‚’æœ‰åŠ¹
                }
            })
            if result.get("success"):
                known_data["alert_rules"].append(result["data"]["rule_id"])
        
        # ç›£è¦–å¯¾è±¡ç™»éŒ²ï¼ˆæ—¢çŸ¥æ•°ï¼‰
        monitoring_targets_count = 3
        for i in range(monitoring_targets_count):
            result = await processor.process_action("register_monitoring_target", {
                "target": {
                    "name": f"Test Target {i}",
                    "type": "service",
                    "endpoint_url": f"http://test-{i}:8080",
                    "health_check_enabled": True
                }
            })
            if result.get("success"):
                known_data["monitoring_targets"].append(result["data"]["target_id"])
        
        # 2.0 çµ±è¨ˆå–å¾—ãƒ»ç²¾åº¦ç¢ºèª
        stats_result = await processor.process_action("get_statistics", {})
        
        assert stats_result.get("success"), "çµ±è¨ˆå–å¾—å¤±æ•—"
        
        stats = stats_result["data"]
        
        # çµ±è¨ˆç²¾åº¦æ¤œè¨¼
        accuracy_checks = []
        
        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆçµ±è¨ˆ
        incident_stats = stats["incident_statistics"]
        accuracy_checks.append({
            "metric": "total_incidents",
            "expected": len(known_data["incidents"]),
            "actual": incident_stats["total_incidents"],
            "accurate": incident_stats["total_incidents"] == len(known_data["incidents"])
        })
        
        # è§£æ±ºç‡è¨ˆç®—ç¢ºèª
        expected_resolution_rate = (resolved_count / len(known_data["incidents"])) * 100 if known_data["incidents"] else 0
        resolution_rate_diff = abs(incident_stats["resolution_rate"] - expected_resolution_rate)
        accuracy_checks.append({
            "metric": "resolution_rate",
            "expected": expected_resolution_rate,
            "actual": incident_stats["resolution_rate"],
            "accurate": resolution_rate_diff < 1.0  # 1%ä»¥å†…ã®èª¤å·®è¨±å®¹
        })
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆçµ±è¨ˆ
        alert_stats = stats["alert_statistics"]
        enabled_rules = len([r for r in known_data["alert_rules"]]) // 2  # åŠåˆ†ãŒæœ‰åŠ¹
        accuracy_checks.append({
            "metric": "alert_rules_active",
            "expected": enabled_rules,
            "actual": alert_stats["alert_rules_active"],
            "accurate": abs(alert_stats["alert_rules_active"] - enabled_rules) <= 1
        })
        
        # ç›£è¦–çµ±è¨ˆ
        monitoring_stats = stats["monitoring_statistics"]
        accuracy_checks.append({
            "metric": "monitoring_targets_count",
            "expected": len(known_data["monitoring_targets"]),
            "actual": monitoring_stats["monitoring_targets_count"],
            "accurate": monitoring_stats["monitoring_targets_count"] == len(known_data["monitoring_targets"])
        })
        
        # ç²¾åº¦è©•ä¾¡
        accurate_metrics = [c for c in accuracy_checks if c["accurate"]]
        accuracy_rate = len(accurate_metrics) / len(accuracy_checks) * 100
        
        self.performance_metrics["statistics_accuracy"] = {
            "total_metrics_checked": len(accuracy_checks),
            "accurate_metrics": len(accurate_metrics),
            "accuracy_rate": accuracy_rate,
            "accuracy_details": accuracy_checks
        }
        
        # ç²¾åº¦åŸºæº–ç¢ºèª
        assert accuracy_rate >= 80, f"çµ±è¨ˆç²¾åº¦ä½ã„: {accuracy_rate:0.1f}%"
        
        print(f"âœ… çµ±è¨ˆç²¾åº¦ãƒ†ã‚¹ãƒˆæˆåŠŸ: {accuracy_rate:0.1f}%ç²¾åº¦, {len(accurate_metrics)}/{len(accuracy_checks)}é …ç›®")
    
    @pytest.mark.asyncio
    async def test_stress_load(self):
        """ã‚¹ãƒˆãƒ¬ã‚¹è² è·ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor()
        
        # ã‚¹ãƒˆãƒ¬ã‚¹è² è·ãƒ†ã‚¹ãƒˆ
        
        # 1.0 å¤§é‡åŒæ™‚æ“ä½œãƒ†ã‚¹ãƒˆ
        stress_operations = []
        
        # å„ç¨®æ“ä½œã‚’æ··åˆ
        for i in range(200):
            operation_type = i % 4
            
            if operation_type == 0:
                # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œçŸ¥
                stress_operations.append(("detect_incident", {
                    "anomaly_data": {
                        "component": f"stress_service_{i}",
                        "metric": "stress_metric",
                        "severity": ["low", "medium", "high"][i % 3]
                    }
                }))
            elif operation_type == 1:
                # å“è³ªè©•ä¾¡
                default_standard_id = list(processor.quality_standards.keys())[0]
                stress_operations.append(("assess_quality", {
                    "standard_id": default_standard_id,
                    "component": f"stress_component_{i}",
                    "metrics": {"test_coverage": 70.0 + (i % 30)}
                }))
            elif operation_type == 2:
                # çµ±è¨ˆå–å¾—
                stress_operations.append(("get_statistics", {}))
            else:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
                stress_operations.append(("learn_incident_patterns", {}))
        
        # 2.0 ã‚¹ãƒˆãƒ¬ã‚¹å®Ÿè¡Œ
        start_time = time.time()
        stress_results = []
        
        # ãƒãƒƒãƒå®Ÿè¡Œã§ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
        batch_size = 20
        for i in range(0, len(stress_operations), batch_size):
            batch = stress_operations[i:i+batch_size]
            batch_results = []
            
            for operation, data in batch:
                result = await processor.process_action(operation, data)
                batch_results.append(result)
            
            stress_results.extend(batch_results)
            
            # å°‘ã—ä¼‘æ†©ï¼ˆãƒ¡ãƒ¢ãƒªè² è·è»½æ¸›ï¼‰
            if i % 100 == 0:
                await asyncio.sleep(0.01)
        
        end_time = time.time()
        total_stress_time = end_time - start_time
        
        # 3.0 ã‚¹ãƒˆãƒ¬ã‚¹çµæœåˆ†æ
        successful_operations = [r for r in stress_results if r.get("success")]
        failed_operations = [r for r in stress_results if not r.get("success")]
        
        stress_success_rate = len(successful_operations) / len(stress_results) * 100
        stress_throughput = len(stress_results) / total_stress_time
        
        self.performance_metrics["stress_test"] = {
            "total_operations": len(stress_operations),
            "successful_operations": len(successful_operations),
            "failed_operations": len(failed_operations),
            "success_rate": stress_success_rate,
            "total_time": total_stress_time,
            "throughput": stress_throughput
        }
        
        # ã‚¹ãƒˆãƒ¬ã‚¹åŸºæº–ç¢ºèª
        assert stress_success_rate >= 90, f"ã‚¹ãƒˆãƒ¬ã‚¹æˆåŠŸç‡ä½ã„: {stress_success_rate:0.1f}%"
        assert stress_throughput >= 20, f"ã‚¹ãƒˆãƒ¬ã‚¹ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä½ã„: {stress_throughput:0.1f} ops/sec"
        
        print(f"âœ… ã‚¹ãƒˆãƒ¬ã‚¹è² è·ãƒ†ã‚¹ãƒˆæˆåŠŸ: {stress_success_rate:0.1f}%æˆåŠŸç‡, {stress_throughput:0.1f} ops/sec")
    
    @pytest.mark.asyncio
    async def test_edge_cases(self):
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ"""
        processor = IncidentProcessor()
        
        # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
        
        edge_test_cases = [
            {
                "name": "ç©ºæ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿",
                "action": "search_similar_incidents",
                "data": {"query": ""},
                "should_succeed": True
            },
            {
                "name": "éå¸¸ã«é•·ã„ãƒ†ã‚­ã‚¹ãƒˆ",
                "action": "detect_incident",
                "data": {
                    "anomaly_data": {
                        "component": "x" * 1000,
                        "metric": "y" * 1000,
                        "severity": "medium"
                    }
                },
                "should_succeed": True
            },
            {
                "name": "ã‚¼ãƒ­å€¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹",
                "action": "assess_quality",
                "data": {
                    "standard_id": list(processor.quality_standards.keys())[0],
                    "component": "test",
                    "metrics": {"test_coverage": 0.0}
                },
                "should_succeed": True
            },
            {
                "name": "æ¥µå¤§å€¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹",
                "action": "assess_quality",
                "data": {
                    "standard_id": list(processor.quality_standards.keys())[0],
                    "component": "test",
                    "metrics": {"test_coverage": 999999.0}
                },
                "should_succeed": True
            },
            {
                "name": "ç‰¹æ®Šæ–‡å­—ãƒ‡ãƒ¼ã‚¿",
                "action": "detect_incident",
                "data": {
                    "anomaly_data": {
                        "component": "test@#$%^&*()",
                        "metric": "test<>?:{}[]",
                        "severity": "low"
                    }
                },
                "should_succeed": True
            },
            {
                "name": "Unicodeæ–‡å­—ãƒ‡ãƒ¼ã‚¿",
                "action": "detect_incident",
                "data": {
                    "anomaly_data": {
                        "component": "ãƒ†ã‚¹ãƒˆã‚µãƒ¼ãƒ“ã‚¹",
                        "metric": "å¿œç­”æ™‚é–“",
                        "severity": "medium"
                    }
                },
                "should_succeed": True
            },
            {
                "name": "åŒä¸€ãƒ‡ãƒ¼ã‚¿é‡è¤‡å®Ÿè¡Œ",
                "action": "detect_incident",
                "data": {
                    "anomaly_data": {
                        "component": "duplicate_service",
                        "metric": "duplicate_metric",
                        "severity": "high"
                    }
                },
                "should_succeed": True
            }
        ]
        
        edge_case_results = []
        
        for test_case in edge_test_cases:
            try:
                # åŒä¸€ãƒ‡ãƒ¼ã‚¿é‡è¤‡ã®å ´åˆã¯è¤‡æ•°å›å®Ÿè¡Œ
                if test_case["name"] == "åŒä¸€ãƒ‡ãƒ¼ã‚¿é‡è¤‡å®Ÿè¡Œ":
                    for _ in range(3):
                        result = await processor.process_action(test_case["action"], test_case["data"])
                else:
                    result = await processor.process_action(test_case["action"], test_case["data"])
                
                # çµæœè©•ä¾¡
                success = result.get("success", False)
                expected_success = test_case["should_succeed"]
                
                edge_case_results.append({
                    "case": test_case["name"],
                    "expected_success": expected_success,
                    "actual_success": success,
                    "correct": success == expected_success,
                    "result": result
                })
                
            except Exception as e:
                edge_case_results.append({
                    "case": test_case["name"],
                    "expected_success": test_case["should_succeed"],
                    "actual_success": False,
                    "correct": not test_case["should_succeed"],
                    "exception": str(e)
                })
        
        # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹çµæœè©•ä¾¡
        correct_results = [r for r in edge_case_results if r["correct"]]
        edge_case_success_rate = len(correct_results) / len(edge_case_results) * 100
        
        self.performance_metrics["edge_cases"] = {
            "total_edge_cases": len(edge_test_cases),
            "correct_results": len(correct_results),
            "success_rate": edge_case_success_rate,
            "results": edge_case_results
        }
        
        # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹åŸºæº–ç¢ºèª
        assert edge_case_success_rate >= 80, f"ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å‡¦ç†ä¸ååˆ†: {edge_case_success_rate:0.1f}%"
        
        print(f"âœ… ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆæˆåŠŸ: {edge_case_success_rate:0.1f}%é©åˆ‡å‡¦ç†, {len(correct_results)}/{len(edge_test_cases)}ã‚±ãƒ¼ã‚¹")