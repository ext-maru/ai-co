#!/usr/bin/env python3
"""
ğŸš¨ Incident Sage A2A Agent - åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
====================================

Elder Loop Phase 4: å³å¯†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—å¯¾å¿œ
Knowledge Sageãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨ã—ãŸåŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ä¸¦è¡Œæ€§ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»çµ±åˆãƒ†ã‚¹ãƒˆ

Author: Claude Elder
Created: 2025-07-23
"""

import asyncio
import json
import time
import logging
from datetime import datetime
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor
import threading

# Incident Sage imports
import sys
sys.path.append("/home/aicompany/ai_co/elders_guild")
from incident_sage.business_logic import IncidentProcessor


class TestIncidentSageA2AComprehensive:
    """Incident Sage A2A AgentåŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self):
        self.test_results = {}
        self.performance_metrics = {}
        self.logger = logging.getLogger("incident_sage_comprehensive_test")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """å…¨åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        print("ğŸš¨ Incident Sage A2A Agent - åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹")
        print("=" * 70)
        
        test_methods = [
            ("performance_test", self.test_performance),
            ("concurrency_test", self.test_concurrency), 
            ("error_handling_test", self.test_error_handling),
            ("data_integrity_test", self.test_data_integrity),
            ("complex_workflow_test", self.test_complex_workflow),
            ("memory_efficiency_test", self.test_memory_efficiency),
            ("incident_lifecycle_test", self.test_incident_lifecycle),
            ("quality_assessment_comprehensive_test", self.test_quality_assessment_comprehensive),
            ("alert_system_integration_test", self.test_alert_system_integration),
            ("monitoring_comprehensive_test", self.test_monitoring_comprehensive),
            ("pattern_learning_advanced_test", self.test_pattern_learning_advanced),
            ("correlation_analysis_detailed_test", self.test_correlation_analysis_detailed),
            ("remediation_effectiveness_test", self.test_remediation_effectiveness),
            ("statistics_accuracy_test", self.test_statistics_accuracy),
            ("stress_test", self.test_stress_load),
            ("edge_cases_test", self.test_edge_cases)
        ]
        
        total_tests = len(test_methods)
        passed_tests = 0
        
        for test_name, test_method in test_methods:
            print(f"\\nğŸ§ª {test_name.replace('_', ' ').title()} å®Ÿè¡Œä¸­...")
            try:
                start_time = time.time()
                result = await test_method()
                end_time = time.time()
                
                self.test_results[test_name] = {
                    "passed": result,
                    "duration": end_time - start_time
                }
                
                if result:
                    passed_tests += 1
                    print(f"   âœ… {test_name} æˆåŠŸ ({self.test_results[test_name]['duration']:.3f}s)")
                else:
                    print(f"   âŒ {test_name} å¤±æ•—")
                    
            except Exception as e:
                print(f"   ğŸ’¥ {test_name} ã‚¨ãƒ©ãƒ¼: {e}")
                self.test_results[test_name] = {
                    "passed": False,
                    "error": str(e),
                    "duration": 0
                }
        
        # ç·åˆçµæœ
        success_rate = (passed_tests / total_tests) * 100
        total_duration = sum(r.get("duration", 0) for r in self.test_results.values())
        
        print(f"\\nğŸ“Š åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 70)
        print(f"åˆæ ¼ãƒ†ã‚¹ãƒˆ: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
        print(f"ç·å®Ÿè¡Œæ™‚é–“: {total_duration:.3f}ç§’")
        print(f"å¹³å‡ãƒ†ã‚¹ãƒˆæ™‚é–“: {total_duration/total_tests:.3f}ç§’")
        
        return {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "success_rate": success_rate,
            "total_duration": total_duration,
            "test_results": self.test_results,
            "performance_metrics": self.performance_metrics
        }
    
    async def test_performance(self) -> bool:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor()
            
            # å¤§é‡æ“ä½œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            operations = [
                ("detect_incident", {"anomaly_data": {"component": f"service_{i}", "severity": "medium"}})
                for i in range(100)
            ]
            
            start_time = time.time()
            
            # ãƒãƒƒãƒå®Ÿè¡Œ
            results = []
            for operation, data in operations:
                result = await processor.process_action(operation, data)
                results.append(result)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è§£æ
            successful_operations = sum(1 for r in results if r.get("success"))
            throughput = successful_operations / total_time
            avg_time_per_operation = total_time / len(operations)
            
            self.performance_metrics["batch_processing"] = {
                "total_operations": len(operations),
                "successful_operations": successful_operations,
                "total_time": total_time,
                "throughput_ops_per_sec": throughput,
                "avg_time_per_operation": avg_time_per_operation
            }
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–ç¢ºèª
            if throughput < 50:  # 50 ops/secæœªæº€ã¯å¤±æ•—
                print(f"     âŒ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä½ã„: {throughput:.1f} ops/sec")
                return False
            
            if avg_time_per_operation > 0.1:  # 100msè¶…ã¯å¤±æ•—
                print(f"     âŒ å¹³å‡å®Ÿè¡Œæ™‚é–“é•·ã„: {avg_time_per_operation:.3f}s")
                return False
            
            print(f"     âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆæˆåŠŸ: {throughput:.1f} ops/sec, {avg_time_per_operation:.3f}s/op")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_concurrency(self) -> bool:
        """ä¸¦è¡Œæ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor()
            
            # ä¸¦è¡Œã‚¿ã‚¹ã‚¯å®šç¾©
            async def concurrent_incident_detection(task_id):
                data = {
                    "anomaly_data": {
                        "component": f"concurrent_service_{task_id}",
                        "metric": "error_rate",
                        "severity": "high",
                        "task_id": task_id
                    }
                }
                return await processor.process_action("detect_incident", data)
            
            # ä¸¦è¡Œå®Ÿè¡Œ
            start_time = time.time()
            concurrent_tasks = [concurrent_incident_detection(i) for i in range(20)]
            results = await asyncio.gather(*concurrent_tasks, return_exceptions=True)
            end_time = time.time()
            
            # çµæœè§£æ
            successful_results = [r for r in results if not isinstance(r, Exception) and r.get("success")]
            failed_results = [r for r in results if isinstance(r, Exception) or not r.get("success")]
            
            concurrent_time = end_time - start_time
            
            self.performance_metrics["concurrency"] = {
                "total_tasks": len(concurrent_tasks),
                "successful_tasks": len(successful_results),
                "failed_tasks": len(failed_results),
                "execution_time": concurrent_time,
                "concurrent_throughput": len(successful_results) / concurrent_time
            }
            
            # ä¸¦è¡Œæ€§åŸºæº–ç¢ºèª
            if len(failed_results) > 2:  # 2ã¤è¶…ã®å¤±æ•—ã¯å•é¡Œ
                print(f"     âŒ ä¸¦è¡Œå‡¦ç†å¤±æ•—å¤šæ•°: {len(failed_results)}")
                return False
            
            if concurrent_time > 5.0:  # 5ç§’è¶…ã¯é…ã„
                print(f"     âŒ ä¸¦è¡Œå®Ÿè¡Œæ™‚é–“é•·ã„: {concurrent_time:.3f}s")
                return False
            
            print(f"     âœ… ä¸¦è¡Œæ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(successful_results)}/{len(concurrent_tasks)} æˆåŠŸ, {concurrent_time:.3f}s")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ ä¸¦è¡Œæ€§ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_error_handling(self) -> bool:
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor()
            
            # æ‚ªæ„ã®ã‚ã‚‹/ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
            error_test_cases = [
                {
                    "name": "NULL ãƒ‡ãƒ¼ã‚¿",
                    "action": "detect_incident",
                    "data": None
                },
                {
                    "name": "ç©ºãƒ‡ãƒ¼ã‚¿",
                    "action": "assess_quality", 
                    "data": {}
                },
                {
                    "name": "ç„¡åŠ¹JSONæ§‹é€ ",
                    "action": "create_alert_rule",
                    "data": {"alert_rule": {"invalid": "structure"}}
                },
                {
                    "name": "å­˜åœ¨ã—ãªã„ãƒªã‚½ãƒ¼ã‚¹",
                    "action": "respond_to_incident",
                    "data": {"incident_id": "non_existent_12345"}
                },
                {
                    "name": "ç„¡åŠ¹ãƒ‡ãƒ¼ã‚¿å‹", 
                    "action": "register_monitoring_target",
                    "data": {"target": "not_a_dict"}
                },
                {
                    "name": "å·¨å¤§ãƒ‡ãƒ¼ã‚¿",
                    "action": "search_similar_incidents",
                    "data": {"query": "x" * 10000}
                },
                {
                    "name": "è² ã®å€¤",
                    "action": "assess_quality",
                    "data": {
                        "standard_id": "test",
                        "component": "test",
                        "metrics": {"test_coverage": -50}
                    }
                }
            ]
            
            error_handling_results = []
            
            for test_case in error_test_cases:
                try:
                    # NULLãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
                    if test_case["data"] is None:
                        test_case["data"] = {}
                    
                    result = await processor.process_action(test_case["action"], test_case["data"])
                    
                    # ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if result.get("success"):
                        error_handling_results.append({
                            "case": test_case["name"],
                            "status": "unexpected_success",
                            "result": result
                        })
                    else:
                        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé©åˆ‡ã‹ç¢ºèª
                        if "error" in result and isinstance(result["error"], str):
                            error_handling_results.append({
                                "case": test_case["name"],
                                "status": "properly_handled",
                                "error": result["error"]
                            })
                        else:
                            error_handling_results.append({
                                "case": test_case["name"],
                                "status": "improper_error_format",
                                "result": result
                            })
                
                except Exception as e:
                    # äºˆæœŸã—ãªã„ä¾‹å¤–
                    error_handling_results.append({
                        "case": test_case["name"],
                        "status": "unhandled_exception",
                        "exception": str(e)
                    })
            
            # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è©•ä¾¡
            properly_handled = len([r for r in error_handling_results if r["status"] == "properly_handled"])
            total_cases = len(error_test_cases)
            
            if properly_handled < total_cases * 0.8:  # 80%æœªæº€ã¯å¤±æ•—
                print(f"     âŒ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸ååˆ†: {properly_handled}/{total_cases}")
                for result in error_handling_results:
                    if result["status"] != "properly_handled":
                        print(f"       â€¢ {result['case']}: {result['status']}")
                return False
            
            print(f"     âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆåŠŸ: {properly_handled}/{total_cases} é©åˆ‡å‡¦ç†")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_data_integrity(self) -> bool:
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor()
            
            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª
            
            # 1. ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆã¨å–å¾—ã®æ•´åˆæ€§
            original_incident_data = {
                "anomaly_data": {
                    "component": "integrity_test_service",
                    "metric": "data_consistency",
                    "value": 42.5,
                    "threshold": 30.0,
                    "severity": "high",
                    "confidence": 0.95
                }
            }
            
            create_result = await processor.process_action("detect_incident", original_incident_data)
            if not create_result.get("success"):
                print(f"     âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆå¤±æ•—")
                return False
            
            incident_id = create_result["data"]["incident_id"]
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰èª­ã¿ç›´ã—
            processor2 = IncidentProcessor()
            if incident_id not in processor2.incidents:
                print(f"     âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–å¤±æ•—")
                return False
            
            stored_incident = processor2.incidents[incident_id]
            
            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç¢ºèª
            if stored_incident.title != create_result["data"]["title"]:
                print(f"     âŒ ã‚¿ã‚¤ãƒˆãƒ«ä¸æ•´åˆ")
                return False
            
            if stored_incident.severity.value != create_result["data"]["severity"]:
                print(f"     âŒ é‡è¦åº¦ä¸æ•´åˆ")
                return False
            
            # 2. å“è³ªåŸºæº–ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
            quality_data = {
                "standard": {
                    "name": "Integrity Test Standard",
                    "description": "ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆç”¨å“è³ªåŸºæº–",
                    "category": "testing",
                    "metrics": {
                        "data_consistency": {
                            "name": "Data Consistency",
                            "target_value": 99.9,
                            "threshold_min": 95.0,
                            "unit": "%",
                            "description": "ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§"
                        }
                    },
                    "compliance_threshold": 95.0
                }
            }
            
            register_result = await processor.process_action("register_quality_standard", quality_data)
            if not register_result.get("success"):
                print(f"     âŒ å“è³ªåŸºæº–ç™»éŒ²å¤±æ•—")
                return False
            
            standard_id = register_result["data"]["standard_id"]
            
            # æ–°ã—ã„ãƒ—ãƒ­ã‚»ãƒƒã‚µã§ç¢ºèª
            processor3 = IncidentProcessor()
            if standard_id not in processor3.quality_standards:
                print(f"     âŒ å“è³ªåŸºæº–ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–å¤±æ•—")
                return False
            
            stored_standard = processor3.quality_standards[standard_id]
            if stored_standard.name != quality_data["standard"]["name"]:
                print(f"     âŒ å“è³ªåŸºæº–åä¸æ•´åˆ")
                return False
            
            print(f"     âœ… ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ: ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ»å“è³ªåŸºæº–æ°¸ç¶šåŒ–ç¢ºèª")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_complex_workflow(self) -> bool:
        """è¤‡é›‘ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor()
            
            # è¤‡é›‘ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚·ãƒŠãƒªã‚ª
            # 1. è¤‡æ•°ã‚µãƒ¼ãƒ“ã‚¹ã§ã®é€£é–çš„ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ
            services = ["frontend", "api_gateway", "user_service", "payment_service", "database"]
            incidents = []
            
            # æ™‚ç³»åˆ—ã§ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç™ºç”Ÿ
            for i, service in enumerate(services):
                incident_data = {
                    "anomaly_data": {
                        "component": service,
                        "metric": "response_time" if i % 2 == 0 else "error_rate",
                        "severity": "critical" if i == 0 else "high",
                        "confidence": 0.9 - (i * 0.1)
                    }
                }
                
                result = await processor.process_action("detect_incident", incident_data)
                if result.get("success"):
                    incidents.append(result["data"]["incident_id"])
                
                # å°‘ã—å¾…æ©Ÿã—ã¦æ™‚ç³»åˆ—ã‚’ä½œã‚‹
                await asyncio.sleep(0.001)
            
            if len(incidents) != len(services):
                print(f"     âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆæ•°ä¸ä¸€è‡´: {len(incidents)} != {len(services)}")
                return False
            
            # 2. ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’å®Ÿè¡Œ
            pattern_result = await processor.process_action("learn_incident_patterns", {})
            if not pattern_result.get("success"):
                print(f"     âŒ ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’å¤±æ•—")
                return False
            
            # 3. ç›¸é–¢åˆ†æå®Ÿè¡Œ
            correlation_result = await processor.process_action("analyze_correlations", {})
            if not correlation_result.get("success"):
                print(f"     âŒ ç›¸é–¢åˆ†æå¤±æ•—")
                return False
            
            correlations = correlation_result["data"]["correlations"]
            
            # 4. è‡ªå‹•ä¿®å¾©è©¦è¡Œ
            remediation_results = []
            for incident_id in incidents[:2]:  # æœ€åˆã®2ã¤ã®ã¿
                remediation_result = await processor.process_action("attempt_automated_remediation", {
                    "incident_id": incident_id
                })
                remediation_results.append(remediation_result)
            
            successful_remediations = [r for r in remediation_results if r.get("success")]
            
            # 5. çµ±è¨ˆåˆ†æ
            stats_result = await processor.process_action("get_statistics", {})
            if not stats_result.get("success"):
                print(f"     âŒ çµ±è¨ˆå–å¾—å¤±æ•—")
                return False
            
            stats = stats_result["data"]
            
            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµæœæ¤œè¨¼
            if stats["incident_statistics"]["total_incidents"] < len(services):
                print(f"     âŒ çµ±è¨ˆã®ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ•°ä¸æ­£")
                return False
            
            if len(successful_remediations) == 0:
                print(f"     âŒ è‡ªå‹•ä¿®å¾©ãŒå…¨ã¦å¤±æ•—")
                return False
            
            print(f"     âœ… è¤‡é›‘ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æˆåŠŸ: {len(incidents)}ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ, {len(correlations)}ç›¸é–¢, {len(successful_remediations)}ä¿®å¾©æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ è¤‡é›‘ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_memory_efficiency(self) -> bool:
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            import psutil
            import os
            
            # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            process = psutil.Process(os.getpid())
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            processor = IncidentProcessor()
            
            # å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            for i in range(500):
                await processor.process_action("detect_incident", {
                    "anomaly_data": {
                        "component": f"memory_test_service_{i}",
                        "metric": "memory_usage",
                        "severity": "low"
                    }
                })
                
                # å“è³ªè©•ä¾¡ã‚‚å®Ÿè¡Œ
                default_standard_id = list(processor.quality_standards.keys())[0]
                await processor.process_action("assess_quality", {
                    "standard_id": default_standard_id,
                    "component": f"component_{i}",
                    "metrics": {"test_coverage": 80.0 + (i % 20)}
                })
            
            # æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = final_memory - initial_memory
            
            self.performance_metrics["memory_usage"] = {
                "initial_memory_mb": initial_memory,
                "final_memory_mb": final_memory,
                "memory_increase_mb": memory_increase,
                "operations_performed": 1000,
                "memory_per_operation_kb": (memory_increase * 1024) / 1000
            }
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§åŸºæº–ç¢ºèª
            if memory_increase > 100:  # 100MBè¶…å¢—åŠ ã¯å•é¡Œ
                print(f"     âŒ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ éå¤§: {memory_increase:.1f}MB")
                return False
            
            memory_per_op = (memory_increase * 1024) / 1000  # KB per operation
            if memory_per_op > 10:  # 10KB/opè¶…ã¯å•é¡Œ
                print(f"     âŒ æ“ä½œã‚ãŸã‚Šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡éå¤§: {memory_per_op:.1f}KB/op")
                return False
            
            print(f"     âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ: +{memory_increase:.1f}MB, {memory_per_op:.1f}KB/op")
            return True
            
        except ImportError:
            print(f"     âš ï¸ psutilæœªåˆ©ç”¨å¯èƒ½ã€ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆã‚¹ã‚­ãƒƒãƒ—")
            return True
        except Exception as e:
            print(f"     ğŸ’¥ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_incident_lifecycle(self) -> bool:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor(test_mode=True)
            processor.reset_for_testing()
            
            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ãƒ†ã‚¹ãƒˆ
            
            # 1. ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œçŸ¥
            detection_data = {
                "anomaly_data": {
                    "component": "lifecycle_test_service",
                    "metric": "availability",
                    "severity": "critical",
                    "confidence": 0.98
                }
            }
            
            detection_result = await processor.process_action("detect_incident", detection_data)
            if not detection_result.get("success"):
                print(f"     âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œçŸ¥å¤±æ•—")
                return False
            
            incident_id = detection_result["data"]["incident_id"]
            initial_status = detection_result["data"]["status"]
            
            if initial_status != "open":
                print(f"     âŒ åˆæœŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ä¸æ­£: {initial_status}")
                return False
            
            # 2. ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œ
            response_result = await processor.process_action("respond_to_incident", {
                "incident_id": incident_id
            })
            
            if not response_result.get("success"):
                print(f"     âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œå¤±æ•—")
                return False
            
            response_status = response_result["data"]["response_status"]
            incident_status_after_response = response_result["data"]["incident_status"]
            
            # 3. è‡ªå‹•ä¿®å¾©è©¦è¡Œ
            remediation_result = await processor.process_action("attempt_automated_remediation", {
                "incident_id": incident_id
            })
            
            if not remediation_result.get("success"):
                print(f"     âŒ è‡ªå‹•ä¿®å¾©è©¦è¡Œå¤±æ•—")
                return False
            
            remediation_status = remediation_result["data"]["status"]
            
            # 4. ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆçŠ¶æ…‹ç¢ºèª
            # test_modeã§ã¯æ°¸ç¶šåŒ–ã•ã‚Œãªã„ãŸã‚ã€åŒã˜ãƒ—ãƒ­ã‚»ãƒƒã‚µã§ç¢ºèª
            if incident_id not in processor.incidents:
                print(f"     âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãŒãƒ¡ãƒ¢ãƒªã«å­˜åœ¨ã—ãªã„")
                return False
            
            final_incident = processor.incidents[incident_id]
            
            # ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«æ¤œè¨¼
            lifecycle_steps = [
                ("detection", detection_result["data"]["incident_id"]),
                ("response", response_status),
                ("remediation", remediation_status),
                ("final_status", final_incident.status.value)
            ]
            
            print(f"     âœ… ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«æˆåŠŸ:")
            for step_name, step_result in lifecycle_steps:
                print(f"       â€¢ {step_name}: {step_result}")
            
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_quality_assessment_comprehensive(self) -> bool:
        """å“è³ªè©•ä¾¡åŒ…æ‹¬ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor()
            
            # è¤‡æ•°å“è³ªåŸºæº–ã§ã®åŒ…æ‹¬è©•ä¾¡
            
            # 1. ã‚«ã‚¹ã‚¿ãƒ å“è³ªåŸºæº–ä½œæˆ
            custom_standards = [
                {
                    "name": "Security Standard",
                    "category": "security",
                    "metrics": {
                        "security_score": {"target_value": 95.0, "threshold_min": 90.0},
                        "vulnerability_count": {"target_value": 0.0, "threshold_min": 2.0}
                    },
                    "compliance_threshold": 90.0
                },
                {
                    "name": "Performance Standard", 
                    "category": "performance",
                    "metrics": {
                        "response_time": {"target_value": 100.0, "threshold_min": 200.0},
                        "throughput": {"target_value": 1000.0, "threshold_min": 500.0}
                    },
                    "compliance_threshold": 85.0
                }
            ]
            
            created_standards = []
            for standard_data in custom_standards:
                result = await processor.process_action("register_quality_standard", {
                    "standard": standard_data
                })
                if result.get("success"):
                    created_standards.append(result["data"]["standard_id"])
            
            if len(created_standards) != len(custom_standards):
                print(f"     âŒ å“è³ªåŸºæº–ä½œæˆæ•°ä¸ä¸€è‡´")
                return False
            
            # 2. è¤‡æ•°ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã®å“è³ªè©•ä¾¡
            components = ["frontend", "backend", "database"]
            assessment_results = []
            
            for component in components:
                for standard_id in created_standards:
                    # ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ
                    if "security" in processor.quality_standards[standard_id].name.lower():
                        metrics = {
                            "security_score": 92.5,
                            "vulnerability_count": 1.0
                        }
                    else:
                        metrics = {
                            "response_time": 150.0,
                            "throughput": 750.0
                        }
                    
                    assessment_result = await processor.process_action("assess_quality", {
                        "standard_id": standard_id,
                        "component": component,
                        "metrics": metrics
                    })
                    
                    if assessment_result.get("success"):
                        assessment_results.append(assessment_result["data"])
            
            # 3. è©•ä¾¡çµæœåˆ†æ
            total_assessments = len(components) * len(created_standards)
            successful_assessments = len(assessment_results)
            
            if successful_assessments != total_assessments:
                print(f"     âŒ è©•ä¾¡å®Ÿè¡Œæ•°ä¸ä¸€è‡´: {successful_assessments} != {total_assessments}")
                return False
            
            # ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ç‡è¨ˆç®—
            compliant_assessments = [a for a in assessment_results if a["is_compliant"]]
            compliance_rate = len(compliant_assessments) / len(assessment_results) * 100
            
            # å¹³å‡å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            avg_quality_score = sum(a["overall_score"] for a in assessment_results) / len(assessment_results)
            
            self.performance_metrics["quality_assessment"] = {
                "total_assessments": total_assessments,
                "successful_assessments": successful_assessments,
                "compliance_rate": compliance_rate,
                "average_quality_score": avg_quality_score
            }
            
            print(f"     âœ… å“è³ªè©•ä¾¡åŒ…æ‹¬ãƒ†ã‚¹ãƒˆæˆåŠŸ: {compliance_rate:.1f}%é©åˆ, å¹³å‡{avg_quality_score:.1f}ç‚¹")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ å“è³ªè©•ä¾¡åŒ…æ‹¬ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_alert_system_integration(self) -> bool:
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor(test_mode=True)
            processor.reset_for_testing()
            
            # ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆã‚·ãƒŠãƒªã‚ª
            
            # 1. è¤‡æ•°ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ä½œæˆ
            alert_rules = [
                {
                    "name": "CPU Alert",
                    "condition_expression": "cpu_usage > 80.0",
                    "severity": "high",
                    "enabled": True
                },
                {
                    "name": "Memory Alert",
                    "condition_expression": "memory_usage > 90.0", 
                    "severity": "critical",
                    "enabled": True
                },
                {
                    "name": "Disk Alert",
                    "condition_expression": "disk_usage > 95.0",
                    "severity": "medium",
                    "enabled": False  # ç„¡åŠ¹
                }
            ]
            
            created_rules = []
            for rule_data in alert_rules:
                result = await processor.process_action("create_alert_rule", {
                    "alert_rule": rule_data
                })
                if result.get("success"):
                    created_rules.append(result["data"]["rule_id"])
            
            if len(created_rules) != len(alert_rules):
                print(f"     âŒ ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ä½œæˆæ•°ä¸ä¸€è‡´")
                return False
            
            # 2. ã‚¢ãƒ©ãƒ¼ãƒˆè©•ä¾¡ã‚·ãƒŠãƒªã‚ª
            test_metrics = [
                {
                    "scenario": "normal",
                    "metrics": {"cpu_usage": 70.0, "memory_usage": 75.0, "disk_usage": 80.0},
                    "expected_alerts": 0
                },
                {
                    "scenario": "cpu_high",
                    "metrics": {"cpu_usage": 85.0, "memory_usage": 75.0, "disk_usage": 80.0},
                    "expected_alerts": 1
                },
                {
                    "scenario": "multiple_alerts",
                    "metrics": {"cpu_usage": 85.0, "memory_usage": 95.0, "disk_usage": 98.0},
                    "expected_alerts": 2  # disk_usageã¯ç„¡åŠ¹ãƒ«ãƒ¼ãƒ«ãªã®ã§é™¤å¤–
                }
            ]
            
            alert_evaluation_results = []
            
            for test_case in test_metrics:
                result = await processor.process_action("evaluate_alert_rules", {
                    "metrics": test_case["metrics"],
                    "reset_cooldown": True  # ãƒ†ã‚¹ãƒˆç”¨ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ãƒªã‚»ãƒƒãƒˆ
                })
                
                if result.get("success"):
                    triggered_count = result["data"]["alert_count"]
                    alert_evaluation_results.append({
                        "scenario": test_case["scenario"],
                        "expected": test_case["expected_alerts"],
                        "actual": triggered_count,
                        "match": triggered_count == test_case["expected_alerts"]
                    })
            
            # 3. ã‚¢ãƒ©ãƒ¼ãƒˆè©•ä¾¡çµæœæ¤œè¨¼
            successful_evaluations = [r for r in alert_evaluation_results if r["match"]]
            
            if len(successful_evaluations) != len(test_metrics):
                print(f"     âŒ ã‚¢ãƒ©ãƒ¼ãƒˆè©•ä¾¡çµæœä¸ä¸€è‡´:")
                for result in alert_evaluation_results:
                    if not result["match"]:
                        print(f"       â€¢ {result['scenario']}: æœŸå¾…{result['expected']}, å®Ÿéš›{result['actual']}")
                return False
            
            print(f"     âœ… ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆæˆåŠŸ: {len(created_rules)}ãƒ«ãƒ¼ãƒ«, {len(successful_evaluations)}ã‚·ãƒŠãƒªã‚ª")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_monitoring_comprehensive(self) -> bool:
        """ç›£è¦–æ©Ÿèƒ½åŒ…æ‹¬ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor()
            
            # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åŒ…æ‹¬ãƒ†ã‚¹ãƒˆ
            
            # 1. è¤‡æ•°ç›£è¦–å¯¾è±¡ç™»éŒ²
            monitoring_targets = [
                {
                    "name": "Web Server",
                    "type": "web_service",
                    "endpoint_url": "http://web-server:80",
                    "health_check_enabled": True
                },
                {
                    "name": "Database Server",
                    "type": "database",
                    "endpoint_url": "http://db-server:5432",
                    "health_check_enabled": True
                },
                {
                    "name": "Cache Server",
                    "type": "cache",
                    "endpoint_url": "http://cache-server:6379",
                    "health_check_enabled": False
                }
            ]
            
            created_targets = []
            for target_data in monitoring_targets:
                result = await processor.process_action("register_monitoring_target", {
                    "target": target_data
                })
                if result.get("success"):
                    created_targets.append(result["data"]["target_id"])
            
            if len(created_targets) != len(monitoring_targets):
                print(f"     âŒ ç›£è¦–å¯¾è±¡ç™»éŒ²æ•°ä¸ä¸€è‡´")
                return False
            
            # 2. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
            health_check_results = []
            for target_id in created_targets:
                result = await processor.process_action("check_target_health", {
                    "target_id": target_id
                })
                if result.get("success"):
                    health_check_results.append(result["data"])
            
            if len(health_check_results) != len(created_targets):
                print(f"     âŒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œæ•°ä¸ä¸€è‡´")
                return False
            
            # 3. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯çµæœåˆ†æ
            healthy_targets = [r for r in health_check_results if r["status"] == "healthy"]
            avg_response_time = sum(r["response_time_ms"] for r in health_check_results) / len(health_check_results)
            avg_uptime = sum(r["uptime_percentage"] for r in health_check_results) / len(health_check_results)
            
            self.performance_metrics["monitoring"] = {
                "total_targets": len(created_targets),
                "healthy_targets": len(healthy_targets),
                "health_rate": len(healthy_targets) / len(created_targets) * 100,
                "average_response_time": avg_response_time,
                "average_uptime": avg_uptime
            }
            
            # åŸºæº–ç¢ºèª
            if len(healthy_targets) == 0:
                print(f"     âŒ å…¨ç›£è¦–å¯¾è±¡ãŒä¸å¥åº·")
                return False
            
            print(f"     âœ… ç›£è¦–æ©Ÿèƒ½åŒ…æ‹¬ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(healthy_targets)}/{len(created_targets)}å¥åº·, å¹³å‡å¿œç­”{avg_response_time:.1f}ms")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ ç›£è¦–æ©Ÿèƒ½åŒ…æ‹¬ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_pattern_learning_advanced(self) -> bool:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’é«˜åº¦ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor()
            
            # é«˜åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ãƒ†ã‚¹ãƒˆ
            
            # 1. è¤‡é›‘ãªã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ä½œæˆ
            incident_patterns = [
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é–¢é€£ãƒ‘ã‚¿ãƒ¼ãƒ³
                *[{
                    "component": f"web_server_{i}",
                    "metric": "response_time",
                    "severity": "high",
                    "category": "performance"
                } for i in range(5)],
                
                # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢é€£ãƒ‘ã‚¿ãƒ¼ãƒ³
                *[{
                    "component": f"auth_service_{i}",
                    "metric": "failed_logins",
                    "severity": "medium",
                    "category": "security"
                } for i in range(3)],
                
                # å¯ç”¨æ€§é–¢é€£ãƒ‘ã‚¿ãƒ¼ãƒ³
                *[{
                    "component": f"database_{i}",
                    "metric": "connection_failure",
                    "severity": "critical",
                    "category": "availability"
                } for i in range(4)]
            ]
            
            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆ
            created_incidents = []
            for pattern in incident_patterns:
                result = await processor.process_action("detect_incident", {
                    "anomaly_data": pattern
                })
                if result.get("success"):
                    created_incidents.append(result["data"]["incident_id"])
            
            if len(created_incidents) != len(incident_patterns):
                print(f"     âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆæ•°ä¸ä¸€è‡´")
                return False
            
            # 2. ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’å®Ÿè¡Œ
            learning_result = await processor.process_action("learn_incident_patterns", {})
            
            if not learning_result.get("success"):
                print(f"     âŒ ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’å¤±æ•—")
                return False
            
            learning_data = learning_result["data"]
            patterns_learned = learning_data["patterns_learned"]
            patterns = learning_data["patterns"]
            
            # 3. å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            expected_categories = ["performance", "security", "availability"]
            learned_categories = [p["category"] for p in patterns]
            
            category_coverage = len(set(learned_categories) & set(expected_categories))
            
            if category_coverage < 2:  # æœ€ä½2ã‚«ãƒ†ã‚´ãƒªå­¦ç¿’å¿…è¦
                print(f"     âŒ ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ã‚«ãƒ†ã‚´ãƒªä¸è¶³: {category_coverage}")
                return False
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³å“è³ªè©•ä¾¡
            pattern_quality_scores = []
            for pattern in patterns:
                quality_score = 0
                # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå…±é€šæ€§
                if len(pattern["common_components"]) > 0:
                    quality_score += 30
                # ã‚¿ã‚°å…±é€šæ€§
                if len(pattern["common_tags"]) > 0:
                    quality_score += 20
                # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ•°
                if pattern["incident_count"] >= 3:
                    quality_score += 30
                # å¹³å‡é‡è¦åº¦
                if 2.0 <= pattern["average_severity"] <= 4.0:
                    quality_score += 20
                
                pattern_quality_scores.append(quality_score)
            
            avg_pattern_quality = sum(pattern_quality_scores) / len(pattern_quality_scores) if pattern_quality_scores else 0
            
            self.performance_metrics["pattern_learning"] = {
                "total_incidents": len(created_incidents),
                "patterns_learned": patterns_learned,
                "category_coverage": category_coverage,
                "average_pattern_quality": avg_pattern_quality
            }
            
            print(f"     âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’é«˜åº¦ãƒ†ã‚¹ãƒˆæˆåŠŸ: {patterns_learned}ãƒ‘ã‚¿ãƒ¼ãƒ³, {category_coverage}ã‚«ãƒ†ã‚´ãƒª, å“è³ª{avg_pattern_quality:.1f}")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’é«˜åº¦ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_correlation_analysis_detailed(self) -> bool:
        """ç›¸é–¢åˆ†æè©³ç´°ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor(test_mode=True)
            processor.reset_for_testing()
            
            # ç›¸é–¢åˆ†æè©³ç´°ãƒ†ã‚¹ãƒˆ
            
            # 1. æ™‚é–“çš„ç›¸é–¢ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆ
            time_correlated_incidents = []
            
            # åŒæ™‚æœŸã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç¾¤1
            for i in range(3):
                result = await processor.process_action("detect_incident", {
                    "anomaly_data": {
                        "component": f"frontend_app_{i}",
                        "metric": "error_rate",
                        "severity": "high"
                    }
                })
                if result.get("success"):
                    time_correlated_incidents.append(result["data"]["incident_id"])
                await asyncio.sleep(0.001)  # çŸ­é–“éš”
            
            # é–“éš”ã‚’é–‹ã‘ã‚‹
            await asyncio.sleep(0.01)
            
            # åŒæ™‚æœŸã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç¾¤2
            for i in range(2):
                result = await processor.process_action("detect_incident", {
                    "anomaly_data": {
                        "component": f"backend_service_{i}",
                        "metric": "response_time",
                        "severity": "medium"
                    }
                })
                if result.get("success"):
                    time_correlated_incidents.append(result["data"]["incident_id"])
                await asyncio.sleep(0.001)
            
            # 2. ç©ºé–“çš„ç›¸é–¢ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆ
            space_correlated_incidents = []
            shared_components = ["payment_service", "user_database"]
            
            for component in shared_components:
                for metric in ["availability", "performance"]:
                    result = await processor.process_action("detect_incident", {
                        "anomaly_data": {
                            "component": component,
                            "metric": metric,
                            "severity": "high"
                        }
                    })
                    if result.get("success"):
                        space_correlated_incidents.append(result["data"]["incident_id"])
            
            # 3. ç›¸é–¢åˆ†æå®Ÿè¡Œ
            correlation_result = await processor.process_action("analyze_correlations", {})
            
            if not correlation_result.get("success"):
                print(f"     âŒ ç›¸é–¢åˆ†æå¤±æ•—")
                return False
            
            correlation_data = correlation_result["data"]
            correlations = correlation_data["correlations"]
            analyzed_incidents = correlation_data["analyzed_incidents"]
            
            # 4. ç›¸é–¢åˆ†æçµæœè©•ä¾¡
            total_incidents_created = len(time_correlated_incidents) + len(space_correlated_incidents)
            
            if analyzed_incidents != total_incidents_created:
                print(f"     âŒ åˆ†æã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ•°ä¸ä¸€è‡´: {analyzed_incidents} != {total_incidents_created}")
                return False
            
            # ç›¸é–¢å“è³ªè©•ä¾¡
            high_confidence_correlations = [c for c in correlations if c["confidence"] >= 0.7]
            temporal_correlations = [c for c in correlations if c["correlation_type"] == "temporal_spatial"]
            
            self.performance_metrics["correlation_analysis"] = {
                "total_incidents_created": total_incidents_created,
                "total_correlations_found": len(correlations),
                "high_confidence_correlations": len(high_confidence_correlations),
                "temporal_correlations": len(temporal_correlations),
                "correlation_discovery_rate": len(correlations) / max(total_incidents_created, 1)
            }
            
            # æœ€ä½é™ã®ç›¸é–¢æ¤œå‡ºç¢ºèª
            if len(correlations) == 0:
                print(f"     âš ï¸ ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼ˆæ­£å¸¸ã€ãƒ‡ãƒ¼ã‚¿ä¾å­˜ï¼‰")
            
            print(f"     âœ… ç›¸é–¢åˆ†æè©³ç´°ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(correlations)}ç›¸é–¢æ¤œå‡º, {len(high_confidence_correlations)}é«˜ä¿¡é ¼åº¦")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ ç›¸é–¢åˆ†æè©³ç´°ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_remediation_effectiveness(self) -> bool:
        """ä¿®å¾©åŠ¹æœãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor()
            
            # ä¿®å¾©åŠ¹æœãƒ†ã‚¹ãƒˆ
            
            # 1. ç•°ãªã‚‹ã‚«ãƒ†ã‚´ãƒªã®ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆ
            test_categories = [
                ("performance", "cache_service", "memory_leak"),
                ("availability", "web_server", "service_down"),
                ("quality", "api_gateway", "validation_error"),
                ("security", "auth_service", "unauthorized_access")
            ]
            
            remediation_results = []
            
            for category, component, metric in test_categories:
                # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆ
                incident_result = await processor.process_action("detect_incident", {
                    "anomaly_data": {
                        "component": component,
                        "metric": metric,
                        "severity": "high",
                        "category": category
                    }
                })
                
                if not incident_result.get("success"):
                    continue
                
                incident_id = incident_result["data"]["incident_id"]
                
                # ä¿®å¾©è©¦è¡Œ
                remediation_result = await processor.process_action("attempt_automated_remediation", {
                    "incident_id": incident_id
                })
                
                if remediation_result.get("success"):
                    remediation_data = remediation_result["data"]
                    remediation_results.append({
                        "category": category,
                        "component": component,
                        "incident_id": incident_id,
                        "status": remediation_data["status"],
                        "actions_taken": remediation_data.get("actions_taken", [])
                    })
            
            # 2. ä¿®å¾©åŠ¹æœåˆ†æ
            successful_remediations = [r for r in remediation_results if r["status"] == "success"]
            failed_remediations = [r for r in remediation_results if r["status"] == "failed"]
            no_action_remediations = [r for r in remediation_results if r["status"] == "no_action"]
            
            success_rate = len(successful_remediations) / len(remediation_results) * 100 if remediation_results else 0
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ä¿®å¾©åŠ¹æœ
            category_effectiveness = {}
            for category in [c[0] for c in test_categories]:
                category_results = [r for r in remediation_results if r["category"] == category]
                category_successes = [r for r in category_results if r["status"] == "success"]
                category_effectiveness[category] = len(category_successes) / len(category_results) * 100 if category_results else 0
            
            self.performance_metrics["remediation_effectiveness"] = {
                "total_remediations": len(remediation_results),
                "successful_remediations": len(successful_remediations),
                "failed_remediations": len(failed_remediations),
                "no_action_remediations": len(no_action_remediations),
                "overall_success_rate": success_rate,
                "category_effectiveness": category_effectiveness
            }
            
            # åŠ¹æœåŸºæº–ç¢ºèª
            if success_rate < 50:  # 50%æœªæº€ã¯å•é¡Œ
                print(f"     âŒ ä¿®å¾©æˆåŠŸç‡ä½ã„: {success_rate:.1f}%")
                return False
            
            # å…¨ã‚«ãƒ†ã‚´ãƒªã§ä¿®å¾©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
            categories_with_actions = len([r for r in remediation_results if r["actions_taken"]])
            if categories_with_actions < len(test_categories) * 0.8:  # 80%æœªæº€ã¯å•é¡Œ
                print(f"     âŒ ä¿®å¾©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®šç¾©ä¸è¶³")
                return False
            
            print(f"     âœ… ä¿®å¾©åŠ¹æœãƒ†ã‚¹ãƒˆæˆåŠŸ: {success_rate:.1f}%æˆåŠŸç‡, {len(test_categories)}ã‚«ãƒ†ã‚´ãƒªå¯¾å¿œ")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ ä¿®å¾©åŠ¹æœãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_statistics_accuracy(self) -> bool:
        """çµ±è¨ˆç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor(test_mode=True)
            processor.reset_for_testing()
            
            # çµ±è¨ˆç²¾åº¦ãƒ†ã‚¹ãƒˆ
            
            # 1. æ—¢çŸ¥ãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆä½œæˆ
            known_data = {
                "incidents": [],
                "quality_assessments": [],
                "alert_rules": [],
                "monitoring_targets": []
            }
            
            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆï¼ˆæ—¢çŸ¥æ•°ï¼‰
            incident_severities = ["low", "medium", "high", "critical"]
            incidents_per_severity = 3
            
            for severity in incident_severities:
                for i in range(incidents_per_severity):
                    result = await processor.process_action("detect_incident", {
                        "anomaly_data": {
                            "component": f"test_service_{severity}_{i}",
                            "metric": "test_metric",
                            "severity": severity
                        }
                    })
                    if result.get("success"):
                        known_data["incidents"].append(result["data"]["incident_id"])
            
            # ã„ãã¤ã‹ã®ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚’è§£æ±ºçŠ¶æ…‹ã«ã™ã‚‹
            resolved_count = 0
            for incident_id in known_data["incidents"][:6]:  # åŠåˆ†ã‚’è§£æ±º
                result = await processor.process_action("respond_to_incident", {
                    "incident_id": incident_id
                })
                if result.get("success") and result["data"]["incident_status"] == "resolved":
                    resolved_count += 1
            
            # å“è³ªè©•ä¾¡å®Ÿè¡Œï¼ˆæ—¢çŸ¥æ•°ï¼‰
            default_standard_id = list(processor.quality_standards.keys())[0]
            quality_assessments = 5
            
            for i in range(quality_assessments):
                result = await processor.process_action("assess_quality", {
                    "standard_id": default_standard_id,
                    "component": f"test_component_{i}",
                    "metrics": {"test_coverage": 80.0 + i}
                })
                if result.get("success"):
                    known_data["quality_assessments"].append(result["data"]["assessment_id"])
            
            # ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ä½œæˆï¼ˆæ—¢çŸ¥æ•°ï¼‰
            alert_rules_count = 4
            for i in range(alert_rules_count):
                result = await processor.process_action("create_alert_rule", {
                    "alert_rule": {
                        "name": f"Test Alert {i}",
                        "condition_expression": f"metric_{i} > {i*10}",
                        "severity": "medium",
                        "enabled": i % 2 == 0  # åŠåˆ†ã‚’æœ‰åŠ¹
                    }
                })
                if result.get("success"):
                    known_data["alert_rules"].append(result["data"]["rule_id"])
            
            # ç›£è¦–å¯¾è±¡ç™»éŒ²ï¼ˆæ—¢çŸ¥æ•°ï¼‰
            monitoring_targets_count = 3
            for i in range(monitoring_targets_count):
                result = await processor.process_action("register_monitoring_target", {
                    "target": {
                        "name": f"Test Target {i}",
                        "type": "service",
                        "endpoint_url": f"http://test-{i}:8080",
                        "health_check_enabled": True
                    }
                })
                if result.get("success"):
                    known_data["monitoring_targets"].append(result["data"]["target_id"])
            
            # 2. çµ±è¨ˆå–å¾—ãƒ»ç²¾åº¦ç¢ºèª
            stats_result = await processor.process_action("get_statistics", {})
            
            if not stats_result.get("success"):
                print(f"     âŒ çµ±è¨ˆå–å¾—å¤±æ•—")
                return False
            
            stats = stats_result["data"]
            
            # çµ±è¨ˆç²¾åº¦æ¤œè¨¼
            accuracy_checks = []
            
            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆçµ±è¨ˆ
            incident_stats = stats["incident_statistics"]
            accuracy_checks.append({
                "metric": "total_incidents",
                "expected": len(known_data["incidents"]),
                "actual": incident_stats["total_incidents"],
                "accurate": incident_stats["total_incidents"] == len(known_data["incidents"])
            })
            
            # è§£æ±ºç‡è¨ˆç®—ç¢ºèª
            expected_resolution_rate = (resolved_count / len(known_data["incidents"])) * 100 if known_data["incidents"] else 0
            resolution_rate_diff = abs(incident_stats["resolution_rate"] - expected_resolution_rate)
            accuracy_checks.append({
                "metric": "resolution_rate",
                "expected": expected_resolution_rate,
                "actual": incident_stats["resolution_rate"],
                "accurate": resolution_rate_diff < 1.0  # 1%ä»¥å†…ã®èª¤å·®è¨±å®¹
            })
            
            # ã‚¢ãƒ©ãƒ¼ãƒˆçµ±è¨ˆ
            alert_stats = stats["alert_statistics"]
            enabled_rules = len([r for r in known_data["alert_rules"]]) // 2  # åŠåˆ†ãŒæœ‰åŠ¹
            accuracy_checks.append({
                "metric": "alert_rules_active",
                "expected": enabled_rules,
                "actual": alert_stats["alert_rules_active"],
                "accurate": abs(alert_stats["alert_rules_active"] - enabled_rules) <= 1
            })
            
            # ç›£è¦–çµ±è¨ˆ
            monitoring_stats = stats["monitoring_statistics"]
            accuracy_checks.append({
                "metric": "monitoring_targets_count",
                "expected": len(known_data["monitoring_targets"]),
                "actual": monitoring_stats["monitoring_targets_count"],
                "accurate": monitoring_stats["monitoring_targets_count"] == len(known_data["monitoring_targets"])
            })
            
            # ç²¾åº¦è©•ä¾¡
            accurate_metrics = [c for c in accuracy_checks if c["accurate"]]
            accuracy_rate = len(accurate_metrics) / len(accuracy_checks) * 100
            
            self.performance_metrics["statistics_accuracy"] = {
                "total_metrics_checked": len(accuracy_checks),
                "accurate_metrics": len(accurate_metrics),
                "accuracy_rate": accuracy_rate,
                "accuracy_details": accuracy_checks
            }
            
            # ç²¾åº¦åŸºæº–ç¢ºèª
            if accuracy_rate < 80:  # 80%æœªæº€ã¯å•é¡Œ
                print(f"     âŒ çµ±è¨ˆç²¾åº¦ä½ã„: {accuracy_rate:.1f}%")
                for check in accuracy_checks:
                    if not check["accurate"]:
                        print(f"       â€¢ {check['metric']}: æœŸå¾…{check['expected']}, å®Ÿéš›{check['actual']}")
                return False
            
            print(f"     âœ… çµ±è¨ˆç²¾åº¦ãƒ†ã‚¹ãƒˆæˆåŠŸ: {accuracy_rate:.1f}%ç²¾åº¦, {len(accurate_metrics)}/{len(accuracy_checks)}é …ç›®")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ çµ±è¨ˆç²¾åº¦ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_stress_load(self) -> bool:
        """ã‚¹ãƒˆãƒ¬ã‚¹è² è·ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor()
            
            # ã‚¹ãƒˆãƒ¬ã‚¹è² è·ãƒ†ã‚¹ãƒˆ
            
            # 1. å¤§é‡åŒæ™‚æ“ä½œãƒ†ã‚¹ãƒˆ
            stress_operations = []
            
            # å„ç¨®æ“ä½œã‚’æ··åˆ
            for i in range(200):
                operation_type = i % 4
                
                if operation_type == 0:
                    # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œçŸ¥
                    stress_operations.append(("detect_incident", {
                        "anomaly_data": {
                            "component": f"stress_service_{i}",
                            "metric": "stress_metric",
                            "severity": ["low", "medium", "high"][i % 3]
                        }
                    }))
                elif operation_type == 1:
                    # å“è³ªè©•ä¾¡
                    default_standard_id = list(processor.quality_standards.keys())[0]
                    stress_operations.append(("assess_quality", {
                        "standard_id": default_standard_id,
                        "component": f"stress_component_{i}",
                        "metrics": {"test_coverage": 70.0 + (i % 30)}
                    }))
                elif operation_type == 2:
                    # çµ±è¨ˆå–å¾—
                    stress_operations.append(("get_statistics", {}))
                else:
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
                    stress_operations.append(("learn_incident_patterns", {}))
            
            # 2. ã‚¹ãƒˆãƒ¬ã‚¹å®Ÿè¡Œ
            start_time = time.time()
            stress_results = []
            
            # ãƒãƒƒãƒå®Ÿè¡Œã§ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
            batch_size = 20
            for i in range(0, len(stress_operations), batch_size):
                batch = stress_operations[i:i+batch_size]
                batch_results = []
                
                for operation, data in batch:
                    result = await processor.process_action(operation, data)
                    batch_results.append(result)
                
                stress_results.extend(batch_results)
                
                # å°‘ã—ä¼‘æ†©ï¼ˆãƒ¡ãƒ¢ãƒªè² è·è»½æ¸›ï¼‰
                if i % 100 == 0:
                    await asyncio.sleep(0.001)
            
            end_time = time.time()
            total_stress_time = end_time - start_time
            
            # 3. ã‚¹ãƒˆãƒ¬ã‚¹çµæœåˆ†æ
            successful_operations = [r for r in stress_results if r.get("success")]
            failed_operations = [r for r in stress_results if not r.get("success")]
            
            stress_success_rate = len(successful_operations) / len(stress_results) * 100
            stress_throughput = len(stress_results) / total_stress_time
            
            self.performance_metrics["stress_test"] = {
                "total_operations": len(stress_operations),
                "successful_operations": len(successful_operations),
                "failed_operations": len(failed_operations),
                "success_rate": stress_success_rate,
                "total_time": total_stress_time,
                "throughput": stress_throughput
            }
            
            # ã‚¹ãƒˆãƒ¬ã‚¹åŸºæº–ç¢ºèª
            if stress_success_rate < 90:  # 90%æœªæº€ã¯å•é¡Œ
                print(f"     âŒ ã‚¹ãƒˆãƒ¬ã‚¹æˆåŠŸç‡ä½ã„: {stress_success_rate:.1f}%")
                return False
            
            if stress_throughput < 20:  # 20 ops/secæœªæº€ã¯å•é¡Œ
                print(f"     âŒ ã‚¹ãƒˆãƒ¬ã‚¹ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä½ã„: {stress_throughput:.1f} ops/sec")
                return False
            
            print(f"     âœ… ã‚¹ãƒˆãƒ¬ã‚¹è² è·ãƒ†ã‚¹ãƒˆæˆåŠŸ: {stress_success_rate:.1f}%æˆåŠŸç‡, {stress_throughput:.1f} ops/sec")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ ã‚¹ãƒˆãƒ¬ã‚¹è² è·ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def test_edge_cases(self) -> bool:
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ"""
        try:
            processor = IncidentProcessor()
            
            # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
            
            edge_test_cases = [
                {
                    "name": "ç©ºæ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿",
                    "action": "search_similar_incidents",
                    "data": {"query": ""},
                    "should_succeed": True
                },
                {
                    "name": "éå¸¸ã«é•·ã„ãƒ†ã‚­ã‚¹ãƒˆ",
                    "action": "detect_incident",
                    "data": {
                        "anomaly_data": {
                            "component": "x" * 1000,
                            "metric": "y" * 1000,
                            "severity": "medium"
                        }
                    },
                    "should_succeed": True
                },
                {
                    "name": "ã‚¼ãƒ­å€¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹",
                    "action": "assess_quality",
                    "data": {
                        "standard_id": list(processor.quality_standards.keys())[0],
                        "component": "test",
                        "metrics": {"test_coverage": 0.0}
                    },
                    "should_succeed": True
                },
                {
                    "name": "æ¥µå¤§å€¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹",
                    "action": "assess_quality",
                    "data": {
                        "standard_id": list(processor.quality_standards.keys())[0],
                        "component": "test",
                        "metrics": {"test_coverage": 999999.0}
                    },
                    "should_succeed": True
                },
                {
                    "name": "ç‰¹æ®Šæ–‡å­—ãƒ‡ãƒ¼ã‚¿",
                    "action": "detect_incident",
                    "data": {
                        "anomaly_data": {
                            "component": "test@#$%^&*()",
                            "metric": "test<>?:{}[]",
                            "severity": "low"
                        }
                    },
                    "should_succeed": True
                },
                {
                    "name": "Unicodeæ–‡å­—ãƒ‡ãƒ¼ã‚¿",
                    "action": "detect_incident",
                    "data": {
                        "anomaly_data": {
                            "component": "ãƒ†ã‚¹ãƒˆã‚µãƒ¼ãƒ“ã‚¹",
                            "metric": "å¿œç­”æ™‚é–“",
                            "severity": "medium"
                        }
                    },
                    "should_succeed": True
                },
                {
                    "name": "åŒä¸€ãƒ‡ãƒ¼ã‚¿é‡è¤‡å®Ÿè¡Œ",
                    "action": "detect_incident",
                    "data": {
                        "anomaly_data": {
                            "component": "duplicate_service",
                            "metric": "duplicate_metric",
                            "severity": "high"
                        }
                    },
                    "should_succeed": True
                }
            ]
            
            edge_case_results = []
            
            for test_case in edge_test_cases:
                try:
                    # åŒä¸€ãƒ‡ãƒ¼ã‚¿é‡è¤‡ã®å ´åˆã¯è¤‡æ•°å›å®Ÿè¡Œ
                    if test_case["name"] == "åŒä¸€ãƒ‡ãƒ¼ã‚¿é‡è¤‡å®Ÿè¡Œ":
                        for _ in range(3):
                            result = await processor.process_action(test_case["action"], test_case["data"])
                    else:
                        result = await processor.process_action(test_case["action"], test_case["data"])
                    
                    # çµæœè©•ä¾¡
                    success = result.get("success", False)
                    expected_success = test_case["should_succeed"]
                    
                    edge_case_results.append({
                        "case": test_case["name"],
                        "expected_success": expected_success,
                        "actual_success": success,
                        "correct": success == expected_success,
                        "result": result
                    })
                    
                except Exception as e:
                    edge_case_results.append({
                        "case": test_case["name"],
                        "expected_success": test_case["should_succeed"],
                        "actual_success": False,
                        "correct": not test_case["should_succeed"],
                        "exception": str(e)
                    })
            
            # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹çµæœè©•ä¾¡
            correct_results = [r for r in edge_case_results if r["correct"]]
            edge_case_success_rate = len(correct_results) / len(edge_case_results) * 100
            
            self.performance_metrics["edge_cases"] = {
                "total_edge_cases": len(edge_test_cases),
                "correct_results": len(correct_results),
                "success_rate": edge_case_success_rate,
                "results": edge_case_results
            }
            
            # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹åŸºæº–ç¢ºèª
            if edge_case_success_rate < 80:  # 80%æœªæº€ã¯å•é¡Œ
                print(f"     âŒ ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å‡¦ç†ä¸ååˆ†: {edge_case_success_rate:.1f}%")
                for result in edge_case_results:
                    if not result["correct"]:
                        print(f"       â€¢ {result['case']}: æœŸå¾…{result['expected_success']}, å®Ÿéš›{result['actual_success']}")
                return False
            
            print(f"     âœ… ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆæˆåŠŸ: {edge_case_success_rate:.1f}%é©åˆ‡å‡¦ç†, {len(correct_results)}/{len(edge_test_cases)}ã‚±ãƒ¼ã‚¹")
            return True
            
        except Exception as e:
            print(f"     ğŸ’¥ ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš¨ Incident Sage A2A Agent - åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 70)
    
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO)
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_suite = TestIncidentSageA2AComprehensive()
    results = await test_suite.run_all_tests()
    
    if results["success_rate"] >= 80.0:
        print(f"\\nğŸ‰ Incident SageåŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆæˆåŠŸï¼")
        print(f"   æˆåŠŸç‡: {results['success_rate']:.1f}%")
        print(f"   å®Ÿè¡Œæ™‚é–“: {results['total_duration']:.3f}ç§’")
        print(f"   å¹³å‡ãƒ†ã‚¹ãƒˆæ™‚é–“: {results['total_duration']/results['total_tests']:.3f}ç§’")
        print(f"   ğŸš¨ Elder Loop Phase 4å®Œäº†")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼
        if test_suite.performance_metrics:
            print(f"\\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼:")
            for metric_name, metric_data in test_suite.performance_metrics.items():
                if isinstance(metric_data, dict) and "throughput" in metric_data:
                    print(f"   â€¢ {metric_name}: {metric_data['throughput']:.1f} ops/sec")
                elif isinstance(metric_data, dict) and "success_rate" in metric_data:
                    print(f"   â€¢ {metric_name}: {metric_data['success_rate']:.1f}% æˆåŠŸç‡")
        
        return True
    else:
        print(f"\\nğŸ”§ Incident SageåŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã§èª¿æ•´ãŒå¿…è¦")
        print(f"   æˆåŠŸç‡: {results['success_rate']:.1f}% (80%æœªæº€)")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)