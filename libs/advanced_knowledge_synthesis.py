#!/usr/bin/env python3
"""
ğŸ”® Advanced Knowledge Synthesis System
é«˜åº¦ãªçŸ¥è­˜åˆæˆã‚·ã‚¹ãƒ†ãƒ 

RAGè³¢è€…ã®é«˜åº¦ãªææ¡ˆã«ã‚ˆã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆã¨æ–°çŸ¥è­˜å‰µå‡º
ãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒãƒ»éŸ³å£°ã®çµ±åˆæ¤œç´¢ã¨çŸ¥è­˜çŸ›ç›¾ã®è‡ªå‹•è§£æ±º

Author: Claude Elder
Date: 2025-07-10
Phase: 2 (é«˜åº¦ãªçŸ¥è­˜åˆæˆå®Ÿç¾)
"""

import asyncio
import json
import logging
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Set, Union
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum
import sqlite3
from collections import defaultdict, deque
import hashlib
import threading
import networkx as nx
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
import pickle

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
PROJECT_ROOT = Path(__file__).parent.parent

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
try:
    from .multidimensional_vector_system import (
        MultiDimensionalVectorSystem, 
        KnowledgeType,
        MultiDimensionalVector
    )
    from .autonomous_learning_system import (
        AutonomousLearningSystem,
        CrossDomainInsight
    )
except ImportError:
    # ãƒ¢ãƒƒã‚¯å®Ÿè£…
    MultiDimensionalVectorSystem = None
    KnowledgeType = None
    MultiDimensionalVector = None
    AutonomousLearningSystem = None
    CrossDomainInsight = None

class ModalityType(Enum):
    """ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚¿ã‚¤ãƒ—"""
    TEXT = "text"
    IMAGE = "image"
    AUDIO = "audio"
    VIDEO = "video"
    CODE = "code"
    STRUCTURED_DATA = "structured_data"
    MIXED = "mixed"

class SynthesisStrategy(Enum):
    """åˆæˆæˆ¦ç•¥"""
    FUSION = "fusion"              # èåˆ
    AGGREGATION = "aggregation"    # é›†ç´„
    ABSTRACTION = "abstraction"    # æŠ½è±¡åŒ–
    INTERPOLATION = "interpolation"  # è£œé–“
    EXTRAPOLATION = "extrapolation"  # å¤–æŒ¿
    CONTRADICTION_RESOLUTION = "contradiction_resolution"  # çŸ›ç›¾è§£æ±º

class KnowledgeRelationType(Enum):
    """çŸ¥è­˜é–¢ä¿‚ã‚¿ã‚¤ãƒ—"""
    SUPPORTS = "supports"          # æ”¯æŒ
    CONTRADICTS = "contradicts"    # çŸ›ç›¾
    EXTENDS = "extends"            # æ‹¡å¼µ
    SPECIALIZES = "specializes"    # ç‰¹æ®ŠåŒ–
    GENERALIZES = "generalizes"    # ä¸€èˆ¬åŒ–
    DEPENDS_ON = "depends_on"      # ä¾å­˜
    RELATED_TO = "related_to"      # é–¢é€£

@dataclass
class MultiModalKnowledge:
    """ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çŸ¥è­˜"""
    knowledge_id: str
    created_at: datetime
    modality: ModalityType
    content: Any  # ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«å¿œã˜ãŸå†…å®¹
    text_representation: str  # ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾
    vector_representation: Optional[np.ndarray]
    metadata: Dict[str, Any]
    quality_score: float
    source_references: List[str]

@dataclass
class KnowledgeContradiction:
    """çŸ¥è­˜çŸ›ç›¾"""
    contradiction_id: str
    detected_at: datetime
    knowledge_ids: List[str]
    contradiction_type: str
    severity: float
    resolution_strategy: SynthesisStrategy
    resolution_confidence: float
    evidence: List[Dict[str, Any]]

@dataclass
class SynthesizedKnowledge:
    """åˆæˆçŸ¥è­˜"""
    synthesis_id: str
    created_at: datetime
    source_knowledge_ids: List[str]
    synthesis_strategy: SynthesisStrategy
    modalities_involved: List[ModalityType]
    synthesized_content: Dict[str, Any]
    confidence_score: float
    novelty_score: float
    validation_results: Dict[str, Any]
    applications: List[str]

@dataclass
class KnowledgeGraph:
    """çŸ¥è­˜ã‚°ãƒ©ãƒ•"""
    graph_id: str
    created_at: datetime
    nodes: Dict[str, MultiModalKnowledge]
    edges: List[Tuple[str, str, KnowledgeRelationType]]
    clusters: Dict[int, List[str]]
    central_concepts: List[str]
    graph_metrics: Dict[str, float]

class MultiModalEncoder:
    """ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.encoders = {
            ModalityType.TEXT: self._encode_text,
            ModalityType.IMAGE: self._encode_image,
            ModalityType.AUDIO: self._encode_audio,
            ModalityType.CODE: self._encode_code,
            ModalityType.STRUCTURED_DATA: self._encode_structured_data
        }
    
    async def encode(self, content: Any, modality: ModalityType) -> Tuple[np.ndarray, str]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        encoder = self.encoders.get(modality, self._encode_default)
        vector, text_repr = await encoder(content)
        return vector, text_repr
    
    async def _encode_text(self, text: str) -> Tuple[np.ndarray, str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        # ç°¡ç•¥åŒ–å®Ÿè£…
        vector = np.random.random(768)  # BERTæ¬¡å…ƒ
        return vector, text[:200]  # æœ€åˆã®200æ–‡å­—
    
    async def _encode_image(self, image_data: Any) -> Tuple[np.ndarray, str]:
        """ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        # ç°¡ç•¥åŒ–å®Ÿè£…
        vector = np.random.random(2048)  # ResNetæ¬¡å…ƒ
        text_repr = "Image content: [visual description placeholder]"
        return vector, text_repr
    
    async def _encode_audio(self, audio_data: Any) -> Tuple[np.ndarray, str]:
        """éŸ³å£°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        # ç°¡ç•¥åŒ–å®Ÿè£…
        vector = np.random.random(512)  # éŸ³å£°ç‰¹å¾´æ¬¡å…ƒ
        text_repr = "Audio content: [audio transcription placeholder]"
        return vector, text_repr
    
    async def _encode_code(self, code: str) -> Tuple[np.ndarray, str]:
        """ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        # ç°¡ç•¥åŒ–å®Ÿè£…
        vector = np.random.random(768)  # CodeBERTæ¬¡å…ƒ
        # ã‚³ãƒ¼ãƒ‰ã®æœ€åˆã®éƒ¨åˆ†ã‚’æŠ½å‡º
        lines = code.split('\n')[:5]
        text_repr = f"Code snippet: {' '.join(lines)}"
        return vector, text_repr
    
    async def _encode_structured_data(self, data: Dict[str, Any]) -> Tuple[np.ndarray, str]:
        """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        # ç°¡ç•¥åŒ–å®Ÿè£…
        vector = np.random.random(256)
        text_repr = f"Structured data with {len(data)} fields"
        return vector, text_repr
    
    async def _encode_default(self, content: Any) -> Tuple[np.ndarray, str]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        vector = np.random.random(512)
        text_repr = str(content)[:200]
        return vector, text_repr

class ContradictionResolver:
    """çŸ›ç›¾è§£æ±ºå™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.resolution_strategies = {
            'temporal': self._resolve_temporal_contradiction,
            'source_credibility': self._resolve_by_credibility,
            'consensus': self._resolve_by_consensus,
            'context_specific': self._resolve_by_context,
            'synthesis': self._resolve_by_synthesis
        }
    
    async def resolve(self, 
                     contradiction: KnowledgeContradiction,
                     knowledge_items: List[MultiModalKnowledge]) -> Dict[str, Any]:
        """çŸ›ç›¾è§£æ±º"""
        try:
            # çŸ›ç›¾ã‚¿ã‚¤ãƒ—åˆ¤å®š
            contradiction_type = await self._classify_contradiction(knowledge_items)
            
            # é©åˆ‡ãªè§£æ±ºæˆ¦ç•¥é¸æŠ
            strategy = self._select_resolution_strategy(contradiction_type)
            
            # è§£æ±ºå®Ÿè¡Œ
            resolution = await strategy(knowledge_items)
            
            return {
                'success': True,
                'resolution': resolution,
                'strategy_used': contradiction_type,
                'confidence': resolution.get('confidence', 0.5)
            }
            
        except Exception as e:
            self.logger.error(f"Contradiction resolution failed: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _classify_contradiction(self, knowledge_items: List[MultiModalKnowledge]) -> str:
        """çŸ›ç›¾åˆ†é¡"""
        # æ™‚é–“çš„çŸ›ç›¾ãƒã‚§ãƒƒã‚¯
        timestamps = [k.created_at for k in knowledge_items]
        if max(timestamps) - min(timestamps) > timedelta(days=30):
            return 'temporal'
        
        # ã‚½ãƒ¼ã‚¹ä¿¡é ¼æ€§ã®å·®ãƒã‚§ãƒƒã‚¯
        quality_scores = [k.quality_score for k in knowledge_items]
        if max(quality_scores) - min(quality_scores) > 0.3:
            return 'source_credibility'
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯åˆæˆã«ã‚ˆã‚‹è§£æ±º
        return 'synthesis'
    
    def _select_resolution_strategy(self, contradiction_type: str):
        """è§£æ±ºæˆ¦ç•¥é¸æŠ"""
        return self.resolution_strategies.get(
            contradiction_type, 
            self._resolve_by_synthesis
        )
    
    async def _resolve_temporal_contradiction(self, 
                                            knowledge_items: List[MultiModalKnowledge]) -> Dict[str, Any]:
        """æ™‚é–“çš„çŸ›ç›¾è§£æ±º"""
        # æœ€æ–°ã®çŸ¥è­˜ã‚’å„ªå…ˆ
        latest_knowledge = max(knowledge_items, key=lambda k: k.created_at)
        
        return {
            'resolved_content': latest_knowledge.content,
            'resolution_method': 'temporal_precedence',
            'selected_knowledge_id': latest_knowledge.knowledge_id,
            'confidence': 0.8,
            'reasoning': 'Selected most recent knowledge item'
        }
    
    async def _resolve_by_credibility(self, 
                                    knowledge_items: List[MultiModalKnowledge]) -> Dict[str, Any]:
        """ä¿¡é ¼æ€§ã«ã‚ˆã‚‹è§£æ±º"""
        # æœ€é«˜å“è³ªã‚¹ã‚³ã‚¢ã®çŸ¥è­˜ã‚’é¸æŠ
        best_knowledge = max(knowledge_items, key=lambda k: k.quality_score)
        
        return {
            'resolved_content': best_knowledge.content,
            'resolution_method': 'credibility_based',
            'selected_knowledge_id': best_knowledge.knowledge_id,
            'confidence': best_knowledge.quality_score,
            'reasoning': 'Selected highest quality knowledge item'
        }
    
    async def _resolve_by_consensus(self, 
                                  knowledge_items: List[MultiModalKnowledge]) -> Dict[str, Any]:
        """ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ã«ã‚ˆã‚‹è§£æ±º"""
        # ç°¡ç•¥åŒ–: å¤šæ•°æ±ºçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        # å®Ÿéš›ã¯ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨
        
        # ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        texts = [k.text_representation for k in knowledge_items]
        # ãƒ€ãƒŸãƒ¼å®Ÿè£…
        consensus_text = texts[0]  # æœ€åˆã®ã‚‚ã®ã‚’é¸æŠ
        
        return {
            'resolved_content': consensus_text,
            'resolution_method': 'consensus',
            'confidence': 0.7,
            'reasoning': 'Consensus among multiple sources'
        }
    
    async def _resolve_by_context(self, 
                                knowledge_items: List[MultiModalKnowledge]) -> Dict[str, Any]:
        """æ–‡è„ˆã«ã‚ˆã‚‹è§£æ±º"""
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆç°¡ç•¥åŒ–ï¼‰
        context_scores = []
        for item in knowledge_items:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—
            score = len(item.metadata.get('context', {})) / 10.0
            context_scores.append(min(1.0, score))
        
        # æœ€é«˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã®ã‚¢ã‚¤ãƒ†ãƒ é¸æŠ
        best_idx = np.argmax(context_scores)
        best_knowledge = knowledge_items[best_idx]
        
        return {
            'resolved_content': best_knowledge.content,
            'resolution_method': 'context_specific',
            'selected_knowledge_id': best_knowledge.knowledge_id,
            'confidence': context_scores[best_idx],
            'reasoning': 'Selected based on contextual relevance'
        }
    
    async def _resolve_by_synthesis(self, 
                                  knowledge_items: List[MultiModalKnowledge]) -> Dict[str, Any]:
        """åˆæˆã«ã‚ˆã‚‹è§£æ±º"""
        # å…¨ã¦ã®çŸ¥è­˜ã‚’çµ±åˆ
        synthesized_content = {
            'perspectives': [],
            'common_elements': [],
            'unique_elements': []
        }
        
        # å„çŸ¥è­˜ã®è¦ç´ ã‚’æŠ½å‡º
        for item in knowledge_items:
            synthesized_content['perspectives'].append({
                'source': item.knowledge_id,
                'content': item.text_representation,
                'confidence': item.quality_score
            })
        
        # å…±é€šè¦ç´ ã®è­˜åˆ¥ï¼ˆç°¡ç•¥åŒ–ï¼‰
        synthesized_content['common_elements'] = ['Core concept present in all sources']
        synthesized_content['unique_elements'] = ['Unique perspective from each source']
        
        return {
            'resolved_content': synthesized_content,
            'resolution_method': 'synthesis',
            'confidence': 0.75,
            'reasoning': 'Synthesized multiple perspectives into unified view'
        }

class KnowledgeSynthesizer:
    """çŸ¥è­˜åˆæˆå™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.synthesis_strategies = {
            SynthesisStrategy.FUSION: self._fuse_knowledge,
            SynthesisStrategy.AGGREGATION: self._aggregate_knowledge,
            SynthesisStrategy.ABSTRACTION: self._abstract_knowledge,
            SynthesisStrategy.INTERPOLATION: self._interpolate_knowledge,
            SynthesisStrategy.EXTRAPOLATION: self._extrapolate_knowledge
        }
    
    async def synthesize(self,
                       knowledge_items: List[MultiModalKnowledge],
                       strategy: SynthesisStrategy) -> SynthesizedKnowledge:
        """çŸ¥è­˜åˆæˆ"""
        try:
            # åˆæˆæˆ¦ç•¥å®Ÿè¡Œ
            synthesis_func = self.synthesis_strategies.get(
                strategy, 
                self._fuse_knowledge
            )
            
            synthesized_content = await synthesis_func(knowledge_items)
            
            # æ–°è¦æ€§è©•ä¾¡
            novelty_score = await self._evaluate_novelty(
                synthesized_content, 
                knowledge_items
            )
            
            # æ¤œè¨¼
            validation_results = await self._validate_synthesis(
                synthesized_content
            )
            
            # å¿œç”¨å¯èƒ½æ€§è©•ä¾¡
            applications = await self._identify_applications(
                synthesized_content
            )
            
            synthesis = SynthesizedKnowledge(
                synthesis_id=f"synth_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                created_at=datetime.now(),
                source_knowledge_ids=[k.knowledge_id for k in knowledge_items],
                synthesis_strategy=strategy,
                modalities_involved=list(set(k.modality for k in knowledge_items)),
                synthesized_content=synthesized_content,
                confidence_score=validation_results.get('confidence', 0.5),
                novelty_score=novelty_score,
                validation_results=validation_results,
                applications=applications
            )
            
            return synthesis
            
        except Exception as e:
            self.logger.error(f"Knowledge synthesis failed: {e}")
            raise
    
    async def _fuse_knowledge(self, 
                            knowledge_items: List[MultiModalKnowledge]) -> Dict[str, Any]:
        """çŸ¥è­˜èåˆ"""
        fused_content = {
            'type': 'fusion',
            'elements': {},
            'unified_representation': None
        }
        
        # ãƒ™ã‚¯ãƒˆãƒ«èåˆ
        if all(k.vector_representation is not None for k in knowledge_items):
            vectors = [k.vector_representation for k in knowledge_items]
            # é‡ã¿ä»˜ãå¹³å‡ï¼ˆå“è³ªã‚¹ã‚³ã‚¢ã§é‡ã¿ä»˜ã‘ï¼‰
            weights = [k.quality_score for k in knowledge_items]
            weights = np.array(weights) / sum(weights)
            
            fused_vector = np.average(vectors, axis=0, weights=weights)
            fused_content['unified_vector'] = fused_vector.tolist()
        
        # ãƒ†ã‚­ã‚¹ãƒˆèåˆ
        texts = [k.text_representation for k in knowledge_items]
        fused_content['unified_text'] = self._fuse_texts(texts)
        
        # ãƒ¢ãƒ€ãƒªãƒ†ã‚£åˆ¥è¦ç´ 
        for item in knowledge_items:
            modality = item.modality.value
            if modality not in fused_content['elements']:
                fused_content['elements'][modality] = []
            fused_content['elements'][modality].append(item.content)
        
        return fused_content
    
    async def _aggregate_knowledge(self, 
                                 knowledge_items: List[MultiModalKnowledge]) -> Dict[str, Any]:
        """çŸ¥è­˜é›†ç´„"""
        aggregated_content = {
            'type': 'aggregation',
            'summary': {},
            'statistics': {},
            'patterns': []
        }
        
        # ãƒ¢ãƒ€ãƒªãƒ†ã‚£åˆ¥é›†ç´„
        modality_groups = defaultdict(list)
        for item in knowledge_items:
            modality_groups[item.modality].append(item)
        
        # å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®çµ±è¨ˆ
        for modality, items in modality_groups.items():
            aggregated_content['statistics'][modality.value] = {
                'count': len(items),
                'avg_quality': np.mean([i.quality_score for i in items]),
                'sources': list(set(ref for i in items for ref in i.source_references))
            }
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
        if len(knowledge_items) >= 3:
            patterns = self._extract_patterns(knowledge_items)
            aggregated_content['patterns'] = patterns
        
        # ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        aggregated_content['summary'] = {
            'total_items': len(knowledge_items),
            'modalities': list(modality_groups.keys()),
            'key_insights': self._extract_key_insights(knowledge_items)
        }
        
        return aggregated_content
    
    async def _abstract_knowledge(self, 
                                knowledge_items: List[MultiModalKnowledge]) -> Dict[str, Any]:
        """çŸ¥è­˜æŠ½è±¡åŒ–"""
        abstracted_content = {
            'type': 'abstraction',
            'abstract_concepts': [],
            'hierarchical_structure': {},
            'general_principles': []
        }
        
        # æ¦‚å¿µæŠ½å‡º
        concepts = self._extract_concepts(knowledge_items)
        abstracted_content['abstract_concepts'] = concepts
        
        # éšå±¤æ§‹é€ æ§‹ç¯‰
        hierarchy = self._build_concept_hierarchy(concepts)
        abstracted_content['hierarchical_structure'] = hierarchy
        
        # ä¸€èˆ¬åŸå‰‡å°å‡º
        principles = self._derive_principles(knowledge_items)
        abstracted_content['general_principles'] = principles
        
        return abstracted_content
    
    async def _interpolate_knowledge(self, 
                                   knowledge_items: List[MultiModalKnowledge]) -> Dict[str, Any]:
        """çŸ¥è­˜è£œé–“"""
        interpolated_content = {
            'type': 'interpolation',
            'filled_gaps': [],
            'intermediate_states': [],
            'confidence_map': {}
        }
        
        # ã‚®ãƒ£ãƒƒãƒ—è­˜åˆ¥
        gaps = self._identify_knowledge_gaps(knowledge_items)
        
        # è£œé–“å®Ÿè¡Œ
        for gap in gaps:
            filled = self._interpolate_gap(gap, knowledge_items)
            interpolated_content['filled_gaps'].append(filled)
        
        # ä¸­é–“çŠ¶æ…‹ç”Ÿæˆ
        if len(knowledge_items) >= 2:
            for i in range(len(knowledge_items) - 1):
                intermediate = self._generate_intermediate_state(
                    knowledge_items[i], 
                    knowledge_items[i + 1]
                )
                interpolated_content['intermediate_states'].append(intermediate)
        
        return interpolated_content
    
    async def _extrapolate_knowledge(self, 
                                   knowledge_items: List[MultiModalKnowledge]) -> Dict[str, Any]:
        """çŸ¥è­˜å¤–æŒ¿"""
        extrapolated_content = {
            'type': 'extrapolation',
            'future_predictions': [],
            'extended_concepts': [],
            'novel_applications': []
        }
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        trends = self._analyze_trends(knowledge_items)
        
        # å°†æ¥äºˆæ¸¬
        for trend in trends:
            prediction = self._predict_future_state(trend)
            extrapolated_content['future_predictions'].append(prediction)
        
        # æ¦‚å¿µæ‹¡å¼µ
        extended = self._extend_concepts(knowledge_items)
        extrapolated_content['extended_concepts'] = extended
        
        # æ–°è¦å¿œç”¨ç™ºè¦‹
        applications = self._discover_novel_applications(knowledge_items)
        extrapolated_content['novel_applications'] = applications
        
        return extrapolated_content
    
    def _fuse_texts(self, texts: List[str]) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆèåˆ"""
        # ç°¡ç•¥åŒ–å®Ÿè£…
        if not texts:
            return ""
        
        # å…±é€šéƒ¨åˆ†ã¨ç‹¬è‡ªéƒ¨åˆ†ã‚’è­˜åˆ¥
        if len(texts) == 1:
            return texts[0]
        
        # ç°¡å˜ãªçµåˆ
        return f"Fused knowledge from {len(texts)} sources: " + " | ".join(texts[:3])
    
    def _extract_patterns(self, knowledge_items: List[MultiModalKnowledge]) -> List[Dict[str, Any]]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        patterns = []
        
        # å“è³ªãƒ‘ã‚¿ãƒ¼ãƒ³
        quality_scores = [k.quality_score for k in knowledge_items]
        if len(set(quality_scores)) < len(quality_scores) / 2:
            patterns.append({
                'type': 'quality_consistency',
                'description': 'Consistent quality across sources'
            })
        
        # ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ‘ã‚¿ãƒ¼ãƒ³
        modalities = [k.modality for k in knowledge_items]
        if len(set(modalities)) > 1:
            patterns.append({
                'type': 'multi_modal',
                'description': 'Knowledge spans multiple modalities'
            })
        
        return patterns
    
    def _extract_key_insights(self, knowledge_items: List[MultiModalKnowledge]) -> List[str]:
        """é‡è¦æ´å¯ŸæŠ½å‡º"""
        insights = []
        
        # é«˜å“è³ªã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ã®æ´å¯Ÿ
        high_quality = [k for k in knowledge_items if k.quality_score > 0.8]
        if high_quality:
            insights.append(f"Found {len(high_quality)} high-quality knowledge items")
        
        # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ´å¯Ÿ
        modalities = set(k.modality for k in knowledge_items)
        if len(modalities) > 2:
            insights.append("Rich multi-modal knowledge representation")
        
        return insights
    
    def _extract_concepts(self, knowledge_items: List[MultiModalKnowledge]) -> List[str]:
        """æ¦‚å¿µæŠ½å‡º"""
        # ç°¡ç•¥åŒ–å®Ÿè£…
        concepts = []
        
        for item in knowledge_items:
            # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¦‚å¿µã‚’æŠ½å‡ºï¼ˆãƒ€ãƒŸãƒ¼å®Ÿè£…ï¼‰
            text = item.text_representation
            if "system" in text.lower():
                concepts.append("system_architecture")
            if "learning" in text.lower():
                concepts.append("machine_learning")
            if "data" in text.lower():
                concepts.append("data_processing")
        
        return list(set(concepts))
    
    def _build_concept_hierarchy(self, concepts: List[str]) -> Dict[str, Any]:
        """æ¦‚å¿µéšå±¤æ§‹ç¯‰"""
        # ç°¡ç•¥åŒ–å®Ÿè£…
        hierarchy = {
            'root': 'knowledge',
            'branches': {}
        }
        
        for concept in concepts:
            if 'system' in concept:
                hierarchy['branches']['technical'] = hierarchy['branches'].get('technical', [])
                hierarchy['branches']['technical'].append(concept)
            else:
                hierarchy['branches']['general'] = hierarchy['branches'].get('general', [])
                hierarchy['branches']['general'].append(concept)
        
        return hierarchy
    
    def _derive_principles(self, knowledge_items: List[MultiModalKnowledge]) -> List[str]:
        """åŸå‰‡å°å‡º"""
        principles = []
        
        # å“è³ªåŸå‰‡
        avg_quality = np.mean([k.quality_score for k in knowledge_items])
        if avg_quality > 0.7:
            principles.append("High-quality knowledge leads to better synthesis")
        
        # å¤šæ§˜æ€§åŸå‰‡
        modalities = set(k.modality for k in knowledge_items)
        if len(modalities) > 1:
            principles.append("Multi-modal integration enhances understanding")
        
        return principles
    
    def _identify_knowledge_gaps(self, knowledge_items: List[MultiModalKnowledge]) -> List[Dict[str, Any]]:
        """çŸ¥è­˜ã‚®ãƒ£ãƒƒãƒ—è­˜åˆ¥"""
        gaps = []
        
        # æ™‚é–“çš„ã‚®ãƒ£ãƒƒãƒ—
        if len(knowledge_items) >= 2:
            timestamps = sorted([k.created_at for k in knowledge_items])
            for i in range(len(timestamps) - 1):
                time_diff = timestamps[i + 1] - timestamps[i]
                if time_diff > timedelta(days=7):
                    gaps.append({
                        'type': 'temporal',
                        'start': timestamps[i],
                        'end': timestamps[i + 1],
                        'duration': time_diff
                    })
        
        return gaps
    
    def _interpolate_gap(self, gap: Dict[str, Any], 
                        knowledge_items: List[MultiModalKnowledge]) -> Dict[str, Any]:
        """ã‚®ãƒ£ãƒƒãƒ—è£œé–“"""
        return {
            'gap_type': gap['type'],
            'interpolated_content': 'Estimated intermediate knowledge',
            'confidence': 0.6
        }
    
    def _generate_intermediate_state(self, 
                                   item1: MultiModalKnowledge,
                                   item2: MultiModalKnowledge) -> Dict[str, Any]:
        """ä¸­é–“çŠ¶æ…‹ç”Ÿæˆ"""
        return {
            'from': item1.knowledge_id,
            'to': item2.knowledge_id,
            'intermediate_content': 'Transitional knowledge state',
            'similarity': 0.7
        }
    
    def _analyze_trends(self, knowledge_items: List[MultiModalKnowledge]) -> List[Dict[str, Any]]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        trends = []
        
        # å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰
        if len(knowledge_items) >= 3:
            quality_scores = [k.quality_score for k in knowledge_items]
            quality_trend = np.polyfit(range(len(quality_scores)), quality_scores, 1)[0]
            
            trends.append({
                'type': 'quality_trend',
                'direction': 'increasing' if quality_trend > 0 else 'decreasing',
                'slope': quality_trend
            })
        
        return trends
    
    def _predict_future_state(self, trend: Dict[str, Any]) -> Dict[str, Any]:
        """å°†æ¥çŠ¶æ…‹äºˆæ¸¬"""
        return {
            'trend_type': trend['type'],
            'prediction': 'Expected future development',
            'confidence': 0.7,
            'timeframe': '30 days'
        }
    
    def _extend_concepts(self, knowledge_items: List[MultiModalKnowledge]) -> List[str]:
        """æ¦‚å¿µæ‹¡å¼µ"""
        extended = []
        
        concepts = self._extract_concepts(knowledge_items)
        for concept in concepts:
            extended.append(f"extended_{concept}")
            extended.append(f"meta_{concept}")
        
        return extended
    
    def _discover_novel_applications(self, 
                                   knowledge_items: List[MultiModalKnowledge]) -> List[str]:
        """æ–°è¦å¿œç”¨ç™ºè¦‹"""
        applications = []
        
        # ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹å¿œç”¨
        modalities = set(k.modality for k in knowledge_items)
        if ModalityType.TEXT in modalities and ModalityType.IMAGE in modalities:
            applications.append("Visual question answering system")
        
        if ModalityType.AUDIO in modalities and ModalityType.TEXT in modalities:
            applications.append("Speech-to-insight system")
        
        return applications
    
    async def _evaluate_novelty(self, 
                              synthesized_content: Dict[str, Any],
                              source_items: List[MultiModalKnowledge]) -> float:
        """æ–°è¦æ€§è©•ä¾¡"""
        # ç°¡ç•¥åŒ–å®Ÿè£…
        novelty_factors = []
        
        # æ–°è¦ç´ ã®æ•°
        if 'novel_applications' in synthesized_content:
            novelty_factors.append(len(synthesized_content['novel_applications']) / 10.0)
        
        # æ‹¡å¼µæ¦‚å¿µã®æ•°
        if 'extended_concepts' in synthesized_content:
            novelty_factors.append(len(synthesized_content['extended_concepts']) / 20.0)
        
        # äºˆæ¸¬ã®æ•°
        if 'future_predictions' in synthesized_content:
            novelty_factors.append(len(synthesized_content['future_predictions']) / 5.0)
        
        return min(1.0, np.mean(novelty_factors) if novelty_factors else 0.5)
    
    async def _validate_synthesis(self, 
                                synthesized_content: Dict[str, Any]) -> Dict[str, Any]:
        """åˆæˆæ¤œè¨¼"""
        validation = {
            'is_valid': True,
            'confidence': 0.8,
            'checks_passed': [],
            'warnings': []
        }
        
        # æ§‹é€ ãƒã‚§ãƒƒã‚¯
        if 'type' in synthesized_content:
            validation['checks_passed'].append('Structure validation')
        else:
            validation['warnings'].append('Missing synthesis type')
            validation['confidence'] *= 0.8
        
        # å†…å®¹ãƒã‚§ãƒƒã‚¯
        if synthesized_content:
            validation['checks_passed'].append('Content validation')
        else:
            validation['warnings'].append('Empty synthesis result')
            validation['is_valid'] = False
        
        return validation
    
    async def _identify_applications(self, 
                                   synthesized_content: Dict[str, Any]) -> List[str]:
        """å¿œç”¨å¯èƒ½æ€§è­˜åˆ¥"""
        applications = []
        
        synthesis_type = synthesized_content.get('type', '')
        
        if synthesis_type == 'fusion':
            applications.extend([
                "Unified knowledge base",
                "Comprehensive decision support"
            ])
        elif synthesis_type == 'abstraction':
            applications.extend([
                "Conceptual framework development",
                "Theory formulation"
            ])
        elif synthesis_type == 'extrapolation':
            applications.extend([
                "Future planning",
                "Innovation roadmap"
            ])
        
        return applications

class AdvancedKnowledgeSynthesisSystem:
    """é«˜åº¦ãªçŸ¥è­˜åˆæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.logger = logging.getLogger(__name__)
        self.config = config or self._default_config()
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.multimodal_encoder = MultiModalEncoder()
        self.contradiction_resolver = ContradictionResolver()
        self.knowledge_synthesizer = KnowledgeSynthesizer()
        
        # çŸ¥è­˜ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.knowledge_base = {}
        self.contradictions = {}
        self.synthesized_knowledge = {}
        self.knowledge_graph = None
        
        # çµ±è¨ˆ
        self.stats = {
            'total_knowledge_items': 0,
            'contradictions_detected': 0,
            'contradictions_resolved': 0,
            'synthesis_operations': 0,
            'novel_knowledge_created': 0
        }
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self._init_database()
        
        self.logger.info("ğŸ”® Advanced Knowledge Synthesis System initialized")
    
    def _default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š"""
        return {
            'contradiction_threshold': 0.7,
            'synthesis_confidence_threshold': 0.8,
            'graph_update_interval': 3600,
            'max_synthesis_batch': 10,
            'database_path': str(PROJECT_ROOT / "data" / "knowledge_synthesis.db")
        }
    
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        try:
            db_path = self.config['database_path']
            Path(db_path).parent.mkdir(parents=True, exist_ok=True)
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çŸ¥è­˜ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS multimodal_knowledge (
                    knowledge_id TEXT PRIMARY KEY,
                    created_at TEXT,
                    modality TEXT,
                    content BLOB,
                    text_representation TEXT,
                    vector_representation BLOB,
                    metadata TEXT,
                    quality_score REAL,
                    source_references TEXT
                );
            """)
            
            # çŸ›ç›¾ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS knowledge_contradictions (
                    contradiction_id TEXT PRIMARY KEY,
                    detected_at TEXT,
                    knowledge_ids TEXT,
                    contradiction_type TEXT,
                    severity REAL,
                    resolution_strategy TEXT,
                    resolution_confidence REAL,
                    evidence TEXT,
                    resolved BOOLEAN
                );
            """)
            
            # åˆæˆçŸ¥è­˜ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS synthesized_knowledge (
                    synthesis_id TEXT PRIMARY KEY,
                    created_at TEXT,
                    source_knowledge_ids TEXT,
                    synthesis_strategy TEXT,
                    modalities_involved TEXT,
                    synthesized_content TEXT,
                    confidence_score REAL,
                    novelty_score REAL,
                    validation_results TEXT,
                    applications TEXT
                );
            """)
            
            # çŸ¥è­˜ã‚°ãƒ©ãƒ•ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS knowledge_graph_snapshots (
                    snapshot_id TEXT PRIMARY KEY,
                    created_at TEXT,
                    nodes_count INTEGER,
                    edges_count INTEGER,
                    clusters_count INTEGER,
                    graph_data BLOB,
                    graph_metrics TEXT
                );
            """)
            
            conn.commit()
            conn.close()
            
            self.logger.info("ğŸ“Š Knowledge synthesis database initialized")
            
        except Exception as e:
            self.logger.error(f"Database initialization failed: {e}")
    
    async def add_knowledge(self,
                          content: Any,
                          modality: ModalityType,
                          metadata: Dict[str, Any] = None,
                          source_references: List[str] = None) -> str:
        """çŸ¥è­˜è¿½åŠ """
        try:
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            vector, text_repr = await self.multimodal_encoder.encode(content, modality)
            
            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            quality_score = await self._calculate_quality_score(
                content, modality, metadata
            )
            
            # çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ ä½œæˆ
            knowledge = MultiModalKnowledge(
                knowledge_id=f"know_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hashlib.md5(text_repr.encode()).hexdigest()[:8]}",
                created_at=datetime.now(),
                modality=modality,
                content=content,
                text_representation=text_repr,
                vector_representation=vector,
                metadata=metadata or {},
                quality_score=quality_score,
                source_references=source_references or []
            )
            
            # ä¿å­˜
            self.knowledge_base[knowledge.knowledge_id] = knowledge
            self.stats['total_knowledge_items'] += 1
            
            # çŸ›ç›¾ãƒã‚§ãƒƒã‚¯
            await self._check_contradictions(knowledge)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
            await self._persist_knowledge(knowledge)
            
            self.logger.info(f"ğŸ“š Added {modality.value} knowledge: {knowledge.knowledge_id}")
            
            return knowledge.knowledge_id
            
        except Exception as e:
            self.logger.error(f"Knowledge addition failed: {e}")
            raise
    
    async def synthesize_knowledge(self,
                                 knowledge_ids: List[str],
                                 strategy: SynthesisStrategy = SynthesisStrategy.FUSION) -> str:
        """çŸ¥è­˜åˆæˆå®Ÿè¡Œ"""
        try:
            # çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ å–å¾—
            knowledge_items = [
                self.knowledge_base[kid] 
                for kid in knowledge_ids 
                if kid in self.knowledge_base
            ]
            
            if len(knowledge_items) < 2:
                raise ValueError("At least 2 knowledge items required for synthesis")
            
            # åˆæˆå®Ÿè¡Œ
            synthesized = await self.knowledge_synthesizer.synthesize(
                knowledge_items, 
                strategy
            )
            
            # ä¿å­˜
            self.synthesized_knowledge[synthesized.synthesis_id] = synthesized
            self.stats['synthesis_operations'] += 1
            
            if synthesized.novelty_score > 0.7:
                self.stats['novel_knowledge_created'] += 1
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
            await self._persist_synthesized_knowledge(synthesized)
            
            self.logger.info(f"âœ¨ Synthesized knowledge: {synthesized.synthesis_id} (novelty: {synthesized.novelty_score:.2f})")
            
            return synthesized.synthesis_id
            
        except Exception as e:
            self.logger.error(f"Knowledge synthesis failed: {e}")
            raise
    
    async def resolve_contradiction(self, contradiction_id: str) -> Dict[str, Any]:
        """çŸ›ç›¾è§£æ±º"""
        try:
            if contradiction_id not in self.contradictions:
                raise ValueError(f"Contradiction {contradiction_id} not found")
            
            contradiction = self.contradictions[contradiction_id]
            
            # é–¢é€£çŸ¥è­˜å–å¾—
            knowledge_items = [
                self.knowledge_base[kid]
                for kid in contradiction.knowledge_ids
                if kid in self.knowledge_base
            ]
            
            # è§£æ±ºå®Ÿè¡Œ
            resolution = await self.contradiction_resolver.resolve(
                contradiction,
                knowledge_items
            )
            
            if resolution['success']:
                self.stats['contradictions_resolved'] += 1
                
                # çŸ›ç›¾è¨˜éŒ²æ›´æ–°
                contradiction.resolution_confidence = resolution['confidence']
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
                await self._update_contradiction_resolution(
                    contradiction_id, 
                    resolution
                )
            
            self.logger.info(f"ğŸ”§ Resolved contradiction: {contradiction_id}")
            
            return resolution
            
        except Exception as e:
            self.logger.error(f"Contradiction resolution failed: {e}")
            return {'success': False, 'error': str(e)}
    
    async def build_knowledge_graph(self) -> KnowledgeGraph:
        """çŸ¥è­˜ã‚°ãƒ©ãƒ•æ§‹ç¯‰"""
        try:
            # NetworkXã‚°ãƒ©ãƒ•ä½œæˆ
            G = nx.Graph()
            
            # ãƒãƒ¼ãƒ‰è¿½åŠ 
            for kid, knowledge in self.knowledge_base.items():
                G.add_node(kid, **{
                    'modality': knowledge.modality.value,
                    'quality': knowledge.quality_score,
                    'created_at': knowledge.created_at.isoformat()
                })
            
            # ã‚¨ãƒƒã‚¸è¿½åŠ ï¼ˆé¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
            knowledge_items = list(self.knowledge_base.values())
            for i in range(len(knowledge_items)):
                for j in range(i + 1, len(knowledge_items)):
                    similarity = await self._calculate_similarity(
                        knowledge_items[i],
                        knowledge_items[j]
                    )
                    
                    if similarity > 0.7:
                        G.add_edge(
                            knowledge_items[i].knowledge_id,
                            knowledge_items[j].knowledge_id,
                            weight=similarity
                        )
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            if len(G.nodes) > 0:
                # ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—ä½œæˆ
                vectors = []
                node_ids = []
                for kid, knowledge in self.knowledge_base.items():
                    if knowledge.vector_representation is not None:
                        vectors.append(knowledge.vector_representation)
                        node_ids.append(kid)
                
                if vectors:
                    # DBSCAN ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
                    clustering = DBSCAN(eps=0.3, min_samples=2)
                    clusters = clustering.fit_predict(np.array(vectors))
                    
                    cluster_dict = defaultdict(list)
                    for node_id, cluster_label in zip(node_ids, clusters):
                        cluster_dict[int(cluster_label)].append(node_id)
                else:
                    cluster_dict = {0: list(G.nodes)}
            else:
                cluster_dict = {}
            
            # ä¸­å¿ƒæ¦‚å¿µè­˜åˆ¥
            if len(G.nodes) > 0:
                centrality = nx.degree_centrality(G)
                central_concepts = sorted(
                    centrality.items(), 
                    key=lambda x: x[1], 
                    reverse=True
                )[:5]
                central_concepts = [c[0] for c in central_concepts]
            else:
                central_concepts = []
            
            # ã‚°ãƒ©ãƒ•ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            graph_metrics = {
                'nodes': len(G.nodes),
                'edges': len(G.edges),
                'density': nx.density(G) if len(G.nodes) > 1 else 0,
                'clusters': len(cluster_dict),
                'average_degree': np.mean([d for n, d in G.degree()]) if G.nodes else 0
            }
            
            # çŸ¥è­˜ã‚°ãƒ©ãƒ•ä½œæˆ
            self.knowledge_graph = KnowledgeGraph(
                graph_id=f"graph_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                created_at=datetime.now(),
                nodes=dict(self.knowledge_base),
                edges=[(u, v, KnowledgeRelationType.RELATED_TO) for u, v in G.edges],
                clusters=dict(cluster_dict),
                central_concepts=central_concepts,
                graph_metrics=graph_metrics
            )
            
            # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä¿å­˜
            await self._save_graph_snapshot(G)
            
            self.logger.info(f"ğŸ•¸ï¸ Built knowledge graph: {graph_metrics}")
            
            return self.knowledge_graph
            
        except Exception as e:
            self.logger.error(f"Knowledge graph building failed: {e}")
            raise
    
    async def query_multimodal(self,
                             query: str,
                             modalities: List[ModalityType] = None,
                             top_k: int = 5) -> List[Dict[str, Any]]:
        """ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¯ã‚¨ãƒª"""
        try:
            # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            query_vector, _ = await self.multimodal_encoder.encode(
                query, 
                ModalityType.TEXT
            )
            
            # å€™è£œæ¤œç´¢
            candidates = []
            for kid, knowledge in self.knowledge_base.items():
                # ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿
                if modalities and knowledge.modality not in modalities:
                    continue
                
                # é¡ä¼¼åº¦è¨ˆç®—
                if knowledge.vector_representation is not None:
                    similarity = cosine_similarity(
                        [query_vector],
                        [knowledge.vector_representation]
                    )[0][0]
                    
                    candidates.append({
                        'knowledge_id': kid,
                        'modality': knowledge.modality.value,
                        'text': knowledge.text_representation,
                        'similarity': float(similarity),
                        'quality': knowledge.quality_score
                    })
            
            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            candidates.sort(key=lambda x: x['similarity'] * x['quality'], reverse=True)
            
            return candidates[:top_k]
            
        except Exception as e:
            self.logger.error(f"Multimodal query failed: {e}")
            return []
    
    async def _calculate_quality_score(self,
                                     content: Any,
                                     modality: ModalityType,
                                     metadata: Dict[str, Any]) -> float:
        """å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        base_score = 0.5
        
        # ãƒ¢ãƒ€ãƒªãƒ†ã‚£åˆ¥èª¿æ•´
        modality_scores = {
            ModalityType.TEXT: 0.1,
            ModalityType.IMAGE: 0.15,
            ModalityType.AUDIO: 0.15,
            ModalityType.CODE: 0.2,
            ModalityType.STRUCTURED_DATA: 0.1
        }
        
        base_score += modality_scores.get(modality, 0)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹èª¿æ•´
        if metadata:
            if 'source_credibility' in metadata:
                base_score += metadata['source_credibility'] * 0.2
            if 'verification_status' in metadata:
                base_score += 0.1
        
        return min(1.0, base_score)
    
    async def _check_contradictions(self, new_knowledge: MultiModalKnowledge):
        """çŸ›ç›¾ãƒã‚§ãƒƒã‚¯"""
        for kid, existing_knowledge in self.knowledge_base.items():
            if kid == new_knowledge.knowledge_id:
                continue
            
            # é¡ä¼¼åº¦è¨ˆç®—
            similarity = await self._calculate_similarity(new_knowledge, existing_knowledge)
            
            # ãƒ†ã‚­ã‚¹ãƒˆçŸ›ç›¾ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡ç•¥åŒ–ï¼‰
            if similarity > 0.8:
                # é«˜é¡ä¼¼åº¦ã ãŒå†…å®¹ãŒçŸ›ç›¾ã™ã‚‹å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                if await self._detect_semantic_contradiction(
                    new_knowledge.text_representation,
                    existing_knowledge.text_representation
                ):
                    # çŸ›ç›¾æ¤œå‡º
                    contradiction = KnowledgeContradiction(
                        contradiction_id=f"contra_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        detected_at=datetime.now(),
                        knowledge_ids=[new_knowledge.knowledge_id, kid],
                        contradiction_type='semantic',
                        severity=0.8,
                        resolution_strategy=SynthesisStrategy.CONTRADICTION_RESOLUTION,
                        resolution_confidence=0.0,
                        evidence=[{
                            'type': 'high_similarity_with_contradiction',
                            'similarity': similarity
                        }]
                    )
                    
                    self.contradictions[contradiction.contradiction_id] = contradiction
                    self.stats['contradictions_detected'] += 1
                    
                    self.logger.warning(f"âš ï¸ Contradiction detected: {contradiction.contradiction_id}")
    
    async def _calculate_similarity(self,
                                  knowledge1: MultiModalKnowledge,
                                  knowledge2: MultiModalKnowledge) -> float:
        """çŸ¥è­˜é–“é¡ä¼¼åº¦è¨ˆç®—"""
        if (knowledge1.vector_representation is not None and 
            knowledge2.vector_representation is not None):
            
            # ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒèª¿æ•´
            min_dim = min(
                len(knowledge1.vector_representation),
                len(knowledge2.vector_representation)
            )
            
            v1 = knowledge1.vector_representation[:min_dim]
            v2 = knowledge2.vector_representation[:min_dim]
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
            similarity = cosine_similarity([v1], [v2])[0][0]
            
            # ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ã‚ˆã‚‹èª¿æ•´
            if knowledge1.modality != knowledge2.modality:
                similarity *= 0.8
            
            return float(similarity)
        
        return 0.0
    
    async def _detect_semantic_contradiction(self, text1: str, text2: str) -> bool:
        """æ„å‘³çš„çŸ›ç›¾æ¤œå‡º"""
        # ç°¡ç•¥åŒ–å®Ÿè£…
        # å®Ÿéš›ã¯é«˜åº¦ãªNLPæŠ€è¡“ã‚’ä½¿ç”¨
        
        # å¦å®šèªãƒã‚§ãƒƒã‚¯
        negation_words = ['not', 'no', 'never', 'none', 'nothing']
        
        text1_lower = text1.lower()
        text2_lower = text2.lower()
        
        # ä¸€æ–¹ã«å¦å®šèªãŒã‚ã‚Šã€ä»–æ–¹ã«ãªã„å ´åˆ
        text1_has_negation = any(neg in text1_lower for neg in negation_words)
        text2_has_negation = any(neg in text2_lower for neg in negation_words)
        
        if text1_has_negation != text2_has_negation:
            return True
        
        return False
    
    async def _persist_knowledge(self, knowledge: MultiModalKnowledge):
        """çŸ¥è­˜æ°¸ç¶šåŒ–"""
        try:
            conn = sqlite3.connect(self.config['database_path'])
            cursor = conn.cursor()
            
            # ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›
            vector_blob = pickle.dumps(knowledge.vector_representation) if knowledge.vector_representation is not None else None
            content_blob = pickle.dumps(knowledge.content)
            
            cursor.execute("""
                INSERT OR REPLACE INTO multimodal_knowledge 
                (knowledge_id, created_at, modality, content, text_representation,
                 vector_representation, metadata, quality_score, source_references)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                knowledge.knowledge_id,
                knowledge.created_at.isoformat(),
                knowledge.modality.value,
                content_blob,
                knowledge.text_representation,
                vector_blob,
                json.dumps(knowledge.metadata),
                knowledge.quality_score,
                json.dumps(knowledge.source_references)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Knowledge persistence failed: {e}")
    
    async def _persist_synthesized_knowledge(self, synthesized: SynthesizedKnowledge):
        """åˆæˆçŸ¥è­˜æ°¸ç¶šåŒ–"""
        try:
            conn = sqlite3.connect(self.config['database_path'])
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO synthesized_knowledge 
                (synthesis_id, created_at, source_knowledge_ids, synthesis_strategy,
                 modalities_involved, synthesized_content, confidence_score,
                 novelty_score, validation_results, applications)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                synthesized.synthesis_id,
                synthesized.created_at.isoformat(),
                json.dumps(synthesized.source_knowledge_ids),
                synthesized.synthesis_strategy.value,
                json.dumps([m.value for m in synthesized.modalities_involved]),
                json.dumps(synthesized.synthesized_content),
                synthesized.confidence_score,
                synthesized.novelty_score,
                json.dumps(synthesized.validation_results),
                json.dumps(synthesized.applications)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Synthesized knowledge persistence failed: {e}")
    
    async def _update_contradiction_resolution(self, 
                                             contradiction_id: str,
                                             resolution: Dict[str, Any]):
        """çŸ›ç›¾è§£æ±ºæ›´æ–°"""
        try:
            conn = sqlite3.connect(self.config['database_path'])
            cursor = conn.cursor()
            
            cursor.execute("""
                UPDATE knowledge_contradictions 
                SET resolved = ?, resolution_confidence = ?
                WHERE contradiction_id = ?
            """, (
                True,
                resolution['confidence'],
                contradiction_id
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Contradiction resolution update failed: {e}")
    
    async def _save_graph_snapshot(self, graph: nx.Graph):
        """ã‚°ãƒ©ãƒ•ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä¿å­˜"""
        try:
            conn = sqlite3.connect(self.config['database_path'])
            cursor = conn.cursor()
            
            # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ”ãƒƒã‚¯ãƒ«åŒ–
            graph_blob = pickle.dumps(graph)
            
            cursor.execute("""
                INSERT INTO knowledge_graph_snapshots 
                (snapshot_id, created_at, nodes_count, edges_count,
                 clusters_count, graph_data, graph_metrics)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                self.knowledge_graph.graph_id,
                self.knowledge_graph.created_at.isoformat(),
                len(self.knowledge_graph.nodes),
                len(self.knowledge_graph.edges),
                len(self.knowledge_graph.clusters),
                graph_blob,
                json.dumps(self.knowledge_graph.graph_metrics)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Graph snapshot save failed: {e}")
    
    async def get_synthesis_statistics(self) -> Dict[str, Any]:
        """åˆæˆçµ±è¨ˆå–å¾—"""
        return {
            'total_knowledge_items': self.stats['total_knowledge_items'],
            'modality_distribution': self._get_modality_distribution(),
            'contradictions_detected': self.stats['contradictions_detected'],
            'contradictions_resolved': self.stats['contradictions_resolved'],
            'resolution_rate': (
                self.stats['contradictions_resolved'] / self.stats['contradictions_detected']
                if self.stats['contradictions_detected'] > 0 else 0
            ),
            'synthesis_operations': self.stats['synthesis_operations'],
            'novel_knowledge_created': self.stats['novel_knowledge_created'],
            'novelty_rate': (
                self.stats['novel_knowledge_created'] / self.stats['synthesis_operations']
                if self.stats['synthesis_operations'] > 0 else 0
            ),
            'graph_metrics': self.knowledge_graph.graph_metrics if self.knowledge_graph else {}
        }
    
    def _get_modality_distribution(self) -> Dict[str, int]:
        """ãƒ¢ãƒ€ãƒªãƒ†ã‚£åˆ†å¸ƒå–å¾—"""
        distribution = defaultdict(int)
        for knowledge in self.knowledge_base.values():
            distribution[knowledge.modality.value] += 1
        return dict(distribution)


# ä½¿ç”¨ä¾‹
async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        synthesis_system = AdvancedKnowledgeSynthesisSystem()
        
        print("ğŸ”® Starting Advanced Knowledge Synthesis System...")
        
        # ã‚µãƒ³ãƒ—ãƒ«çŸ¥è­˜è¿½åŠ 
        print("\nğŸ“š Adding multimodal knowledge...")
        
        # ãƒ†ã‚­ã‚¹ãƒˆçŸ¥è­˜
        text_id1 = await synthesis_system.add_knowledge(
            content="pgvector enables semantic search with high-dimensional vectors",
            modality=ModalityType.TEXT,
            metadata={'source': 'technical_doc', 'confidence': 0.9},
            source_references=['pgvector_official_docs']
        )
        
        text_id2 = await synthesis_system.add_knowledge(
            content="Semantic search uses embeddings to find similar content",
            modality=ModalityType.TEXT,
            metadata={'source': 'research_paper', 'confidence': 0.85},
            source_references=['semantic_search_survey_2024']
        )
        
        # ã‚³ãƒ¼ãƒ‰çŸ¥è­˜
        code_id = await synthesis_system.add_knowledge(
            content="""
            def semantic_search(query, vectors):
                query_embedding = encode(query)
                similarities = cosine_similarity(query_embedding, vectors)
                return top_k_indices(similarities)
            """,
            modality=ModalityType.CODE,
            metadata={'language': 'python', 'tested': True},
            source_references=['implementation_guide']
        )
        
        # ç”»åƒçŸ¥è­˜ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
        image_id = await synthesis_system.add_knowledge(
            content={'type': 'diagram', 'description': 'Vector space visualization'},
            modality=ModalityType.IMAGE,
            metadata={'format': 'png', 'resolution': '1920x1080'},
            source_references=['visualization_toolkit']
        )
        
        print(f"Added {synthesis_system.stats['total_knowledge_items']} knowledge items")
        
        # çŸ›ç›¾ã™ã‚‹çŸ¥è­˜ã‚’è¿½åŠ ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        print("\nâš ï¸ Adding contradictory knowledge...")
        
        contradiction_id = await synthesis_system.add_knowledge(
            content="pgvector does not support semantic search",  # çŸ›ç›¾
            modality=ModalityType.TEXT,
            metadata={'source': 'unreliable_blog', 'confidence': 0.3},
            source_references=['random_blog_post']
        )
        
        # çŸ¥è­˜åˆæˆ
        print("\nâœ¨ Synthesizing knowledge...")
        
        # èåˆæˆ¦ç•¥
        fusion_id = await synthesis_system.synthesize_knowledge(
            knowledge_ids=[text_id1, text_id2, code_id],
            strategy=SynthesisStrategy.FUSION
        )
        
        # æŠ½è±¡åŒ–æˆ¦ç•¥
        abstraction_id = await synthesis_system.synthesize_knowledge(
            knowledge_ids=[text_id1, text_id2],
            strategy=SynthesisStrategy.ABSTRACTION
        )
        
        # çŸ›ç›¾è§£æ±º
        if synthesis_system.contradictions:
            print("\nğŸ”§ Resolving contradictions...")
            for contra_id in list(synthesis_system.contradictions.keys()):
                resolution = await synthesis_system.resolve_contradiction(contra_id)
                print(f"Resolved: {resolution['success']} (confidence: {resolution.get('confidence', 0):.2f})")
        
        # çŸ¥è­˜ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        print("\nğŸ•¸ï¸ Building knowledge graph...")
        knowledge_graph = await synthesis_system.build_knowledge_graph()
        
        # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¯ã‚¨ãƒª
        print("\nğŸ” Testing multimodal query...")
        results = await synthesis_system.query_multimodal(
            query="How does pgvector implement semantic search?",
            modalities=[ModalityType.TEXT, ModalityType.CODE],
            top_k=3
        )
        
        print(f"Found {len(results)} relevant items:")
        for result in results:
            print(f"  - {result['modality']}: {result['text'][:50]}... (similarity: {result['similarity']:.3f})")
        
        # çµ±è¨ˆè¡¨ç¤º
        print("\nğŸ“Š Synthesis Statistics:")
        stats = await synthesis_system.get_synthesis_statistics()
        
        print(f"  Total Knowledge Items: {stats['total_knowledge_items']}")
        print(f"  Modality Distribution: {stats['modality_distribution']}")
        print(f"  Contradictions: {stats['contradictions_detected']} detected, {stats['contradictions_resolved']} resolved")
        print(f"  Resolution Rate: {stats['resolution_rate']:.2%}")
        print(f"  Synthesis Operations: {stats['synthesis_operations']}")
        print(f"  Novel Knowledge Created: {stats['novel_knowledge_created']}")
        print(f"  Novelty Rate: {stats['novelty_rate']:.2%}")
        
        if stats['graph_metrics']:
            print(f"\n  Knowledge Graph:")
            print(f"    Nodes: {stats['graph_metrics']['nodes']}")
            print(f"    Edges: {stats['graph_metrics']['edges']}")
            print(f"    Density: {stats['graph_metrics']['density']:.3f}")
            print(f"    Clusters: {stats['graph_metrics']['clusters']}")
        
        print("\nğŸ‰ Advanced Knowledge Synthesis System Phase 2 demonstration completed!")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())