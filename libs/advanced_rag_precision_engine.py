#!/usr/bin/env python3
"""
Advanced RAG Precision Engine
æœ€æ–°ã®è«–æ–‡ãƒ»ç ”ç©¶ã«åŸºã¥ãRAGç²¾åº¦å‘ä¸Šã‚·ã‚¹ãƒ†ãƒ 

ğŸ¯ å®Ÿè£…æ‰‹æ³•:
1. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  (Cosine + BM25)
2. å‹•çš„ä¿¡é ¼åº¦è¨ˆç®—å¼ (Multi-factor Confidence)
3. RAGASæ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹ (5æŒ‡æ¨™çµ±åˆ)
4. O1-Embedderæ–¹å¼ (æ¨è«–æ‹¡å¼µåŸ‹ã‚è¾¼ã¿)
5. å¤šæ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (Position-weighted Ranking)
"""

import asyncio
import json
import logging
import math
import re
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
import numpy as np
from collections import Counter, defaultdict
import sqlite3

# å¿…è¦ã«å¿œã˜ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¯èƒ½ãªç§‘å­¦è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.feature_extraction.text import TfidfVectorizer

    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ scikit-learn not available, using manual implementations")


@dataclass
class SearchResult:
    """æ¤œç´¢çµæœ"""

    doc_id: str
    content: str
    title: str
    hybrid_score: float
    cosine_score: float
    bm25_score: float
    confidence: float
    metadata: Dict[str, Any]


@dataclass
class RAGASMetrics:
    """RAGASæ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    faithfulness: float
    answer_relevancy: float
    context_precision: float
    context_recall: float
    groundedness: float
    overall_score: float


class AdvancedRAGPrecisionEngine:
    """Advanced RAG Precision Engine - æœ€æ–°æ‰‹æ³•çµ±åˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.logger = self._setup_logger()

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
        self.db_path = "/home/aicompany/ai_co/data/advanced_rag_precision.db"
        self._setup_database()

        # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.search_config = {
            "hybrid_alpha": 0.7,  # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢é‡ã¿
            "hybrid_beta": 0.3,  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢é‡ã¿
            "max_results": 10,
            "confidence_threshold": 0.6,
            "position_decay": 0.85,  # é †ä½æ¸›è¡°ç‡
        }

        # RAGASé‡ã¿è¨­å®š
        self.ragas_weights = {
            "faithfulness": 0.25,
            "answer_relevancy": 0.25,
            "context_precision": 0.20,
            "context_recall": 0.20,
            "groundedness": 0.10,
        }

        # æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.document_store: Dict[str, Dict[str, Any]] = {}
        self.document_embeddings: Dict[str, List[float]] = {}
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None

        self.logger.info("ğŸ¯ Advanced RAG Precision Engine initialized")

    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼è¨­å®š"""
        logger = logging.getLogger("advanced_rag_precision")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s - RAG Precision - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def _setup_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š"""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # æ¤œç´¢çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS search_results (
                result_id TEXT PRIMARY KEY,
                query TEXT,
                doc_id TEXT,
                hybrid_score REAL,
                cosine_score REAL,
                bm25_score REAL,
                confidence REAL,
                timestamp TEXT
            )
        """
        )

        # RAGASãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS ragas_metrics (
                metric_id TEXT PRIMARY KEY,
                query TEXT,
                generated_answer TEXT,
                faithfulness REAL,
                answer_relevancy REAL,
                context_precision REAL,
                context_recall REAL,
                groundedness REAL,
                overall_score REAL,
                timestamp TEXT
            )
        """
        )

        # æ”¹å–„å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS improvement_history (
                improvement_id TEXT PRIMARY KEY,
                method_name TEXT,
                baseline_score REAL,
                improved_score REAL,
                improvement_rate REAL,
                test_cases_count INTEGER,
                timestamp TEXT
            )
        """
        )

        conn.commit()
        conn.close()

    async def initialize_document_store(self, documents: List[Dict[str, Any]]):
        """æ–‡æ›¸ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–"""
        self.logger.info(
            f"ğŸ”„ Initializing document store with {len(documents)} documents..."
        )

        # æ–‡æ›¸ã®æ ¼ç´
        for doc in documents:
            doc_id = doc.get("id", f"doc_{len(self.document_store)}")
            self.document_store[doc_id] = {
                "title": doc.get("title", ""),
                "content": doc.get("content", ""),
                "metadata": doc.get("metadata", {}),
            }

        # åŸ‹ã‚è¾¼ã¿ã¨TF-IDFã®ç”Ÿæˆ
        await self._generate_embeddings()
        await self._build_tfidf_index()

        self.logger.info(
            f"âœ… Document store initialized with {len(self.document_store)} documents"
        )

    async def _generate_embeddings(self):
        """æ–‡æ›¸åŸ‹ã‚è¾¼ã¿ã®ç”Ÿæˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        self.logger.info("ğŸ§  Generating document embeddings...")

        for doc_id, doc_data in self.document_store.items():
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯é«˜åº¦ãªåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            content = doc_data["content"]

            # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            embedding = await self._simulate_embedding(content)
            self.document_embeddings[doc_id] = embedding

        self.logger.info(
            f"âœ… Generated embeddings for {len(self.document_embeddings)} documents"
        )

    async def _simulate_embedding(self, text: str, dim: int = 384) -> List[float]:
        """åŸ‹ã‚è¾¼ã¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ã‚·ãƒ¼ãƒ‰å€¤ã§ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’å›ºå®š
        seed = sum(ord(c) for c in text[:100]) % (2**32)
        np.random.seed(seed)

        # æ­£è¦åŒ–ã•ã‚ŒãŸãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«
        vector = np.random.randn(dim)
        vector = vector / np.linalg.norm(vector)

        return vector.tolist()

    async def _build_tfidf_index(self):
        """TF-IDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰"""
        self.logger.info("ğŸ“Š Building TF-IDF index...")

        if not SKLEARN_AVAILABLE:
            self.logger.warning("scikit-learn not available, skipping TF-IDF indexing")
            return

        # å…¨æ–‡æ›¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
        documents = [doc_data["content"] for doc_data in self.document_store.values()]

        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000, stop_words="english", ngram_range=(1, 2)
        )

        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)
        self.logger.info("âœ… TF-IDF index built successfully")

    async def hybrid_search(self, query: str, top_k: int = None) -> List[SearchResult]:
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  (Cosine + BM25)

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            top_k: è¿”ã™çµæœæ•°

        Returns:
            List[SearchResult]: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸæ¤œç´¢çµæœ
        """
        if top_k is None:
            top_k = self.search_config["max_results"]

        self.logger.info(f"ğŸ” Hybrid search for: '{query[:50]}...'")

        # 1. ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        query_embedding = await self._simulate_embedding(query)

        # 2. å„æ–‡æ›¸ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
        results = []

        for doc_id, doc_data in self.document_store.items():
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
            cosine_score = await self._calculate_cosine_similarity(
                query_embedding, self.document_embeddings[doc_id]
            )

            # BM25ã‚¹ã‚³ã‚¢è¨ˆç®—
            bm25_score = await self._calculate_bm25_score(query, doc_data["content"])

            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢è¨ˆç®—
            hybrid_score = (
                self.search_config["hybrid_alpha"] * cosine_score
                + self.search_config["hybrid_beta"] * bm25_score
            )

            # ä¿¡é ¼åº¦è¨ˆç®—
            confidence = await self._calculate_multi_factor_confidence(
                cosine_score, bm25_score, doc_data["content"], query
            )

            if hybrid_score > 0.05:  # æœ€å°é–¾å€¤ã‚’ä¸‹ã’ã¦çµæœã‚’å–å¾—
                results.append(
                    SearchResult(
                        doc_id=doc_id,
                        content=doc_data["content"][:500] + "...",
                        title=doc_data["title"],
                        hybrid_score=hybrid_score,
                        cosine_score=cosine_score,
                        bm25_score=bm25_score,
                        confidence=confidence,
                        metadata=doc_data["metadata"],
                    )
                )

        # 3. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x.hybrid_score, reverse=True)

        # 4. é †ä½ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘ï¼ˆPosition-weighted Rankingï¼‰
        for i, result in enumerate(results):
            position_weight = self.search_config["position_decay"] ** i
            result.hybrid_score *= position_weight

        # 5. çµæœã‚’è¨˜éŒ²
        await self._record_search_results(query, results[:top_k])

        self.logger.info(f"âœ… Found {len(results[:top_k])} relevant results")
        return results[:top_k]

    async def _calculate_cosine_similarity(
        self, vec1: List[float], vec2: List[float]
    ) -> float:
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—"""
        if SKLEARN_AVAILABLE:
            return float(cosine_similarity([vec1], [vec2])[0][0])
        else:
            # æ‰‹å‹•å®Ÿè£…
            dot_product = sum(a * b for a, b in zip(vec1, vec2))
            magnitude1 = math.sqrt(sum(a * a for a in vec1))
            magnitude2 = math.sqrt(sum(b * b for b in vec2))

            if magnitude1 == 0 or magnitude2 == 0:
                return 0.0

            return dot_product / (magnitude1 * magnitude2)

    async def _calculate_bm25_score(self, query: str, document: str) -> float:
        """BM25ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # BM25ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        k1 = 1.2
        b = 0.75

        # ã‚¯ã‚¨ãƒªã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‰å‡¦ç†
        query_terms = set(query.lower().split())
        doc_terms = document.lower().split()
        doc_length = len(doc_terms)

        if doc_length == 0:
            return 0.0

        # å¹³å‡æ–‡æ›¸é•·ï¼ˆç°¡æ˜“è¨ˆç®—ï¼‰
        avg_doc_length = sum(
            len(doc["content"].split()) for doc in self.document_store.values()
        ) / len(self.document_store)

        score = 0.0
        for term in query_terms:
            # èªã®å‡ºç¾é »åº¦
            tf = doc_terms.count(term)
            if tf == 0:
                continue

            # æ–‡æ›¸é »åº¦ï¼ˆç°¡æ˜“ç‰ˆ: å…¨æ–‡æ›¸ã§ã®å‡ºç¾ç‡ï¼‰
            df = sum(
                1
                for doc in self.document_store.values()
                if term in doc["content"].lower()
            )

            # IDFè¨ˆç®—
            idf = math.log((len(self.document_store) - df + 0.5) / (df + 0.5))

            # BM25ã‚¹ã‚³ã‚¢è¨ˆç®—
            numerator = tf * (k1 + 1)
            denominator = tf + k1 * (1 - b + b * (doc_length / avg_doc_length))

            score += idf * (numerator / denominator)

        # æ­£è¦åŒ–
        return min(score / len(query_terms), 1.0) if query_terms else 0.0

    async def _calculate_multi_factor_confidence(
        self, cosine_score: float, bm25_score: float, document: str, query: str
    ) -> float:
        """
        å¤šè¦ç´ ä¿¡é ¼åº¦è¨ˆç®—

        Args:
            cosine_score: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
            bm25_score: BM25ã‚¹ã‚³ã‚¢
            document: æ–‡æ›¸å†…å®¹
            query: ã‚¯ã‚¨ãƒª

        Returns:
            float: çµ±åˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        """
        # 1. åŸºæœ¬ä¿¡é ¼åº¦ (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹)
        base_confidence = 0.7 * cosine_score + 0.3 * bm25_score

        # 2. æ¤œç´¢æ–‡æ›¸ã¨ã®æ•´åˆæ€§
        query_terms = set(query.lower().split())
        doc_terms = set(document.lower().split())

        if query_terms:
            alignment_score = len(query_terms.intersection(doc_terms)) / len(
                query_terms
            )
        else:
            alignment_score = 0.0

        # 3. å¿œç­”ã®ä¸€è²«æ€§ï¼ˆæ–‡æ›¸å†…ã§ã®èªã®åˆ†å¸ƒï¼‰
        doc_words = document.lower().split()
        if doc_words:
            # é‡è¦èªã®å¯†åº¦
            important_words = [word for word in doc_words if len(word) > 4]
            density_score = len(important_words) / len(doc_words)
        else:
            density_score = 0.0

        # 4. çµ±åˆä¿¡é ¼åº¦è¨ˆç®—
        confidence_weights = [0.5, 0.3, 0.2]
        final_confidence = (
            confidence_weights[0] * base_confidence
            + confidence_weights[1] * alignment_score
            + confidence_weights[2] * density_score
        )

        return min(final_confidence, 1.0)

    async def calculate_ragas_metrics(
        self,
        query: str,
        generated_answer: str,
        retrieved_contexts: List[str],
        ground_truth: str = None,
    ) -> RAGASMetrics:
        """
        RAGASæ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—

        Args:
            query: è³ªå•
            generated_answer: ç”Ÿæˆã•ã‚ŒãŸå›ç­”
            retrieved_contexts: æ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            ground_truth: æ­£è§£å›ç­”ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            RAGASMetrics: æ‹¡å¼µRAGASè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        self.logger.info("ğŸ“Š Calculating enhanced RAGAS metrics...")

        # 1. Faithfulness (å¿ å®Ÿæ€§)
        faithfulness = await self._calculate_faithfulness(
            generated_answer, retrieved_contexts
        )

        # 2. Answer Relevancy (å›ç­”é–¢é€£æ€§)
        answer_relevancy = await self._calculate_answer_relevancy(
            generated_answer, query
        )

        # 3. Context Precision (ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç²¾åº¦)
        context_precision = await self._calculate_context_precision(
            retrieved_contexts, ground_truth
        )

        # 4. Context Recall (ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†ç¾ç‡)
        context_recall = await self._calculate_context_recall(
            retrieved_contexts, ground_truth
        )

        # 5. Response Groundedness (å¿œç­”æ ¹æ‹ æ€§) - æ–°æŒ‡æ¨™
        groundedness = await self._calculate_response_groundedness(
            generated_answer, retrieved_contexts
        )

        # 6. ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        overall_score = (
            self.ragas_weights["faithfulness"] * faithfulness
            + self.ragas_weights["answer_relevancy"] * answer_relevancy
            + self.ragas_weights["context_precision"] * context_precision
            + self.ragas_weights["context_recall"] * context_recall
            + self.ragas_weights["groundedness"] * groundedness
        )

        metrics = RAGASMetrics(
            faithfulness=faithfulness,
            answer_relevancy=answer_relevancy,
            context_precision=context_precision,
            context_recall=context_recall,
            groundedness=groundedness,
            overall_score=overall_score,
        )

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
        await self._record_ragas_metrics(query, generated_answer, metrics)

        self.logger.info(
            f"âœ… RAGAS metrics calculated: Overall score = {overall_score:.3f}"
        )
        return metrics

    async def _calculate_faithfulness(self, answer: str, contexts: List[str]) -> float:
        """å¿ å®Ÿæ€§è¨ˆç®—"""
        if not contexts or not answer:
            return 0.0

        # answerãŒæ–‡å­—åˆ—ã§ãªã„å ´åˆã®å¯¾å¿œ
        if isinstance(answer, (list, tuple)):
            answer = " ".join(str(item) for item in answer)
        elif not isinstance(answer, str):
            answer = str(answer)

        # å›ç­”ã®å„æ–‡ã«ã¤ã„ã¦ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§æ”¯æŒã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        answer_sentences = re.split(r"[.!?]+", answer)
        supported_count = 0

        for sentence in answer_sentences:
            if len(sentence.strip()) < 10:  # çŸ­ã™ãã‚‹æ–‡ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®æ”¯æŒåº¦ãƒã‚§ãƒƒã‚¯
            sentence_words = set(sentence.lower().split())

            for context in contexts:
                context_words = set(context.lower().split())
                overlap = len(sentence_words.intersection(context_words))

                if overlap >= len(sentence_words) * 0.5:  # 50%ä»¥ä¸Šã®é‡è¤‡ã§æ”¯æŒã¨ã¿ãªã™
                    supported_count += 1
                    break

        return supported_count / max(len(answer_sentences), 1)

    async def _calculate_answer_relevancy(self, answer: str, query: str) -> float:
        """å›ç­”é–¢é€£æ€§è¨ˆç®—"""
        if not answer or not query:
            return 0.0

        # answerãŒæ–‡å­—åˆ—ã§ãªã„å ´åˆã®å¯¾å¿œ
        if isinstance(answer, (list, tuple)):
            answer = " ".join(str(item) for item in answer)
        elif not isinstance(answer, str):
            answer = str(answer)

        # ã‚¯ã‚¨ãƒªã¨å›ç­”ã®èªå½™é‡è¤‡åº¦
        query_words = set(query.lower().split())
        answer_words = set(answer.lower().split())

        if not query_words:
            return 0.0

        overlap = len(query_words.intersection(answer_words))
        return overlap / len(query_words)

    async def _calculate_context_precision(
        self, contexts: List[str], ground_truth: str
    ) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç²¾åº¦è¨ˆç®—"""
        if not contexts or not ground_truth:
            return 0.0

        relevant_count = 0
        for i, context in enumerate(contexts):
            # é †ä½ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
            position_weight = 1.0 / (i + 1)

            # æ­£è§£ã¨ã®é–¢é€£åº¦ãƒã‚§ãƒƒã‚¯
            context_words = set(context.lower().split())
            truth_words = set(ground_truth.lower().split())

            overlap = len(context_words.intersection(truth_words))
            if overlap >= len(truth_words) * 0.3:  # 30%ä»¥ä¸Šã®é‡è¤‡ã§é–¢é€£ã¨ã¿ãªã™
                relevant_count += position_weight

        return relevant_count / len(contexts)

    async def _calculate_context_recall(
        self, contexts: List[str], ground_truth: str
    ) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†ç¾ç‡è¨ˆç®—"""
        if not contexts or not ground_truth:
            return 0.0

        # æ­£è§£æƒ…å ±ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã©ã®ç¨‹åº¦ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã‚‹ã‹
        truth_words = set(ground_truth.lower().split())
        covered_words = set()

        for context in contexts:
            context_words = set(context.lower().split())
            covered_words.update(context_words.intersection(truth_words))

        return len(covered_words) / max(len(truth_words), 1)

    async def _calculate_response_groundedness(
        self, answer: str, contexts: List[str]
    ) -> float:
        """å¿œç­”æ ¹æ‹ æ€§è¨ˆç®—ï¼ˆæ–°æŒ‡æ¨™ï¼‰"""
        if not answer or not contexts:
            return 0.0

        # answerãŒæ–‡å­—åˆ—ã§ãªã„å ´åˆã®å¯¾å¿œ
        if isinstance(answer, (list, tuple)):
            answer = " ".join(str(item) for item in answer)
        elif not isinstance(answer, str):
            answer = str(answer)

        # å›ç­”ã®å„é‡è¦èªã«ã¤ã„ã¦ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®æ ¹æ‹ ã‚’ç¢ºèª
        answer_words = [word for word in answer.lower().split() if len(word) > 3]
        grounded_words = 0

        all_context_words = set()
        for context in contexts:
            all_context_words.update(context.lower().split())

        for word in answer_words:
            if word in all_context_words:
                grounded_words += 1

        return grounded_words / max(len(answer_words), 1)

    async def o1_embedder_reasoning(self, query: str) -> str:
        """
        O1-Embedderæ–¹å¼: æ¨è«–æ‹¡å¼µåŸ‹ã‚è¾¼ã¿

        Args:
            query: å…ƒã®ã‚¯ã‚¨ãƒª

        Returns:
            str: æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã§æ‹¡å¼µã•ã‚ŒãŸã‚¯ã‚¨ãƒª
        """
        self.logger.info(f"ğŸ§  O1-Embedder reasoning for: '{query}'")

        # æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
        thinking_steps = []

        # 1. ã‚¯ã‚¨ãƒªåˆ†æ
        if "å®Ÿè£…" in query or "é–‹ç™º" in query:
            thinking_steps.append("ã“ã‚Œã¯é–‹ç™ºãƒ»å®Ÿè£…ã«é–¢ã™ã‚‹è³ªå•ã§ã™ã€‚")
            thinking_steps.append("æŠ€è¡“ä»•æ§˜ã€è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã€å®Ÿè£…ä¾‹ãŒå¿…è¦ã§ã™ã€‚")

        if "æœ€é©åŒ–" in query or "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹" in query:
            thinking_steps.append("ã“ã‚Œã¯æ€§èƒ½æ”¹å–„ã«é–¢ã™ã‚‹è³ªå•ã§ã™ã€‚")
            thinking_steps.append("ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã€æ”¹å–„æ‰‹æ³•ã€æ¸¬å®šæ–¹æ³•ãŒé‡è¦ã§ã™ã€‚")

        if "ã‚¨ãƒ©ãƒ¼" in query or "ãƒã‚°" in query or "ä¿®æ­£" in query:
            thinking_steps.append("ã“ã‚Œã¯å•é¡Œè§£æ±ºã«é–¢ã™ã‚‹è³ªå•ã§ã™ã€‚")
            thinking_steps.append("åŸå› åˆ†æã€è§£æ±ºç­–ã€äºˆé˜²ç­–ãŒå¿…è¦ã§ã™ã€‚")

        # 2. é–¢é€£æŠ€è¡“ã®æ¨è«–
        tech_keywords = [
            "Elder",
            "Flow",
            "RAG",
            "API",
            "OAuth",
            "WebSocket",
            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
        ]
        found_tech = [tech for tech in tech_keywords if tech.lower() in query.lower()]

        if found_tech:
            thinking_steps.append(f"é–¢é€£æŠ€è¡“: {', '.join(found_tech)}")
            thinking_steps.append("ã“ã‚Œã‚‰ã®æŠ€è¡“ã«ç‰¹åŒ–ã—ãŸæƒ…å ±ã‚’æ¤œç´¢ã™ã¹ãã§ã™ã€‚")

        # 3. æ‹¡å¼µã‚¯ã‚¨ãƒªç”Ÿæˆ
        thinking_text = " ".join(thinking_steps)
        enhanced_query = f"{query} [THINKING] {thinking_text}"

        self.logger.info(
            f"âœ… Enhanced query with reasoning: {len(enhanced_query)} chars"
        )
        return enhanced_query

    async def adaptive_retrieval_strategy(self, query: str) -> str:
        """
        é©å¿œçš„æ¤œç´¢æˆ¦ç•¥é¸æŠ

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª

        Returns:
            str: é¸æŠã•ã‚ŒãŸæ¤œç´¢æˆ¦ç•¥
        """
        # ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦ã®è¨ˆç®—
        complexity_factors = {
            "length": len(query.split()) / 20,  # èªæ•°ã«ã‚ˆã‚‹è¤‡é›‘åº¦
            "technical_terms": sum(1 for word in query.split() if len(word) > 6)
            / len(query.split()),
            "question_depth": query.count("ãªãœ")
            + query.count("ã©ã®ã‚ˆã†ã«")
            + query.count("why")
            + query.count("how"),
        }

        complexity = sum(complexity_factors.values()) / len(complexity_factors)

        if complexity < 0.3:
            strategy = "simple_vector_search"
        elif complexity < 0.7:
            strategy = "hybrid_search"
        else:
            strategy = "multi_hop_reasoning"

        self.logger.info(
            f"ğŸ¯ Selected strategy: {strategy} (complexity: {complexity:.2f})"
        )
        return strategy

    async def _record_search_results(self, query: str, results: List[SearchResult]):
        """æ¤œç´¢çµæœã®è¨˜éŒ²"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            for i, result in enumerate(results):
                cursor.execute(
                    """
                    INSERT INTO search_results
                    (result_id, query, doc_id, hybrid_score, cosine_score, bm25_score, confidence, timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        f"result_{datetime.now().timestamp()}_{i}",
                        query,
                        result.doc_id,
                        result.hybrid_score,
                        result.cosine_score,
                        result.bm25_score,
                        result.confidence,
                        datetime.now().isoformat(),
                    ),
                )

            conn.commit()
            conn.close()

        except Exception as e:
            self.logger.error(f"Failed to record search results: {e}")

    async def _record_ragas_metrics(
        self, query: str, answer: str, metrics: RAGASMetrics
    ):
        """RAGASãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨˜éŒ²"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            cursor.execute(
                """
                INSERT INTO ragas_metrics
                (metric_id, query, generated_answer, faithfulness, answer_relevancy,
                 context_precision, context_recall, groundedness, overall_score, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    f"metric_{datetime.now().timestamp()}",
                    query,
                    answer,
                    metrics.faithfulness,
                    metrics.answer_relevancy,
                    metrics.context_precision,
                    metrics.context_recall,
                    metrics.groundedness,
                    metrics.overall_score,
                    datetime.now().isoformat(),
                ),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            self.logger.error(f"Failed to record RAGAS metrics: {e}")

    async def benchmark_improvements(
        self, test_cases: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        æ”¹å–„åŠ¹æœã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

        Args:
            test_cases: ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ

        Returns:
            Dict[str, Any]: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        """
        self.logger.info(f"ğŸ”¬ Running benchmark with {len(test_cases)} test cases...")

        baseline_scores = []
        improved_scores = []

        for test_case in test_cases:
            query = test_case["query"]
            expected_docs = test_case.get("expected_docs", [])

            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¤œç´¢ï¼ˆå˜ç´”ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰
            baseline_results = await self._baseline_search(query)
            baseline_score = self._calculate_relevance_score(
                baseline_results, expected_docs
            )
            baseline_scores.append(baseline_score)

            # æ”¹å–„ç‰ˆæ¤œç´¢ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼‰
            improved_results = await self.hybrid_search(query)
            improved_score = self._calculate_relevance_score(
                improved_results, expected_docs
            )
            improved_scores.append(improved_score)

        # çµ±è¨ˆè¨ˆç®—
        avg_baseline = sum(baseline_scores) / len(baseline_scores)
        avg_improved = sum(improved_scores) / len(improved_scores)
        improvement_rate = (
            (avg_improved - avg_baseline) / max(avg_baseline, 0.001)
        ) * 100  # ã‚¼ãƒ­é™¤ç®—å¯¾ç­–

        benchmark_result = {
            "test_cases_count": len(test_cases),
            "baseline_average": avg_baseline,
            "improved_average": avg_improved,
            "improvement_rate_percent": improvement_rate,
            "individual_results": [
                {
                    "query": test_cases[i]["query"],
                    "baseline": baseline_scores[i],
                    "improved": improved_scores[i],
                    "improvement": (
                        (improved_scores[i] - baseline_scores[i])
                        / max(baseline_scores[i], 0.001)
                    )
                    * 100,
                }
                for i in range(len(test_cases))
            ],
        }

        # çµæœã‚’è¨˜éŒ²
        await self._record_improvement_history(
            "hybrid_search", avg_baseline, avg_improved, len(test_cases)
        )

        self.logger.info(f"âœ… Benchmark complete: {improvement_rate:.1f}% improvement")
        return benchmark_result

    async def _baseline_search(self, query: str) -> List[SearchResult]:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¤œç´¢ï¼ˆå˜ç´”ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰"""
        query_embedding = await self._simulate_embedding(query)
        results = []

        for doc_id, doc_data in self.document_store.items():
            cosine_score = await self._calculate_cosine_similarity(
                query_embedding, self.document_embeddings[doc_id]
            )

            if cosine_score > 0.05:  # é–¾å€¤ã‚’ä¸‹ã’ã¦çµæœã‚’å–å¾—
                results.append(
                    SearchResult(
                        doc_id=doc_id,
                        content=doc_data["content"][:500] + "...",
                        title=doc_data["title"],
                        hybrid_score=cosine_score,
                        cosine_score=cosine_score,
                        bm25_score=0.0,
                        confidence=cosine_score,
                        metadata=doc_data["metadata"],
                    )
                )

        results.sort(key=lambda x: x.cosine_score, reverse=True)
        return results[: self.search_config["max_results"]]

    def _calculate_relevance_score(
        self, results: List[SearchResult], expected_docs: List[str]
    ) -> float:
        """é–¢é€£æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not expected_docs:
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢

        found_relevant = 0
        for result in results[:5]:  # ä¸Šä½5ä»¶ã§è©•ä¾¡
            if result.doc_id in expected_docs:
                found_relevant += 1

        return found_relevant / min(len(expected_docs), 5)

    async def _record_improvement_history(
        self, method_name: str, baseline: float, improved: float, test_count: int
    ):
        """æ”¹å–„å±¥æ­´ã®è¨˜éŒ²"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            improvement_rate = (
                ((improved - baseline) / baseline) * 100 if baseline > 0 else 0
            )

            cursor.execute(
                """
                INSERT INTO improvement_history
                (improvement_id, method_name, baseline_score, improved_score,
                 improvement_rate, test_cases_count, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    f"improvement_{datetime.now().timestamp()}",
                    method_name,
                    baseline,
                    improved,
                    improvement_rate,
                    test_count,
                    datetime.now().isoformat(),
                ),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            self.logger.error(f"Failed to record improvement history: {e}")


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
async def demo_advanced_rag_precision():
    """Advanced RAG Precision Engine ãƒ‡ãƒ¢"""
    print("ğŸ¯ Advanced RAG Precision Engine Demo")
    print("=" * 60)

    engine = AdvancedRAGPrecisionEngine()

    # ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ã®æº–å‚™
    sample_documents = [
        {
            "id": "doc_elder_flow",
            "title": "Elder Flowé–‹ç™ºã‚¬ã‚¤ãƒ‰",
            "content": "Elder Flowã¯è‡ªå‹•åŒ–é–‹ç™ºãƒ•ãƒ­ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚4è³¢è€…ã¨ã®é€£æºã«ã‚ˆã‚ŠMind Reading Protocolã¨çµ±åˆã—ã€ã‚°ãƒ©ãƒ³ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼maruã®æ„å›³ã‚’ç†è§£ã—ã¦è‡ªå‹•å®Ÿè¡Œã—ã¾ã™ã€‚OAuth2.0èªè¨¼ã‚·ã‚¹ãƒ†ãƒ ã‚„WebSocketé€šä¿¡ã®å®Ÿè£…ã‚’æ”¯æ´ã—ã¾ã™ã€‚",
            "metadata": {"category": "development", "importance": "high"},
        },
        {
            "id": "doc_rag_system",
            "title": "RAGæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…",
            "content": "RAGã‚·ã‚¹ãƒ†ãƒ ã¯Retrieval-Augmented Generationã®ç•¥ã§ã€æ¤œç´¢æŠ€è¡“ã¨ç”ŸæˆAIã‚’çµ„ã¿åˆã‚ã›ãŸã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’çµ±åˆã—ã€æ–‡è„ˆç†è§£ã‚’æ·±åŒ–ã•ã›ã¾ã™ã€‚BM25ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ä½µç”¨ã—ã¾ã™ã€‚",
            "metadata": {"category": "rag", "importance": "high"},
        },
        {
            "id": "doc_performance",
            "title": "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¬ã‚¤ãƒ‰",
            "content": "ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã«ã¯è¤‡æ•°ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚Šã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ´»ç”¨ã€éåŒæœŸå‡¦ç†ã®å°å…¥ãªã©ãŒåŠ¹æœçš„ã§ã™ã€‚ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã—ã€æ®µéšçš„ã«æ”¹å–„ã—ã¾ã™ã€‚",
            "metadata": {"category": "optimization", "importance": "medium"},
        },
        {
            "id": "doc_security",
            "title": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»ã‚·ã‚¹ãƒ†ãƒ ",
            "content": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»ã‚·ã‚¹ãƒ†ãƒ ã¯è„†å¼±æ€§ã®æ¤œå‡ºã¨å¯¾ç­–ã‚’è‡ªå‹•åŒ–ã—ã¾ã™ã€‚OAuth2.0èªè¨¼ã€API ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–ã‚’å®Ÿè£…ã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã«ã‚ˆã‚Šä¸æ­£ã‚¢ã‚¯ã‚»ã‚¹ã‚’é˜²æ­¢ã—ã¾ã™ã€‚",
            "metadata": {"category": "security", "importance": "high"},
        },
        {
            "id": "doc_websocket",
            "title": "WebSocketå®Ÿè£…ã‚¬ã‚¤ãƒ‰",
            "content": "WebSocketã‚’ä½¿ç”¨ã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šä¿¡æ©Ÿèƒ½ã®å®Ÿè£…æ–¹æ³•ã€‚æ¥ç¶šç®¡ç†ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ³ã‚°ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å«ã‚€å®Œå…¨ãªã‚¬ã‚¤ãƒ‰ã§ã™ã€‚ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹é–“ã®é€šä¿¡æœ€é©åŒ–ã«ã‚‚å¯¾å¿œã—ã¾ã™ã€‚",
            "metadata": {"category": "websocket", "importance": "medium"},
        },
    ]

    # æ–‡æ›¸ã‚¹ãƒˆã‚¢åˆæœŸåŒ–
    await engine.initialize_document_store(sample_documents)

    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_queries = [
        "Elder Flowã§OAuth2.0èªè¨¼ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„",
        "RAGã‚·ã‚¹ãƒ†ãƒ ã®æ¤œç´¢ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹æ–¹æ³•",
        "WebSocketã‚’ä½¿ã£ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šä¿¡ã®æœ€é©åŒ–",
        "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã®ç‰¹å®šã¨è§£æ±ºç­–",
        "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»ã®è‡ªå‹•åŒ–æ‰‹æ³•",
    ]

    print("\nğŸ” Hybrid Search Results:")
    print("-" * 40)

    all_results = []

    for i, query in enumerate(test_queries, 1):
        print(f"\n[Query {i}] {query}")

        # O1-Embedderæ¨è«–æ‹¡å¼µ
        enhanced_query = await engine.o1_embedder_reasoning(query)

        # é©å¿œçš„æ¤œç´¢æˆ¦ç•¥
        strategy = await engine.adaptive_retrieval_strategy(query)

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Ÿè¡Œ
        results = await engine.hybrid_search(query, top_k=3)

        print(f"   ğŸ§  Enhanced Query: {len(enhanced_query)} chars")
        print(f"   ğŸ¯ Strategy: {strategy}")
        print(f"   ğŸ“Š Results: {len(results)} documents")

        for j, result in enumerate(results, 1):
            print(f"     [{j}] {result.title}")
            print(
                f"         Hybrid: {result.hybrid_score:.3f} | "
                f"Cosine: {result.cosine_score:.3f} | "
                f"BM25: {result.bm25_score:.3f} | "
                f"Confidence: {result.confidence:.3f}"
            )

        all_results.extend(results)

    # RAGASãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ãƒ‡ãƒ¢
    print(f"\nğŸ“Š RAGAS Metrics Demo:")
    print("-" * 40)

    sample_query = "RAGã‚·ã‚¹ãƒ†ãƒ ã®ç²¾åº¦å‘ä¸Šæ–¹æ³•ã‚’æ•™ãˆã¦"
    sample_answer = "RAGã‚·ã‚¹ãƒ†ãƒ ã®ç²¾åº¦å‘ä¸Šã«ã¯ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¡ç”¨ã€å‹•çš„ä¿¡é ¼åº¦è¨ˆç®—ã€RAGASæ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ´»ç”¨ãŒåŠ¹æœçš„ã§ã™ã€‚"
    sample_contexts = [doc["content"] for doc in sample_documents[:3]]

    ragas_metrics = await engine.calculate_ragas_metrics(
        query=sample_query,
        generated_answer=sample_answer,
        retrieved_contexts=sample_contexts,
    )

    print(f"   Faithfulness: {ragas_metrics.faithfulness:.3f}")
    print(f"   Answer Relevancy: {ragas_metrics.answer_relevancy:.3f}")
    print(f"   Context Precision: {ragas_metrics.context_precision:.3f}")
    print(f"   Context Recall: {ragas_metrics.context_recall:.3f}")
    print(f"   Groundedness: {ragas_metrics.groundedness:.3f}")
    print(f"   ğŸ¯ Overall Score: {ragas_metrics.overall_score:.3f}")

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ”¬ Benchmark Test:")
    print("-" * 40)

    test_cases = [
        {"query": "Elder Flowå®Ÿè£…", "expected_docs": ["doc_elder_flow"]},
        {"query": "RAGæ¤œç´¢ç²¾åº¦å‘ä¸Š", "expected_docs": ["doc_rag_system"]},
        {"query": "WebSocketæœ€é©åŒ–", "expected_docs": ["doc_websocket"]},
        {"query": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–", "expected_docs": ["doc_security"]},
        {"query": "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„", "expected_docs": ["doc_performance"]},
    ]

    benchmark_results = await engine.benchmark_improvements(test_cases)

    print(f"   Test Cases: {benchmark_results['test_cases_count']}")
    print(f"   Baseline Average: {benchmark_results['baseline_average']:.3f}")
    print(f"   Improved Average: {benchmark_results['improved_average']:.3f}")
    print(
        f"   ğŸ¯ Improvement Rate: {benchmark_results['improvement_rate_percent']:.1f}%"
    )

    # å€‹åˆ¥çµæœ
    print(f"\n   Individual Results:")
    for result in benchmark_results["individual_results"]:
        print(f"     '{result['query']}': {result['improvement']:.1f}% improvement")

    print(f"\nâœ¨ Advanced RAG Precision Engine Demo Complete!")
    print(f"ğŸ¯ Successfully demonstrated:")
    print(f"   â€¢ Hybrid Search Algorithm (Cosine + BM25)")
    print(f"   â€¢ Multi-factor Confidence Calculation")
    print(f"   â€¢ Enhanced RAGAS Metrics (5 indicators)")
    print(f"   â€¢ O1-Embedder Reasoning Enhancement")
    print(f"   â€¢ Adaptive Retrieval Strategy")
    print(f"   â€¢ Comprehensive Benchmarking")


if __name__ == "__main__":
    asyncio.run(demo_advanced_rag_precision())
