#!/usr/bin/env python3
"""
AI Automation Performance Monitor - AIËá™ÂãïÂåñ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÁõ£Ë¶ñ„Ç∑„Çπ„ÉÜ„É†
„Ç®„É´„ÉÄ„Éº„Ç∫„ÇÆ„É´„ÉâÂÆüÊà¶ÊäïÂÖ•„Çø„Çπ„ÇØ4 - „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÁõ£Ë¶ñ„Å®„É¨„Éù„Éº„ÉàÁîüÊàê

Ê©üËÉΩ:
- Four Sages AIËá™ÂãïÂåñ„Ç∑„Çπ„ÉÜ„É†„ÅÆ„É™„Ç¢„É´„Çø„Ç§„É†Áõ£Ë¶ñ
- „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ„ÉªÂàÜÊûê
- Ëá™ÂãïÂåñÂäπÊûúÊ∏¨ÂÆö„Å®„É¨„Éù„Éº„ÉàÁîüÊàê
- „Ç¢„É©„Éº„ÉàÊ§úÂá∫„Å®Ëá™ÂãïÂØæÂøú
- Áµ±Ë®à„É¨„Éù„Éº„Éà„Å®„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ
"""

import asyncio
import json
import logging
import sqlite3
import statistics
from collections import defaultdict
from collections import deque
from dataclasses import dataclass
from dataclasses import field
from datetime import datetime
from datetime import timedelta
from pathlib import Path
from typing import Any
from typing import Dict
from typing import List
from typing import Optional
from typing import Tuple

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetric:
    """„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„É°„Éà„É™„ÇØ„Çπ"""

    metric_name: str
    value: float
    timestamp: datetime
    source_system: str
    metric_type: str  # "counter", "gauge", "histogram", "timer"
    tags: Dict[str, str] = field(default_factory=dict)
    unit: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return {
            "metric_name": self.metric_name,
            "value": self.value,
            "timestamp": self.timestamp.isoformat(),
            "source_system": self.source_system,
            "metric_type": self.metric_type,
            "tags": self.tags,
            "unit": self.unit,
        }


@dataclass
class AlertRule:
    """„Ç¢„É©„Éº„Éà„É´„Éº„É´"""

    rule_id: str
    metric_name: str
    condition: str  # "gt", "lt", "eq", "ne"
    threshold: float
    duration_seconds: int
    severity: str  # "critical", "warning", "info"
    description: str
    enabled: bool = True

    def evaluate(self, metric_value: float, duration: int) -> bool:
        """„Ç¢„É©„Éº„ÉàÊù°‰ª∂Ë©ï‰æ°"""
        if not self.enabled:
            return False

        if duration < self.duration_seconds:
            return False

        if self.condition == "gt":
            return metric_value > self.threshold
        elif self.condition == "lt":
            return metric_value < self.threshold
        elif self.condition == "eq":
            return abs(metric_value - self.threshold) < 0.001
        elif self.condition == "ne":
            return abs(metric_value - self.threshold) >= 0.001

        return False


class AIAutomationPerformanceMonitor:
    """AIËá™ÂãïÂåñ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÁõ£Ë¶ñ„Ç∑„Çπ„ÉÜ„É†"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = {
            "collection_interval": 30,  # 30ÁßíÈñìÈöî
            "retention_days": 30,
            "alert_check_interval": 60,
            "dashboard_update_interval": 300,  # 5ÂàÜÈñìÈöî
            "max_metrics_memory": 10000,
        }

        if config:
            self.config.update(config)

        # „Éá„Éº„Çø„Çπ„Éà„É¨„Éº„Ç∏
        self.db_path = Path("data/ai_automation_performance.db")
        self.reports_path = Path("reports/performance")
        self.reports_path.mkdir(parents=True, exist_ok=True)

        # „É°„Éà„É™„ÇØ„ÇπÁÆ°ÁêÜ
        self.metrics_buffer: deque = deque(maxlen=self.config["max_metrics_memory"])
        self.metric_aggregates: Dict[str, Dict[str, Any]] = defaultdict(dict)
        self.active_alerts: Dict[str, Dict[str, Any]] = {}

        # „Ç¢„É©„Éº„Éà„É´„Éº„É´
        self.alert_rules: List[AlertRule] = []
        self._setup_default_alert_rules()

        # „Ç∑„Çπ„ÉÜ„É†Áä∂ÊÖãËøΩË∑°
        self.system_status = {
            "four_sages_integration": "unknown",
            "autonomous_learning": "unknown",
            "performance_optimization": "unknown",
            "last_health_check": None,
        }

        # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÁµ±Ë®à
        self.performance_stats = {
            "total_metrics_collected": 0,
            "alerts_triggered": 0,
            "reports_generated": 0,
            "system_uptime_start": datetime.now(),
            "last_dashboard_update": None,
        }

        # ‰∏¶Ë°åÂá¶ÁêÜÂà∂Âæ°
        self.monitoring_active = False
        self.monitoring_task = None

        self._init_database()
        logger.info("AI Automation Performance Monitor initialized")

    def _init_database(self):
        """„Éá„Éº„Çø„Éô„Éº„ÇπÂàùÊúüÂåñ"""
        self.db_path.parent.mkdir(exist_ok=True)
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()

        # „É°„Éà„É™„ÇØ„Çπ„ÉÜ„Éº„Éñ„É´
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS performance_metrics (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            metric_name TEXT NOT NULL,
            value REAL NOT NULL,
            timestamp TIMESTAMP NOT NULL,
            source_system TEXT NOT NULL,
            metric_type TEXT NOT NULL,
            tags TEXT,
            unit TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """
        )

        # „Ç¢„É©„Éº„ÉàÂ±•Ê≠¥„ÉÜ„Éº„Éñ„É´
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS alert_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            alert_id TEXT NOT NULL,
            rule_id TEXT NOT NULL,
            metric_name TEXT NOT NULL,
            trigger_value REAL NOT NULL,
            threshold REAL NOT NULL,
            severity TEXT NOT NULL,
            description TEXT,
            triggered_at TIMESTAMP NOT NULL,
            resolved_at TIMESTAMP,
            duration_seconds INTEGER
        )
        """
        )

        # „É¨„Éù„Éº„ÉàÂ±•Ê≠¥„ÉÜ„Éº„Éñ„É´
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS report_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            report_id TEXT NOT NULL,
            report_type TEXT NOT NULL,
            generated_at TIMESTAMP NOT NULL,
            file_path TEXT,
            metrics_count INTEGER,
            time_range_hours INTEGER,
            summary TEXT
        )
        """
        )

        # „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Çµ„Éû„É™„Éº„ÉÜ„Éº„Éñ„É´
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS performance_summary (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            summary_date DATE NOT NULL,
            metrics_collected INTEGER DEFAULT 0,
            alerts_triggered INTEGER DEFAULT 0,
            avg_response_time REAL DEFAULT 0.0,
            success_rate REAL DEFAULT 0.0,
            system_health_score REAL DEFAULT 0.0,
            automation_efficiency REAL DEFAULT 0.0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """
        )

        # „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ‰ΩúÊàê
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_metrics_timestamp ON performance_metrics(timestamp)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_metrics_source ON performance_metrics(source_system)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_alerts_triggered ON alert_history(triggered_at)")

        conn.commit()
        conn.close()

    def _setup_default_alert_rules(self):
        """„Éá„Éï„Ç©„É´„Éà„Ç¢„É©„Éº„Éà„É´„Éº„É´Ë®≠ÂÆö"""
        default_rules = [
            AlertRule(
                rule_id="four_sages_consensus_rate_low",
                metric_name="four_sages.consensus_rate",
                condition="lt",
                threshold=0.7,
                duration_seconds=300,
                severity="warning",
                description="Four Sages consensus rate below 70%",
            ),
            AlertRule(
                rule_id="autonomous_learning_accuracy_low",
                metric_name="autonomous_learning.prediction_accuracy",
                condition="lt",
                threshold=0.6,
                duration_seconds=600,
                severity="critical",
                description="Autonomous learning prediction accuracy below 60%",
            ),
            AlertRule(
                rule_id="system_response_time_high",
                metric_name="system.response_time",
                condition="gt",
                threshold=5.0,
                duration_seconds=180,
                severity="warning",
                description="System response time above 5 seconds",
            ),
            AlertRule(
                rule_id="pattern_discovery_rate_low",
                metric_name="learning.pattern_discovery_rate",
                condition="lt",
                threshold=0.1,
                duration_seconds=1800,  # 30ÂàÜ
                severity="info",
                description="Pattern discovery rate below 0.1 patterns/minute",
            ),
            AlertRule(
                rule_id="automation_success_rate_low",
                metric_name="automation.success_rate",
                condition="lt",
                threshold=0.8,
                duration_seconds=900,  # 15ÂàÜ
                severity="warning",
                description="Automation success rate below 80%",
            ),
        ]

        self.alert_rules.extend(default_rules)

    async def start_monitoring(self):
        """Áõ£Ë¶ñÈñãÂßã"""
        if self.monitoring_active:
            logger.warning("Monitoring is already active")
            return

        self.monitoring_active = True
        logger.info("üöÄ Starting AI Automation Performance Monitoring")

        # ‰∏¶Ë°åÁõ£Ë¶ñ„Çø„Çπ„ÇØ
        await asyncio.gather(
            self._metrics_collection_loop(),
            self._alert_processing_loop(),
            self._performance_analysis_loop(),
            self._dashboard_update_loop(),
            self._health_check_loop(),
        )

    async def stop_monitoring(self):
        """Áõ£Ë¶ñÂÅúÊ≠¢"""
        self.monitoring_active = False
        logger.info("üõë Stopping AI Automation Performance Monitoring")

        if self.monitoring_task:
            self.monitoring_task.cancel()
            try:
                await self.monitoring_task
            except asyncio.CancelledError:
                pass

    async def _metrics_collection_loop(self):
        """„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ„É´„Éº„Éó"""
        while self.monitoring_active:
            try:
                # Four Sages„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ
                four_sages_metrics = await self._collect_four_sages_metrics()
                for metric in four_sages_metrics:
                    await self.record_metric(metric)

                # Ëá™ÂæãÂ≠¶Áøí„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ
                learning_metrics = await self._collect_autonomous_learning_metrics()
                for metric in learning_metrics:
                    await self.record_metric(metric)

                # „Ç∑„Çπ„ÉÜ„É†„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ
                system_metrics = await self._collect_system_metrics()
                for metric in system_metrics:
                    await self.record_metric(metric)

                # Áµ±Ë®àÊõ¥Êñ∞
                self.performance_stats["total_metrics_collected"] += len(
                    four_sages_metrics + learning_metrics + system_metrics
                )

                await asyncio.sleep(self.config["collection_interval"])

            except Exception as e:
                logger.error(f"Metrics collection error: {e}")
                await asyncio.sleep(self.config["collection_interval"])

    async def _collect_four_sages_metrics(self) -> List[PerformanceMetric]:
        """Four Sages„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ"""
        metrics = []

        try:
            # Four SagesÁµ±Âêà„Ç∑„Çπ„ÉÜ„É†„Åã„Çâ„É°„Éà„É™„ÇØ„ÇπÂèñÂæóÔºà„É¢„ÉÉ„ÇØÂÆüË£ÖÔºâ
            metrics.extend(
                [
                    PerformanceMetric(
                        metric_name="four_sages.consensus_rate",
                        value=0.88,  # „É¢„ÉÉ„ÇØÂÄ§
                        timestamp=datetime.now(),
                        source_system="four_sages_integration",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                    PerformanceMetric(
                        metric_name="four_sages.response_time",
                        value=1.2,  # „É¢„ÉÉ„ÇØÂÄ§
                        timestamp=datetime.now(),
                        source_system="four_sages_integration",
                        metric_type="timer",
                        unit="seconds",
                    ),
                    PerformanceMetric(
                        metric_name="four_sages.active_sessions",
                        value=3,  # „É¢„ÉÉ„ÇØÂÄ§
                        timestamp=datetime.now(),
                        source_system="four_sages_integration",
                        metric_type="gauge",
                        unit="count",
                    ),
                ]
            )

        except Exception as e:
            logger.warning(f"Failed to collect Four Sages metrics: {e}")

        return metrics

    async def _collect_autonomous_learning_metrics(self) -> List[PerformanceMetric]:
        """Ëá™ÂæãÂ≠¶Áøí„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ"""
        metrics = []

        try:
            # Ëá™ÂæãÂ≠¶Áøí„Ç∑„Çπ„ÉÜ„É†„Åã„Çâ„É°„Éà„É™„ÇØ„ÇπÂèñÂæóÔºà„É¢„ÉÉ„ÇØÂÆüË£ÖÔºâ
            metrics.extend(
                [
                    PerformanceMetric(
                        metric_name="autonomous_learning.prediction_accuracy",
                        value=0.75,  # „É¢„ÉÉ„ÇØÂÄ§
                        timestamp=datetime.now(),
                        source_system="autonomous_learning",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                    PerformanceMetric(
                        metric_name="learning.pattern_discovery_rate",
                        value=0.15,  # „É¢„ÉÉ„ÇØÂÄ§
                        timestamp=datetime.now(),
                        source_system="autonomous_learning",
                        metric_type="gauge",
                        unit="patterns_per_minute",
                    ),
                    PerformanceMetric(
                        metric_name="learning.active_patterns",
                        value=12,  # „É¢„ÉÉ„ÇØÂÄ§
                        timestamp=datetime.now(),
                        source_system="autonomous_learning",
                        metric_type="gauge",
                        unit="count",
                    ),
                ]
            )

        except Exception as e:
            logger.warning(f"Failed to collect autonomous learning metrics: {e}")

        return metrics

    async def _collect_system_metrics(self) -> List[PerformanceMetric]:
        """„Ç∑„Çπ„ÉÜ„É†„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ"""
        metrics = []

        try:
            # „Ç∑„Çπ„ÉÜ„É†„É¨„Éô„É´„É°„Éà„É™„ÇØ„Çπ
            import psutil

            metrics.extend(
                [
                    PerformanceMetric(
                        metric_name="system.cpu_usage",
                        value=psutil.cpu_percent(interval=1),
                        timestamp=datetime.now(),
                        source_system="system",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                    PerformanceMetric(
                        metric_name="system.memory_usage",
                        value=psutil.virtual_memory().percent,
                        timestamp=datetime.now(),
                        source_system="system",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                    PerformanceMetric(
                        metric_name="automation.success_rate",
                        value=0.92,  # „É¢„ÉÉ„ÇØÂÄ§
                        timestamp=datetime.now(),
                        source_system="automation",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                ]
            )

        except Exception as e:
            logger.warning(f"Failed to collect system metrics: {e}")
            # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÁî®„É¢„ÉÉ„ÇØ„É°„Éà„É™„ÇØ„Çπ
            metrics.extend(
                [
                    PerformanceMetric(
                        metric_name="system.cpu_usage",
                        value=45.0,
                        timestamp=datetime.now(),
                        source_system="system",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                    PerformanceMetric(
                        metric_name="automation.success_rate",
                        value=0.92,
                        timestamp=datetime.now(),
                        source_system="automation",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                ]
            )

        return metrics

    async def record_metric(self, metric: PerformanceMetric):
        """„É°„Éà„É™„ÇØ„ÇπË®òÈå≤"""
        # „É°„É¢„É™„Éê„ÉÉ„Éï„Ç°„Å´ËøΩÂä†
        self.metrics_buffer.append(metric)

        # „Éá„Éº„Çø„Éô„Éº„Çπ„Å´‰øùÂ≠ò
        await self._save_metric_to_db(metric)

        # ÈõÜË®à„Éá„Éº„ÇøÊõ¥Êñ∞
        self._update_metric_aggregates(metric)

    async def _save_metric_to_db(self, metric: PerformanceMetric):
        """„É°„Éà„É™„ÇØ„Çπ„Çí„Éá„Éº„Çø„Éô„Éº„Çπ„Å´‰øùÂ≠ò"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            cursor.execute(
                """
                INSERT INTO performance_metrics
                (metric_name, value, timestamp, source_system, metric_type, tags, unit)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    metric.metric_name,
                    metric.value,
                    metric.timestamp,
                    metric.source_system,
                    metric.metric_type,
                    json.dumps(metric.tags),
                    metric.unit,
                ),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"Failed to save metric to database: {e}")

    def _update_metric_aggregates(self, metric: PerformanceMetric):
        """„É°„Éà„É™„ÇØ„ÇπÈõÜË®à„Éá„Éº„ÇøÊõ¥Êñ∞"""
        metric_key = f"{metric.source_system}.{metric.metric_name}"

        if metric_key not in self.metric_aggregates:
            self.metric_aggregates[metric_key] = {
                "values": deque(maxlen=100),
                "last_value": 0.0,
                "min_value": float("inf"),
                "max_value": float("-inf"),
                "avg_value": 0.0,
                "last_updated": datetime.now(),
            }

        aggregate = self.metric_aggregates[metric_key]
        aggregate["values"].append(metric.value)
        aggregate["last_value"] = metric.value
        aggregate["min_value"] = min(aggregate["min_value"], metric.value)
        aggregate["max_value"] = max(aggregate["max_value"], metric.value)
        aggregate["avg_value"] = statistics.mean(aggregate["values"])
        aggregate["last_updated"] = datetime.now()

    async def _alert_processing_loop(self):
        """„Ç¢„É©„Éº„ÉàÂá¶ÁêÜ„É´„Éº„Éó"""
        while self.monitoring_active:
            try:
                # „Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Ç¢„É©„Éº„Éà„ÉÅ„Çß„ÉÉ„ÇØ
                await self._check_alert_conditions()

                # „Ç¢„É©„Éº„ÉàËá™ÂãïËß£Ê±∫„ÉÅ„Çß„ÉÉ„ÇØ
                await self._check_alert_resolution()

                await asyncio.sleep(self.config["alert_check_interval"])

            except Exception as e:
                logger.error(f"Alert processing error: {e}")
                await asyncio.sleep(self.config["alert_check_interval"])

    async def _check_alert_conditions(self):
        """„Ç¢„É©„Éº„ÉàÊù°‰ª∂„ÉÅ„Çß„ÉÉ„ÇØ"""
        current_time = datetime.now()

        for rule in self.alert_rules:
            if not rule.enabled:
                continue

            # ÊúÄËøë„ÅÆ„É°„Éà„É™„ÇØ„ÇπÂÄ§„ÇíÂèñÂæó
            recent_values = self._get_recent_metric_values(rule.metric_name, rule.duration_seconds)

            if not recent_values:
                continue

            # Êù°‰ª∂Ë©ï‰æ°
            latest_value = recent_values[-1][1]  # (timestamp, value)
            duration = (current_time - recent_values[0][0]).total_seconds()

            if rule.evaluate(latest_value, duration):
                # „Ç¢„É©„Éº„Éà„Éà„É™„Ç¨„Éº
                alert_id = f"{rule.rule_id}_{int(current_time.timestamp())}"

                if alert_id not in self.active_alerts:
                    await self._trigger_alert(alert_id, rule, latest_value, current_time)

    def _get_recent_metric_values(self, metric_name: str, duration_seconds: int) -> List[Tuple[datetime, float]]:
        """ÊúÄËøë„ÅÆ„É°„Éà„É™„ÇØ„ÇπÂÄ§ÂèñÂæó"""
        cutoff_time = datetime.now() - timedelta(seconds=duration_seconds)

        # „É°„É¢„É™„Éê„ÉÉ„Éï„Ç°„Åã„ÇâÊ§úÁ¥¢
        recent_values = []
        for metric in self.metrics_buffer:
            if metric.metric_name == metric_name and metric.timestamp >= cutoff_time:
                recent_values.append((metric.timestamp, metric.value))

        return sorted(recent_values, key=lambda x: x[0])

    async def _trigger_alert(self, alert_id: str, rule: AlertRule, trigger_value: float, timestamp: datetime):
        """„Ç¢„É©„Éº„Éà„Éà„É™„Ç¨„Éº"""
        alert_data = {
            "alert_id": alert_id,
            "rule_id": rule.rule_id,
            "metric_name": rule.metric_name,
            "trigger_value": trigger_value,
            "threshold": rule.threshold,
            "severity": rule.severity,
            "description": rule.description,
            "triggered_at": timestamp,
            "resolved": False,
        }

        self.active_alerts[alert_id] = alert_data

        # „Éá„Éº„Çø„Éô„Éº„Çπ„Å´Ë®òÈå≤
        await self._save_alert_to_db(alert_data)

        # Áµ±Ë®àÊõ¥Êñ∞
        self.performance_stats["alerts_triggered"] += 1

        # „É≠„Ç∞Âá∫Âäõ
        logger.warning(f"üö® ALERT TRIGGERED: {rule.description} (Value: {trigger_value}, Threshold: {rule.threshold})")

        # Ëá™ÂãïÂØæÂøúÂÆüË°å
        await self._handle_alert_auto_response(alert_data)

    async def _save_alert_to_db(self, alert_data: Dict[str, Any]):
        """„Ç¢„É©„Éº„Éà„Çí„Éá„Éº„Çø„Éô„Éº„Çπ„Å´‰øùÂ≠ò"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            cursor.execute(
                """
                INSERT INTO alert_history
                (alert_id, rule_id, metric_name, trigger_value, threshold,
                 severity, description, triggered_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    alert_data["alert_id"],
                    alert_data["rule_id"],
                    alert_data["metric_name"],
                    alert_data["trigger_value"],
                    alert_data["threshold"],
                    alert_data["severity"],
                    alert_data["description"],
                    alert_data["triggered_at"],
                ),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"Failed to save alert to database: {e}")

    async def _handle_alert_auto_response(self, alert_data: Dict[str, Any]):
        """„Ç¢„É©„Éº„ÉàËá™ÂãïÂØæÂøú"""
        rule_id = alert_data["rule_id"]

        # „É´„Éº„É´Âà•Ëá™ÂãïÂØæÂøú
        if rule_id == "four_sages_consensus_rate_low":
            logger.info("üîß Auto-response: Initiating Four Sages optimization")
            # Four SagesÊúÄÈÅ©ÂåñÂá¶ÁêÜÔºàÂÆüË£ÖÁúÅÁï•Ôºâ

        elif rule_id == "autonomous_learning_accuracy_low":
            logger.info("üîß Auto-response: Triggering learning parameter adjustment")
            # Â≠¶Áøí„Éë„É©„É°„Éº„ÇøË™øÊï¥ÔºàÂÆüË£ÖÁúÅÁï•Ôºâ

        elif rule_id == "system_response_time_high":
            logger.info("üîß Auto-response: System performance optimization")
            # „Ç∑„Çπ„ÉÜ„É†ÊúÄÈÅ©ÂåñÂá¶ÁêÜÔºàÂÆüË£ÖÁúÅÁï•Ôºâ

        # Ëá™ÂãïÂØæÂøú„ÅÆË®òÈå≤
        alert_data["auto_response_applied"] = True
        alert_data["auto_response_timestamp"] = datetime.now()

    async def _check_alert_resolution(self):
        """„Ç¢„É©„Éº„ÉàËá™ÂãïËß£Ê±∫„ÉÅ„Çß„ÉÉ„ÇØ"""
        current_time = datetime.now()
        resolved_alerts = []

        for alert_id, alert_data in self.active_alerts.items():
            if alert_data["resolved"]:
                continue

            # ÂØæÂøú„Åô„Çã„É´„Éº„É´„ÇíÂèñÂæó
            rule = next((r for r in self.alert_rules if r.rule_id == alert_data["rule_id"]), None)
            if not rule:
                continue

            # ÊúÄÊñ∞„ÅÆ„É°„Éà„É™„ÇØ„ÇπÂÄ§„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            recent_values = self._get_recent_metric_values(rule.metric_name, 60)  # 1ÂàÜÈñì

            if recent_values:
                latest_value = recent_values[-1][1]

                # „Ç¢„É©„Éº„ÉàÊù°‰ª∂„ÅåËß£Ê∂à„Åï„Çå„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
                if not rule.evaluate(latest_value, 60):
                    # „Ç¢„É©„Éº„ÉàËß£Ê±∫
                    await self._resolve_alert(alert_id, current_time)
                    resolved_alerts.append(alert_id)

        # Ëß£Ê±∫„Åï„Çå„Åü„Ç¢„É©„Éº„Éà„ÇíÂâäÈô§
        for alert_id in resolved_alerts:
            del self.active_alerts[alert_id]

    async def _resolve_alert(self, alert_id: str, resolved_time: datetime):
        """„Ç¢„É©„Éº„ÉàËß£Ê±∫"""
        alert_data = self.active_alerts[alert_id]
        duration = (resolved_time - alert_data["triggered_at"]).total_seconds()

        # „Éá„Éº„Çø„Éô„Éº„ÇπÊõ¥Êñ∞
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            cursor.execute(
                """
                UPDATE alert_history
                SET resolved_at = ?, duration_seconds = ?
                WHERE alert_id = ?
            """,
                (resolved_time, int(duration), alert_id),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"Failed to update alert resolution: {e}")

        logger.info(f"‚úÖ ALERT RESOLVED: {alert_data['description']} (Duration: {int(duration)}s)")

    async def _performance_analysis_loop(self):
        """„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂàÜÊûê„É´„Éº„Éó"""
        while self.monitoring_active:
            try:
                # Êó•Ê¨°„Çµ„Éû„É™„ÉºÁîüÊàê
                await self._generate_daily_summary()

                # „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Éà„É¨„É≥„ÉâÂàÜÊûê
                await self._analyze_performance_trends()

                # Ëá™ÂãïÊúÄÈÅ©ÂåñÊèêÊ°à
                await self._generate_optimization_recommendations()

                await asyncio.sleep(3600)  # 1ÊôÇÈñìÈñìÈöî

            except Exception as e:
                logger.error(f"Performance analysis error: {e}")
                await asyncio.sleep(3600)

    async def _generate_daily_summary(self):
        """Êó•Ê¨°„Çµ„Éû„É™„ÉºÁîüÊàê"""
        today = datetime.now().date()

        # ‰ªäÊó•„ÅÆ„É°„Éà„É™„ÇØ„ÇπÈõÜË®à
        today_metrics = [m for m in self.metrics_buffer if m.timestamp.date() == today]

        if not today_metrics:
            return

        # „Çµ„Éû„É™„ÉºË®àÁÆó
        metrics_count = len(today_metrics)

        # ÂøúÁ≠îÊôÇÈñì„ÅÆÂπ≥ÂùáÔºàFour Sages„Åã„ÇâÔºâ
        response_times = [m.value for m in today_metrics if m.metric_name == "four_sages.response_time"]
        avg_response_time = statistics.mean(response_times) if response_times else 0.0

        # ÊàêÂäüÁéá„ÅÆÂπ≥Âùá
        success_rates = [m.value for m in today_metrics if "success_rate" in m.metric_name]
        avg_success_rate = statistics.mean(success_rates) if success_rates else 0.0

        # „Ç∑„Çπ„ÉÜ„É†„Éò„É´„Çπ„Çπ„Ç≥„Ç¢Ë®àÁÆó
        health_score = self._calculate_system_health_score()

        # Ëá™ÂãïÂåñÂäπÁéáË®àÁÆó
        automation_efficiency = self._calculate_automation_efficiency()

        # „Éá„Éº„Çø„Éô„Éº„Çπ„Å´‰øùÂ≠ò
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            cursor.execute(
                """
                INSERT OR REPLACE INTO performance_summary
                (summary_date, metrics_collected, alerts_triggered, avg_response_time,
                 success_rate, system_health_score, automation_efficiency)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    today,
                    metrics_count,
                    self.performance_stats["alerts_triggered"],
                    avg_response_time,
                    avg_success_rate,
                    health_score,
                    automation_efficiency,
                ),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"Failed to save daily summary: {e}")

    def _calculate_system_health_score(self) -> float:
        """„Ç∑„Çπ„ÉÜ„É†„Éò„É´„Çπ„Çπ„Ç≥„Ç¢Ë®àÁÆó"""
        # ÂêÑÁ®Æ„É°„Éà„É™„ÇØ„Çπ„Åã„Çâ„Éò„É´„Çπ„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó
        scores = []

        # Four Sages„Ç≥„É≥„Çª„É≥„Çµ„ÇπÁéá
        consensus_metrics = [m for m in self.metrics_buffer if m.metric_name == "four_sages.consensus_rate"]
        if consensus_metrics:
            latest_consensus = consensus_metrics[-1].value
            scores.append(min(1.0, latest_consensus / 0.8))  # 80%„ÇíÂü∫Ê∫ñ

        # Ëá™ÂæãÂ≠¶ÁøíÁ≤æÂ∫¶
        accuracy_metrics = [
            m for m in self.metrics_buffer if m.metric_name == "autonomous_learning.prediction_accuracy"
        ]
        if accuracy_metrics:
            latest_accuracy = accuracy_metrics[-1].value
            scores.append(min(1.0, latest_accuracy / 0.7))  # 70%„ÇíÂü∫Ê∫ñ

        # „Ç∑„Çπ„ÉÜ„É†„É™„ÇΩ„Éº„Çπ‰ΩøÁî®ÁéáÔºàÈÄÜÁõ∏Èñ¢Ôºâ
        cpu_metrics = [m for m in self.metrics_buffer if m.metric_name == "system.cpu_usage"]
        if cpu_metrics:
            latest_cpu = cpu_metrics[-1].value
            scores.append(max(0.0, 1.0 - latest_cpu / 100.0))

        # „Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Ç¢„É©„Éº„ÉàÊï∞ÔºàÈÄÜÁõ∏Èñ¢Ôºâ
        alert_penalty = min(0.5, len(self.active_alerts) * 0.1)
        scores.append(1.0 - alert_penalty)

        return statistics.mean(scores) if scores else 0.5

    def _calculate_automation_efficiency(self) -> float:
        """Ëá™ÂãïÂåñÂäπÁéáË®àÁÆó"""
        # ÊàêÂäüÁéá„Å®ÂøúÁ≠îÊôÇÈñì„Åã„ÇâÂäπÁéá„ÇíË®àÁÆó
        success_rates = [m.value for m in self.metrics_buffer if "success_rate" in m.metric_name]
        response_times = [m.value for m in self.metrics_buffer if "response_time" in m.metric_name]

        if not success_rates or not response_times:
            return 0.5

        avg_success_rate = statistics.mean(success_rates)
        avg_response_time = statistics.mean(response_times)

        # ÂäπÁéá = ÊàêÂäüÁéá / (1 + Ê≠£Ë¶èÂåñÂøúÁ≠îÊôÇÈñì)
        normalized_response_time = min(1.0, avg_response_time / 10.0)  # 10Áßí„ÇíÂü∫Ê∫ñ
        efficiency = avg_success_rate / (1 + normalized_response_time)

        return min(1.0, efficiency)

    async def _analyze_performance_trends(self):
        """„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Éà„É¨„É≥„ÉâÂàÜÊûê"""
        # ÈÅéÂéª7Êó•Èñì„ÅÆ„Éà„É¨„É≥„ÉâÂàÜÊûê
        week_ago = datetime.now() - timedelta(days=7)

        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            cursor.execute(
                """
                SELECT summary_date, system_health_score, automation_efficiency
                FROM performance_summary
                WHERE summary_date >= ?
                ORDER BY summary_date
            """,
                (week_ago.date(),),
            )

            results = cursor.fetchall()
            conn.close()

            if len(results) >= 3:
                # „Éà„É¨„É≥„ÉâË®àÁÆó
                health_scores = [r[1] for r in results]
                efficiency_scores = [r[2] for r in results]

                health_trend = self._calculate_trend_slope(health_scores)
                efficiency_trend = self._calculate_trend_slope(efficiency_scores)

                logger.info(f"üìà Performance Trends: Health={health_trend:.3f}, Efficiency={efficiency_trend:.3f}")

        except Exception as e:
            logger.error(f"Failed to analyze performance trends: {e}")

    def _calculate_trend_slope(self, values: List[float]) -> float:
        """„Éà„É¨„É≥„ÉâÂÇæ„ÅçË®àÁÆó"""
        if len(values) < 2:
            return 0.0

        n = len(values)
        x = list(range(n))

        x_mean = statistics.mean(x)
        y_mean = statistics.mean(values)

        numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
        denominator = sum((x[i] - x_mean) ** 2 for i in range(n))

        return numerator / denominator if denominator != 0 else 0.0

    async def _generate_optimization_recommendations(self):
        """ÊúÄÈÅ©ÂåñÊé®Â•®ÁîüÊàê"""
        recommendations = []

        # „Ç∑„Çπ„ÉÜ„É†„Éò„É´„Çπ„Çπ„Ç≥„Ç¢„Åå‰Ωé„ÅÑÂ†¥Âêà
        current_health = self._calculate_system_health_score()
        if current_health < 0.7:
            recommendations.append("System health score is low - consider system optimization")

        # „Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Ç¢„É©„Éº„Éà„ÅåÂ§ö„ÅÑÂ†¥Âêà
        if len(self.active_alerts) > 3:
            recommendations.append("Multiple active alerts - review system configuration")

        # ÂøúÁ≠îÊôÇÈñì„ÅåÈ´ò„ÅÑÂ†¥Âêà
        recent_response_times = [m.value for m in list(self.metrics_buffer)[-50:] if "response_time" in m.metric_name]
        if recent_response_times and statistics.mean(recent_response_times) > 3.0:
            recommendations.append("High response times detected - consider performance tuning")

        if recommendations:
            logger.info(f"üí° Optimization Recommendations: {'; '.join(recommendations)}")

    async def _dashboard_update_loop(self):
        """„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„ÉâÊõ¥Êñ∞„É´„Éº„Éó"""
        while self.monitoring_active:
            try:
                # „ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ„Éá„Éº„ÇøÁîüÊàê
                dashboard_data = await self._generate_dashboard_data()

                # „ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ„Éï„Ç°„Ç§„É´Êõ¥Êñ∞
                await self._update_dashboard_file(dashboard_data)

                self.performance_stats["last_dashboard_update"] = datetime.now()

                await asyncio.sleep(self.config["dashboard_update_interval"])

            except Exception as e:
                logger.error(f"Dashboard update error: {e}")
                await asyncio.sleep(self.config["dashboard_update_interval"])

    async def _generate_dashboard_data(self) -> Dict[str, Any]:
        """„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ„Éá„Éº„ÇøÁîüÊàê"""
        current_time = datetime.now()

        # ÊúÄÊñ∞„É°„Éà„É™„ÇØ„Çπ
        latest_metrics = {}
        for metric_key, aggregate in self.metric_aggregates.items():
            latest_metrics[metric_key] = {
                "current": aggregate["last_value"],
                "min": aggregate["min_value"],
                "max": aggregate["max_value"],
                "avg": aggregate["avg_value"],
                "last_updated": aggregate["last_updated"].isoformat(),
            }

        # „Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Ç¢„É©„Éº„Éà
        active_alerts_summary = []
        for alert_id, alert_data in self.active_alerts.items():
            active_alerts_summary.append(
                {
                    "alert_id": alert_id,
                    "severity": alert_data["severity"],
                    "description": alert_data["description"],
                    "triggered_at": alert_data["triggered_at"].isoformat(),
                    "duration": int((current_time - alert_data["triggered_at"]).total_seconds()),
                }
            )

        # „Ç∑„Çπ„ÉÜ„É†Áä∂ÊÖã
        system_health = self._calculate_system_health_score()
        automation_efficiency = self._calculate_automation_efficiency()

        return {
            "timestamp": current_time.isoformat(),
            "system_overview": {
                "health_score": system_health,
                "automation_efficiency": automation_efficiency,
                "uptime_seconds": int((current_time - self.performance_stats["system_uptime_start"]).total_seconds()),
                "total_metrics_collected": self.performance_stats["total_metrics_collected"],
                "alerts_triggered": self.performance_stats["alerts_triggered"],
            },
            "latest_metrics": latest_metrics,
            "active_alerts": active_alerts_summary,
            "system_status": self.system_status,
        }

    async def _update_dashboard_file(self, dashboard_data: Dict[str, Any]):
        """„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ„Éï„Ç°„Ç§„É´Êõ¥Êñ∞"""
        dashboard_file = self.reports_path / "dashboard.json"

        try:
            with open(dashboard_file, "w") as f:
                json.dump(dashboard_data, f, indent=2, default=str)

        except Exception as e:
            logger.error(f"Failed to update dashboard file: {e}")

    async def _health_check_loop(self):
        """„Éò„É´„Çπ„ÉÅ„Çß„ÉÉ„ÇØ„É´„Éº„Éó"""
        while self.monitoring_active:
            try:
                # Four SagesÁµ±ÂêàÁä∂ÊÖã„ÉÅ„Çß„ÉÉ„ÇØ
                four_sages_status = await self._check_four_sages_health()
                self.system_status["four_sages_integration"] = four_sages_status

                # Ëá™ÂæãÂ≠¶Áøí„Ç∑„Çπ„ÉÜ„É†Áä∂ÊÖã„ÉÅ„Çß„ÉÉ„ÇØ
                learning_status = await self._check_autonomous_learning_health()
                self.system_status["autonomous_learning"] = learning_status

                # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©ÂåñÁä∂ÊÖã„ÉÅ„Çß„ÉÉ„ÇØ
                optimization_status = await self._check_optimization_health()
                self.system_status["performance_optimization"] = optimization_status

                self.system_status["last_health_check"] = datetime.now().isoformat()

                await asyncio.sleep(300)  # 5ÂàÜÈñìÈöî

            except Exception as e:
                logger.error(f"Health check error: {e}")
                await asyncio.sleep(300)

    async def _check_four_sages_health(self) -> str:
        """Four SagesÂÅ•Â∫∑Áä∂ÊÖã„ÉÅ„Çß„ÉÉ„ÇØ"""
        try:
            # ÊúÄËøë„ÅÆ„É°„Éà„É™„ÇØ„Çπ„Åã„ÇâÂà§ÂÆö
            recent_metrics = [
                m
                for m in self.metrics_buffer
                if m.source_system == "four_sages_integration" and (datetime.now() - m.timestamp).total_seconds() < 300
            ]

            if not recent_metrics:
                return "unknown"

            consensus_rates = [m.value for m in recent_metrics if m.metric_name == "four_sages.consensus_rate"]
            if consensus_rates and statistics.mean(consensus_rates) > 0.8:
                return "healthy"
            elif consensus_rates and statistics.mean(consensus_rates) > 0.6:
                return "warning"
            else:
                return "critical"

        except Exception:
            return "error"

    async def _check_autonomous_learning_health(self) -> str:
        """Ëá™ÂæãÂ≠¶ÁøíÂÅ•Â∫∑Áä∂ÊÖã„ÉÅ„Çß„ÉÉ„ÇØ"""
        try:
            recent_metrics = [
                m
                for m in self.metrics_buffer
                if m.source_system == "autonomous_learning" and (datetime.now() - m.timestamp).total_seconds() < 300
            ]

            if not recent_metrics:
                return "unknown"

            accuracy_values = [m.value for m in recent_metrics if "accuracy" in m.metric_name]
            if accuracy_values and statistics.mean(accuracy_values) > 0.7:
                return "healthy"
            elif accuracy_values and statistics.mean(accuracy_values) > 0.5:
                return "warning"
            else:
                return "critical"

        except Exception:
            return "error"

    async def _check_optimization_health(self) -> str:
        """ÊúÄÈÅ©ÂåñÂÅ•Â∫∑Áä∂ÊÖã„ÉÅ„Çß„ÉÉ„ÇØ"""
        try:
            # Ëá™ÂãïÂåñÂäπÁéá„Åã„ÇâÂà§ÂÆö
            efficiency = self._calculate_automation_efficiency()

            if efficiency > 0.8:
                return "healthy"
            elif efficiency > 0.6:
                return "warning"
            else:
                return "critical"

        except Exception:
            return "error"

    async def generate_performance_report(self, hours: int = 24) -> Dict[str, Any]:
        """„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„É¨„Éù„Éº„ÉàÁîüÊàê"""
        start_time = datetime.now() - timedelta(hours=hours)
        report_id = f"performance_report_{int(datetime.now().timestamp())}"

        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            # „É°„Éà„É™„ÇØ„ÇπÁµ±Ë®à
            cursor.execute(
                """
                SELECT metric_name, COUNT(*), AVG(value), MIN(value), MAX(value)
                FROM performance_metrics
                WHERE timestamp >= ?
                GROUP BY metric_name
            """,
                (start_time,),
            )

            metrics_stats = {}
            for row in cursor.fetchall():
                metrics_stats[row[0]] = {"count": row[1], "average": row[2], "minimum": row[3], "maximum": row[4]}

            # „Ç¢„É©„Éº„ÉàÁµ±Ë®à
            cursor.execute(
                """
                SELECT severity, COUNT(*)
                FROM alert_history
                WHERE triggered_at >= ?
                GROUP BY severity
            """,
                (start_time,),
            )

            alerts_stats = {row[0]: row[1] for row in cursor.fetchall()}

            conn.close()

            # „É¨„Éù„Éº„Éà‰ΩúÊàê
            report = {
                "report_id": report_id,
                "generated_at": datetime.now().isoformat(),
                "time_range": {"start": start_time.isoformat(), "end": datetime.now().isoformat(), "hours": hours},
                "metrics_statistics": metrics_stats,
                "alerts_statistics": alerts_stats,
                "system_health": {
                    "current_score": self._calculate_system_health_score(),
                    "automation_efficiency": self._calculate_automation_efficiency(),
                    "active_alerts_count": len(self.active_alerts),
                },
                "performance_summary": {
                    "total_metrics_collected": len([m for m in self.metrics_buffer if m.timestamp >= start_time]),
                    "system_status": self.system_status.copy(),
                    "uptime_hours": (datetime.now() - self.performance_stats["system_uptime_start"]).total_seconds()
                    / 3600,
                },
            }

            # „É¨„Éù„Éº„Éà„Éï„Ç°„Ç§„É´‰øùÂ≠ò
            report_file = self.reports_path / f"{report_id}.json"
            with open(report_file, "w") as f:
                json.dump(report, f, indent=2, default=str)

            # „É¨„Éù„Éº„ÉàÂ±•Ê≠¥„Å´Ë®òÈå≤
            await self._save_report_history(report_id, "performance", report_file, len(metrics_stats), hours)

            self.performance_stats["reports_generated"] += 1

            logger.info(f"üìä Performance report generated: {report_id}")
            return report

        except Exception as e:
            logger.error(f"Failed to generate performance report: {e}")
            return {"error": str(e)}

    async def _save_report_history(
        self, report_id: str, report_type: str, file_path: Path, metrics_count: int, time_range_hours: int
    ):
        """„É¨„Éù„Éº„ÉàÂ±•Ê≠¥‰øùÂ≠ò"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            cursor.execute(
                """
                INSERT INTO report_history
                (report_id, report_type, generated_at, file_path, metrics_count, time_range_hours)
                VALUES (?, ?, ?, ?, ?, ?)
            """,
                (report_id, report_type, datetime.now(), str(file_path), metrics_count, time_range_hours),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"Failed to save report history: {e}")

    def get_current_status(self) -> Dict[str, Any]:
        """ÁèæÂú®„ÅÆÁä∂ÊÖãÂèñÂæó"""
        return {
            "monitoring_active": self.monitoring_active,
            "system_status": self.system_status.copy(),
            "performance_stats": self.performance_stats.copy(),
            "active_alerts_count": len(self.active_alerts),
            "metrics_in_buffer": len(self.metrics_buffer),
            "system_health_score": self._calculate_system_health_score(),
            "automation_efficiency": self._calculate_automation_efficiency(),
        }


# „Éá„É¢ÂÆüË°å
if __name__ == "__main__":

    async def demo():
        print("üöÄ AI Automation Performance Monitor Demo")
        print("=" * 50)

        # Áõ£Ë¶ñ„Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñ
        monitor = AIAutomationPerformanceMonitor({"collection_interval": 5, "alert_check_interval": 10})  # „Éá„É¢Áî®„Å´Áü≠Á∏Æ

        print("‚úÖ Performance monitor initialized")

        # „ÉÜ„Çπ„Éà„É°„Éà„É™„ÇØ„ÇπËøΩÂä†
        print("\n1. Adding test metrics...")
        test_metrics = [
            PerformanceMetric("four_sages.consensus_rate", 0.85, datetime.now(), "four_sages_integration", "gauge"),
            PerformanceMetric(
                "autonomous_learning.prediction_accuracy", 0.72, datetime.now(), "autonomous_learning", "gauge"
            ),
            PerformanceMetric("system.response_time", 2.1, datetime.now(), "system", "timer"),
            PerformanceMetric("automation.success_rate", 0.91, datetime.now(), "automation", "gauge"),
        ]

        for metric in test_metrics:
            await monitor.record_metric(metric)
            print(f"  üìä Recorded: {metric.metric_name} = {metric.value}")

        # Áä∂ÊÖãÁ¢∫Ë™ç
        print("\n2. Checking system status...")
        status = monitor.get_current_status()
        print(f"  üè• System Health Score: {status['system_health_score']:.3f}")
        print(f"  ‚ö° Automation Efficiency: {status['automation_efficiency']:.3f}")
        print(f"  üìã Metrics in Buffer: {status['metrics_in_buffer']}")

        # „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„É¨„Éù„Éº„ÉàÁîüÊàê
        print("\n3. Generating performance report...")
        report = await monitor.generate_performance_report(hours=1)
        print(f"  üìä Report ID: {report['report_id']}")
        print(f"  üìà Metrics Statistics: {len(report['metrics_statistics'])} metric types")
        print(f"  üéØ System Health: {report['system_health']['current_score']:.3f}")

        # „ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ„Éá„Éº„ÇøÁîüÊàê
        print("\n4. Generating dashboard data...")
        dashboard = await monitor._generate_dashboard_data()
        print(f"  üìä Dashboard updated at: {dashboard['timestamp']}")
        print(f"  üìà Total metrics: {dashboard['system_overview']['total_metrics_collected']}")
        print(f"  üö® Active alerts: {len(dashboard['active_alerts'])}")

        print("\n‚ú® AI Automation Performance Monitor Features:")
        print("  ‚úÖ Real-time metrics collection and storage")
        print("  ‚úÖ Automatic alert detection and response")
        print("  ‚úÖ Performance trend analysis")
        print("  ‚úÖ System health monitoring")
        print("  ‚úÖ Comprehensive reporting")
        print("  ‚úÖ Dashboard data generation")
        print("  ‚úÖ Automation efficiency tracking")

        print("\nüéØ AI Automation Performance Monitor Demo - COMPLETED")

    asyncio.run(demo())
