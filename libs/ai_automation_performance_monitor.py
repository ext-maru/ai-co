#!/usr/bin/env python3
"""
AI Automation Performance Monitor - AIè‡ªå‹•åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰å®Ÿæˆ¦æŠ•å…¥ã‚¿ã‚¹ã‚¯4 - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

æ©Ÿèƒ½:
- Four Sages AIè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ»åˆ†æ
- è‡ªå‹•åŒ–åŠ¹æœæ¸¬å®šã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
- ã‚¢ãƒ©ãƒ¼ãƒˆæ¤œå‡ºã¨è‡ªå‹•å¯¾å¿œ
- çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
"""

import asyncio
import json
import logging
import sqlite3
import statistics
from collections import defaultdict
from collections import deque
from dataclasses import dataclass
from dataclasses import field
from datetime import datetime
from datetime import timedelta
from pathlib import Path
from typing import Any
from typing import Dict
from typing import List
from typing import Optional
from typing import Tuple

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetric:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    metric_name: str
    value: float
    timestamp: datetime
    source_system: str
    metric_type: str  # "counter", "gauge", "histogram", "timer"
    tags: Dict[str, str] = field(default_factory=dict)
    unit: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return {
            "metric_name": self.metric_name,
            "value": self.value,
            "timestamp": self.timestamp.isoformat(),
            "source_system": self.source_system,
            "metric_type": self.metric_type,
            "tags": self.tags,
            "unit": self.unit,
        }


@dataclass
class AlertRule:
    """ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«"""

    rule_id: str
    metric_name: str
    condition: str  # "gt", "lt", "eq", "ne"
    threshold: float
    duration_seconds: int
    severity: str  # "critical", "warning", "info"
    description: str
    enabled: bool = True

    def evaluate(self, metric_value: float, duration: int) -> bool:
        """ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶è©•ä¾¡"""
        if not self.enabled:
            return False

        if duration < self.duration_seconds:
            return False

        if self.condition == "gt":
            return metric_value > self.threshold
        elif self.condition == "lt":
            return metric_value < self.threshold
        elif self.condition == "eq":
            return abs(metric_value - self.threshold) < 0.001
        elif self.condition == "ne":
            return abs(metric_value - self.threshold) >= 0.001

        return False


class AIAutomationPerformanceMonitor:
    """AIè‡ªå‹•åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = {
            "collection_interval": 30,  # 30ç§’é–“éš”
            "retention_days": 30,
            "alert_check_interval": 60,
            "dashboard_update_interval": 300,  # 5åˆ†é–“éš”
            "max_metrics_memory": 10000,
        }

        if config:
            self.config.update(config)

        # ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.db_path = Path("data/ai_automation_performance.db")
        self.reports_path = Path("reports/performance")
        self.reports_path.mkdir(parents=True, exist_ok=True)

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç®¡ç†
        self.metrics_buffer: deque = deque(maxlen=self.config["max_metrics_memory"])
        self.metric_aggregates: Dict[str, Dict[str, Any]] = defaultdict(dict)
        self.active_alerts: Dict[str, Dict[str, Any]] = {}

        # ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«
        self.alert_rules: List[AlertRule] = []
        self._setup_default_alert_rules()

        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¿½è·¡
        self.system_status = {
            "four_sages_integration": "unknown",
            "autonomous_learning": "unknown",
            "performance_optimization": "unknown",
            "last_health_check": None,
        }

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        self.performance_stats = {
            "total_metrics_collected": 0,
            "alerts_triggered": 0,
            "reports_generated": 0,
            "system_uptime_start": datetime.now(),
            "last_dashboard_update": None,
        }

        # ä¸¦è¡Œå‡¦ç†åˆ¶å¾¡
        self.monitoring_active = False
        self.monitoring_task = None

        self._init_database()
        logger.info("AI Automation Performance Monitor initialized")

    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        self.db_path.parent.mkdir(exist_ok=True)
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS performance_metrics (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            metric_name TEXT NOT NULL,
            value REAL NOT NULL,
            timestamp TIMESTAMP NOT NULL,
            source_system TEXT NOT NULL,
            metric_type TEXT NOT NULL,
            tags TEXT,
            unit TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """
        )

        # ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS alert_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            alert_id TEXT NOT NULL,
            rule_id TEXT NOT NULL,
            metric_name TEXT NOT NULL,
            trigger_value REAL NOT NULL,
            threshold REAL NOT NULL,
            severity TEXT NOT NULL,
            description TEXT,
            triggered_at TIMESTAMP NOT NULL,
            resolved_at TIMESTAMP,
            duration_seconds INTEGER
        )
        """
        )

        # ãƒ¬ãƒãƒ¼ãƒˆå±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS report_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            report_id TEXT NOT NULL,
            report_type TEXT NOT NULL,
            generated_at TIMESTAMP NOT NULL,
            file_path TEXT,
            metrics_count INTEGER,
            time_range_hours INTEGER,
            summary TEXT
        )
        """
        )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS performance_summary (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            summary_date DATE NOT NULL,
            metrics_collected INTEGER DEFAULT 0,
            alerts_triggered INTEGER DEFAULT 0,
            avg_response_time REAL DEFAULT 0.0,
            success_rate REAL DEFAULT 0.0,
            system_health_score REAL DEFAULT 0.0,
            automation_efficiency REAL DEFAULT 0.0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """
        )

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_metrics_timestamp ON performance_metrics(timestamp)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_metrics_source ON performance_metrics(source_system)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_alerts_triggered ON alert_history(triggered_at)")

        conn.commit()
        conn.close()

    def _setup_default_alert_rules(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«è¨­å®š"""
        default_rules = [
            AlertRule(
                rule_id="four_sages_consensus_rate_low",
                metric_name="four_sages.consensus_rate",
                condition="lt",
                threshold=0.7,
                duration_seconds=300,
                severity="warning",
                description="Four Sages consensus rate below 70%",
            ),
            AlertRule(
                rule_id="autonomous_learning_accuracy_low",
                metric_name="autonomous_learning.prediction_accuracy",
                condition="lt",
                threshold=0.6,
                duration_seconds=600,
                severity="critical",
                description="Autonomous learning prediction accuracy below 60%",
            ),
            AlertRule(
                rule_id="system_response_time_high",
                metric_name="system.response_time",
                condition="gt",
                threshold=5.0,
                duration_seconds=180,
                severity="warning",
                description="System response time above 5 seconds",
            ),
            AlertRule(
                rule_id="pattern_discovery_rate_low",
                metric_name="learning.pattern_discovery_rate",
                condition="lt",
                threshold=0.1,
                duration_seconds=1800,  # 30åˆ†
                severity="info",
                description="Pattern discovery rate below 0.1 patterns/minute",
            ),
            AlertRule(
                rule_id="automation_success_rate_low",
                metric_name="automation.success_rate",
                condition="lt",
                threshold=0.8,
                duration_seconds=900,  # 15åˆ†
                severity="warning",
                description="Automation success rate below 80%",
            ),
        ]

        self.alert_rules.extend(default_rules)

    async def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        if self.monitoring_active:
            logger.warning("Monitoring is already active")
            return

        self.monitoring_active = True
        logger.info("ğŸš€ Starting AI Automation Performance Monitoring")

        # ä¸¦è¡Œç›£è¦–ã‚¿ã‚¹ã‚¯
        await asyncio.gather(
            self._metrics_collection_loop(),
            self._alert_processing_loop(),
            self._performance_analysis_loop(),
            self._dashboard_update_loop(),
            self._health_check_loop(),
        )

    async def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.monitoring_active = False
        logger.info("ğŸ›‘ Stopping AI Automation Performance Monitoring")

        if self.monitoring_task:
            self.monitoring_task.cancel()
            try:
                await self.monitoring_task
            except asyncio.CancelledError:
                pass

    async def _metrics_collection_loop(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                # Four Sagesãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
                four_sages_metrics = await self._collect_four_sages_metrics()
                for metric in four_sages_metrics:
                    await self.record_metric(metric)

                # è‡ªå¾‹å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
                learning_metrics = await self._collect_autonomous_learning_metrics()
                for metric in learning_metrics:
                    await self.record_metric(metric)

                # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
                system_metrics = await self._collect_system_metrics()
                for metric in system_metrics:
                    await self.record_metric(metric)

                # çµ±è¨ˆæ›´æ–°
                self.performance_stats["total_metrics_collected"] += len(
                    four_sages_metrics + learning_metrics + system_metrics
                )

                await asyncio.sleep(self.config["collection_interval"])

            except Exception as e:
                logger.error(f"Metrics collection error: {e}")
                await asyncio.sleep(self.config["collection_interval"])

    async def _collect_four_sages_metrics(self) -> List[PerformanceMetric]:
        """Four Sagesãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        metrics = []

        try:
            # Four Sagesçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ï¼ˆãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰
            metrics.extend(
                [
                    PerformanceMetric(
                        metric_name="four_sages.consensus_rate",
                        value=0.88,  # ãƒ¢ãƒƒã‚¯å€¤
                        timestamp=datetime.now(),
                        source_system="four_sages_integration",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                    PerformanceMetric(
                        metric_name="four_sages.response_time",
                        value=1.2,  # ãƒ¢ãƒƒã‚¯å€¤
                        timestamp=datetime.now(),
                        source_system="four_sages_integration",
                        metric_type="timer",
                        unit="seconds",
                    ),
                    PerformanceMetric(
                        metric_name="four_sages.active_sessions",
                        value=3,  # ãƒ¢ãƒƒã‚¯å€¤
                        timestamp=datetime.now(),
                        source_system="four_sages_integration",
                        metric_type="gauge",
                        unit="count",
                    ),
                ]
            )

        except Exception as e:
            logger.warning(f"Failed to collect Four Sages metrics: {e}")

        return metrics

    async def _collect_autonomous_learning_metrics(self) -> List[PerformanceMetric]:
        """è‡ªå¾‹å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        metrics = []

        try:
            # è‡ªå¾‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ï¼ˆãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰
            metrics.extend(
                [
                    PerformanceMetric(
                        metric_name="autonomous_learning.prediction_accuracy",
                        value=0.75,  # ãƒ¢ãƒƒã‚¯å€¤
                        timestamp=datetime.now(),
                        source_system="autonomous_learning",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                    PerformanceMetric(
                        metric_name="learning.pattern_discovery_rate",
                        value=0.15,  # ãƒ¢ãƒƒã‚¯å€¤
                        timestamp=datetime.now(),
                        source_system="autonomous_learning",
                        metric_type="gauge",
                        unit="patterns_per_minute",
                    ),
                    PerformanceMetric(
                        metric_name="learning.active_patterns",
                        value=12,  # ãƒ¢ãƒƒã‚¯å€¤
                        timestamp=datetime.now(),
                        source_system="autonomous_learning",
                        metric_type="gauge",
                        unit="count",
                    ),
                ]
            )

        except Exception as e:
            logger.warning(f"Failed to collect autonomous learning metrics: {e}")

        return metrics

    async def _collect_system_metrics(self) -> List[PerformanceMetric]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        metrics = []

        try:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            import psutil

            metrics.extend(
                [
                    PerformanceMetric(
                        metric_name="system.cpu_usage",
                        value=psutil.cpu_percent(interval=1),
                        timestamp=datetime.now(),
                        source_system="system",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                    PerformanceMetric(
                        metric_name="system.memory_usage",
                        value=psutil.virtual_memory().percent,
                        timestamp=datetime.now(),
                        source_system="system",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                    PerformanceMetric(
                        metric_name="automation.success_rate",
                        value=0.92,  # ãƒ¢ãƒƒã‚¯å€¤
                        timestamp=datetime.now(),
                        source_system="automation",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                ]
            )

        except Exception as e:
            logger.warning(f"Failed to collect system metrics: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ¢ãƒƒã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            metrics.extend(
                [
                    PerformanceMetric(
                        metric_name="system.cpu_usage",
                        value=45.0,
                        timestamp=datetime.now(),
                        source_system="system",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                    PerformanceMetric(
                        metric_name="automation.success_rate",
                        value=0.92,
                        timestamp=datetime.now(),
                        source_system="automation",
                        metric_type="gauge",
                        unit="percentage",
                    ),
                ]
            )

        return metrics

    async def record_metric(self, metric: PerformanceMetric):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²"""
        # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
        self.metrics_buffer.append(metric)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        await self._save_metric_to_db(metric)

        # é›†è¨ˆãƒ‡ãƒ¼ã‚¿æ›´æ–°
        self._update_metric_aggregates(metric)

    async def _save_metric_to_db(self, metric: PerformanceMetric):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            cursor.execute(
                """
                INSERT INTO performance_metrics
                (metric_name, value, timestamp, source_system, metric_type, tags, unit)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    metric.metric_name,
                    metric.value,
                    metric.timestamp,
                    metric.source_system,
                    metric.metric_type,
                    json.dumps(metric.tags),
                    metric.unit,
                ),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"Failed to save metric to database: {e}")

    def _update_metric_aggregates(self, metric: PerformanceMetric):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹é›†è¨ˆãƒ‡ãƒ¼ã‚¿æ›´æ–°"""
        metric_key = f"{metric.source_system}.{metric.metric_name}"

        if metric_key not in self.metric_aggregates:
            self.metric_aggregates[metric_key] = {
                "values": deque(maxlen=100),
                "last_value": 0.0,
                "min_value": float("inf"),
                "max_value": float("-inf"),
                "avg_value": 0.0,
                "last_updated": datetime.now(),
            }

        aggregate = self.metric_aggregates[metric_key]
        aggregate["values"].append(metric.value)
        aggregate["last_value"] = metric.value
        aggregate["min_value"] = min(aggregate["min_value"], metric.value)
        aggregate["max_value"] = max(aggregate["max_value"], metric.value)
        aggregate["avg_value"] = statistics.mean(aggregate["values"])
        aggregate["last_updated"] = datetime.now()

    async def _alert_processing_loop(self):
        """ã‚¢ãƒ©ãƒ¼ãƒˆå‡¦ç†ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
                await self._check_alert_conditions()

                # ã‚¢ãƒ©ãƒ¼ãƒˆè‡ªå‹•è§£æ±ºãƒã‚§ãƒƒã‚¯
                await self._check_alert_resolution()

                await asyncio.sleep(self.config["alert_check_interval"])

            except Exception as e:
                logger.error(f"Alert processing error: {e}")
                await asyncio.sleep(self.config["alert_check_interval"])

    async def _check_alert_conditions(self):
        """ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ãƒã‚§ãƒƒã‚¯"""
        current_time = datetime.now()

        for rule in self.alert_rules:
            if not rule.enabled:
                continue

            # æœ€è¿‘ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ã‚’å–å¾—
            recent_values = self._get_recent_metric_values(rule.metric_name, rule.duration_seconds)

            if not recent_values:
                continue

            # æ¡ä»¶è©•ä¾¡
            latest_value = recent_values[-1][1]  # (timestamp, value)
            duration = (current_time - recent_values[0][0]).total_seconds()

            if rule.evaluate(latest_value, duration):
                # ã‚¢ãƒ©ãƒ¼ãƒˆãƒˆãƒªã‚¬ãƒ¼
                alert_id = f"{rule.rule_id}_{int(current_time.timestamp())}"

                if alert_id not in self.active_alerts:
                    await self._trigger_alert(alert_id, rule, latest_value, current_time)

    def _get_recent_metric_values(self, metric_name: str, duration_seconds: int) -> List[Tuple[datetime, float]]:
        """æœ€è¿‘ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤å–å¾—"""
        cutoff_time = datetime.now() - timedelta(seconds=duration_seconds)

        # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡ã‹ã‚‰æ¤œç´¢
        recent_values = []
        for metric in self.metrics_buffer:
            if metric.metric_name == metric_name and metric.timestamp >= cutoff_time:
                recent_values.append((metric.timestamp, metric.value))

        return sorted(recent_values, key=lambda x: x[0])

    async def _trigger_alert(self, alert_id: str, rule: AlertRule, trigger_value: float, timestamp: datetime):
        """ã‚¢ãƒ©ãƒ¼ãƒˆãƒˆãƒªã‚¬ãƒ¼"""
        alert_data = {
            "alert_id": alert_id,
            "rule_id": rule.rule_id,
            "metric_name": rule.metric_name,
            "trigger_value": trigger_value,
            "threshold": rule.threshold,
            "severity": rule.severity,
            "description": rule.description,
            "triggered_at": timestamp,
            "resolved": False,
        }

        self.active_alerts[alert_id] = alert_data

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
        await self._save_alert_to_db(alert_data)

        # çµ±è¨ˆæ›´æ–°
        self.performance_stats["alerts_triggered"] += 1

        # ãƒ­ã‚°å‡ºåŠ›
        logger.warning(f"ğŸš¨ ALERT TRIGGERED: {rule.description} (Value: {trigger_value}, Threshold: {rule.threshold})")

        # è‡ªå‹•å¯¾å¿œå®Ÿè¡Œ
        await self._handle_alert_auto_response(alert_data)

    async def _save_alert_to_db(self, alert_data: Dict[str, Any]):
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            cursor.execute(
                """
                INSERT INTO alert_history
                (alert_id, rule_id, metric_name, trigger_value, threshold,
                 severity, description, triggered_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    alert_data["alert_id"],
                    alert_data["rule_id"],
                    alert_data["metric_name"],
                    alert_data["trigger_value"],
                    alert_data["threshold"],
                    alert_data["severity"],
                    alert_data["description"],
                    alert_data["triggered_at"],
                ),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"Failed to save alert to database: {e}")

    async def _handle_alert_auto_response(self, alert_data: Dict[str, Any]):
        """ã‚¢ãƒ©ãƒ¼ãƒˆè‡ªå‹•å¯¾å¿œ"""
        rule_id = alert_data["rule_id"]

        # ãƒ«ãƒ¼ãƒ«åˆ¥è‡ªå‹•å¯¾å¿œ
        if rule_id == "four_sages_consensus_rate_low":
            logger.info("ğŸ”§ Auto-response: Initiating Four Sages optimization")
            # Four Sagesæœ€é©åŒ–å‡¦ç†ï¼ˆå®Ÿè£…çœç•¥ï¼‰

        elif rule_id == "autonomous_learning_accuracy_low":
            logger.info("ğŸ”§ Auto-response: Triggering learning parameter adjustment")
            # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ï¼ˆå®Ÿè£…çœç•¥ï¼‰

        elif rule_id == "system_response_time_high":
            logger.info("ğŸ”§ Auto-response: System performance optimization")
            # ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–å‡¦ç†ï¼ˆå®Ÿè£…çœç•¥ï¼‰

        # è‡ªå‹•å¯¾å¿œã®è¨˜éŒ²
        alert_data["auto_response_applied"] = True
        alert_data["auto_response_timestamp"] = datetime.now()

    async def _check_alert_resolution(self):
        """ã‚¢ãƒ©ãƒ¼ãƒˆè‡ªå‹•è§£æ±ºãƒã‚§ãƒƒã‚¯"""
        current_time = datetime.now()
        resolved_alerts = []

        for alert_id, alert_data in self.active_alerts.items():
            if alert_data["resolved"]:
                continue

            # å¯¾å¿œã™ã‚‹ãƒ«ãƒ¼ãƒ«ã‚’å–å¾—
            rule = next((r for r in self.alert_rules if r.rule_id == alert_data["rule_id"]), None)
            if not rule:
                continue

            # æœ€æ–°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ã‚’ãƒã‚§ãƒƒã‚¯
            recent_values = self._get_recent_metric_values(rule.metric_name, 60)  # 1åˆ†é–“

            if recent_values:
                latest_value = recent_values[-1][1]

                # ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ãŒè§£æ¶ˆã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if not rule.evaluate(latest_value, 60):
                    # ã‚¢ãƒ©ãƒ¼ãƒˆè§£æ±º
                    await self._resolve_alert(alert_id, current_time)
                    resolved_alerts.append(alert_id)

        # è§£æ±ºã•ã‚ŒãŸã‚¢ãƒ©ãƒ¼ãƒˆã‚’å‰Šé™¤
        for alert_id in resolved_alerts:
            del self.active_alerts[alert_id]

    async def _resolve_alert(self, alert_id: str, resolved_time: datetime):
        """ã‚¢ãƒ©ãƒ¼ãƒˆè§£æ±º"""
        alert_data = self.active_alerts[alert_id]
        duration = (resolved_time - alert_data["triggered_at"]).total_seconds()

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            cursor.execute(
                """
                UPDATE alert_history
                SET resolved_at = ?, duration_seconds = ?
                WHERE alert_id = ?
            """,
                (resolved_time, int(duration), alert_id),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"Failed to update alert resolution: {e}")

        logger.info(f"âœ… ALERT RESOLVED: {alert_data['description']} (Duration: {int(duration)}s)")

    async def _performance_analysis_loop(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                # æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
                await self._generate_daily_summary()

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
                await self._analyze_performance_trends()

                # è‡ªå‹•æœ€é©åŒ–ææ¡ˆ
                await self._generate_optimization_recommendations()

                await asyncio.sleep(3600)  # 1æ™‚é–“é–“éš”

            except Exception as e:
                logger.error(f"Performance analysis error: {e}")
                await asyncio.sleep(3600)

    async def _generate_daily_summary(self):
        """æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        today = datetime.now().date()

        # ä»Šæ—¥ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹é›†è¨ˆ
        today_metrics = [m for m in self.metrics_buffer if m.timestamp.date() == today]

        if not today_metrics:
            return

        # ã‚µãƒãƒªãƒ¼è¨ˆç®—
        metrics_count = len(today_metrics)

        # å¿œç­”æ™‚é–“ã®å¹³å‡ï¼ˆFour Sagesã‹ã‚‰ï¼‰
        response_times = [m.value for m in today_metrics if m.metric_name == "four_sages.response_time"]
        avg_response_time = statistics.mean(response_times) if response_times else 0.0

        # æˆåŠŸç‡ã®å¹³å‡
        success_rates = [m.value for m in today_metrics if "success_rate" in m.metric_name]
        avg_success_rate = statistics.mean(success_rates) if success_rates else 0.0

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—
        health_score = self._calculate_system_health_score()

        # è‡ªå‹•åŒ–åŠ¹ç‡è¨ˆç®—
        automation_efficiency = self._calculate_automation_efficiency()

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            cursor.execute(
                """
                INSERT OR REPLACE INTO performance_summary
                (summary_date, metrics_collected, alerts_triggered, avg_response_time,
                 success_rate, system_health_score, automation_efficiency)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    today,
                    metrics_count,
                    self.performance_stats["alerts_triggered"],
                    avg_response_time,
                    avg_success_rate,
                    health_score,
                    automation_efficiency,
                ),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"Failed to save daily summary: {e}")

    def _calculate_system_health_score(self) -> float:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # å„ç¨®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒ˜ãƒ«ã‚¹ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        scores = []

        # Four Sagesã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ç‡
        consensus_metrics = [m for m in self.metrics_buffer if m.metric_name == "four_sages.consensus_rate"]
        if consensus_metrics:
            latest_consensus = consensus_metrics[-1].value
            scores.append(min(1.0, latest_consensus / 0.8))  # 80%ã‚’åŸºæº–

        # è‡ªå¾‹å­¦ç¿’ç²¾åº¦
        accuracy_metrics = [
            m for m in self.metrics_buffer if m.metric_name == "autonomous_learning.prediction_accuracy"
        ]
        if accuracy_metrics:
            latest_accuracy = accuracy_metrics[-1].value
            scores.append(min(1.0, latest_accuracy / 0.7))  # 70%ã‚’åŸºæº–

        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ï¼ˆé€†ç›¸é–¢ï¼‰
        cpu_metrics = [m for m in self.metrics_buffer if m.metric_name == "system.cpu_usage"]
        if cpu_metrics:
            latest_cpu = cpu_metrics[-1].value
            scores.append(max(0.0, 1.0 - latest_cpu / 100.0))

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆæ•°ï¼ˆé€†ç›¸é–¢ï¼‰
        alert_penalty = min(0.5, len(self.active_alerts) * 0.1)
        scores.append(1.0 - alert_penalty)

        return statistics.mean(scores) if scores else 0.5

    def _calculate_automation_efficiency(self) -> float:
        """è‡ªå‹•åŒ–åŠ¹ç‡è¨ˆç®—"""
        # æˆåŠŸç‡ã¨å¿œç­”æ™‚é–“ã‹ã‚‰åŠ¹ç‡ã‚’è¨ˆç®—
        success_rates = [m.value for m in self.metrics_buffer if "success_rate" in m.metric_name]
        response_times = [m.value for m in self.metrics_buffer if "response_time" in m.metric_name]

        if not success_rates or not response_times:
            return 0.5

        avg_success_rate = statistics.mean(success_rates)
        avg_response_time = statistics.mean(response_times)

        # åŠ¹ç‡ = æˆåŠŸç‡ / (1 + æ­£è¦åŒ–å¿œç­”æ™‚é–“)
        normalized_response_time = min(1.0, avg_response_time / 10.0)  # 10ç§’ã‚’åŸºæº–
        efficiency = avg_success_rate / (1 + normalized_response_time)

        return min(1.0, efficiency)

    async def _analyze_performance_trends(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        # éå»7æ—¥é–“ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        week_ago = datetime.now() - timedelta(days=7)

        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            cursor.execute(
                """
                SELECT summary_date, system_health_score, automation_efficiency
                FROM performance_summary
                WHERE summary_date >= ?
                ORDER BY summary_date
            """,
                (week_ago.date(),),
            )

            results = cursor.fetchall()
            conn.close()

            if len(results) >= 3:
                # ãƒˆãƒ¬ãƒ³ãƒ‰è¨ˆç®—
                health_scores = [r[1] for r in results]
                efficiency_scores = [r[2] for r in results]

                health_trend = self._calculate_trend_slope(health_scores)
                efficiency_trend = self._calculate_trend_slope(efficiency_scores)

                logger.info(f"ğŸ“ˆ Performance Trends: Health={health_trend:.3f}, Efficiency={efficiency_trend:.3f}")

        except Exception as e:
            logger.error(f"Failed to analyze performance trends: {e}")

    def _calculate_trend_slope(self, values: List[float]) -> float:
        """ãƒˆãƒ¬ãƒ³ãƒ‰å‚¾ãè¨ˆç®—"""
        if len(values) < 2:
            return 0.0

        n = len(values)
        x = list(range(n))

        x_mean = statistics.mean(x)
        y_mean = statistics.mean(values)

        numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
        denominator = sum((x[i] - x_mean) ** 2 for i in range(n))

        return numerator / denominator if denominator != 0 else 0.0

    async def _generate_optimization_recommendations(self):
        """æœ€é©åŒ–æ¨å¥¨ç”Ÿæˆ"""
        recommendations = []

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ã‚¹ã‚³ã‚¢ãŒä½ã„å ´åˆ
        current_health = self._calculate_system_health_score()
        if current_health < 0.7:
            recommendations.append("System health score is low - consider system optimization")

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆãŒå¤šã„å ´åˆ
        if len(self.active_alerts) > 3:
            recommendations.append("Multiple active alerts - review system configuration")

        # å¿œç­”æ™‚é–“ãŒé«˜ã„å ´åˆ
        recent_response_times = [m.value for m in list(self.metrics_buffer)[-50:] if "response_time" in m.metric_name]
        if recent_response_times and statistics.mean(recent_response_times) > 3.0:
            recommendations.append("High response times detected - consider performance tuning")

        if recommendations:
            logger.info(f"ğŸ’¡ Optimization Recommendations: {'; '.join(recommendations)}")

    async def _dashboard_update_loop(self):
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ›´æ–°ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                dashboard_data = await self._generate_dashboard_data()

                # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
                await self._update_dashboard_file(dashboard_data)

                self.performance_stats["last_dashboard_update"] = datetime.now()

                await asyncio.sleep(self.config["dashboard_update_interval"])

            except Exception as e:
                logger.error(f"Dashboard update error: {e}")
                await asyncio.sleep(self.config["dashboard_update_interval"])

    async def _generate_dashboard_data(self) -> Dict[str, Any]:
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        current_time = datetime.now()

        # æœ€æ–°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        latest_metrics = {}
        for metric_key, aggregate in self.metric_aggregates.items():
            latest_metrics[metric_key] = {
                "current": aggregate["last_value"],
                "min": aggregate["min_value"],
                "max": aggregate["max_value"],
                "avg": aggregate["avg_value"],
                "last_updated": aggregate["last_updated"].isoformat(),
            }

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆ
        active_alerts_summary = []
        for alert_id, alert_data in self.active_alerts.items():
            active_alerts_summary.append(
                {
                    "alert_id": alert_id,
                    "severity": alert_data["severity"],
                    "description": alert_data["description"],
                    "triggered_at": alert_data["triggered_at"].isoformat(),
                    "duration": int((current_time - alert_data["triggered_at"]).total_seconds()),
                }
            )

        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        system_health = self._calculate_system_health_score()
        automation_efficiency = self._calculate_automation_efficiency()

        return {
            "timestamp": current_time.isoformat(),
            "system_overview": {
                "health_score": system_health,
                "automation_efficiency": automation_efficiency,
                "uptime_seconds": int((current_time - self.performance_stats["system_uptime_start"]).total_seconds()),
                "total_metrics_collected": self.performance_stats["total_metrics_collected"],
                "alerts_triggered": self.performance_stats["alerts_triggered"],
            },
            "latest_metrics": latest_metrics,
            "active_alerts": active_alerts_summary,
            "system_status": self.system_status,
        }

    async def _update_dashboard_file(self, dashboard_data: Dict[str, Any]):
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°"""
        dashboard_file = self.reports_path / "dashboard.json"

        try:
            with open(dashboard_file, "w") as f:
                json.dump(dashboard_data, f, indent=2, default=str)

        except Exception as e:
            logger.error(f"Failed to update dashboard file: {e}")

    async def _health_check_loop(self):
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                # Four Sagesçµ±åˆçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
                four_sages_status = await self._check_four_sages_health()
                self.system_status["four_sages_integration"] = four_sages_status

                # è‡ªå¾‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
                learning_status = await self._check_autonomous_learning_health()
                self.system_status["autonomous_learning"] = learning_status

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
                optimization_status = await self._check_optimization_health()
                self.system_status["performance_optimization"] = optimization_status

                self.system_status["last_health_check"] = datetime.now().isoformat()

                await asyncio.sleep(300)  # 5åˆ†é–“éš”

            except Exception as e:
                logger.error(f"Health check error: {e}")
                await asyncio.sleep(300)

    async def _check_four_sages_health(self) -> str:
        """Four Sageså¥åº·çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯"""
        try:
            # æœ€è¿‘ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰åˆ¤å®š
            recent_metrics = [
                m
                for m in self.metrics_buffer
                if m.source_system == "four_sages_integration" and (datetime.now() - m.timestamp).total_seconds() < 300
            ]

            if not recent_metrics:
                return "unknown"

            consensus_rates = [m.value for m in recent_metrics if m.metric_name == "four_sages.consensus_rate"]
            if consensus_rates and statistics.mean(consensus_rates) > 0.8:
                return "healthy"
            elif consensus_rates and statistics.mean(consensus_rates) > 0.6:
                return "warning"
            else:
                return "critical"

        except Exception:
            return "error"

    async def _check_autonomous_learning_health(self) -> str:
        """è‡ªå¾‹å­¦ç¿’å¥åº·çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯"""
        try:
            recent_metrics = [
                m
                for m in self.metrics_buffer
                if m.source_system == "autonomous_learning" and (datetime.now() - m.timestamp).total_seconds() < 300
            ]

            if not recent_metrics:
                return "unknown"

            accuracy_values = [m.value for m in recent_metrics if "accuracy" in m.metric_name]
            if accuracy_values and statistics.mean(accuracy_values) > 0.7:
                return "healthy"
            elif accuracy_values and statistics.mean(accuracy_values) > 0.5:
                return "warning"
            else:
                return "critical"

        except Exception:
            return "error"

    async def _check_optimization_health(self) -> str:
        """æœ€é©åŒ–å¥åº·çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯"""
        try:
            # è‡ªå‹•åŒ–åŠ¹ç‡ã‹ã‚‰åˆ¤å®š
            efficiency = self._calculate_automation_efficiency()

            if efficiency > 0.8:
                return "healthy"
            elif efficiency > 0.6:
                return "warning"
            else:
                return "critical"

        except Exception:
            return "error"

    async def generate_performance_report(self, hours: int = 24) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        start_time = datetime.now() - timedelta(hours=hours)
        report_id = f"performance_report_{int(datetime.now().timestamp())}"

        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±è¨ˆ
            cursor.execute(
                """
                SELECT metric_name, COUNT(*), AVG(value), MIN(value), MAX(value)
                FROM performance_metrics
                WHERE timestamp >= ?
                GROUP BY metric_name
            """,
                (start_time,),
            )

            metrics_stats = {}
            for row in cursor.fetchall():
                metrics_stats[row[0]] = {"count": row[1], "average": row[2], "minimum": row[3], "maximum": row[4]}

            # ã‚¢ãƒ©ãƒ¼ãƒˆçµ±è¨ˆ
            cursor.execute(
                """
                SELECT severity, COUNT(*)
                FROM alert_history
                WHERE triggered_at >= ?
                GROUP BY severity
            """,
                (start_time,),
            )

            alerts_stats = {row[0]: row[1] for row in cursor.fetchall()}

            conn.close()

            # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
            report = {
                "report_id": report_id,
                "generated_at": datetime.now().isoformat(),
                "time_range": {"start": start_time.isoformat(), "end": datetime.now().isoformat(), "hours": hours},
                "metrics_statistics": metrics_stats,
                "alerts_statistics": alerts_stats,
                "system_health": {
                    "current_score": self._calculate_system_health_score(),
                    "automation_efficiency": self._calculate_automation_efficiency(),
                    "active_alerts_count": len(self.active_alerts),
                },
                "performance_summary": {
                    "total_metrics_collected": len([m for m in self.metrics_buffer if m.timestamp >= start_time]),
                    "system_status": self.system_status.copy(),
                    "uptime_hours": (datetime.now() - self.performance_stats["system_uptime_start"]).total_seconds()
                    / 3600,
                },
            }

            # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            report_file = self.reports_path / f"{report_id}.json"
            with open(report_file, "w") as f:
                json.dump(report, f, indent=2, default=str)

            # ãƒ¬ãƒãƒ¼ãƒˆå±¥æ­´ã«è¨˜éŒ²
            await self._save_report_history(report_id, "performance", report_file, len(metrics_stats), hours)

            self.performance_stats["reports_generated"] += 1

            logger.info(f"ğŸ“Š Performance report generated: {report_id}")
            return report

        except Exception as e:
            logger.error(f"Failed to generate performance report: {e}")
            return {"error": str(e)}

    async def _save_report_history(
        self, report_id: str, report_type: str, file_path: Path, metrics_count: int, time_range_hours: int
    ):
        """ãƒ¬ãƒãƒ¼ãƒˆå±¥æ­´ä¿å­˜"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            cursor.execute(
                """
                INSERT INTO report_history
                (report_id, report_type, generated_at, file_path, metrics_count, time_range_hours)
                VALUES (?, ?, ?, ?, ?, ?)
            """,
                (report_id, report_type, datetime.now(), str(file_path), metrics_count, time_range_hours),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"Failed to save report history: {e}")

    def get_current_status(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®çŠ¶æ…‹å–å¾—"""
        return {
            "monitoring_active": self.monitoring_active,
            "system_status": self.system_status.copy(),
            "performance_stats": self.performance_stats.copy(),
            "active_alerts_count": len(self.active_alerts),
            "metrics_in_buffer": len(self.metrics_buffer),
            "system_health_score": self._calculate_system_health_score(),
            "automation_efficiency": self._calculate_automation_efficiency(),
        }


# ãƒ‡ãƒ¢å®Ÿè¡Œ
if __name__ == "__main__":

    async def demo():
        print("ğŸš€ AI Automation Performance Monitor Demo")
        print("=" * 50)

        # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        monitor = AIAutomationPerformanceMonitor({"collection_interval": 5, "alert_check_interval": 10})  # ãƒ‡ãƒ¢ç”¨ã«çŸ­ç¸®

        print("âœ… Performance monitor initialized")

        # ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ 
        print("\n1. Adding test metrics...")
        test_metrics = [
            PerformanceMetric("four_sages.consensus_rate", 0.85, datetime.now(), "four_sages_integration", "gauge"),
            PerformanceMetric(
                "autonomous_learning.prediction_accuracy", 0.72, datetime.now(), "autonomous_learning", "gauge"
            ),
            PerformanceMetric("system.response_time", 2.1, datetime.now(), "system", "timer"),
            PerformanceMetric("automation.success_rate", 0.91, datetime.now(), "automation", "gauge"),
        ]

        for metric in test_metrics:
            await monitor.record_metric(metric)
            print(f"  ğŸ“Š Recorded: {metric.metric_name} = {metric.value}")

        # çŠ¶æ…‹ç¢ºèª
        print("\n2. Checking system status...")
        status = monitor.get_current_status()
        print(f"  ğŸ¥ System Health Score: {status['system_health_score']:.3f}")
        print(f"  âš¡ Automation Efficiency: {status['automation_efficiency']:.3f}")
        print(f"  ğŸ“‹ Metrics in Buffer: {status['metrics_in_buffer']}")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        print("\n3. Generating performance report...")
        report = await monitor.generate_performance_report(hours=1)
        print(f"  ğŸ“Š Report ID: {report['report_id']}")
        print(f"  ğŸ“ˆ Metrics Statistics: {len(report['metrics_statistics'])} metric types")
        print(f"  ğŸ¯ System Health: {report['system_health']['current_score']:.3f}")

        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        print("\n4. Generating dashboard data...")
        dashboard = await monitor._generate_dashboard_data()
        print(f"  ğŸ“Š Dashboard updated at: {dashboard['timestamp']}")
        print(f"  ğŸ“ˆ Total metrics: {dashboard['system_overview']['total_metrics_collected']}")
        print(f"  ğŸš¨ Active alerts: {len(dashboard['active_alerts'])}")

        print("\nâœ¨ AI Automation Performance Monitor Features:")
        print("  âœ… Real-time metrics collection and storage")
        print("  âœ… Automatic alert detection and response")
        print("  âœ… Performance trend analysis")
        print("  âœ… System health monitoring")
        print("  âœ… Comprehensive reporting")
        print("  âœ… Dashboard data generation")
        print("  âœ… Automation efficiency tracking")

        print("\nğŸ¯ AI Automation Performance Monitor Demo - COMPLETED")

    asyncio.run(demo())
