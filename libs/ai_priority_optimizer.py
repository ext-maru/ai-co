#!/usr/bin/env python3
"""
AIé§†å‹•å‹ã‚¿ã‚¹ã‚¯å„ªå…ˆé †ä½æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
4è³¢è€…å”èª¿ã«ã‚ˆã‚‹è‡ªå¾‹çš„ãªå„ªå…ˆåº¦æ±ºå®š
"""

import asyncio
import json
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
PROJECT_ROOT = Path(__file__).parent.parent
import sys
sys.path.insert(0, str(PROJECT_ROOT))

from libs.knowledge_base_manager import KnowledgeBaseManager
from features.database.task_history_db import TaskHistoryDB


class TaskType(Enum):
    """ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—å®šç¾©"""
    PROJECT_INTERNAL = "project_internal"      # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã‚¿ã‚¹ã‚¯
    CROSS_PROJECT = "cross_project"           # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¨ªæ–­ã‚¿ã‚¹ã‚¯
    SYSTEM_WIDE = "system_wide"               # ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã‚¿ã‚¹ã‚¯
    INCIDENT = "incident"                     # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œ
    TECHNICAL_DEBT = "technical_debt"         # æŠ€è¡“çš„è² å‚µ


@dataclass
class Task:
    """ã‚¿ã‚¹ã‚¯å®šç¾©"""
    id: str
    name: str
    type: TaskType
    project: str
    dependencies: List[str] = field(default_factory=list)
    estimated_hours: float = 0
    business_value: float = 0
    technical_complexity: float = 0
    incident_risk: float = 0
    created_at: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class PriorityScore:
    """å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢"""
    total_score: float
    business_impact: float
    technical_urgency: float
    risk_mitigation: float
    resource_efficiency: float
    reasoning: Dict[str, str]
    confidence: float


class SageEvaluator:
    """è³¢è€…è©•ä¾¡åŸºåº•ã‚¯ãƒ©ã‚¹"""

    def __init__(self, name: str):
        self.name = name
        self.learning_data = []

    async def evaluate(self, task: Task, context: Dict[str, Any]) -> Dict[str, float]:
        """ã‚¿ã‚¹ã‚¯è©•ä¾¡ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        raise NotImplementedError


class TaskSage(SageEvaluator):
    """ã‚¿ã‚¹ã‚¯è³¢è€… - ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤è©•ä¾¡"""

    def __init__(self):
        super().__init__("ğŸ“‹ ã‚¿ã‚¹ã‚¯è³¢è€…")

    async def evaluate(self, task: Task, context: Dict[str, Any]) -> Dict[str, float]:
        """ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆè©•ä¾¡"""
        # ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã®è©•ä¾¡
        business_score = task.business_value

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé‡è¦åº¦ã«ã‚ˆã‚‹èª¿æ•´
        project_importance = context.get("project_importance", {}).get(task.project, 1.0)
        business_score *= project_importance

        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹èª¿æ•´
        if task.type == TaskType.INCIDENT:
            business_score *= 2.0  # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã¯å€ç‡
        elif task.type == TaskType.CROSS_PROJECT:
            business_score *= 1.5  # æ¨ªæ–­ã‚¿ã‚¹ã‚¯ã¯1.5å€

        # ä¾å­˜é–¢ä¿‚ã«ã‚ˆã‚‹åŠ ç‚¹
        dependency_bonus = len(task.dependencies) * 0.1
        business_score *= (1 + dependency_bonus)

        return {
            "business_impact": min(business_score, 10.0),
            "dependency_importance": dependency_bonus,
            "project_priority": project_importance
        }


class KnowledgeSage(SageEvaluator):
    """ãƒŠãƒ¬ãƒƒã‚¸è³¢è€… - æŠ€è¡“çš„è² å‚µè©•ä¾¡"""

    def __init__(self):
        super().__init__("ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…")

    async def evaluate(self, task: Task, context: Dict[str, Any]) -> Dict[str, float]:
        """æŠ€è¡“çš„è² å‚µã®è©•ä¾¡"""
        debt_score = 0

        # æŠ€è¡“çš„è² å‚µã‚¿ã‚¹ã‚¯ã®è©•ä¾¡
        if task.type == TaskType.TECHNICAL_DEBT:
            debt_score = 8.0  # ãƒ™ãƒ¼ã‚¹é«˜ã‚¹ã‚³ã‚¢

            # æ”¾ç½®æœŸé–“ã«ã‚ˆã‚‹åŠ ç‚¹
            age_days = (datetime.now() - task.created_at).days
            debt_score += min(age_days / 30, 2.0)  # æœ€å¤§2ç‚¹åŠ ç‚¹
        else:
            # é€šå¸¸ã‚¿ã‚¹ã‚¯ã§ã‚‚è¤‡é›‘åº¦ã‚’è€ƒæ…®
            debt_score = task.technical_complexity * 0.5

        # éå»ã®é¡ä¼¼ã‚¿ã‚¹ã‚¯ã‹ã‚‰å­¦ç¿’
        similar_tasks = context.get("similar_tasks", [])
        if similar_tasks:
            avg_impact = sum(t.get("impact", 0) for t in similar_tasks) / len(similar_tasks)
            debt_score *= (1 + avg_impact * 0.1)

        return {
            "technical_debt_score": min(debt_score, 10.0),
            "complexity_factor": task.technical_complexity,
            "age_factor": (datetime.now() - task.created_at).days / 30
        }


class IncidentSage(SageEvaluator):
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€… - ãƒªã‚¹ã‚¯è©•ä¾¡"""

    def __init__(self):
        super().__init__("ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…")

    async def evaluate(self, task: Task, context: Dict[str, Any]) -> Dict[str, float]:
        """ãƒªã‚¹ã‚¯è©•ä¾¡"""
        risk_score = task.incident_risk

        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚¿ã‚¹ã‚¯ã¯æœ€é«˜å„ªå…ˆåº¦
        if task.type == TaskType.INCIDENT:
            risk_score = 10.0
        else:
            # äºˆé˜²çš„ãƒªã‚¹ã‚¯è©•ä¾¡
            if "security" in task.name.lower() or "critical" in task.name.lower():
                risk_score *= 2.0

            # ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã¸ã®å½±éŸ¿
            if task.type == TaskType.SYSTEM_WIDE:
                risk_score *= 1.5

        # éå»ã®ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå±¥æ­´ã‹ã‚‰å­¦ç¿’
        incident_history = context.get("incident_history", [])
        if task.project in [inc.get("project") for inc in incident_history]:
            risk_score *= 1.3  # ãƒªã‚¹ã‚¯å±¥æ­´ãŒã‚ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯åŠ ç‚¹

        return {
            "risk_score": min(risk_score, 10.0),
            "incident_probability": risk_score / 10.0,
            "impact_scope": "system" if task.type == TaskType.SYSTEM_WIDE else "project"
        }


class RAGSage(SageEvaluator):
    """RAGè³¢è€… - ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–è©•ä¾¡"""

    def __init__(self):
        super().__init__("ğŸ” RAGè³¢è€…")

    async def evaluate(self, task: Task, context: Dict[str, Any]) -> Dict[str, float]:
        """ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡æ€§è©•ä¾¡"""
        # åˆ©ç”¨å¯èƒ½ãƒªã‚½ãƒ¼ã‚¹ã®ç¢ºèª
        available_resources = context.get("available_resources", 100)
        current_load = context.get("current_load", 0)

        # ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ã‚¹ã‚³ã‚¢
        efficiency_score = 0

        # å°ã•ãªã‚¿ã‚¹ã‚¯ã§å¤§ããªä¾¡å€¤
        if task.estimated_hours < 4 and task.business_value > 5:
            efficiency_score = 8.0
        # é©æ­£è¦æ¨¡ã®ã‚¿ã‚¹ã‚¯
        elif 4 <= task.estimated_hours <= 16:
            efficiency_score = 6.0
        # å¤§è¦æ¨¡ã‚¿ã‚¹ã‚¯
        else:
            efficiency_score = 4.0

        # ãƒªã‚½ãƒ¼ã‚¹ä½™è£•åº¦ã«ã‚ˆã‚‹èª¿æ•´
        resource_availability = (available_resources - current_load) / available_resources
        efficiency_score *= (1 + resource_availability * 0.5)

        # ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½æ€§
        if not task.dependencies:
            efficiency_score *= 1.2  # ä¾å­˜ãªã—ã¯ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½

        return {
            "resource_efficiency": min(efficiency_score, 10.0),
            "parallel_executable": len(task.dependencies) == 0,
            "resource_fit": resource_availability
        }


class AIPriorityOptimizer:
    """AIé§†å‹•å‹å„ªå…ˆé †ä½æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.task_sage = TaskSage()
        self.knowledge_sage = KnowledgeSage()
        self.incident_sage = IncidentSage()
        self.rag_sage = RAGSage()

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self.learning_history_file = PROJECT_ROOT / "knowledge_base" / "ai_priority_learning.json"
        self.learning_history = self.load_learning_history()

        # ãƒ­ã‚¬ãƒ¼è¨­å®š
        self.logger = logging.getLogger("AIPriorityOptimizer")
        self.logger.setLevel(logging.INFO)

        # äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´
        self.feedback_history = []

    def load_learning_history(self) -> List[Dict[str, Any]]:
        """å­¦ç¿’å±¥æ­´ã®èª­ã¿è¾¼ã¿"""
        if self.learning_history_file.exists():
            with open(self.learning_history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []

    def save_learning_history(self):
        """å­¦ç¿’å±¥æ­´ã®ä¿å­˜"""
        self.learning_history_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.learning_history_file, 'w', encoding='utf-8') as f:
            json.dump(self.learning_history, f, indent=2, ensure_ascii=False, default=str)

    async def calculate_priority(self, task: Task, context: Dict[str, Any]) -> PriorityScore:
        """AIé§†å‹•ã«ã‚ˆã‚‹å„ªå…ˆåº¦è¨ˆç®—"""
        # 4è³¢è€…ã«ã‚ˆã‚‹è©•ä¾¡ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        evaluations = await asyncio.gather(
            self.task_sage.evaluate(task, context),
            self.knowledge_sage.evaluate(task, context),
            self.incident_sage.evaluate(task, context),
            self.rag_sage.evaluate(task, context)
        )

        # è©•ä¾¡çµæœã®çµ±åˆ
        task_eval = evaluations[0]
        knowledge_eval = evaluations[1]
        incident_eval = evaluations[2]
        rag_eval = evaluations[3]

        # AIæœ€é©åŒ–ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘ï¼ˆå­¦ç¿’ã«ã‚ˆã‚Šèª¿æ•´ï¼‰
        weights = self.get_dynamic_weights(task, context)

        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        business_impact = task_eval["business_impact"] * weights["business"]
        technical_urgency = knowledge_eval["technical_debt_score"] * weights["technical"]
        risk_mitigation = incident_eval["risk_score"] * weights["risk"]
        resource_efficiency = rag_eval["resource_efficiency"] * weights["resource"]

        # ç·åˆã‚¹ã‚³ã‚¢
        total_score = (
            business_impact +
            technical_urgency +
            risk_mitigation +
            resource_efficiency
        ) / 4.0

        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self.calculate_confidence(task, context)

        # æ¨è«–ç†ç”±ã®ç”Ÿæˆ
        reasoning = {
            "business": f"ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ {task.business_value:.1f} Ã— ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé‡è¦åº¦",
            "technical": f"æŠ€è¡“çš„è¤‡é›‘åº¦ {task.technical_complexity:.1f} + è² å‚µã‚¹ã‚³ã‚¢",
            "risk": f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ« {task.incident_risk:.1f} Ã— ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç¯„å›²",
            "resource": f"æ¨å®šæ™‚é–“ {task.estimated_hours:.1f}h ã§ã®åŠ¹ç‡æ€§"
        }

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è¨˜éŒ²
        self.record_learning_data(task, total_score, weights, context)

        return PriorityScore(
            total_score=total_score,
            business_impact=business_impact,
            technical_urgency=technical_urgency,
            risk_mitigation=risk_mitigation,
            resource_efficiency=resource_efficiency,
            reasoning=reasoning,
            confidence=confidence
        )

    def get_dynamic_weights(self, task: Task, context: Dict[str, Any]) -> Dict[str, float]:
        """å‹•çš„ãªé‡ã¿ä»˜ã‘å–å¾—ï¼ˆå­¦ç¿’ãƒ™ãƒ¼ã‚¹ï¼‰"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é‡ã¿
        weights = {
            "business": 0.25,
            "technical": 0.25,
            "risk": 0.25,
            "resource": 0.25
        }

        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹èª¿æ•´
        if task.type == TaskType.INCIDENT:
            weights["risk"] = 0.5
            weights["business"] = 0.3
            weights["technical"] = 0.1
            weights["resource"] = 0.1
        elif task.type == TaskType.TECHNICAL_DEBT:
            weights["technical"] = 0.4
            weights["risk"] = 0.3
            weights["business"] = 0.2
            weights["resource"] = 0.1
        elif task.type == TaskType.CROSS_PROJECT:
            weights["business"] = 0.4
            weights["resource"] = 0.3
            weights["technical"] = 0.2
            weights["risk"] = 0.1

        # å­¦ç¿’ã«ã‚ˆã‚‹èª¿æ•´
        if self.learning_history:
            similar_tasks = self.find_similar_tasks(task)
            if similar_tasks:
                # éå»ã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å­¦ç¿’
                avg_weights = self.calculate_average_weights(similar_tasks)
                # å¾ã€…ã«å­¦ç¿’çµæœã‚’åæ˜ ï¼ˆæ€¥æ¿€ãªå¤‰åŒ–ã‚’é¿ã‘ã‚‹ï¼‰
                for key in weights:
                    weights[key] = weights[key] * 0.7 + avg_weights.get(key, weights[key]) * 0.3

        return weights

    def calculate_confidence(self, task: Task, context: Dict[str, Any]) -> float:
        """å„ªå…ˆåº¦è¨ˆç®—ã®ä¿¡é ¼åº¦"""
        confidence = 0.5  # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦

        # é¡ä¼¼ã‚¿ã‚¹ã‚¯ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹
        similar_tasks = self.find_similar_tasks(task)
        if len(similar_tasks) > 10:
            confidence += 0.3
        elif len(similar_tasks) > 5:
            confidence += 0.2
        elif len(similar_tasks) > 0:
            confidence += 0.1

        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´
        positive_feedback = sum(1 for f in self.feedback_history if f["positive"])
        if len(self.feedback_history) > 0:
            feedback_rate = positive_feedback / len(self.feedback_history)
            confidence += feedback_rate * 0.2

        return min(confidence, 1.0)

    def find_similar_tasks(self, task: Task) -> List[Dict[str, Any]]:
        """é¡ä¼¼ã‚¿ã‚¹ã‚¯ã®æ¤œç´¢"""
        similar = []
        for record in self.learning_history:
            if (record["task_type"] == task.type.value and
                abs(record["business_value"] - task.business_value) < 2.0):
                similar.append(record)
        return similar

    def calculate_average_weights(self, tasks: List[Dict[str, Any]]) -> Dict[str, float]:
        """å¹³å‡é‡ã¿ä»˜ã‘ã®è¨ˆç®—"""
        if not tasks:
            return {}

        weights_sum = {"business": 0, "technical": 0, "risk": 0, "resource": 0}
        for task in tasks:
            for key in weights_sum:
                weights_sum[key] += task["weights"].get(key, 0)

        return {key: val / len(tasks) for key, val in weights_sum.items()}

    def record_learning_data(self, task: Task, score: float, weights: Dict[str, float], context: Dict[str, Any]):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²"""
        learning_record = {
            "timestamp": datetime.now().isoformat(),
            "task_id": task.id,
            "task_type": task.type.value,
            "business_value": task.business_value,
            "technical_complexity": task.technical_complexity,
            "incident_risk": task.incident_risk,
            "priority_score": score,
            "weights": weights,
            "context_summary": {
                "available_resources": context.get("available_resources", 0),
                "current_load": context.get("current_load", 0),
                "active_projects": len(context.get("project_importance", {}))
            }
        }

        self.learning_history.append(learning_record)

        # å®šæœŸçš„ã«ä¿å­˜ï¼ˆ100ä»¶ã”ã¨ï¼‰
        if len(self.learning_history) % 100 == 0:
            self.save_learning_history()

    async def batch_prioritize(self, tasks: List[Task], context: Dict[str, Any]) -> List[Tuple[Task, PriorityScore]]:
        """ãƒãƒƒãƒã§ã®å„ªå…ˆé †ä½ä»˜ã‘"""
        # ä¸¦åˆ—ã§å…¨ã‚¿ã‚¹ã‚¯ã‚’è©•ä¾¡
        priorities = await asyncio.gather(
            *[self.calculate_priority(task, context) for task in tasks]
        )

        # ã‚¿ã‚¹ã‚¯ã¨å„ªå…ˆåº¦ã®ãƒšã‚¢ã‚’ä½œæˆ
        task_priorities = list(zip(tasks, priorities))

        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
        task_priorities.sort(key=lambda x: x[1].total_score, reverse=True)

        return task_priorities

    def receive_feedback(self, task_id: str, feedback: str, adjustment: Optional[float] = None):
        """äººé–“ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å—ä¿¡"""
        feedback_record = {
            "timestamp": datetime.now().isoformat(),
            "task_id": task_id,
            "feedback": feedback,
            "positive": feedback.lower() in ["good", "correct", "ok", "è‰¯ã„", "æ­£ã—ã„"],
            "adjustment": adjustment
        }

        self.feedback_history.append(feedback_record)

        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ãå­¦ç¿’
        if adjustment is not None:
            # è©²å½“ã‚¿ã‚¹ã‚¯ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’èª¿æ•´
            for record in self.learning_history:
                if record["task_id"] == task_id:
                    record["human_adjusted_score"] = adjustment
                    break

        # å³åº§ã«ä¿å­˜
        self.save_learning_history()

        self.logger.info(f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å—ä¿¡: {task_id} - {feedback}")

    def generate_explanation(self, task: Task, score: PriorityScore) -> str:
        """å„ªå…ˆåº¦æ±ºå®šã®èª¬æ˜ç”Ÿæˆ"""
        explanation = f"""
ğŸ¯ ã‚¿ã‚¹ã‚¯: {task.name}
ğŸ“Š å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢: {score.total_score:.2f} (ä¿¡é ¼åº¦: {score.confidence:.0%})

ğŸ“‹ è©•ä¾¡å†…è¨³:
- ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ: {score.business_impact:.2f}
  {score.reasoning['business']}

- æŠ€è¡“çš„ç·Šæ€¥åº¦: {score.technical_urgency:.2f}
  {score.reasoning['technical']}

- ãƒªã‚¹ã‚¯è»½æ¸›: {score.risk_mitigation:.2f}
  {score.reasoning['risk']}

- ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡: {score.resource_efficiency:.2f}
  {score.reasoning['resource']}

ğŸ’¡ ã“ã®å„ªå…ˆåº¦ã¯{len(self.learning_history)}ä»¶ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚
"""
        return explanation


# ä½¿ç”¨ä¾‹
async def demo():
    """ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    optimizer = AIPriorityOptimizer()

    # ã‚µãƒ³ãƒ—ãƒ«ã‚¿ã‚¹ã‚¯
    tasks = [
        Task(
            id="task-001",
            name="ç·Šæ€¥ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ãƒƒãƒé©ç”¨",
            type=TaskType.INCIDENT,
            project="api",
            business_value=8.0,
            technical_complexity=3.0,
            incident_risk=9.0,
            estimated_hours=2.0
        ),
        Task(
            id="task-002",
            name="æ–°æ©Ÿèƒ½ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
            type=TaskType.PROJECT_INTERNAL,
            project="frontend",
            business_value=7.0,
            technical_complexity=6.0,
            incident_risk=2.0,
            estimated_hours=40.0
        ),
        Task(
            id="task-003",
            name="æŠ€è¡“çš„è² å‚µï¼šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–",
            type=TaskType.TECHNICAL_DEBT,
            project="api",
            business_value=4.0,
            technical_complexity=8.0,
            incident_risk=3.0,
            estimated_hours=16.0
        )
    ]

    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
    context = {
        "available_resources": 100,
        "current_load": 30,
        "project_importance": {
            "api": 1.2,
            "frontend": 1.0
        },
        "incident_history": [
            {"project": "api", "severity": "high"}
        ]
    }

    # å„ªå…ˆé †ä½ä»˜ã‘å®Ÿè¡Œ
    prioritized = await optimizer.batch_prioritize(tasks, context)

    print("ğŸ›ï¸ AIé§†å‹•å‹å„ªå…ˆé †ä½ä»˜ã‘çµæœ")
    print("=" * 60)

    for i, (task, score) in enumerate(prioritized, 1):
        print(f"\n{i}ä½: {task.name}")
        print(f"   ã‚¹ã‚³ã‚¢: {score.total_score:.2f}")
        print(f"   ä¿¡é ¼åº¦: {score.confidence:.0%}")

    # èª¬æ˜ç”Ÿæˆ
    top_task, top_score = prioritized[0]
    explanation = optimizer.generate_explanation(top_task, top_score)
    print("\n" + explanation)


if __name__ == "__main__":
    asyncio.run(demo())
