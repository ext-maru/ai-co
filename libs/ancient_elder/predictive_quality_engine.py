#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ›ï¸ Ancient Elder - Predictive Quality Engine
ã‚¨ãƒ³ã‚·ã‚§ãƒ³ãƒˆã‚¨ãƒ«ãƒ€ãƒ¼äºˆæ¸¬å“è³ªã‚¨ãƒ³ã‚¸ãƒ³

Tier 4: Predictive Quality Analysis
AIé§†å‹•å‹å“è³ªäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 

Creation: 2025-01-20
Author: Claude Elder
"""

import ast
import re
import sys
import time
import math
import statistics
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple, Union
from collections import Counter
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class PredictionResult:
    """å“è³ªäºˆæ¸¬çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    bug_probability: float = 0.0  # ãƒã‚°ç™ºç”Ÿç¢ºç‡ (0-100%)
    performance_risk: float = 0.0  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒªã‚¹ã‚¯ (0-100%)
    security_risk: float = 0.0  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ (0-100%)
    maintainability_score: float = 0.0  # ä¿å®ˆæ€§ã‚¹ã‚³ã‚¢ (0-100)
    complexity_index: float = 0.0  # è¤‡é›‘åº¦æŒ‡æ•°
    quality_trend: str = "stable"  # å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰: improving/stable/declining
    risk_factors: List[str] = field(default_factory=list)
    improvement_suggestions: List[str] = field(default_factory=list)
    confidence: float = 0.0  # äºˆæ¸¬ä¿¡é ¼åº¦ (0-100%)


@dataclass
class CodeFeatures:
    """ã‚³ãƒ¼ãƒ‰ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (5)
    lines_of_code: int = 0
    number_of_functions: int = 0
    number_of_classes: int = 0
    cyclomatic_complexity: float = 0.0
    nesting_depth: int = 0
    
    # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ (5)
    comment_ratio: float = 0.0
    docstring_coverage: float = 0.0
    variable_name_quality: float = 0.0
    function_length_avg: float = 0.0
    parameter_count_avg: float = 0.0
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (5)
    loop_complexity: int = 0
    nested_loop_count: int = 0
    recursive_function_count: int = 0
    string_concatenation_count: int = 0
    list_comprehension_count: int = 0
    
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (5)
    dangerous_function_count: int = 0
    input_validation_ratio: float = 0.0
    hardcoded_secret_count: int = 0
    sql_query_count: int = 0
    file_operation_count: int = 0
    
    # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (5)
    import_count: int = 0
    external_dependency_count: int = 0
    coupling_factor: float = 0.0
    inheritance_depth: int = 0
    method_override_count: int = 0


class PredictiveQualityEngine:
    """
    ğŸ§  äºˆæ¸¬å“è³ªã‚¨ãƒ³ã‚¸ãƒ³
    
    AIé§†å‹•ã®å“è³ªäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ :
    - ãƒã‚°ç™ºç”Ÿç¢ºç‡äºˆæ¸¬
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒªã‚¹ã‚¯è©•ä¾¡
    - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§äºˆæ¸¬
    - ä¿å®ˆæ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
    - æ”¹å–„ææ¡ˆç”Ÿæˆ
    """
    
    def __init__(self):
        """ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–"""
        self._bug_patterns = self._load_bug_patterns()
        self._performance_patterns = self._load_performance_patterns()
        self._security_patterns = self._load_security_patterns()
        self._quality_weights = self._load_quality_weights()
        logger.info("ğŸ§  PredictiveQualityEngine initialized")
    
    def _load_bug_patterns(self) -> Dict[str, float]:
        """ãƒã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³é‡ã¿ä»˜ã‘"""
        return {
            'division_without_check': 0.7,
            'index_out_of_bounds': 0.6,
            'null_pointer_access': 0.8,
            'infinite_loop_risk': 0.9,
            'unhandled_exception': 0.5,
            'resource_leak': 0.4,
            'race_condition': 0.6,
            'type_mismatch': 0.3,
        }
    
    def _load_performance_patterns(self) -> Dict[str, float]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³é‡ã¿ä»˜ã‘"""
        return {
            'nested_loops': 0.8,
            'inefficient_search': 0.6,
            'string_concatenation': 0.4,
            'recursive_without_memo': 0.7,
            'large_data_in_memory': 0.5,
            'repeated_calculations': 0.6,
            'io_in_loop': 0.9,
            'inefficient_data_structure': 0.5,
        }
    
    def _load_security_patterns(self) -> Dict[str, float]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ã‚¿ãƒ¼ãƒ³é‡ã¿ä»˜ã‘"""
        return {
            'sql_injection': 0.9,
            'command_injection': 1.0,
            'xss_vulnerability': 0.8,
            'hardcoded_credentials': 0.7,
            'insecure_random': 0.4,
            'path_traversal': 0.6,
            'unsafe_deserialization': 0.8,
            'weak_crypto': 0.5,
        }
    
    def _load_quality_weights(self) -> Dict[str, float]:
        """å“è³ªé‡ã¿ä»˜ã‘è¨­å®š"""
        return {
            'complexity': 0.25,
            'maintainability': 0.25,
            'performance': 0.20,
            'security': 0.20,
            'testability': 0.10,
        }
    
    def predict_quality(self, code: str) -> PredictionResult:
        """
        ğŸ”® å“è³ªäºˆæ¸¬å®Ÿè¡Œ
        
        Args:
            code: åˆ†æå¯¾è±¡ã‚³ãƒ¼ãƒ‰
            
        Returns:
            PredictionResult: äºˆæ¸¬çµæœ
        """
        if not code or not code.strip():
            return PredictionResult(
                bug_probability=100.0,
                performance_risk=100.0,
                security_risk=100.0,
                maintainability_score=0.0,
                complexity_index=0.0,
                quality_trend="declining",
                risk_factors=["Empty code"],
                improvement_suggestions=["ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„"],
                confidence=100.0
            )
        
        # ã‚³ãƒ¼ãƒ‰ç‰¹å¾´é‡æŠ½å‡º
        features = self._extract_features(code)
        
        # å„ãƒªã‚¹ã‚¯äºˆæ¸¬
        bug_prob = self._predict_bug_probability(features, code)
        perf_risk = self._predict_performance_risk(features, code)
        sec_risk = self._predict_security_risk(features, code)
        maintain_score = self._calculate_maintainability_score(features, code)
        complexity = self._calculate_complexity_index(features)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        trend = self._analyze_quality_trend(features)
        
        # ãƒªã‚¹ã‚¯è¦å› ç‰¹å®š
        risk_factors = self._identify_risk_factors(features, code)
        
        # æ”¹å–„ææ¡ˆç”Ÿæˆ
        suggestions = self._generate_improvement_suggestions(features, code)
        
        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_confidence(features)
        
        return PredictionResult(
            bug_probability=bug_prob,
            performance_risk=perf_risk,
            security_risk=sec_risk,
            maintainability_score=maintain_score,
            complexity_index=complexity,
            quality_trend=trend,
            risk_factors=risk_factors,
            improvement_suggestions=suggestions,
            confidence=confidence
        )
    
    def _extract_features(self, code: str) -> CodeFeatures:
        """
        ğŸ“Š ã‚³ãƒ¼ãƒ‰ç‰¹å¾´é‡æŠ½å‡º
        
        25ã®ç‰¹å¾´é‡ã‚’æŠ½å‡º:
        - åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (5)
        - å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ (5) 
        - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (5)
        - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (5)
        - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (5)
        """
        features = CodeFeatures()
        lines = code.split('\n')
        
        try:
            tree = ast.parse(code)
        except:
            # ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯åŸºæœ¬çš„ãªæ–‡å­—åˆ—è§£æã®ã¿
            return self._extract_features_fallback(code)
        
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        features.lines_of_code = len([line for line in lines if line.strip()])
        features.number_of_functions = len([node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)])
        features.number_of_classes = len([node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)])
        features.cyclomatic_complexity = self._calculate_cyclomatic_complexity(tree)
        features.nesting_depth = self._calculate_nesting_depth(tree)
        
        # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        features.comment_ratio = self._calculate_comment_ratio(lines)
        features.docstring_coverage = self._calculate_docstring_coverage(tree)
        features.variable_name_quality = self._evaluate_variable_names(tree)
        features.function_length_avg = self._calculate_avg_function_length(tree, lines)
        features.parameter_count_avg = self._calculate_avg_parameter_count(tree)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        features.loop_complexity = self._calculate_loop_complexity(tree)
        features.nested_loop_count = self._count_nested_loops(tree)
        features.recursive_function_count = self._count_recursive_functions(tree)
        features.string_concatenation_count = len(re.findall(r'\+.*[\'"]', code))
        features.list_comprehension_count = len([node for node in ast.walk(tree) if isinstance(node, ast.ListComp)])
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        features.dangerous_function_count = self._count_dangerous_functions(code)
        features.input_validation_ratio = self._calculate_input_validation_ratio(tree)
        features.hardcoded_secret_count = self._count_hardcoded_secrets(code)
        features.sql_query_count = len(re.findall(r'SELECT|INSERT|UPDATE|DELETE', code, re.IGNORECASE))
        features.file_operation_count = len(re.findall(r'\bopen\s*\(', code))
        
        # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        features.import_count = len([node for node in ast.walk(tree) if isinstance(node, (ast.Import, ast.ImportFrom))])
        features.external_dependency_count = self._count_external_dependencies(tree)
        features.coupling_factor = self._calculate_coupling_factor(tree)
        features.inheritance_depth = self._calculate_inheritance_depth(tree)
        features.method_override_count = self._count_method_overrides(tree)
        
        return features
    
    def _extract_features_fallback(self, code: str) -> CodeFeatures:
        """ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡æŠ½å‡º"""
        features = CodeFeatures()
        lines = code.split('\n')
        
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆæ–‡å­—åˆ—ãƒ™ãƒ¼ã‚¹ï¼‰
        features.lines_of_code = len([line for line in lines if line.strip()])
        features.number_of_functions = len(re.findall(r'def\s+\w+', code))
        features.number_of_classes = len(re.findall(r'class\s+\w+', code))
        features.nesting_depth = max(len(line) - len(line.lstrip()) for line in lines) // 4
        
        # ãã®ä»–ã¯0ã¾ãŸã¯ä½ã„å€¤ã§åˆæœŸåŒ–
        features.comment_ratio = len(re.findall(r'#.*', code)) / max(len(lines), 1)
        features.dangerous_function_count = self._count_dangerous_functions(code)
        
        return features
    
    def _predict_bug_probability(self, features: CodeFeatures, code: str) -> float:
        """ğŸ› ãƒã‚°ç™ºç”Ÿç¢ºç‡äºˆæ¸¬"""
        bug_score = 0.0
        
        # è¤‡é›‘åº¦ã«ã‚ˆã‚‹ãƒã‚°ç¢ºç‡
        if features.cyclomatic_complexity > 10:
            bug_score += 30.0
        elif features.cyclomatic_complexity > 5:
            bug_score += 15.0
        
        # ãƒã‚¹ãƒˆæ·±åº¦
        if features.nesting_depth > 4:
            bug_score += 25.0
        elif features.nesting_depth > 2:
            bug_score += 10.0
        
        # é–¢æ•°é•·
        if features.function_length_avg > 50:
            bug_score += 20.0
        elif features.function_length_avg > 25:
            bug_score += 10.0
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æ•°
        if features.parameter_count_avg > 7:
            bug_score += 15.0
        elif features.parameter_count_avg > 4:
            bug_score += 5.0
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸è¶³
        if features.docstring_coverage < 0.3:
            bug_score += 10.0
        
        # å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        if '/' in code and 'if' not in code:  # ã‚¼ãƒ­é™¤ç®—ãƒªã‚¹ã‚¯
            bug_score += 20.0
        
        if 'while True' in code and 'break' not in code:  # ç„¡é™ãƒ«ãƒ¼ãƒ—ãƒªã‚¹ã‚¯
            bug_score += 25.0
        
        return min(bug_score, 100.0)
    
    def _predict_performance_risk(self, features: CodeFeatures, code: str) -> float:
        """âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒªã‚¹ã‚¯äºˆæ¸¬"""
        perf_score = 0.0
        
        # ãƒã‚¹ãƒˆãƒ«ãƒ¼ãƒ—
        if features.nested_loop_count > 2:
            perf_score += 40.0
        elif features.nested_loop_count > 0:
            perf_score += 20.0
        
        # æ–‡å­—åˆ—é€£çµ
        if features.string_concatenation_count > 10:
            perf_score += 30.0
        elif features.string_concatenation_count > 5:
            perf_score += 15.0
        
        # å†å¸°ãªã—ãƒ¡ãƒ¢åŒ–
        if features.recursive_function_count > 0:
            if '@lru_cache' not in code and '@cache' not in code:
                perf_score += 25.0
        
        # å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        large_ranges = re.findall(r'range\((\d+)\)', code)
        for range_val in large_ranges:
            if int(range_val) > 10000:
                perf_score += 20.0
        
        # ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜ã®ä¸ä½¿ç”¨
        if 'append' in code and features.list_comprehension_count == 0:
            perf_score += 10.0
        
        return min(perf_score, 100.0)
    
    def _predict_security_risk(self, features: CodeFeatures, code: str) -> float:
        """ğŸ›¡ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯äºˆæ¸¬"""
        sec_score = 0.0
        
        # å±é™ºé–¢æ•°
        if features.dangerous_function_count > 0:
            sec_score += features.dangerous_function_count * 20.0
        
        # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ
        if features.hardcoded_secret_count > 0:
            sec_score += features.hardcoded_secret_count * 15.0
        
        # å…¥åŠ›æ¤œè¨¼ä¸è¶³
        if features.input_validation_ratio < 0.5:
            sec_score += 20.0
        
        # SQLã‚¯ã‚¨ãƒª
        if features.sql_query_count > 0 and '%' in code:
            sec_score += 30.0  # SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ ãƒªã‚¹ã‚¯
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ
        if features.file_operation_count > 0:
            if 'os.path.join' not in code:
                sec_score += 15.0  # ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ« ãƒªã‚¹ã‚¯
        
        return min(sec_score, 100.0)
    
    def _calculate_maintainability_score(self, features: CodeFeatures, code: str) -> float:
        """ğŸ”§ ä¿å®ˆæ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 100.0
        
        # è¤‡é›‘åº¦ãƒšãƒŠãƒ«ãƒ†ã‚£
        score -= features.cyclomatic_complexity * 2
        
        # ãƒã‚¹ãƒˆæ·±åº¦ãƒšãƒŠãƒ«ãƒ†ã‚£
        score -= features.nesting_depth * 5
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒœãƒ¼ãƒŠã‚¹
        score += features.docstring_coverage * 20
        
        # å¤‰æ•°åå“è³ªãƒœãƒ¼ãƒŠã‚¹
        score += features.variable_name_quality * 10
        
        # é–¢æ•°é•·ãƒšãƒŠãƒ«ãƒ†ã‚£
        if features.function_length_avg > 50:
            score -= 20
        elif features.function_length_avg > 25:
            score -= 10
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æ•°ãƒšãƒŠãƒ«ãƒ†ã‚£
        if features.parameter_count_avg > 7:
            score -= 15
        elif features.parameter_count_avg > 4:
            score -= 5
        
        return max(score, 0.0)
    
    def _calculate_complexity_index(self, features: CodeFeatures) -> float:
        """ğŸ“Š è¤‡é›‘åº¦æŒ‡æ•°è¨ˆç®—"""
        # åŠ é‡è¤‡é›‘åº¦è¨ˆç®—
        complexity = (
            features.cyclomatic_complexity * 0.3 +
            features.nesting_depth * 0.2 +
            features.function_length_avg * 0.1 +
            features.parameter_count_avg * 0.15 +
            features.loop_complexity * 0.25
        )
        return min(complexity, 100.0)
    
    def _analyze_quality_trend(self, features: CodeFeatures) -> str:
        """ğŸ“ˆ å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        positive_factors = 0
        negative_factors = 0
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–è¦å› 
        if features.docstring_coverage > 0.7:
            positive_factors += 1
        if features.variable_name_quality > 0.8:
            positive_factors += 1
        if features.list_comprehension_count > 0:
            positive_factors += 1
        if features.function_length_avg < 20:
            positive_factors += 1
        
        # ãƒã‚¬ãƒ†ã‚£ãƒ–è¦å› 
        if features.cyclomatic_complexity > 10:
            negative_factors += 1
        if features.nesting_depth > 4:
            negative_factors += 1
        if features.nested_loop_count > 1:
            negative_factors += 1
        if features.dangerous_function_count > 0:
            negative_factors += 1
        
        if positive_factors > negative_factors:
            return "improving"
        elif negative_factors > positive_factors:
            return "declining"
        else:
            return "stable"
    
    def _identify_risk_factors(self, features: CodeFeatures, code: str) -> List[str]:
        """âš ï¸ ãƒªã‚¹ã‚¯è¦å› ç‰¹å®š"""
        risks = []
        
        if features.cyclomatic_complexity > 10:
            risks.append("é«˜ã„å¾ªç’°çš„è¤‡é›‘åº¦")
        if features.nesting_depth > 4:
            risks.append("éåº¦ãªãƒã‚¹ãƒˆ")
        if features.function_length_avg > 50:
            risks.append("é•·ã„é–¢æ•°")
        if features.parameter_count_avg > 7:
            risks.append("å¤šã™ãã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼")
        if features.docstring_coverage < 0.3:
            risks.append("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸è¶³")
        if features.dangerous_function_count > 0:
            risks.append("å±é™ºãªé–¢æ•°ã®ä½¿ç”¨")
        if features.nested_loop_count > 2:
            risks.append("éåº¦ãªãƒã‚¹ãƒˆãƒ«ãƒ¼ãƒ—")
        
        return risks
    
    def _generate_improvement_suggestions(self, features: CodeFeatures, code: str) -> List[str]:
        """ğŸ’¡ æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        suggestions = []
        
        if features.cyclomatic_complexity > 10:
            suggestions.append("é–¢æ•°ã‚’å°ã•ãåˆ†å‰²ã—ã¦è¤‡é›‘åº¦ã‚’ä¸‹ã’ã¦ãã ã•ã„")
        if features.nesting_depth > 4:
            suggestions.append("æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³ã‚„ã‚¬ãƒ¼ãƒ‰å¥ã‚’ä½¿ã£ã¦ãƒã‚¹ãƒˆã‚’æµ…ãã—ã¦ãã ã•ã„")
        if features.function_length_avg > 50:
            suggestions.append("é•·ã„é–¢æ•°ã‚’è¤‡æ•°ã®å°ã•ãªé–¢æ•°ã«åˆ†å‰²ã—ã¦ãã ã•ã„")
        if features.parameter_count_avg > 7:
            suggestions.append("ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã¾ã¨ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        if features.docstring_coverage < 0.3:
            suggestions.append("é–¢æ•°ã¨ã‚¯ãƒ©ã‚¹ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ³ã‚°ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
        if features.list_comprehension_count == 0 and 'append' in code:
            suggestions.append("ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜ã®ä½¿ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        if features.string_concatenation_count > 5:
            suggestions.append("æ–‡å­—åˆ—çµåˆã«f-stringã‚„join()ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
        if features.dangerous_function_count > 0:
            suggestions.append("ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ã®ã‚ã‚‹é–¢æ•°ã®ä½¿ç”¨ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„")
        
        return suggestions
    
    def _calculate_confidence(self, features: CodeFeatures) -> float:
        """ğŸ¯ äºˆæ¸¬ä¿¡é ¼åº¦è¨ˆç®—"""
        confidence = 50.0  # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        
        # ã‚³ãƒ¼ãƒ‰ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´
        if features.lines_of_code > 20:
            confidence += 20.0
        elif features.lines_of_code > 10:
            confidence += 10.0
        
        # æ©Ÿèƒ½ã®è¤‡é›‘ã•ã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´
        if features.number_of_functions > 3:
            confidence += 15.0
        elif features.number_of_functions > 1:
            confidence += 10.0
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ˜ç¢ºã•ã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´
        if features.cyclomatic_complexity > 0:
            confidence += 10.0
        if features.nesting_depth > 0:
            confidence += 5.0
        
        return min(confidence, 100.0)
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _calculate_cyclomatic_complexity(self, tree: ast.AST) -> float:
        """å¾ªç’°çš„è¤‡é›‘åº¦è¨ˆç®—"""
        complexity = 1  # ãƒ™ãƒ¼ã‚¹è¤‡é›‘åº¦
        
        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1
            elif isinstance(node, (ast.ExceptHandler, ast.Try)):
                complexity += 1
        
        return float(complexity)
    
    def _calculate_nesting_depth(self, tree: ast.AST) -> int:
        """ãƒã‚¹ãƒˆæ·±åº¦è¨ˆç®—"""
        max_depth = 0
        
        def calculate_depth(node, current_depth=0):
            nonlocal max_depth
            max_depth = max(max_depth, current_depth)
            
            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor, ast.With, ast.Try)):
                current_depth += 1
            
            for child in ast.iter_child_nodes(node):
                calculate_depth(child, current_depth)
        
        calculate_depth(tree)
        return max_depth
    
    def _calculate_comment_ratio(self, lines: List[str]) -> float:
        """ã‚³ãƒ¡ãƒ³ãƒˆæ¯”ç‡è¨ˆç®—"""
        comment_lines = sum(1 for line in lines if line.strip().startswith('#'))
        total_lines = len([line for line in lines if line.strip()])
        return comment_lines / max(total_lines, 1)
    
    def _calculate_docstring_coverage(self, tree: ast.AST) -> float:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ³ã‚° ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—"""
        functions = [node for node in ast.walk(tree) if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))]
        classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
        
        total_items = len(functions) + len(classes)
        if total_items == 0:
            return 1.0  # é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹ãŒãªã„å ´åˆã¯100%
        
        documented = 0
        for item in functions + classes:
            if (item.body and isinstance(item.body[0], ast.Expr) and 
                isinstance(item.body[0].value, ast.Constant) and
                isinstance(item.body[0].value.value, str)):
                documented += 1
        
        return documented / total_items
    
    def _evaluate_variable_names(self, tree: ast.AST) -> float:
        """å¤‰æ•°åå“è³ªè©•ä¾¡"""
        names = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Name):
                names.append(node.id)
        
        if not names:
            return 1.0
        
        good_names = 0
        for name in names:
            if len(name) > 2 and name.islower() and '_' in name or name.isupper():
                good_names += 1
        
        return good_names / len(names)
    
    def _calculate_avg_function_length(self, tree: ast.AST, lines: List[str]) -> float:
        """å¹³å‡é–¢æ•°é•·è¨ˆç®—"""
        functions = [node for node in ast.walk(tree) if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))]
        
        if not functions:
            return 0.0
        
        lengths = []
        for func in functions:
            start_line = func.lineno
            end_line = func.end_lineno or start_line
            length = end_line - start_line + 1
            lengths.append(length)
        
        return statistics.mean(lengths) if lengths else 0.0
    
    def _calculate_avg_parameter_count(self, tree: ast.AST) -> float:
        """å¹³å‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æ•°è¨ˆç®—"""
        functions = [node for node in ast.walk(tree) if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))]
        
        if not functions:
            return 0.0
        
        param_counts = [len(func.args.args) for func in functions]
        return statistics.mean(param_counts) if param_counts else 0.0
    
    def _calculate_loop_complexity(self, tree: ast.AST) -> int:
        """ãƒ«ãƒ¼ãƒ—è¤‡é›‘åº¦è¨ˆç®—"""
        loops = [node for node in ast.walk(tree) if isinstance(node, (ast.For, ast.AsyncFor, ast.While))]
        return len(loops)
    
    def _count_nested_loops(self, tree: ast.AST) -> int:
        """ãƒã‚¹ãƒˆãƒ«ãƒ¼ãƒ—æ•°è¨ˆç®—"""
        nested_count = 0
        
        def count_nested(node, in_loop=False):
            nonlocal nested_count
            
            if isinstance(node, (ast.For, ast.AsyncFor, ast.While)):
                if in_loop:
                    nested_count += 1
                in_loop = True
            
            for child in ast.iter_child_nodes(node):
                count_nested(child, in_loop)
        
        count_nested(tree)
        return nested_count
    
    def _count_recursive_functions(self, tree: ast.AST) -> int:
        """å†å¸°é–¢æ•°æ•°ã‚«ã‚¦ãƒ³ãƒˆ"""
        functions = [node for node in ast.walk(tree) if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))]
        recursive_count = 0
        
        for func in functions:
            func_name = func.name
            for node in ast.walk(func):
                if isinstance(node, ast.Call) and isinstance(node.func, ast.Name):
                    if node.func.id == func_name:
                        recursive_count += 1
                        break
        
        return recursive_count
    
    def _count_dangerous_functions(self, code: str) -> int:
        """å±é™ºé–¢æ•°ã‚«ã‚¦ãƒ³ãƒˆ"""
        dangerous_patterns = [
            r'os\.system\s*\(',
            r'eval\s*\(',
            r'exec\s*\(',
            r'subprocess\.call\s*\(',
            r'input\s*\(',
        ]
        
        count = 0
        for pattern in dangerous_patterns:
            count += len(re.findall(pattern, code))
        
        return count
    
    def _calculate_input_validation_ratio(self, tree: ast.AST) -> float:
        """å…¥åŠ›æ¤œè¨¼æ¯”ç‡è¨ˆç®—"""
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸå®Ÿè£…
        # å®Ÿéš›ã«ã¯ã€å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã«å¯¾ã™ã‚‹æ¤œè¨¼ã®å‰²åˆã‚’è¨ˆç®—
        functions = [node for node in ast.walk(tree) if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))]
        
        if not functions:
            return 1.0
        
        validated_functions = 0
        for func in functions:
            # é–¢æ•°å†…ã«ifæ–‡ãŒã‚ã‚‹å ´åˆã¯æ¤œè¨¼ãŒã‚ã‚‹ã¨ä»®å®š
            for node in ast.walk(func):
                if isinstance(node, ast.If):
                    validated_functions += 1
                    break
        
        return validated_functions / len(functions)
    
    def _count_hardcoded_secrets(self, code: str) -> int:
        """ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆæ•°"""
        secret_patterns = [
            r'password\s*=\s*[\'"][^\'"]{6,}[\'"]',
            r'api_key\s*=\s*[\'"][^\'"]{10,}[\'"]',
            r'secret\s*=\s*[\'"][^\'"]{8,}[\'"]',
            r'token\s*=\s*[\'"][^\'"]{12,}[\'"]',
        ]
        
        count = 0
        for pattern in secret_patterns:
            count += len(re.findall(pattern, code, re.IGNORECASE))
        
        return count
    
    def _count_external_dependencies(self, tree: ast.AST) -> int:
        """å¤–éƒ¨ä¾å­˜é–¢ä¿‚ã‚«ã‚¦ãƒ³ãƒˆ"""
        imports = [node for node in ast.walk(tree) if isinstance(node, (ast.Import, ast.ImportFrom))]
        
        external_count = 0
        standard_libs = {'os', 'sys', 'time', 'datetime', 're', 'math', 'random', 'json'}
        
        for imp in imports:
            if isinstance(imp, ast.Import):
                for alias in imp.names:
                    if alias.name not in standard_libs:
                        external_count += 1
            elif isinstance(imp, ast.ImportFrom):
                if imp.module and imp.module not in standard_libs:
                    external_count += 1
        
        return external_count
    
    def _calculate_coupling_factor(self, tree: ast.AST) -> float:
        """çµåˆåº¦è¨ˆç®—"""
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸå®Ÿè£…
        classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
        
        if len(classes) <= 1:
            return 0.0
        
        # ã‚¯ãƒ©ã‚¹é–“ã®ç›¸äº’å‚ç…§æ•°ã‚’è¨ˆç®—
        coupling_count = 0
        class_names = [cls.name for cls in classes]
        
        for cls in classes:
            for node in ast.walk(cls):
                if isinstance(node, ast.Name) and node.id in class_names:
                    coupling_count += 1
        
        max_possible_coupling = len(classes) * (len(classes) - 1)
        return coupling_count / max(max_possible_coupling, 1)
    
    def _calculate_inheritance_depth(self, tree: ast.AST) -> int:
        """ç¶™æ‰¿æ·±åº¦è¨ˆç®—"""
        classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
        
        max_depth = 0
        for cls in classes:
            depth = len(cls.bases)  # ç°¡ç•¥åŒ–: ç›´æ¥ã®åŸºåº•ã‚¯ãƒ©ã‚¹æ•°
            max_depth = max(max_depth, depth)
        
        return max_depth
    
    def _count_method_overrides(self, tree: ast.AST) -> int:
        """ãƒ¡ã‚½ãƒƒãƒ‰ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰æ•°"""
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸå®Ÿè£…
        methods = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        
        override_count = 0
        special_methods = ['__init__', '__str__', '__repr__', '__eq__', '__hash__']
        
        for method in methods:
            if method.name in special_methods:
                override_count += 1
        
        return override_count


# Ancient Elder Magic Integration
def predict_ancient_elder_quality(code: str) -> PredictionResult:
    """
    ğŸ›ï¸ Ancient Elderé­”æ³•çµ±åˆäºˆæ¸¬é–¢æ•°
    
    Args:
        code: äºˆæ¸¬å¯¾è±¡ã‚³ãƒ¼ãƒ‰
        
    Returns:
        PredictionResult: Ancient Elderæ‰¿èªæ¸ˆã¿äºˆæ¸¬çµæœ
    """
    engine = PredictiveQualityEngine()
    result = engine.predict_quality(code)
    
    logger.info(f"ğŸ›ï¸ Ancient Elder quality prediction completed: "
               f"Bug Probability={result.bug_probability:.1f}%, "
               f"Performance Risk={result.performance_risk:.1f}%, "
               f"Security Risk={result.security_risk:.1f}%")
    
    return result


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    engine = PredictiveQualityEngine()
    
    test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

def process_data(data):
    result = []
    for item in data:
        for subitem in item:
            if subitem > 0:
                result.append(subitem * 2)
    return result
"""
    
    result = engine.predict_quality(test_code)
    print(f"ğŸ§  Prediction Result:")
    print(f"   Bug Probability: {result.bug_probability:.1f}%")
    print(f"   Performance Risk: {result.performance_risk:.1f}%")
    print(f"   Security Risk: {result.security_risk:.1f}%")
    print(f"   Maintainability Score: {result.maintainability_score:.1f}")
    print(f"   Quality Trend: {result.quality_trend}")
    print(f"   Confidence: {result.confidence:.1f}%")