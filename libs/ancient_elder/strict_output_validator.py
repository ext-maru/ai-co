#!/usr/bin/env python3
"""
ğŸ›ï¸ ã‚¨ãƒ³ã‚·ã‚§ãƒ³ãƒˆã‚¨ãƒ«ãƒ€ãƒ¼å¤ä»£é­”æ³• - StrictOutputValidator

ç”Ÿæˆç‰©ã¸ã®å³æ ¼æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
Tier 3: Output Validation Engine (ç”Ÿæˆç‰©æ¤œè¨¼ã‚¨ãƒ³ã‚¸ãƒ³)

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ä»¥ä¸‹ã®å³æ ¼ãƒ†ã‚¹ãƒˆã‚’å®Ÿè£…ï¼š
- æ§‹æ–‡å®Œç’§æ€§ãƒã‚§ãƒƒã‚¯
- è«–ç†ä¸€è²«æ€§æ¤œè¨¼
- æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¾µå…¥ãƒ†ã‚¹ãƒˆ
- ä¿å®ˆæ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è§£æ
"""

import ast
import re
import time
import inspect
import tokenize
import io
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass
from enum import Enum
import logging
import statistics
from pathlib import Path
import traceback

# ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰å“è³ªã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆ
try:
    from libs.elders_code_quality_engine import EldersCodeQualityEngine
except ImportError:
    logging.warning("EldersCodeQualityEngine not available")
    EldersCodeQualityEngine = None

# æ—¢å­˜å¤ä»£é­”æ³•ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆ
try:
    from libs.ancient_elder.base import AncientMagicBase
except ImportError:
    logging.warning("AncientMagicBase not available")
    AncientMagicBase = object

@dataclass
class ValidationResult:
    """æ¤œè¨¼çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    is_valid: bool
    score: float  # 0-100
    issues: List[Dict[str, Any]]
    suggestions: List[str]
    execution_time: float
    details: Dict[str, Any]

class SecurityRiskLevel(Enum):
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«"""
    NONE = 0
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4

class StrictOutputValidator(AncientMagicBase):
    """ğŸ›¡ï¸ å³æ ¼ãªç”Ÿæˆç‰©æ¤œè¨¼ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        """ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–"""
        super().__init__()
        
        # EldersCodeQualityEngineã®å®‰å…¨ãªåˆæœŸåŒ–
        try:
            if EldersCodeQualityEngine:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®åˆæœŸåŒ–ã‚’è©¦è¡Œ
                self.quality_engine = EldersCodeQualityEngine({})
            else:
                self.quality_engine = None
        except Exception as e:
            logging.warning(f"Could not initialize EldersCodeQualityEngine: {e}")
            self.quality_engine = None
            
        self.security_patterns = self._load_security_patterns()
        self.performance_thresholds = self._load_performance_thresholds()
        
        # æ¤œè¨¼çµ±è¨ˆ
        self.validation_history = []
        
    def validate_code_output(self, code_output: str) -> ValidationResult:
        """ã‚³ãƒ¼ãƒ‰ç”Ÿæˆç‰©ã®å³æ ¼æ¤œè¨¼
        
        Args:
            code_output: æ¤œè¨¼å¯¾è±¡ã®ã‚³ãƒ¼ãƒ‰æ–‡å­—åˆ—
            
        Returns:
            ValidationResult: åŒ…æ‹¬çš„æ¤œè¨¼çµæœ
        """
        start_time = time.time()
        
        try:
            checks = [
                self._syntax_perfection_check(code_output),
                self._logic_consistency_check(code_output),
                self._performance_benchmark(code_output),
                self._security_penetration_test(code_output),
                self._maintainability_score(code_output),
                self._scalability_analysis(code_output),
            ]
            
            result = self._comprehensive_evaluation(checks, code_output)
            result.execution_time = time.time() - start_time
            
            # æ¤œè¨¼å±¥æ­´ã«è¿½åŠ 
            self.validation_history.append(result)
            
            return result
            
        except Exception as e:
            return ValidationResult(
                is_valid=False,
                score=0.0,
                issues=[{"type": "validation_error", "message": str(e)}],
                suggestions=["Fix validation error"],
                execution_time=time.time() - start_time,
                details={"error": str(e), "traceback": traceback.format_exc()}
            )
    
    def _syntax_perfection_check(self, code: str) -> Dict[str, Any]:
        """æ§‹æ–‡å®Œç’§æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            # ASTãƒ‘ãƒ¼ã‚¹è©¦è¡Œ
            tree = ast.parse(code)
            
            # æ§‹æ–‡è§£æè©³ç´°ãƒã‚§ãƒƒã‚¯
            issues = []
            
            # ä¸å®Œå…¨ãªæ–‡ã®æ¤œå‡º
            for node in ast.walk(tree):
                if isinstance(node, ast.If) and not node.body:
                    issues.append({
                        "type": "incomplete_if",
                        "line": node.lineno,
                        "message": "Empty if statement body"
                    })
                elif isinstance(node, ast.FunctionDef) and not node.body:
                    issues.append({
                        "type": "empty_function",
                        "line": node.lineno,
                        "message": "Empty function body"
                    })
            
            return {
                "check_name": "syntax_perfection",
                "passed": len(issues) == 0,
                "score": max(0, 100 - len(issues) * 10),
                "issues": issues,
                "details": {"ast_nodes": len(list(ast.walk(tree)))}
            }
            
        except SyntaxError as e:
            return {
                "check_name": "syntax_perfection",
                "passed": False,
                "score": 0,
                "issues": [{
                    "type": "syntax_error",
                    "line": e.lineno,
                    "message": str(e)
                }],
                "details": {"syntax_error": str(e)}
            }
    
    def _logic_consistency_check(self, code: str) -> Dict[str, Any]:
        """è«–ç†ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            tree = ast.parse(code)
            issues = []
            
            # è«–ç†çš„ä¸æ•´åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    # é–¢æ•°ã®è«–ç†ãƒã‚§ãƒƒã‚¯
                    func_issues = self._check_function_logic(node)
                    issues.extend(func_issues)
            
            return {
                "check_name": "logic_consistency",
                "passed": len(issues) == 0,
                "score": max(0, 100 - len(issues) * 15),
                "issues": issues,
                "details": {"functions_checked": len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)])}
            }
            
        except Exception as e:
            return {
                "check_name": "logic_consistency",
                "passed": False,
                "score": 0,
                "issues": [{"type": "analysis_error", "message": str(e)}],
                "details": {"error": str(e)}
            }
    
    def _check_function_logic(self, func_node: ast.FunctionDef) -> List[Dict[str, Any]]:
        """é–¢æ•°ã®è«–ç†ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        # æ­£æ•°ã‚’è² æ•°ã§è¿”ã™ãƒ‘ã‚¿ãƒ¼ãƒ³ãªã©ã®æ¤œå‡º
        for node in ast.walk(func_node):
            if isinstance(node, ast.Return) and isinstance(node.value, ast.BinOp):
                if isinstance(node.value.op, ast.Mult):
                    # x * -1 ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
                    if (isinstance(node.value.right, ast.UnaryOp) and 
                        isinstance(node.value.right.op, ast.USub)):
                        issues.append({
                            "type": "logic_inconsistency",
                            "line": node.lineno,
                            "message": "Potential logic inconsistency: positive input to negative output"
                        })
        
        return issues
    
    def _performance_benchmark(self, code: str) -> Dict[str, Any]:
        """æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        try:
            tree = ast.parse(code)
            issues = []
            complexity_score = 0
            
            # ã‚µã‚¤ã‚¯ãƒ­ãƒãƒ†ã‚£ãƒƒã‚¯è¤‡é›‘åº¦ã®è¨ˆç®—
            for node in ast.walk(tree):
                if isinstance(node, (ast.If, ast.While, ast.For)):
                    complexity_score += 1
                elif isinstance(node, ast.FunctionDef):
                    # ãƒã‚¹ãƒˆã—ãŸãƒ«ãƒ¼ãƒ—ã®æ¤œå‡ºï¼ˆO(nÂ²)ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
                    nested_loops = self._detect_nested_loops(node)
                    if nested_loops > 1:
                        issues.append({
                            "type": "performance_issue",
                            "line": node.lineno,
                            "message": f"Detected O(n^{nested_loops}) complexity in function {node.name}"
                        })
            
            # æ€§èƒ½ã‚¹ã‚³ã‚¢è¨ˆç®—
            performance_score = max(0, 100 - complexity_score * 5 - len(issues) * 20)
            
            return {
                "check_name": "performance_benchmark",
                "passed": len(issues) == 0,
                "score": performance_score,
                "issues": issues,
                "details": {
                    "complexity_score": complexity_score,
                    "nested_loops_detected": len(issues)
                }
            }
            
        except Exception as e:
            return {
                "check_name": "performance_benchmark",
                "passed": False,
                "score": 50,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢
                "issues": [{"type": "analysis_error", "message": str(e)}],
                "details": {"error": str(e)}
            }
    
    def _detect_nested_loops(self, func_node: ast.FunctionDef) -> int:
        """ãƒã‚¹ãƒˆã—ãŸãƒ«ãƒ¼ãƒ—ã®æ·±åº¦ã‚’æ¤œå‡º"""
        max_depth = 0
        
        def count_depth(node, current_depth=0):
            nonlocal max_depth
            if isinstance(node, (ast.For, ast.While)):
                current_depth += 1
                max_depth = max(max_depth, current_depth)
            
            for child in ast.iter_child_nodes(node):
                count_depth(child, current_depth)
        
        count_depth(func_node)
        return max_depth
    
    def _security_penetration_test(self, code: str) -> Dict[str, Any]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¾µå…¥ãƒ†ã‚¹ãƒˆ"""
        issues = []
        risk_level = SecurityRiskLevel.NONE
        
        # å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        dangerous_patterns = [
            (r'os\.system\s*\(', SecurityRiskLevel.CRITICAL, "Arbitrary command execution"),
            (r'eval\s*\(', SecurityRiskLevel.CRITICAL, "Code injection vulnerability"),
            (r'exec\s*\(', SecurityRiskLevel.HIGH, "Dynamic code execution"),
            (r'subprocess\.call\s*\(', SecurityRiskLevel.MEDIUM, "Subprocess execution"),
            (r'__import__\s*\(', SecurityRiskLevel.MEDIUM, "Dynamic import"),
        ]
        
        for pattern, level, message in dangerous_patterns:
            matches = re.finditer(pattern, code)
            for match in matches:
                line_no = code[:match.start()].count('\n') + 1
                issues.append({
                    "type": "security_risk",
                    "line": line_no,
                    "level": level.name,
                    "message": message,
                    "pattern": pattern
                })
                if level.value > risk_level.value:
                    risk_level = level
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢è¨ˆç®—
        security_score = max(0, 100 - sum(issue.get("level_value", 0) for issue in issues))
        
        return {
            "check_name": "security_penetration",
            "passed": risk_level == SecurityRiskLevel.NONE,
            "score": security_score,
            "issues": issues,
            "details": {
                "max_risk_level": risk_level.name,
                "patterns_checked": len(dangerous_patterns)
            }
        }
    
    def _maintainability_score(self, code: str) -> Dict[str, Any]:
        """ä¿å®ˆæ€§ã‚¹ã‚³ã‚¢"""
        try:
            tree = ast.parse(code)
            issues = []
            
            # ä¿å®ˆæ€§ã®å•é¡Œã‚’æ¤œå‡º
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãƒã‚§ãƒƒã‚¯
                    if len(node.args.args) > 10:
                        issues.append({
                            "type": "too_many_parameters",
                            "line": node.lineno,
                            "message": f"Function {node.name} has {len(node.args.args)} parameters (max: 10)"
                        })
                    
                    # é–¢æ•°ã®è¤‡é›‘åº¦ãƒã‚§ãƒƒã‚¯
                    lines = len(code.split('\n'))
                    if lines > 50:
                        issues.append({
                            "type": "long_function",
                            "line": node.lineno,
                            "message": f"Function {node.name} is too long ({lines} lines)"
                        })
            
            maintainability_score = max(0, 100 - len(issues) * 15)
            
            return {
                "check_name": "maintainability_score",
                "passed": len(issues) == 0,
                "score": maintainability_score,
                "issues": issues,
                "details": {"total_lines": len(code.split('\n'))}
            }
            
        except Exception as e:
            return {
                "check_name": "maintainability_score",
                "passed": False,
                "score": 50,
                "issues": [{"type": "analysis_error", "message": str(e)}],
                "details": {"error": str(e)}
            }
    
    def _scalability_analysis(self, code: str) -> Dict[str, Any]:
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è§£æ"""
        issues = []
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’é˜»å®³ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        scalability_patterns = [
            (r'global\s+\w+', "Global variable usage reduces scalability"),
            (r'singleton', "Singleton pattern may limit scalability"),
            (r'time\.sleep\s*\(', "Blocking sleep calls reduce scalability"),
        ]
        
        for pattern, message in scalability_patterns:
            matches = re.finditer(pattern, code, re.IGNORECASE)
            for match in matches:
                line_no = code[:match.start()].count('\n') + 1
                issues.append({
                    "type": "scalability_issue",
                    "line": line_no,
                    "message": message,
                    "pattern": pattern
                })
        
        scalability_score = max(0, 100 - len(issues) * 20)
        
        return {
            "check_name": "scalability_analysis",
            "passed": len(issues) == 0,
            "score": scalability_score,
            "issues": issues,
            "details": {"patterns_checked": len(scalability_patterns)}
        }
    
    def _comprehensive_evaluation(self, checks: List[Dict[str, Any]], code: str) -> ValidationResult:
        """åŒ…æ‹¬çš„è©•ä¾¡"""
        all_issues = []
        all_suggestions = []
        scores = []
        
        for check in checks:
            all_issues.extend(check.get("issues", []))
            scores.append(check.get("score", 0))
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        overall_score = statistics.mean(scores) if scores else 0
        
        # ææ¡ˆç”Ÿæˆ
        if overall_score < 70:
            all_suggestions.append("Code quality is below acceptable threshold")
        if any(issue.get("type") == "security_risk" for issue in all_issues):
            all_suggestions.append("Address security vulnerabilities immediately")
        if any(issue.get("type") == "performance_issue" for issue in all_issues):
            all_suggestions.append("Optimize performance bottlenecks")
        
        return ValidationResult(
            is_valid=overall_score >= 70,
            score=overall_score,
            issues=all_issues,
            suggestions=all_suggestions,
            execution_time=0.0,  # å‘¼ã³å‡ºã—å…ƒã§è¨­å®š
            details={
                "individual_scores": {check["check_name"]: check["score"] for check in checks},
                "total_checks": len(checks),
                "code_length": len(code)
            }
        )
    
    def validate_design_output(self, design_output: Dict[str, Any]) -> ValidationResult:
        """è¨­è¨ˆç”Ÿæˆç‰©ã®å³æ ¼æ¤œè¨¼"""
        start_time = time.time()
        
        checks = [
            self._architecture_soundness(design_output),
            self._design_pattern_compliance(design_output),
            self._future_extensibility(design_output),
            self._technical_debt_prediction(design_output),
        ]
        
        all_issues = []
        scores = []
        
        for check in checks:
            all_issues.extend(check.get("issues", []))
            scores.append(check.get("score", 0))
        
        overall_score = statistics.mean(scores) if scores else 0
        
        return ValidationResult(
            is_valid=overall_score >= 70,
            score=overall_score,
            issues=all_issues,
            suggestions=["Review design architecture", "Consider scalability"],
            execution_time=time.time() - start_time,
            details={"design_type": "architecture"}
        )
    
    def _architecture_soundness(self, design: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        issues = []
        score = 80  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹é€ ã®ãƒã‚§ãƒƒã‚¯
        if "layers" not in design:
            issues.append({
                "type": "missing_layers",
                "message": "Architecture layers not defined"
            })
            score -= 20
        
        return {
            "check_name": "architecture_soundness",
            "passed": len(issues) == 0,
            "score": max(0, score),
            "issues": issues,
            "details": {"design_keys": list(design.keys())}
        }
    
    def _design_pattern_compliance(self, design: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³æº–æ‹ æ€§ãƒã‚§ãƒƒã‚¯"""
        issues = []
        score = 75
        
        patterns = design.get("patterns", [])
        if not patterns:
            issues.append({
                "type": "no_patterns",
                "message": "No design patterns specified"
            })
            score -= 25
        
        return {
            "check_name": "design_pattern_compliance",
            "passed": len(issues) == 0,
            "score": max(0, score),
            "issues": issues,
            "details": {"patterns": patterns}
        }
    
    def _future_extensibility(self, design: Dict[str, Any]) -> Dict[str, Any]:
        """å°†æ¥æ‹¡å¼µæ€§ãƒã‚§ãƒƒã‚¯"""
        score = 70
        issues = []
        
        # æ‹¡å¼µæ€§æŒ‡æ¨™ã®ãƒã‚§ãƒƒã‚¯
        if "extensibility" not in design:
            issues.append({
                "type": "no_extensibility_plan",
                "message": "Future extensibility not considered"
            })
            score -= 30
        
        return {
            "check_name": "future_extensibility",
            "passed": len(issues) == 0,
            "score": max(0, score),
            "issues": issues,
            "details": {}
        }
    
    def _technical_debt_prediction(self, design: Dict[str, Any]) -> Dict[str, Any]:
        """æŠ€è¡“è² å‚µäºˆæ¸¬"""
        debt_indicators = [
            "TODO", "HACK", "FIXME", "TEMPORARY", "WORKAROUND"
        ]
        
        issues = []
        debt_score = 0
        
        # æ–‡å­—åˆ—è¡¨ç¾ã§ã®æŠ€è¡“è² å‚µæŒ‡æ¨™ãƒã‚§ãƒƒã‚¯
        design_str = str(design)
        for indicator in debt_indicators:
            if indicator in design_str.upper():
                issues.append({
                    "type": "technical_debt",
                    "message": f"Technical debt indicator found: {indicator}"
                })
                debt_score += 10
        
        score = max(0, 100 - debt_score)
        
        return {
            "check_name": "technical_debt_prediction",
            "passed": len(issues) == 0,
            "score": score,
            "issues": issues,
            "details": {"debt_indicators_checked": len(debt_indicators)}
        }
    
    def _load_security_patterns(self) -> List[str]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ã‚¿ãƒ¼ãƒ³ã®èª­ã¿è¾¼ã¿"""
        return [
            r'os\.system',
            r'eval\s*\(',
            r'exec\s*\(',
            r'subprocess\.call',
            r'__import__'
        ]
    
    def _load_performance_thresholds(self) -> Dict[str, Any]:
        """æ€§èƒ½é–¾å€¤ã®èª­ã¿è¾¼ã¿"""
        return {
            "max_complexity": 10,
            "max_nested_loops": 2,
            "max_function_lines": 50
        }
    
    def get_validation_statistics(self) -> Dict[str, Any]:
        """æ¤œè¨¼çµ±è¨ˆæƒ…å ±å–å¾—"""
        if not self.validation_history:
            return {"message": "No validation history available"}
        
        scores = [result.score for result in self.validation_history]
        
        return {
            "total_validations": len(self.validation_history),
            "average_score": statistics.mean(scores),
            "median_score": statistics.median(scores),
            "min_score": min(scores),
            "max_score": max(scores),
            "success_rate": len([r for r in self.validation_history if r.is_valid]) / len(self.validation_history)
        }

# ä¾¿åˆ©é–¢æ•°
def validate_code(code: str) -> ValidationResult:
    """ã‚³ãƒ¼ãƒ‰æ¤œè¨¼ã®ä¾¿åˆ©é–¢æ•°"""
    validator = StrictOutputValidator()
    return validator.validate_code_output(code)

def validate_design(design: Dict[str, Any]) -> ValidationResult:
    """è¨­è¨ˆæ¤œè¨¼ã®ä¾¿åˆ©é–¢æ•°"""
    validator = StrictOutputValidator()
    return validator.validate_design_output(design)

if __name__ == "__main__":
    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    sample_code = """
def hello_world():
    print("Hello, World!")
    return True
"""
    
    validator = StrictOutputValidator()
    result = validator.validate_code_output(sample_code)
    
    print(f"Validation Result: {result.is_valid}")
    print(f"Score: {result.score:.2f}")
    print(f"Issues: {len(result.issues)}")
    print(f"Execution Time: {result.execution_time:.4f}s")