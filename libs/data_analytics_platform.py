#!/usr/bin/env python3
"""
é«˜åº¦ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆåˆ†æãƒ»äºˆæ¸¬ã™ã‚‹åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ 

è¨­è¨ˆ: RAGã‚¨ãƒ«ãƒ€ãƒ¼ Ã— ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼
æ‰¿èª: ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºè©•è­°ä¼šï¼ˆäºˆå®šï¼‰
å®Ÿè£…æ—¥: 2025å¹´7æœˆ9æ—¥
"""

import asyncio
import json
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import pandas as pd
import numpy as np
from dataclasses import dataclass
from enum import Enum
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AnalyticsType(Enum):
    """åˆ†æã‚¿ã‚¤ãƒ—"""
    COMMIT_PATTERN = "commit_pattern"      # ã‚³ãƒŸãƒƒãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    SAGE_PERFORMANCE = "sage_performance"  # 4è³¢è€…ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
    SYSTEM_HEALTH = "system_health"        # ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹äºˆæ¸¬
    PROTOCOL_EFFICIENCY = "protocol_efficiency"  # ãƒ—ãƒ­ãƒˆã‚³ãƒ«åŠ¹ç‡åˆ†æ
    ERROR_PREDICTION = "error_prediction"  # ã‚¨ãƒ©ãƒ¼äºˆæ¸¬
    BOTTLENECK_DETECTION = "bottleneck_detection"  # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º

@dataclass
class AnalyticsResult:
    """åˆ†æçµæœ"""
    type: AnalyticsType
    timestamp: datetime
    metrics: Dict[str, Any]
    insights: List[str]
    predictions: Dict[str, Any]
    recommendations: List[str]
    confidence: float

class DataCollector:
    """ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.logs_dir = project_root / "logs"
        self.db_path = project_root / "elder_dashboard.db"
        
    async def collect_commit_data(self) -> pd.DataFrame:
        """ã‚³ãƒŸãƒƒãƒˆãƒ‡ãƒ¼ã‚¿åé›†"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            
            # ãƒ—ãƒ­ãƒˆã‚³ãƒ«å±¥æ­´ã‚’å–å¾—
            query = """
                SELECT 
                    timestamp,
                    protocol,
                    message,
                    approved,
                    execution_time,
                    sage_count,
                    risk_score,
                    files_changed,
                    complexity
                FROM protocol_history
                ORDER BY timestamp
            """
            
            df = pd.read_sql_query(query, conn)
            conn.close()
            
            # æ—¥æ™‚å‹ã«å¤‰æ›
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            logger.info(f"ğŸ“Š {len(df)}ä»¶ã®ã‚³ãƒŸãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åé›†")
            return df
            
        except Exception as e:
            logger.error(f"âŒ ã‚³ãƒŸãƒƒãƒˆãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    async def collect_sage_consultation_data(self) -> pd.DataFrame:
        """4è³¢è€…ç›¸è«‡ãƒ‡ãƒ¼ã‚¿åé›†"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            
            query = """
                SELECT 
                    sc.sage_name,
                    sc.approval,
                    sc.risk_score,
                    sc.advice,
                    sc.timestamp,
                    ph.protocol,
                    ph.complexity
                FROM sage_consultations sc
                JOIN protocol_history ph ON sc.protocol_id = ph.id
                ORDER BY sc.timestamp
            """
            
            df = pd.read_sql_query(query, conn)
            conn.close()
            
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            logger.info(f"ğŸ§™â€â™‚ï¸ {len(df)}ä»¶ã®è³¢è€…ç›¸è«‡ãƒ‡ãƒ¼ã‚¿ã‚’åé›†")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è³¢è€…ç›¸è«‡ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    async def collect_system_metrics(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        metrics = {
            "timestamp": datetime.now(),
            "active_workers": 0,
            "error_logs": 0,
            "warning_logs": 0,
            "total_log_files": 0,
            "disk_usage_mb": 0
        }
        
        try:
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ
            log_files = list(self.logs_dir.glob("*.log"))
            metrics["total_log_files"] = len(log_files)
            
            # ã‚¨ãƒ©ãƒ¼ãƒ»è­¦å‘Šã‚«ã‚¦ãƒ³ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
            for log_file in log_files[:10]:  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                try:
                    with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        metrics["error_logs"] += content.count("ERROR")
                        metrics["warning_logs"] += content.count("WARNING")
                except:
                    pass
            
            # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡
            total_size = sum(f.stat().st_size for f in log_files)
            metrics["disk_usage_mb"] = total_size / (1024 * 1024)
            
            logger.info(f"ğŸ“ˆ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†å®Œäº†")
            return metrics
            
        except Exception as e:
            logger.error(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return metrics

class AnalyticsEngine:
    """åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.ml_models = {}  # æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ ¼ç´ç”¨
        
    async def analyze_commit_patterns(self, df: pd.DataFrame) -> AnalyticsResult:
        """ã‚³ãƒŸãƒƒãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        insights = []
        predictions = {}
        metrics = {}
        
        if df.empty:
            return self._empty_result(AnalyticsType.COMMIT_PATTERN)
        
        # åŸºæœ¬çµ±è¨ˆ
        metrics["total_commits"] = len(df)
        metrics["approval_rate"] = df['approved'].mean() * 100
        metrics["avg_execution_time"] = df['execution_time'].mean()
        metrics["avg_complexity"] = df['complexity'].mean()
        
        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«åˆ¥åˆ†æ
        protocol_stats = df.groupby('protocol').agg({
            'approved': ['count', 'mean'],
            'execution_time': 'mean',
            'complexity': 'mean'
        }).round(2)
        
        # MultiIndexã‚’å‡¦ç†ã—ã‚„ã™ã„å½¢ã«å¤‰æ›
        protocol_distribution = {}
        for protocol in protocol_stats.index:
            protocol_distribution[protocol] = {
                'count': int(protocol_stats.loc[protocol, ('approved', 'count')]),
                'approval_rate': float(protocol_stats.loc[protocol, ('approved', 'mean')]),
                'avg_execution_time': float(protocol_stats.loc[protocol, ('execution_time', 'mean')]),
                'avg_complexity': float(protocol_stats.loc[protocol, ('complexity', 'mean')])
            }
        
        metrics["protocol_distribution"] = protocol_distribution
        
        # æ™‚ç³»åˆ—åˆ†æ
        df['hour'] = df['timestamp'].dt.hour
        hourly_commits = df.groupby('hour').size()
        peak_hour = hourly_commits.idxmax()
        
        insights.append(f"ğŸ“Š ãƒ”ãƒ¼ã‚¯ã‚³ãƒŸãƒƒãƒˆæ™‚é–“: {peak_hour}æ™‚å°")
        insights.append(f"âš¡ å¹³å‡å®Ÿè¡Œæ™‚é–“: {metrics['avg_execution_time']:.1f}ç§’")
        
        # äºˆæ¸¬
        if len(df) > 10:
            # ç°¡æ˜“çš„ãªæ¬¡å›ã‚³ãƒŸãƒƒãƒˆæ™‚é–“äºˆæ¸¬
            commit_intervals = df['timestamp'].diff().dropna()
            avg_interval = commit_intervals.mean()
            next_commit = df['timestamp'].iloc[-1] + avg_interval
            predictions["next_commit_time"] = next_commit.isoformat()
            predictions["expected_protocol"] = df['protocol'].mode()[0]
        
        # æ¨å¥¨äº‹é …
        recommendations = []
        if metrics["avg_execution_time"] > 10:
            recommendations.append("ğŸš€ å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ãŸã‚ã€Lightning Protocol ã®æ´»ç”¨ã‚’æ¨å¥¨")
        if metrics["approval_rate"] < 90:
            recommendations.append("âš ï¸ æ‰¿èªç‡ãŒä½ä¸‹å‚¾å‘ã€‚å“è³ªãƒã‚§ãƒƒã‚¯ã®å¼·åŒ–ã‚’æ¨å¥¨")
        
        return AnalyticsResult(
            type=AnalyticsType.COMMIT_PATTERN,
            timestamp=datetime.now(),
            metrics=metrics,
            insights=insights,
            predictions=predictions,
            recommendations=recommendations,
            confidence=0.85
        )
    
    async def analyze_sage_performance(self, df: pd.DataFrame) -> AnalyticsResult:
        """4è³¢è€…ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        insights = []
        predictions = {}
        metrics = {}
        
        if df.empty:
            return self._empty_result(AnalyticsType.SAGE_PERFORMANCE)
        
        # è³¢è€…åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        sage_stats = df.groupby('sage_name').agg({
            'approval': ['count', 'mean'],
            'risk_score': 'mean'
        }).round(3)
        
        # MultiIndexã‚’å‡¦ç†ã—ã‚„ã™ã„å½¢ã«å¤‰æ›
        sage_performance = {}
        for sage in sage_stats.index:
            sage_performance[sage] = {
                'consultation_count': int(sage_stats.loc[sage, ('approval', 'count')]),
                'approval_rate': float(sage_stats.loc[sage, ('approval', 'mean')]),
                'avg_risk_score': float(sage_stats.loc[sage, ('risk_score', 'mean')])
            }
        
        metrics["sage_performance"] = sage_performance
        
        # è³¢è€…é–“ã®ç›¸é–¢åˆ†æ
        sage_approvals = df.pivot_table(
            index='timestamp', 
            columns='sage_name', 
            values='approval',
            aggfunc='mean'
        )
        
        if len(sage_approvals.columns) > 1:
            correlation = sage_approvals.corr()
            metrics["sage_correlation"] = correlation.to_dict()
            
            # é«˜ç›¸é–¢ãƒšã‚¢ã®æ¤œå‡º
            high_corr_pairs = []
            for i in range(len(correlation.columns)):
                for j in range(i+1, len(correlation.columns)):
                    corr_value = correlation.iloc[i, j]
                    if corr_value > 0.7:
                        high_corr_pairs.append({
                            "pair": f"{correlation.columns[i]} - {correlation.columns[j]}",
                            "correlation": corr_value
                        })
            
            if high_corr_pairs:
                insights.append(f"ğŸ¤ é«˜ç›¸é–¢è³¢è€…ãƒšã‚¢æ¤œå‡º: {len(high_corr_pairs)}çµ„")
        
        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«åˆ¥ã®è³¢è€…æ‰¿èªç‡
        protocol_sage_approval = df.groupby(['protocol', 'sage_name'])['approval'].mean()
        
        # MultiIndexã‚’å‡¦ç†ã—ã‚„ã™ã„å½¢ã«å¤‰æ›
        approval_dict = {}
        for (protocol, sage_name), approval_rate in protocol_sage_approval.items():
            if protocol not in approval_dict:
                approval_dict[protocol] = {}
            approval_dict[protocol][sage_name] = float(approval_rate)
        
        metrics["protocol_sage_approval"] = approval_dict
        
        # æ¨å¥¨äº‹é …
        recommendations = []
        for sage, stats in sage_stats.iterrows():
            approval_rate = stats[('approval', 'mean')] * 100
            if approval_rate < 80:
                recommendations.append(f"âš ï¸ {sage}ã®æ‰¿èªç‡ãŒ{approval_rate:.1f}%ã¨ä½ã„")
        
        return AnalyticsResult(
            type=AnalyticsType.SAGE_PERFORMANCE,
            timestamp=datetime.now(),
            metrics=metrics,
            insights=insights,
            predictions=predictions,
            recommendations=recommendations,
            confidence=0.90
        )
    
    async def predict_system_health(self, 
                                  commit_df: pd.DataFrame, 
                                  system_metrics: Dict) -> AnalyticsResult:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹äºˆæ¸¬"""
        insights = []
        predictions = {}
        metrics = {}
        
        # ç¾åœ¨ã®ãƒ˜ãƒ«ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—
        health_score = 100.0
        
        # ã‚¨ãƒ©ãƒ¼ç‡ã«ã‚ˆã‚‹æ¸›ç‚¹
        if system_metrics["error_logs"] > 100:
            health_score -= 20
            insights.append("âš ï¸ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ãŒå¤šæ•°æ¤œå‡º")
        
        # è­¦å‘Šç‡ã«ã‚ˆã‚‹æ¸›ç‚¹
        if system_metrics["warning_logs"] > 500:
            health_score -= 10
            insights.append("âš ï¸ è­¦å‘Šãƒ­ã‚°ãŒå¢—åŠ å‚¾å‘")
        
        # ã‚³ãƒŸãƒƒãƒˆæ‰¿èªç‡ã«ã‚ˆã‚‹è©•ä¾¡
        if not commit_df.empty:
            approval_rate = commit_df['approved'].mean() * 100
            if approval_rate < 80:
                health_score -= 15
                insights.append(f"ğŸ“‰ ã‚³ãƒŸãƒƒãƒˆæ‰¿èªç‡ãŒ{approval_rate:.1f}%ã¨ä½ä¸‹")
        
        metrics["current_health_score"] = health_score
        metrics["error_rate"] = system_metrics["error_logs"] / max(system_metrics["total_log_files"], 1)
        metrics["warning_rate"] = system_metrics["warning_logs"] / max(system_metrics["total_log_files"], 1)
        
        # ãƒ˜ãƒ«ã‚¹äºˆæ¸¬ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if health_score >= 80:
            predictions["next_24h_health"] = "è‰¯å¥½"
            predictions["maintenance_required"] = False
        elif health_score >= 60:
            predictions["next_24h_health"] = "æ³¨æ„"
            predictions["maintenance_required"] = True
            predictions["maintenance_type"] = "äºˆé˜²çš„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹"
        else:
            predictions["next_24h_health"] = "è¦å¯¾å¿œ"
            predictions["maintenance_required"] = True
            predictions["maintenance_type"] = "ç·Šæ€¥ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹"
        
        # æ¨å¥¨äº‹é …
        recommendations = []
        if health_score < 80:
            recommendations.append("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã®å®Ÿè¡Œã‚’æ¨å¥¨")
        if metrics["error_rate"] > 0.1:
            recommendations.append("ğŸš¨ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®è©³ç´°åˆ†æãŒå¿…è¦")
        
        return AnalyticsResult(
            type=AnalyticsType.SYSTEM_HEALTH,
            timestamp=datetime.now(),
            metrics=metrics,
            insights=insights,
            predictions=predictions,
            recommendations=recommendations,
            confidence=0.75
        )
    
    def _empty_result(self, analytics_type: AnalyticsType) -> AnalyticsResult:
        """ç©ºã®çµæœã‚’è¿”ã™"""
        return AnalyticsResult(
            type=analytics_type,
            timestamp=datetime.now(),
            metrics={},
            insights=["ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™"],
            predictions={},
            recommendations=["ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿åé›†ãŒå¿…è¦ã§ã™"],
            confidence=0.0
        )

class PredictiveAnalytics:
    """äºˆæ¸¬åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.models = {}
        
    async def train_models(self, commit_df: pd.DataFrame, sage_df: pd.DataFrame):
        """äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        logger.info("ğŸ¤– äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´é–‹å§‹")
        
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ãªçµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        # å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯scikit-learnç­‰ã‚’ä½¿ç”¨
        
        if not commit_df.empty:
            # ã‚³ãƒŸãƒƒãƒˆé–“éš”äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
            commit_intervals = commit_df['timestamp'].diff().dropna()
            self.models['commit_interval'] = {
                'mean': commit_intervals.mean(),
                'std': commit_intervals.std()
            }
            
            # ãƒ—ãƒ­ãƒˆã‚³ãƒ«é¸æŠäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
            protocol_dist = commit_df['protocol'].value_counts(normalize=True)
            self.models['protocol_selection'] = protocol_dist.to_dict()
        
        logger.info("âœ… äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´å®Œäº†")
    
    async def predict_next_commit(self) -> Dict[str, Any]:
        """æ¬¡å›ã‚³ãƒŸãƒƒãƒˆäºˆæ¸¬"""
        predictions = {}
        
        if 'commit_interval' in self.models:
            model = self.models['commit_interval']
            # æ­£è¦åˆ†å¸ƒã‚’ä»®å®šã—ãŸäºˆæ¸¬
            next_interval = np.random.normal(
                model['mean'].total_seconds(),
                model['std'].total_seconds()
            )
            predictions['next_commit_in_seconds'] = max(0, next_interval)
            predictions['confidence'] = 0.7
        
        if 'protocol_selection' in self.models:
            # ç¢ºç‡ã«åŸºã¥ããƒ—ãƒ­ãƒˆã‚³ãƒ«äºˆæ¸¬
            protocols = list(self.models['protocol_selection'].keys())
            probabilities = list(self.models['protocol_selection'].values())
            predictions['likely_protocol'] = np.random.choice(protocols, p=probabilities)
        
        return predictions

class AnalyticsReporter:
    """åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.reports_dir = project_root / "analytics_reports"
        self.reports_dir.mkdir(exist_ok=True)
        
    async def generate_comprehensive_report(self, results: List[AnalyticsResult]) -> Path:
        """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        timestamp = datetime.now()
        report = {
            "title": "ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ",
            "generated_at": timestamp.isoformat(),
            "summary": self._generate_summary(results),
            "detailed_results": [self._result_to_dict(r) for r in results],
            "executive_insights": self._generate_executive_insights(results),
            "action_items": self._generate_action_items(results)
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_file = self.reports_dir / f"analytics_report_{timestamp.strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_file}")
        return report_file
    
    def _generate_summary(self, results: List[AnalyticsResult]) -> Dict[str, Any]:
        """ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        return {
            "total_analyses": len(results),
            "average_confidence": np.mean([r.confidence for r in results]),
            "key_findings": sum(len(r.insights) for r in results),
            "recommendations": sum(len(r.recommendations) for r in results)
        }
    
    def _result_to_dict(self, result: AnalyticsResult) -> Dict[str, Any]:
        """çµæœã‚’è¾æ›¸ã«å¤‰æ›"""
        return {
            "type": result.type.value,
            "timestamp": result.timestamp.isoformat(),
            "metrics": result.metrics,
            "insights": result.insights,
            "predictions": result.predictions,
            "recommendations": result.recommendations,
            "confidence": result.confidence
        }
    
    def _generate_executive_insights(self, results: List[AnalyticsResult]) -> List[str]:
        """ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–å‘ã‘æ´å¯Ÿ"""
        insights = []
        
        # å„åˆ†æçµæœã‹ã‚‰é‡è¦ãªæ´å¯Ÿã‚’æŠ½å‡º
        for result in results:
            if result.confidence > 0.8 and result.insights:
                insights.extend(result.insights[:2])  # ä¸Šä½2ã¤ã®æ´å¯Ÿ
        
        return insights[:5]  # æœ€å¤§5ã¤
    
    def _generate_action_items(self, results: List[AnalyticsResult]) -> List[str]:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ç”Ÿæˆ"""
        action_items = []
        
        # æ¨å¥¨äº‹é …ã‚’å„ªå…ˆåº¦ä»˜ã‘ã—ã¦é›†ç´„
        all_recommendations = []
        for result in results:
            for rec in result.recommendations:
                all_recommendations.append({
                    'recommendation': rec,
                    'confidence': result.confidence,
                    'type': result.type.value
                })
        
        # ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        all_recommendations.sort(key=lambda x: x['confidence'], reverse=True)
        
        # ä¸Šä½ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚’é¸æŠ
        for item in all_recommendations[:5]:
            action_items.append(f"[{item['type']}] {item['recommendation']}")
        
        return action_items

class DataAnalyticsPlatform:
    """é«˜åº¦ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.collector = DataCollector(project_root)
        self.analytics = AnalyticsEngine()
        self.predictive = PredictiveAnalytics()
        self.reporter = AnalyticsReporter(project_root)
        
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆæœŸåŒ–å®Œäº†")
    
    async def run_full_analysis(self) -> Path:
        """å®Œå…¨åˆ†æå®Ÿè¡Œ"""
        logger.info("ğŸš€ å®Œå…¨åˆ†æé–‹å§‹")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿åé›†ãƒ•ã‚§ãƒ¼ã‚º
            logger.info("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿åé›†ãƒ•ã‚§ãƒ¼ã‚º")
            commit_df = await self.collector.collect_commit_data()
            sage_df = await self.collector.collect_sage_consultation_data()
            system_metrics = await self.collector.collect_system_metrics()
            
            # äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            await self.predictive.train_models(commit_df, sage_df)
            
            # åˆ†æå®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º
            logger.info("ğŸ” åˆ†æå®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º")
            results = []
            
            # ã‚³ãƒŸãƒƒãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            commit_analysis = await self.analytics.analyze_commit_patterns(commit_df)
            results.append(commit_analysis)
            
            # 4è³¢è€…ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
            sage_analysis = await self.analytics.analyze_sage_performance(sage_df)
            results.append(sage_analysis)
            
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹äºˆæ¸¬
            health_prediction = await self.analytics.predict_system_health(commit_df, system_metrics)
            results.append(health_prediction)
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º
            logger.info("ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º")
            report_path = await self.reporter.generate_comprehensive_report(results)
            
            logger.info("âœ… å®Œå…¨åˆ†æå®Œäº†")
            return report_path
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            raise

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
async def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    platform = DataAnalyticsPlatform(Path("/home/aicompany/ai_co"))
    report_path = await platform.run_full_analysis()
    print(f"ğŸ“Š åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")

if __name__ == "__main__":
    asyncio.run(main())