#!/usr/bin/env python3
"""
Demand Predictor AI
éœ€è¦äºˆæ¸¬AIãƒ¢ãƒ‡ãƒ«

ğŸ”® nWo Prophetic Development Matrix - Demand Prediction AI
Think it, Rule it, Own it - æœªæ¥éœ€è¦äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import logging
# import pickle  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: pickleã®ä½¿ç”¨ã‚’å»ƒæ­¢
import re
from collections import defaultdict


class PredictionType(Enum):
    """äºˆæ¸¬ã‚¿ã‚¤ãƒ—"""

    TECHNOLOGY_DEMAND = "technology_demand"
    MARKET_TREND = "market_trend"
    SKILL_DEMAND = "skill_demand"
    FRAMEWORK_ADOPTION = "framework_adoption"
    TOOL_USAGE = "tool_usage"
    LANGUAGE_POPULARITY = "language_popularity"


class TimeFrame(Enum):
    """äºˆæ¸¬æœŸé–“"""

    SHORT_TERM = "short_term"  # 1-3ãƒ¶æœˆ
    MEDIUM_TERM = "medium_term"  # 3-12ãƒ¶æœˆ
    LONG_TERM = "long_term"  # 1-3å¹´


@dataclass
class DemandFeature:
    """éœ€è¦ç‰¹å¾´é‡"""

    name: str
    value: float
    importance: float
    category: str
    timestamp: str


@dataclass
class Prediction:
    """äºˆæ¸¬çµæœ"""

    target: str
    prediction_type: PredictionType
    timeframe: TimeFrame
    predicted_value: float
    confidence: float
    trend_direction: str  # "increasing", "decreasing", "stable"
    features_used: List[DemandFeature]
    model_version: str
    created_at: str


@dataclass
class PatternAnalysis:
    """ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""

    pattern_name: str
    description: str
    frequency: float
    correlation_strength: float
    examples: List[str]
    confidence: float


@dataclass
class ForecastReport:
    """äºˆæ¸¬ãƒ¬ãƒãƒ¼ãƒˆ"""

    timeframe: TimeFrame
    predictions: List[Prediction]
    pattern_analysis: List[PatternAnalysis]
    market_insights: List[str]
    recommendations: List[str]
    risk_factors: List[str]
    confidence_level: float
    generated_at: str


class DemandPredictorAI:
    """Demand Predictor AI - éœ€è¦äºˆæ¸¬AIã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, model_path: Optional[str] = None):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.logger = self._setup_logger()

        # ãƒ¢ãƒ‡ãƒ«ç®¡ç†
        self.model_path = (
            Path(model_path) if model_path else Path("models/demand_prediction.pkl")
        )
        self.model_path.parent.mkdir(parents=True, exist_ok=True)

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        self.training_data = []
        self.feature_importance = {}

        # äºˆæ¸¬å±¥æ­´
        self.prediction_history = []

        # ãƒ‘ã‚¿ãƒ¼ãƒ³è¾æ›¸
        self.demand_patterns = self._load_demand_patterns()

        # æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.tech_keywords = self._load_tech_keywords()

        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self.model = self._load_or_create_model()

        self.logger.info("ğŸ”® Demand Predictor AI v1.0 initialized")

    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼è¨­å®š"""
        logger = logging.getLogger("demand_predictor")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s - Demand Predictor - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def _load_demand_patterns(self) -> Dict[str, Dict]:
        """éœ€è¦ãƒ‘ã‚¿ãƒ¼ãƒ³è¾æ›¸"""
        return {
            "exponential_growth": {
                "description": "æŒ‡æ•°é–¢æ•°çš„æˆé•·ãƒ‘ã‚¿ãƒ¼ãƒ³",
                "indicators": ["rapid_adoption", "viral_spread", "network_effect"],
                "duration_months": [6, 24],
                "confidence_threshold": 0.8,
            },
            "linear_growth": {
                "description": "ç·šå½¢æˆé•·ãƒ‘ã‚¿ãƒ¼ãƒ³",
                "indicators": [
                    "steady_adoption",
                    "market_expansion",
                    "gradual_improvement",
                ],
                "duration_months": [12, 36],
                "confidence_threshold": 0.7,
            },
            "s_curve": {
                "description": "Så­—ã‚«ãƒ¼ãƒ–æ¡ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³",
                "indicators": [
                    "innovation_diffusion",
                    "market_saturation",
                    "early_majority",
                ],
                "duration_months": [18, 48],
                "confidence_threshold": 0.75,
            },
            "hype_cycle": {
                "description": "ãƒã‚¤ãƒ—ã‚µã‚¤ã‚¯ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³",
                "indicators": [
                    "peak_hype",
                    "trough_disillusionment",
                    "plateau_productivity",
                ],
                "duration_months": [24, 60],
                "confidence_threshold": 0.6,
            },
            "seasonal": {
                "description": "å­£ç¯€æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³",
                "indicators": ["conference_cycles", "hiring_seasons", "project_cycles"],
                "duration_months": [3, 12],
                "confidence_threshold": 0.8,
            },
        }

    def _load_tech_keywords(self) -> Dict[str, List[str]]:
        """æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸"""
        return {
            "ai_ml": [
                "artificial intelligence",
                "machine learning",
                "deep learning",
                "neural networks",
                "transformer",
                "gpt",
                "llm",
                "nlp",
                "computer vision",
                "reinforcement learning",
            ],
            "web_frameworks": [
                "react",
                "vue",
                "angular",
                "svelte",
                "nextjs",
                "nuxt",
                "express",
                "fastapi",
                "django",
                "flask",
                "spring",
            ],
            "cloud_native": [
                "kubernetes",
                "docker",
                "microservices",
                "serverless",
                "aws",
                "gcp",
                "azure",
                "terraform",
                "helm",
            ],
            "programming_languages": [
                "python",
                "javascript",
                "typescript",
                "rust",
                "go",
                "java",
                "kotlin",
                "swift",
                "dart",
                "c++",
            ],
            "database_tech": [
                "postgresql",
                "mongodb",
                "redis",
                "elasticsearch",
                "cassandra",
                "neo4j",
                "influxdb",
                "snowflake",
            ],
            "devops_tools": [
                "jenkins",
                "gitlab ci",
                "github actions",
                "ansible",
                "prometheus",
                "grafana",
                "elk stack",
                "datadog",
            ],
        }

    def _load_or_create_model(self) -> Dict:
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã¾ãŸã¯ä½œæˆï¼ˆã‚»ã‚­ãƒ¥ã‚¢ç‰ˆï¼‰"""
        if self.model_path.exists():
            try:
                import json
                with open(self.model_path, "r") as f:
                    model = json.load(f)
                self.logger.info("ğŸ“¦ Existing model loaded")
                return model
            except Exception as e:
                self.logger.warning(f"Model loading failed: {e}")

        # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = {
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "weights": {},
            "bias": 0.0,
            "feature_names": [],
            "training_history": [],
            "performance_metrics": {},
        }

        self.logger.info("ğŸ†• New model created")
        return model

    async def train_model(self, historical_data: List[Dict]) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«è¨“ç·´

        Args:
            historical_data: éå»ãƒ‡ãƒ¼ã‚¿

        Returns:
            Dict: è¨“ç·´çµæœ
        """
        self.logger.info("ğŸ“ Starting model training...")

        if not historical_data:
            raise ValueError("Training data is empty")

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        processed_data = self._preprocess_training_data(historical_data)

        # ç‰¹å¾´é‡æŠ½å‡º
        features, targets = self._extract_features_targets(processed_data)

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªç·šå½¢å›å¸°ï¼‰
        training_result = self._train_linear_model(features, targets)

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self._save_model()

        self.logger.info("âœ… Model training completed")

        return training_result

    def _preprocess_training_data(self, data: List[Dict]) -> List[Dict]:
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        processed = []

        for record in data:
            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            if not self._validate_training_record(record):
                continue

            # æ­£è¦åŒ–
            normalized = self._normalize_record(record)
            processed.append(normalized)

        return processed

    def _validate_training_record(self, record: Dict) -> bool:
        """è¨“ç·´ãƒ¬ã‚³ãƒ¼ãƒ‰æ¤œè¨¼"""
        required_fields = ["timestamp", "technology", "demand_score"]
        return all(field in record for field in required_fields)

    def _normalize_record(self, record: Dict) -> Dict:
        """ãƒ¬ã‚³ãƒ¼ãƒ‰æ­£è¦åŒ–"""
        normalized = record.copy()

        # éœ€è¦ã‚¹ã‚³ã‚¢æ­£è¦åŒ– (0-1ã®ç¯„å›²)
        if "demand_score" in normalized:
            score = float(normalized["demand_score"])
            normalized["demand_score"] = max(0.0, min(1.0, score / 100.0))

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ­£è¦åŒ–
        if "timestamp" in normalized:
            if isinstance(normalized["timestamp"], str):
                normalized["timestamp"] = datetime.fromisoformat(
                    normalized["timestamp"]
                )

        return normalized

    def _extract_features_targets(
        self, data: List[Dict]
    ) -> Tuple[List[List[float]], List[float]]:
        """ç‰¹å¾´é‡ãƒ»ç›®æ¨™å€¤æŠ½å‡º"""
        features = []
        targets = []

        for record in data:
            # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ
            feature_vector = self._create_feature_vector(record)
            features.append(feature_vector)

            # ç›®æ¨™å€¤
            targets.append(record["demand_score"])

        return features, targets

    def _create_feature_vector(self, record: Dict) -> List[float]:
        """ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ"""
        vector = []

        # æŠ€è¡“ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡
        tech = record.get("technology", "").lower()
        for category, keywords in self.tech_keywords.items():
            match_score = sum(1 for keyword in keywords if keyword in tech)
            vector.append(match_score / len(keywords))

        # æ™‚ç³»åˆ—ç‰¹å¾´é‡
        timestamp = record.get("timestamp", datetime.now())
        if isinstance(timestamp, str):
            timestamp = datetime.fromisoformat(timestamp)

        # æœˆã€å››åŠæœŸã€å¹´ã®ç‰¹å¾´é‡
        vector.append(timestamp.month / 12.0)
        vector.append((timestamp.month - 1) // 3 / 4.0)  # å››åŠæœŸ

        # ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡
        trend_indicators = record.get("trend_indicators", {})
        vector.append(trend_indicators.get("github_stars", 0) / 10000.0)
        vector.append(trend_indicators.get("job_postings", 0) / 1000.0)
        vector.append(trend_indicators.get("search_volume", 0) / 100.0)

        return vector

    def _train_linear_model(
        self, features: List[List[float]], targets: List[float]
    ) -> Dict:
        """ç·šå½¢ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        if not features or not targets:
            raise ValueError("Empty features or targets")

        # NumPyé…åˆ—ã«å¤‰æ›
        X = np.array(features)
        y = np.array(targets)

        # æ­£è¦æ–¹ç¨‹å¼ã§é‡ã¿è¨ˆç®—
        X_bias = np.c_[np.ones(X.shape[0]), X]  # ãƒã‚¤ã‚¢ã‚¹é …è¿½åŠ 

        try:
            # (X^T X)^-1 X^T y
            weights = np.linalg.solve(X_bias.T @ X_bias, X_bias.T @ y)

            self.model["bias"] = weights[0]
            self.model["weights"] = weights[1:].tolist()
            self.model["feature_names"] = [
                f"feature_{i}" for i in range(len(weights) - 1)
            ]

        except np.linalg.LinAlgError:
            # ç‰¹ç•°è¡Œåˆ—ã®å ´åˆã¯æ“¬ä¼¼é€†è¡Œåˆ—ã‚’ä½¿ç”¨
            weights = np.linalg.pinv(X_bias.T @ X_bias) @ X_bias.T @ y
            self.model["bias"] = weights[0]
            self.model["weights"] = weights[1:].tolist()
            self.model["feature_names"] = [
                f"feature_{i}" for i in range(len(weights) - 1)
            ]

        # æ€§èƒ½è©•ä¾¡
        predictions = X_bias @ weights
        mse = np.mean((y - predictions) ** 2)
        r2 = 1 - (np.sum((y - predictions) ** 2) / np.sum((y - np.mean(y)) ** 2))

        performance = {
            "mse": float(mse),
            "r2_score": float(r2),
            "training_samples": len(targets),
            "feature_count": len(features[0]) if features else 0,
        }

        self.model["performance_metrics"] = performance

        return {
            "training_completed": True,
            "performance": performance,
            "model_version": self.model["version"],
        }

    def _save_model(self):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        try:
            import json
            with open(self.model_path, "w") as f:
                json.dump(self.model, f, indent=2, default=str)  # default=strã§æ—¥ä»˜ç­‰ã‚’å‡¦ç†
            self.logger.info(f"ğŸ’¾ Model saved: {self.model_path}")
        except Exception as e:
            self.logger.error(f"Model save error: {e}")

    async def predict_demand(self, features: Dict[str, Any]) -> Prediction:
        """
        éœ€è¦äºˆæ¸¬

        Args:
            features: ç‰¹å¾´é‡è¾æ›¸

        Returns:
            Prediction: äºˆæ¸¬çµæœ
        """
        self.logger.info(
            f"ğŸ”® Predicting demand for: {features.get('technology', 'unknown')}"
        )

        # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ
        feature_vector = self._create_feature_vector_from_dict(features)

        # äºˆæ¸¬å®Ÿè¡Œ
        predicted_value = self._predict_single(feature_vector)

        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_prediction_confidence(
            feature_vector, predicted_value
        )

        # ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘åˆ¤å®š
        trend_direction = self._determine_trend_direction(features, predicted_value)

        # ä½¿ç”¨ç‰¹å¾´é‡
        features_used = self._get_important_features(feature_vector)

        prediction = Prediction(
            target=features.get("technology", "unknown"),
            prediction_type=PredictionType.TECHNOLOGY_DEMAND,
            timeframe=TimeFrame.MEDIUM_TERM,
            predicted_value=predicted_value,
            confidence=confidence,
            trend_direction=trend_direction,
            features_used=features_used,
            model_version=self.model["version"],
            created_at=datetime.now().isoformat(),
        )

        self.prediction_history.append(prediction)

        self.logger.info(
            f"âœ… Prediction: {predicted_value:.3f} (confidence: {confidence:.2f})"
        )

        return prediction

    def _create_feature_vector_from_dict(self, features: Dict) -> List[float]:
        """è¾æ›¸ã‹ã‚‰ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ"""
        return self._create_feature_vector(features)

    def _predict_single(self, feature_vector: List[float]) -> float:
        """å˜ä¸€äºˆæ¸¬"""
        if not self.model.get("weights"):
            # è¨“ç·´ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ äºˆæ¸¬
            return np.random.uniform(0.3, 0.8)

        # ç·šå½¢äºˆæ¸¬
        weights = np.array(self.model["weights"])
        bias = self.model["bias"]

        # ãƒ™ã‚¯ãƒˆãƒ«é•·ã‚’èª¿æ•´
        if len(feature_vector) != len(weights):
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯ãƒˆãƒªãƒŸãƒ³ã‚°
            if len(feature_vector) < len(weights):
                feature_vector.extend([0.0] * (len(weights) - len(feature_vector)))
            else:
                feature_vector = feature_vector[: len(weights)]

        prediction = bias + np.dot(weights, feature_vector)

        # 0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
        return max(0.0, min(1.0, prediction))

    def _calculate_prediction_confidence(
        self, feature_vector: List[float], predicted_value: float
    ) -> float:
        """äºˆæ¸¬ä¿¡é ¼åº¦è¨ˆç®—"""
        # åŸºæœ¬ä¿¡é ¼åº¦
        base_confidence = 0.7

        # ç‰¹å¾´é‡ã®å®Œå…¨æ€§
        completeness = sum(1 for x in feature_vector if x > 0) / len(feature_vector)
        confidence = base_confidence + (completeness * 0.2)

        # äºˆæ¸¬å€¤ã®å¦¥å½“æ€§
        if 0.2 <= predicted_value <= 0.9:
            confidence += 0.1

        return min(1.0, confidence)

    def _determine_trend_direction(self, features: Dict, predicted_value: float) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘åˆ¤å®š"""
        # ç°¡å˜ãªé–¾å€¤ãƒ™ãƒ¼ã‚¹åˆ¤å®š
        if predicted_value > 0.7:
            return "increasing"
        elif predicted_value < 0.4:
            return "decreasing"
        else:
            return "stable"

    def _get_important_features(
        self, feature_vector: List[float]
    ) -> List[DemandFeature]:
        """é‡è¦ç‰¹å¾´é‡å–å¾—"""
        features = []

        feature_names = [
            "AI/ML Category",
            "Web Frameworks",
            "Cloud Native",
            "Programming Languages",
            "Database Tech",
            "DevOps Tools",
            "Monthly Trend",
            "Quarterly Trend",
            "GitHub Stars",
            "Job Postings",
            "Search Volume",
        ]

        for i, (name, value) in enumerate(zip(feature_names, feature_vector)):
            if value > 0.1:  # é–¾å€¤ä»¥ä¸Šã®ç‰¹å¾´é‡ã®ã¿
                importance = value * np.random.uniform(0.8, 1.2)  # é‡è¦åº¦æ¨¡æ“¬

                features.append(
                    DemandFeature(
                        name=name,
                        value=value,
                        importance=importance,
                        category="technical",
                        timestamp=datetime.now().isoformat(),
                    )
                )

        return sorted(features, key=lambda f: f.importance, reverse=True)[:5]

    async def analyze_patterns(self) -> List[PatternAnalysis]:
        """
        ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ

        Returns:
            List[PatternAnalysis]: åˆ†æçµæœ
        """
        self.logger.info("ğŸ“Š Analyzing demand patterns...")

        patterns = []

        for pattern_name, pattern_info in self.demand_patterns.items():
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Ÿè¡Œ
            analysis = await self._analyze_single_pattern(pattern_name, pattern_info)
            patterns.append(analysis)

        # ç›¸é–¢åˆ†æ
        correlation_patterns = self._analyze_correlations()
        patterns.extend(correlation_patterns)

        self.logger.info(f"âœ… Found {len(patterns)} patterns")

        return patterns

    async def _analyze_single_pattern(
        self, pattern_name: str, pattern_info: Dict
    ) -> PatternAnalysis:
        """å˜ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        # æ¨¡æ“¬åˆ†æçµæœ
        frequency = np.random.uniform(0.3, 0.8)
        correlation_strength = np.random.uniform(0.5, 0.9)

        examples = self._generate_pattern_examples(pattern_name)

        return PatternAnalysis(
            pattern_name=pattern_name,
            description=pattern_info["description"],
            frequency=frequency,
            correlation_strength=correlation_strength,
            examples=examples,
            confidence=pattern_info["confidence_threshold"],
        )

    def _analyze_correlations(self) -> List[PatternAnalysis]:
        """ç›¸é–¢åˆ†æ"""
        correlations = [
            PatternAnalysis(
                pattern_name="ai_ml_correlation",
                description="AI/MLæŠ€è¡“ã¨å¸‚å ´éœ€è¦ã®æ­£ã®ç›¸é–¢",
                frequency=0.85,
                correlation_strength=0.92,
                examples=["PyTorch + æ±‚äººå¢—åŠ ", "ChatGPT + Pythonéœ€è¦"],
                confidence=0.9,
            ),
            PatternAnalysis(
                pattern_name="framework_lifecycle",
                description="ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³",
                frequency=0.7,
                correlation_strength=0.75,
                examples=["Reactæˆç†ŸæœŸ", "Vueæˆé•·æœŸ", "Svelteå°å…¥æœŸ"],
                confidence=0.8,
            ),
        ]

        return correlations

    def _generate_pattern_examples(self, pattern_name: str) -> List[str]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ä¾‹ç”Ÿæˆ"""
        examples_map = {
            "exponential_growth": [
                "ChatGPT adoption in Q4 2022",
                "React Hook surge in 2019",
                "Docker containerization boom",
            ],
            "linear_growth": [
                "TypeScript steady adoption",
                "AWS cloud migration trends",
                "Remote work tool adoption",
            ],
            "s_curve": [
                "Kubernetes enterprise adoption",
                "GraphQL API transition",
                "Serverless architecture adoption",
            ],
            "hype_cycle": [
                "Blockchain development hype",
                "NoSQL database evolution",
                "Microservices architecture",
            ],
            "seasonal": [
                "Conference-driven framework interest",
                "Hiring season technology demand",
                "Year-end project technology choices",
            ],
        }

        return examples_map.get(
            pattern_name, ["Generic example 1", "Generic example 2"]
        )

    async def generate_forecast(self, timeframe: TimeFrame) -> ForecastReport:
        """
        äºˆæ¸¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

        Args:
            timeframe: äºˆæ¸¬æœŸé–“

        Returns:
            ForecastReport: äºˆæ¸¬ãƒ¬ãƒãƒ¼ãƒˆ
        """
        self.logger.info(f"ğŸ“ˆ Generating forecast for {timeframe.value}...")

        # è¤‡æ•°æŠ€è¡“ã®äºˆæ¸¬
        technologies = [
            "artificial intelligence",
            "rust programming",
            "kubernetes",
            "react framework",
            "python",
            "machine learning",
            "cloud computing",
            "microservices",
            "data science",
        ]

        predictions = []
        for tech in technologies:
            features = {
                "technology": tech,
                "timestamp": datetime.now(),
                "trend_indicators": {
                    "github_stars": np.random.randint(1000, 50000),
                    "job_postings": np.random.randint(100, 5000),
                    "search_volume": np.random.randint(10, 100),
                },
            }

            prediction = await self.predict_demand(features)
            prediction.timeframe = timeframe
            predictions.append(prediction)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        pattern_analysis = await self.analyze_patterns()

        # å¸‚å ´æ´å¯Ÿç”Ÿæˆ
        market_insights = self._generate_market_insights(predictions, timeframe)

        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendations = self._generate_recommendations(predictions, pattern_analysis)

        # ãƒªã‚¹ã‚¯è¦å› 
        risk_factors = self._identify_risk_factors(predictions, timeframe)

        # å…¨ä½“ä¿¡é ¼åº¦
        confidence_level = self._calculate_overall_confidence(predictions)

        report = ForecastReport(
            timeframe=timeframe,
            predictions=predictions,
            pattern_analysis=pattern_analysis,
            market_insights=market_insights,
            recommendations=recommendations,
            risk_factors=risk_factors,
            confidence_level=confidence_level,
            generated_at=datetime.now().isoformat(),
        )

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        await self._save_forecast_report(report)

        self.logger.info("ğŸ“Š Forecast report generated")

        return report

    def _generate_market_insights(
        self, predictions: List[Prediction], timeframe: TimeFrame
    ) -> List[str]:
        """å¸‚å ´æ´å¯Ÿç”Ÿæˆ"""
        insights = []

        # é«˜éœ€è¦æŠ€è¡“
        high_demand = [p for p in predictions if p.predicted_value > 0.7]
        if high_demand:
            top_tech = max(high_demand, key=lambda p: p.predicted_value)
            insights.append(f"{top_tech.target}ãŒ{timeframe.value}ã§æœ€é«˜éœ€è¦ã‚’ç¤ºã™äºˆæ¸¬")

        # æˆé•·ãƒˆãƒ¬ãƒ³ãƒ‰
        growing = [p for p in predictions if p.trend_direction == "increasing"]
        insights.append(f"{len(growing)}ã®æŠ€è¡“ãŒæˆé•·ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç¤ºã—ã¦ã„ã‚‹")

        # AI/MLé–¢é€£
        ai_predictions = [
            p
            for p in predictions
            if "ai" in p.target.lower() or "machine" in p.target.lower()
        ]
        if ai_predictions:
            avg_ai_demand = sum(p.predicted_value for p in ai_predictions) / len(
                ai_predictions
            )
            insights.append(f"AI/MLåˆ†é‡ã®å¹³å‡éœ€è¦äºˆæ¸¬: {avg_ai_demand:.2f}")

        return insights

    def _generate_recommendations(
        self, predictions: List[Prediction], patterns: List[PatternAnalysis]
    ) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        # é«˜éœ€è¦æŠ€è¡“ã¸ã®æŠ•è³‡æ¨å¥¨
        high_demand = [p for p in predictions if p.predicted_value > 0.8]
        for pred in high_demand[:3]:
            recommendations.append(
                f"{pred.target}ã¸ã®æŠ•è³‡ã‚’å¼·ãæ¨å¥¨ï¼ˆäºˆæ¸¬éœ€è¦: {pred.predicted_value:.2f}ï¼‰"
            )

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹æ¨å¥¨
        strong_patterns = [p for p in patterns if p.correlation_strength > 0.8]
        for pattern in strong_patterns[:2]:
            recommendations.append(
                f"{pattern.pattern_name}ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæˆ¦ç•¥ç­–å®šã‚’æ¨å¥¨"
            )

        # ãƒªã‚¹ã‚¯åˆ†æ•£
        recommendations.append("è¤‡æ•°æŠ€è¡“ã¸ã®åˆ†æ•£æŠ•è³‡ã§ãƒªã‚¹ã‚¯è»½æ¸›ã‚’æ¨å¥¨")

        return recommendations

    def _identify_risk_factors(
        self, predictions: List[Prediction], timeframe: TimeFrame
    ) -> List[str]:
        """ãƒªã‚¹ã‚¯è¦å› ç‰¹å®š"""
        risks = []

        # ä½ä¿¡é ¼åº¦äºˆæ¸¬
        low_confidence = [p for p in predictions if p.confidence < 0.6]
        if low_confidence:
            risks.append(f"{len(low_confidence)}ã®æŠ€è¡“äºˆæ¸¬ã§ä¿¡é ¼åº¦ãŒä½ã„")

        # æ€¥æ¿€ãªå¤‰åŒ–
        extreme_values = [
            p for p in predictions if p.predicted_value > 0.9 or p.predicted_value < 0.2
        ]
        if extreme_values:
            risks.append("æ¥µç«¯ãªéœ€è¦å¤‰å‹•ã®å¯èƒ½æ€§")

        # æœŸé–“ä¾å­˜ãƒªã‚¹ã‚¯
        if timeframe == TimeFrame.LONG_TERM:
            risks.append("é•·æœŸäºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§å¢—å¤§")

        risks.append("æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã®æ€¥æ¿€ãªå¤‰åŒ–ãƒªã‚¹ã‚¯")
        risks.append("å¸‚å ´ç’°å¢ƒå¤‰åŒ–ã«ã‚ˆã‚‹äºˆæ¸¬ç²¾åº¦ä½ä¸‹")

        return risks

    def _calculate_overall_confidence(self, predictions: List[Prediction]) -> float:
        """å…¨ä½“ä¿¡é ¼åº¦è¨ˆç®—"""
        if not predictions:
            return 0.0

        return sum(p.confidence for p in predictions) / len(predictions)

    async def _save_forecast_report(self, report: ForecastReport):
        """äºˆæ¸¬ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        report_dir = Path("data/forecast_reports")
        report_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = report_dir / f"forecast_{report.timeframe.value}_{timestamp}.json"

        try:
            with open(report_file, "w", encoding="utf-8") as f:
                json.dump(asdict(report), f, indent=2, ensure_ascii=False, default=str)

            self.logger.info(f"ğŸ“„ Forecast report saved: {report_file}")
        except Exception as e:
            self.logger.error(f"Report save error: {e}")

    def get_model_info(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—"""
        return {
            "version": self.model.get("version", "unknown"),
            "created_at": self.model.get("created_at", "unknown"),
            "performance": self.model.get("performance_metrics", {}),
            "feature_count": len(self.model.get("feature_names", [])),
            "prediction_count": len(self.prediction_history),
        }

    def get_prediction_history(self, limit: int = 10) -> List[Prediction]:
        """äºˆæ¸¬å±¥æ­´å–å¾—"""
        return self.prediction_history[-limit:]

    async def retrain_model(self, new_data: List[Dict]) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«å†è¨“ç·´"""
        self.logger.info("ğŸ”„ Retraining model with new data...")

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨æ–°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
        combined_data = self.training_data + new_data

        # å†è¨“ç·´å®Ÿè¡Œ
        result = await self.train_model(combined_data)

        self.logger.info("âœ… Model retrained successfully")

        return result


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
async def demo_demand_predictor():
    """Demand Predictor AIã®ãƒ‡ãƒ¢"""
    print("ğŸ”® Demand Predictor AI Demo")
    print("=" * 50)

    predictor = DemandPredictorAI()

    # ã‚µãƒ³ãƒ—ãƒ«è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    training_data = [
        {
            "timestamp": "2024-01-01",
            "technology": "python",
            "demand_score": 85,
            "trend_indicators": {
                "github_stars": 45000,
                "job_postings": 3500,
                "search_volume": 90,
            },
        },
        {
            "timestamp": "2024-02-01",
            "technology": "javascript",
            "demand_score": 92,
            "trend_indicators": {
                "github_stars": 60000,
                "job_postings": 4200,
                "search_volume": 95,
            },
        },
        {
            "timestamp": "2024-03-01",
            "technology": "rust",
            "demand_score": 75,
            "trend_indicators": {
                "github_stars": 30000,
                "job_postings": 800,
                "search_volume": 70,
            },
        },
    ]

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    print("\nğŸ“ Training model...")
    training_result = await predictor.train_model(training_data)
    print(f"Training completed: RÂ² = {training_result['performance']['r2_score']:.3f}")

    # éœ€è¦äºˆæ¸¬
    print("\nğŸ”® Making predictions...")
    test_features = {
        "technology": "artificial intelligence",
        "timestamp": datetime.now(),
        "trend_indicators": {
            "github_stars": 25000,
            "job_postings": 2000,
            "search_volume": 85,
        },
    }

    prediction = await predictor.predict_demand(test_features)
    print(f"AI Demand Prediction: {prediction.predicted_value:.3f}")
    print(f"Confidence: {prediction.confidence:.3f}")
    print(f"Trend: {prediction.trend_direction}")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    print("\nğŸ“Š Analyzing patterns...")
    patterns = await predictor.analyze_patterns()
    print(f"Found {len(patterns)} patterns:")
    for pattern in patterns[:3]:
        print(f"  - {pattern.pattern_name}: {pattern.correlation_strength:.2f}")

    # äºˆæ¸¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\nğŸ“ˆ Generating forecast report...")
    forecast = await predictor.generate_forecast(TimeFrame.MEDIUM_TERM)

    print(f"ğŸ“Š Forecast Summary:")
    print(f"   Predictions: {len(forecast.predictions)}")
    print(f"   Confidence: {forecast.confidence_level:.2f}")
    print(
        f"   Top Technology: {max(forecast.predictions, key=lambda p: p.predicted_value).target}"
    )
    print(f"   Market Insights: {len(forecast.market_insights)}")
    print(f"   Recommendations: {len(forecast.recommendations)}")

    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
    print("\nğŸ”§ Model Info:")
    model_info = predictor.get_model_info()
    for key, value in model_info.items():
        print(f"   {key}: {value}")


if __name__ == "__main__":
    asyncio.run(demo_demand_predictor())
