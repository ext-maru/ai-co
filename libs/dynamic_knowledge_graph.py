#!/usr/bin/env python3
"""
ğŸŒ å‹•çš„çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚·ã‚¹ãƒ†ãƒ 
çŸ¥è­˜ã®é–¢é€£æ€§è‡ªå‹•ç™ºè¦‹ã¨å‹•çš„æ›´æ–°ã§å¤šè¨€èªå¯¾å¿œå¼·åŒ–

ä½œæˆæ—¥: 2025å¹´7æœˆ8æ—¥
ä½œæˆè€…: ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼ï¼ˆé–‹ç™ºå®Ÿè¡Œè²¬ä»»è€…ï¼‰
æ‰¿èª: ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ãƒ»RAGè³¢è€…ã«ã‚ˆã‚‹å”è­°æ¸ˆã¿
"""

import asyncio
import hashlib
import json
import logging
import math
import re
import sys
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from itertools import combinations
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import numpy as np

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .predictive_incident_manager import PredictiveIncidentManager
    from .quantum_collaboration_engine import QuantumCollaborationEngine
except ImportError:
    # ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¹
    class QuantumCollaborationEngine:
        """QuantumCollaborationEngine - ã‚¨ãƒ³ã‚¸ãƒ³ã‚¯ãƒ©ã‚¹"""
        async def quantum_consensus(self, request):
            """quantum_consensusãƒ¡ã‚½ãƒƒãƒ‰"""
            return type(
                "MockConsensus",
                (),
                {
                    "solution": "Apply knowledge clustering",
                    "confidence": 0.85,
                    "coherence": 0.8,
                },
            )()

    class PredictiveIncidentManager:
        """PredictiveIncidentManager - ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ã‚¹"""
        def get_prediction_metrics(self):
            """prediction_metricså–å¾—ãƒ¡ã‚½ãƒƒãƒ‰"""
            return {"overall_accuracy": 0.9}


# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logger = logging.getLogger(__name__)


class NodeType(Enum):
    """ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—å®šç¾©"""

    CONCEPT = "concept"
    ENTITY = "entity"
    RELATION = "relation"
    DOCUMENT = "document"
    EVENT = "event"


class RelationType(Enum):
    """é–¢ä¿‚ã‚¿ã‚¤ãƒ—å®šç¾©"""

    RELATED_TO = "related_to"
    IS_TYPE_OF = "is_type_of"
    CAUSES = "causes"
    REQUIRES = "requires"
    IMPROVES = "improves"
    AFFECTS = "affects"
    CONTAINS = "contains"
    PART_OF = "part_of"


@dataclass
class KnowledgeNode:
    """çŸ¥è­˜ãƒãƒ¼ãƒ‰"""

    node_id: str
    content: str
    node_type: str
    importance_score: float = 0.5
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    embedding_vector: Optional[List[float]] = None


@dataclass
class KnowledgeEdge:
    """çŸ¥è­˜ã‚¨ãƒƒã‚¸"""

    source_id: str
    target_id: str
    relation_type: str
    strength: float
    confidence: float = 0.8
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    evidence_count: int = 1


@dataclass
class ConceptRelation:
    """ã‚³ãƒ³ã‚»ãƒ—ãƒˆé–¢ä¿‚"""

    concept_a: str
    concept_b: str
    relation_type: str
    confidence: float
    context: str = ""
    evidence: List[str] = field(default_factory=list)


@dataclass
class SemanticEmbedding:
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿"""

    text: str
    vector: List[float]
    model_name: str = "simplified_embedding"

    def cosine_similarity(self, other: "SemanticEmbedding") -> float:
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—"""
        v1 = np.array(self.vector)
        v2 = np.array(other.vector)

        dot_product = np.dot(v1, v2)
        norm_v1 = np.linalg.norm(v1)
        norm_v2 = np.linalg.norm(v2)

        if norm_v1 == 0 or norm_v2 == 0:
            return 0.0

        similarity = dot_product / (norm_v1 * norm_v2)
        return max(0.0, min(1.0, similarity))


@dataclass
class KnowledgeCluster:
    """çŸ¥è­˜ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼"""

    cluster_id: str
    nodes: List[KnowledgeNode]
    centroid: List[float]
    coherence_score: float = 0.0
    topic_label: str = ""


@dataclass
class GraphUpdate:
    """ã‚°ãƒ©ãƒ•æ›´æ–°"""

    update_type: str
    target: str
    data: Dict[str, Any]
    timestamp: datetime = field(default_factory=datetime.now)
    confidence: float = 0.8


@dataclass
class DiscoveryResult:
    """ç™ºè¦‹çµæœ"""

    discovery_type: str
    entities: List[Any]
    confidence: float
    metadata: Dict[str, Any] = field(default_factory=dict)


class DynamicKnowledgeGraph:
    """å‹•çš„çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.quantum_engine = QuantumCollaborationEngine()
        self.predictive_manager = PredictiveIncidentManager()

        # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿
        self.nodes: Dict[str, KnowledgeNode] = {}
        self.edges: List[KnowledgeEdge] = []
        self.clusters: List[KnowledgeCluster] = []

        # åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.embedding_cache: Dict[str, SemanticEmbedding] = {}

        # é€²åŒ–è¿½è·¡
        self.evolution_history: List[Dict[str, Any]] = []

        # å¤šè¨€èªå¯¾å¿œ
        self.language_mappings: Dict[str, Dict[str, str]] = defaultdict(dict)

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
        self.similarity_threshold = 0.7
        self.max_relations_per_concept = 10
        self.embedding_dimension = 128

        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "total_discoveries": 0,
            "relation_discoveries": 0,
            "concept_updates": 0,
            "inference_operations": 0,
        }

        logger.info("ğŸŒ å‹•çš„çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    def create_node(
        self, content: str, node_type: str, metadata: Optional[Dict[str, Any]] = None
    ) -> KnowledgeNode:
        """çŸ¥è­˜ãƒãƒ¼ãƒ‰ä½œæˆ"""
        node_id = self._generate_node_id(content)

        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        embedding = self.generate_embedding(content)

        node = KnowledgeNode(
            node_id=node_id,
            content=content,
            node_type=node_type,
            metadata=metadata or {},
            embedding_vector=embedding.vector,
        )

        # é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        node.importance_score = self._calculate_initial_importance(
            content, metadata or {}
        )

        self.nodes[node_id] = node

        logger.debug(f"ğŸ“ ãƒãƒ¼ãƒ‰ä½œæˆ: {node_id[:8]}... ({node_type})")
        return node

    def _generate_node_id(self, content: str) -> str:
        """ãƒãƒ¼ãƒ‰IDç”Ÿæˆ"""
        content_hash = hashlib.md5(content.encode()).hexdigest()
        return f"node_{content_hash[:12]}"

    def _calculate_initial_importance(
        self, content: str, metadata: Dict[str, Any]
    ) -> float:
        """åˆæœŸé‡è¦åº¦è¨ˆç®—"""
        base_score = 0.5

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·ã«ã‚ˆã‚‹èª¿æ•´
        length_factor = min(1.0, len(content) / 200)
        base_score += length_factor * 0.2

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹èª¿æ•´
        if metadata.get("confidence", 0) > 0.8:
            base_score += 0.1

        if metadata.get("domain") in ["AI", "ML", "system"]:
            base_score += 0.15

        return min(1.0, base_score)

    def calculate_node_importance(
        self, node: KnowledgeNode, connections: List[Dict[str, Any]]
    ) -> float:
        """ãƒãƒ¼ãƒ‰é‡è¦åº¦è¨ˆç®—"""
        # æ¥ç¶šæ•°ã«ã‚ˆã‚‹é‡è¦åº¦ï¼ˆé—¾å€¤ã‚’ä¸‹ã’ã¦é«˜ã‚¹ã‚³ã‚¢åŒ–ï¼‰
        connection_score = min(1.0, len(connections) / 5)  # 10â†’5ã«å¤‰æ›´

        # æ¥ç¶šå¼·åº¦ã®å¹³å‡
        if connections:
            avg_strength = np.mean([conn.get("strength", 0.5) for conn in connections])
        else:
            avg_strength = 0.0

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®é‡è¦åº¦
        content_score = self._analyze_content_importance(node.content)

        # å¤šæ¥ç¶šãƒœãƒ¼ãƒŠã‚¹
        connection_bonus = 0.2 if len(connections) >= 4 else 0.0

        # æœ€çµ‚é‡è¦åº¦è¨ˆç®—ï¼ˆé‡ã¿èª¿æ•´ï¼‰
        importance = (
            connection_score * 0.5
            + avg_strength * 0.3
            + content_score * 0.2
            + connection_bonus
        )

        return min(1.0, importance)

    def _analyze_content_importance(self, content: str) -> float:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡è¦åº¦åˆ†æ"""
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œå‡º
        important_keywords = [
            "optimization",
            "performance",
            "machine learning",
            "algorithm",
            "system",
            "security",
            "efficiency",
            "innovation",
            "architecture",
        ]

        content_lower = content.lower()
        keyword_count = sum(
            1 for keyword in important_keywords if keyword in content_lower
        )

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦ã«ã‚ˆã‚‹é‡è¦åº¦
        keyword_density = keyword_count / len(important_keywords)

        # æ–‡ã®è¤‡é›‘ã•ï¼ˆèªæ•°ï¼‰
        word_count = len(content.split())
        complexity_score = min(1.0, word_count / 50)

        return keyword_density * 0.7 + complexity_score * 0.3

    def calculate_node_similarity(
        self, node1: KnowledgeNode, node2: KnowledgeNode
    ) -> float:
        """ãƒãƒ¼ãƒ‰é¡ä¼¼åº¦è¨ˆç®—"""
        # åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦
        if node1.embedding_vector and node2.embedding_vector:
            embedding1 = SemanticEmbedding(node1.content, node1.embedding_vector)
            embedding2 = SemanticEmbedding(node2.content, node2.embedding_vector)
            semantic_similarity = embedding1.cosine_similarity(embedding2)
        else:
            semantic_similarity = self._calculate_text_similarity(
                node1.content, node2.content
            )

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é¡ä¼¼åº¦
        metadata_similarity = self._calculate_metadata_similarity(
            node1.metadata, node2.metadata
        )

        # ã‚¿ã‚¤ãƒ—ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹
        type_bonus = 0.1 if node1.node_type == node2.node_type else 0.0

        # çµ±åˆé¡ä¼¼åº¦
        total_similarity = (
            semantic_similarity * 0.7 + metadata_similarity * 0.2 + type_bonus
        )

        return min(1.0, total_similarity)

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 and not words2:
            return 1.0

        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))

        return intersection / union if union > 0 else 0.0

    def _calculate_metadata_similarity(
        self, meta1: Dict[str, Any], meta2: Dict[str, Any]
    ) -> float:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é¡ä¼¼åº¦è¨ˆç®—"""
        if not meta1 and not meta2:
            return 1.0

        common_keys = set(meta1.keys()).intersection(set(meta2.keys()))
        if not common_keys:
            return 0.0

        matches = 0
        for key in common_keys:
            if meta1[key] == meta2[key]:
                matches += 1

        return matches / len(common_keys)

    def create_edge(
        self,
        source_id: str,
        target_id: str,
        relation_type: str,
        strength: float,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> KnowledgeEdge:
        """çŸ¥è­˜ã‚¨ãƒƒã‚¸ä½œæˆ"""
        edge = KnowledgeEdge(
            source_id=source_id,
            target_id=target_id,
            relation_type=relation_type,
            strength=strength,
            metadata=metadata or {},
        )

        self.edges.append(edge)

        logger.debug(
            f"ğŸ”— ã‚¨ãƒƒã‚¸ä½œæˆ: {source_id[:8]}...â†’{target_id[:8]}... ({relation_type})"
        )
        return edge

    def calculate_edge_strength(
        self, concept1: str, concept2: str, context: Dict[str, Any]
    ) -> float:
        """ã‚¨ãƒƒã‚¸å¼·åº¦è¨ˆç®—"""
        # å…±èµ·é »åº¦ã«ã‚ˆã‚‹å¼·åº¦
        co_occurrence = context.get("co_occurrence_frequency", 0)
        co_occurrence_score = min(1.0, co_occurrence / 20)

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼åº¦
        semantic_similarity = context.get("semantic_similarity", 0.5)

        # ãƒ‰ãƒ¡ã‚¤ãƒ³é‡è¤‡åº¦
        domain_overlap = context.get("domain_overlap", 0.5)

        # ç·åˆå¼·åº¦è¨ˆç®—
        strength = (
            co_occurrence_score * 0.4 + semantic_similarity * 0.4 + domain_overlap * 0.2
        )

        return min(1.0, strength)

    def update_edge_weights(
        self, edge: KnowledgeEdge, new_evidence: Dict[str, Any]
    ) -> float:
        """å‹•çš„ã‚¨ãƒƒã‚¸é‡ã¿æ›´æ–°"""
        support_count = new_evidence.get("support_count", 0)
        contradiction_count = new_evidence.get("contradiction_count", 0)
        confidence_boost = new_evidence.get("confidence_boost", 0.0)

        # è¨¼æ‹ ã«åŸºã¥ãèª¿æ•´
        evidence_factor = (support_count - contradiction_count) / max(
            1, support_count + contradiction_count
        )
        evidence_adjustment = evidence_factor * 0.2

        # æ–°ã—ã„å¼·åº¦è¨ˆç®—
        new_strength = edge.strength + evidence_adjustment + confidence_boost
        new_strength = max(0.0, min(1.0, new_strength))

        # ã‚¨ãƒƒã‚¸æ›´æ–°
        edge.strength = new_strength
        edge.evidence_count += support_count
        edge.updated_at = datetime.now()

        return new_strength

    def generate_embedding(self, text: str) -> SemanticEmbedding:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if text in self.embedding_cache:
            return self.embedding_cache[text]

        # ç°¡æ˜“çš„ãªåŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
        vector = self._generate_simple_embedding(text)

        embedding = SemanticEmbedding(text, vector)
        self.embedding_cache[text] = embedding

        return embedding

    def _generate_simple_embedding(self, text: str) -> List[float]:
        """ç°¡æ˜“åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        # å˜èªãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´æŠ½å‡º
        words = text.lower().split()

        # ç‰¹å¾´èªå½™
        feature_vocab = [
            "machine",
            "learning",
            "algorithm",
            "data",
            "model",
            "system",
            "performance",
            "optimization",
            "neural",
            "network",
            "deep",
            "training",
            "prediction",
            "analysis",
            "processing",
            "memory",
            "cpu",
            "database",
            "query",
            "cache",
            "security",
            "error",
            "bug",
            "fix",
            "improvement",
            "enhancement",
            "scalability",
            "efficiency",
            "architecture",
            "design",
            "implementation",
            "testing",
            "debugging",
            "monitoring",
            "logging",
            "metrics",
        ]

        # TF-IDFé¢¨ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«
        vector = []
        for vocab_word in feature_vocab[: self.embedding_dimension]:
            # å˜èªã®å‡ºç¾å›æ•°
            count = words.count(vocab_word)
            # æ­£è¦åŒ–ã•ã‚ŒãŸç‰¹å¾´å€¤
            feature_value = min(1.0, count / len(words)) if words else 0.0
            vector.append(feature_value)

        # ãƒ™ã‚¯ãƒˆãƒ«ã‚’embedding_dimensionã«èª¿æ•´
        while len(vector) < self.embedding_dimension:
            vector.append(np.random.normal(0, 0.1))

        # æ­£è¦åŒ–
        vector_array = np.array(vector[: self.embedding_dimension])
        norm = np.linalg.norm(vector_array)
        if norm > 0:
            vector_array = vector_array / norm

        return vector_array.tolist()

    def find_similar_embeddings(
        self,
        query_embedding: SemanticEmbedding,
        candidate_embeddings: List[SemanticEmbedding],
        top_k: int = 5,
    ) -> List[Tuple[SemanticEmbedding, float]]:
        """åŸ‹ã‚è¾¼ã¿é¡ä¼¼æ€§æ¤œç´¢"""
        similarities = []

        for candidate in candidate_embeddings:
            similarity = query_embedding.cosine_similarity(candidate)
            similarities.append((candidate, similarity))

        # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        similarities.sort(key=lambda x: x[1], reverse=True)

        return similarities[:top_k]

    async def discover_concept_relations(
        self, text_corpus: List[str]
    ) -> List[ConceptRelation]:
        """ã‚³ãƒ³ã‚»ãƒ—ãƒˆé–¢ä¿‚ç™ºè¦‹"""
        self.stats["total_discoveries"] += 1
        relations = []

        # å„ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚³ãƒ³ã‚»ãƒ—ãƒˆæŠ½å‡º
        all_concepts = []
        for text in text_corpus:
            concepts = self._extract_concepts(text)
            all_concepts.extend([(concept, text) for concept in concepts])

        # ã‚³ãƒ³ã‚»ãƒ—ãƒˆãƒšã‚¢ã®é–¢ä¿‚åˆ†æ
        concept_pairs = list(combinations([c[0] for c in all_concepts], 2))

        for concept_a, concept_b in concept_pairs:
            # é–¢ä¿‚ã‚¿ã‚¤ãƒ—åˆ†é¡
            context_texts = [
                text
                for concept, text in all_concepts
                if concept in [concept_a, concept_b]
            ]
            combined_context = " ".join(context_texts)

            relation_type = self.classify_relation_type(
                concept_a, concept_b, combined_context
            )

            # ä¿¡é ¼åº¦è¨ˆç®—
            confidence = self._calculate_relation_confidence(
                concept_a, concept_b, context_texts
            )

            if confidence > 0.5:  # é–¾å€¤ä»¥ä¸Šã®é–¢ä¿‚ã®ã¿
                relation = ConceptRelation(
                    concept_a=concept_a,
                    concept_b=concept_b,
                    relation_type=relation_type,
                    confidence=confidence,
                    context=combined_context[:200],
                    evidence=context_texts[:3],
                )
                relations.append(relation)

        # é‡å­å”èª¿ã‚¨ãƒ³ã‚¸ãƒ³ã«ã‚ˆã‚‹é–¢ä¿‚ç²¾æŸ»
        if relations:
            quantum_request = {
                "problem": "validate_concept_relations",
                "relations": [
                    {"a": r.concept_a, "b": r.concept_b, "type": r.relation_type}
                    for r in relations
                ],
                "corpus_size": len(text_corpus),
            }

            try:
                quantum_result = await self.quantum_engine.quantum_consensus(
                    quantum_request
                )
                # é‡å­çµæœã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´
                quantum_boost = quantum_result.confidence * 0.1
                for relation in relations:
                    relation.confidence = min(0.98, relation.confidence + quantum_boost)

            except Exception as e:
                logger.warning(f"é‡å­é–¢ä¿‚ç²¾æŸ»ã‚¨ãƒ©ãƒ¼: {e}")

        self.stats["relation_discoveries"] += len(relations)
        logger.info(f"ğŸ” é–¢ä¿‚ç™ºè¦‹å®Œäº†: {len(relations)}ä»¶")

        return relations

    def _extract_concepts(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚³ãƒ³ã‚»ãƒ—ãƒˆæŠ½å‡º"""
        # ç°¡æ˜“çš„ãªã‚³ãƒ³ã‚»ãƒ—ãƒˆæŠ½å‡ºï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯NLPãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ï¼‰

        # æŠ€è¡“æ¦‚å¿µã®æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        concept_patterns = [
            r"\b(?:machine learning|deep learning|neural network|algorithm|model)\b",
            r"\b(?:database|query|index|optimization|performance)\b",
            r"\b(?:system|architecture|design|implementation|framework)\b",
            r"\b(?:security|authentication|encryption|vulnerability)\b",
            r"\b(?:api|service|microservice|endpoint|interface)\b",
            r"\b(?:cache|memory|cpu|storage|bandwidth)\b",
        ]

        concepts = set()
        text_lower = text.lower()

        for pattern in concept_patterns:
            matches = re.findall(pattern, text_lower)
            concepts.update(matches)

        # åè©å¥ã®æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        words = text.split()
        for i, word in enumerate(words):
            if len(word) > 3 and word.isalpha():
                # è¤‡åˆèªã®æ¤œå‡º
                if i < len(words) - 1:
                    compound = f"{word} {words[i+1]}"
                    if len(compound) < 30:  # é©åº¦ãªé•·ã•
                        concepts.add(compound.lower())

                concepts.add(word.lower())

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_concepts = []
        stop_words = {
            "the",
            "and",
            "for",
            "with",
            "this",
            "that",
            "from",
            "they",
            "have",
            "will",
        }

        for concept in concepts:
            if len(concept) > 3 and concept not in stop_words and not concept.isdigit():
                filtered_concepts.append(concept)

        return filtered_concepts[:20]  # ä¸Šä½20æ¦‚å¿µ

    def classify_relation_type(
            # è¤‡é›‘ãªæ¡ä»¶åˆ¤å®š
        self, concept_a: str, concept_b: str, context: str
    ) -> str:
        """é–¢ä¿‚ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        context_lower = context.lower()

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®åˆ†é¡
        if any(pattern in context_lower for pattern in ["is a", "type of", "kind of"]):
            return RelationType.IS_TYPE_OF.value

        if any(
            pattern in context_lower for pattern in ["causes", "leads to", "results in"]
        ):
            return RelationType.CAUSES.value

        if any(
            pattern in context_lower for pattern in ["requires", "needs", "depends on"]
        ):
            return RelationType.REQUIRES.value

        if any(
            pattern in context_lower
            for pattern in ["improves", "enhances", "optimizes"]
        ):
            return RelationType.IMPROVES.value

        if any(
            pattern in context_lower for pattern in ["affects", "influences", "impacts"]
        ):
            return RelationType.AFFECTS.value

        if any(pattern in context_lower for pattern in ["contains", "includes", "has"]):
            return RelationType.CONTAINS.value

        if any(
            pattern in context_lower
            for pattern in ["part of", "component of", "element of"]
        ):
            return RelationType.PART_OF.value

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return RelationType.RELATED_TO.value

    def _calculate_relation_confidence(
        self, concept_a: str, concept_b: str, context_texts: List[str]
    ) -> float:
        """é–¢ä¿‚ä¿¡é ¼åº¦è¨ˆç®—"""
        # å…±èµ·é »åº¦
        co_occurrence_count = sum(
            1
            for text in context_texts
            if concept_a.lower() in text.lower() and concept_b.lower() in text.lower()
        )

        co_occurrence_score = min(1.0, co_occurrence_count / len(context_texts))

        # è·é›¢ã«ã‚ˆã‚‹èª¿æ•´
        min_distance = float("inf")
        for text in context_texts:
            text_lower = text.lower()
            if concept_a.lower() in text_lower and concept_b.lower() in text_lower:
                pos_a = text_lower.find(concept_a.lower())
                pos_b = text_lower.find(concept_b.lower())
                distance = abs(pos_a - pos_b)
                min_distance = min(min_distance, distance)

        if min_distance != float("inf"):
            distance_score = max(0.1, 1.0 - min_distance / 200)
        else:
            distance_score = 0.1

        # ç·åˆä¿¡é ¼åº¦
        confidence = co_occurrence_score * 0.6 + distance_score * 0.4
        return min(1.0, confidence)

    def cluster_knowledge(
        self, nodes: List[KnowledgeNode], num_clusters: int = 3
    ) -> List[KnowledgeCluster]:
        """çŸ¥è­˜ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        if len(nodes) < num_clusters:
            # ãƒãƒ¼ãƒ‰ãŒå°‘ãªã„å ´åˆã¯å…¨ã¦1ã¤ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«
            cluster = KnowledgeCluster(
                cluster_id="cluster_all",
                nodes=nodes,
                centroid=[0.5] * self.embedding_dimension,
                topic_label="mixed_concepts",
            )
            return [cluster]

        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æº–å‚™
        embeddings = []
        valid_nodes = []

        for node in nodes:
            if node.embedding_vector:
                embeddings.append(node.embedding_vector)
                valid_nodes.append(node)
            else:
                # åŸ‹ã‚è¾¼ã¿ãŒãªã„å ´åˆã¯ç”Ÿæˆ
                embedding = self.generate_embedding(node.content)
                embeddings.append(embedding.vector)
                valid_nodes.append(node)
                node.embedding_vector = embedding.vector

        if not embeddings:
            return []

        embeddings_array = np.array(embeddings)

        # K-means ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
        clusters = self._simple_kmeans(embeddings_array, valid_nodes, num_clusters)

        return clusters

    def _simple_kmeans(
        self, embeddings: np.ndarray, nodes: List[KnowledgeNode], k: int
    ) -> List[KnowledgeCluster]:
        """ã‚·ãƒ³ãƒ—ãƒ«K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        n_samples, n_features = embeddings.shape

        # åˆæœŸã‚»ãƒ³ã‚¿ãƒ­ã‚¤ãƒ‰ï¼ˆãƒ©ãƒ³ãƒ€ãƒ é¸æŠï¼‰
        np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
        centroids = embeddings[np.random.choice(n_samples, k, replace=False)]

        max_iterations = 100
        tolerance = 1e-4

        for iteration in range(max_iterations):
            # å„ç‚¹ã‚’æœ€è¿‘ã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«å‰²ã‚Šå½“ã¦
            distances = np.sqrt(
                ((embeddings - centroids[:, np.newaxis]) ** 2).sum(axis=2)
            )
            labels = np.argmin(distances, axis=0)

            # æ–°ã—ã„ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—
            new_centroids = np.array(
                [embeddings[labels == i].mean(axis=0) for i in range(k)]
            )

            # åæŸãƒã‚§ãƒƒã‚¯
            if np.all(np.abs(centroids - new_centroids) < tolerance):
                break

            centroids = new_centroids

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆ
        clusters = []
        for i in range(k):
            cluster_nodes = [nodes[j] for j in range(len(nodes)) if labels[j] == i]

            if cluster_nodes:
                cluster = KnowledgeCluster(
                    cluster_id=f"cluster_{i}",
                    nodes=cluster_nodes,
                    centroid=centroids[i].tolist(),
                    topic_label=self._generate_topic_label(cluster_nodes),
                )
                cluster.coherence_score = self.measure_cluster_coherence(cluster)
                clusters.append(cluster)

        return clusters

    def _generate_topic_label(self, nodes: List[KnowledgeNode]) -> str:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒˆãƒ”ãƒƒã‚¯ãƒ©ãƒ™ãƒ«ç”Ÿæˆ"""
        # é »å‡ºå˜èªã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯æ¨å®š
        all_words = []
        for node in nodes:
            words = node.content.lower().split()
            all_words.extend(words)

        # é »åº¦è¨ˆç®—
        word_freq = defaultdict(int)
        for word in all_words:
            if len(word) > 3 and word.isalpha():
                word_freq[word] += 1

        # ä¸Šä½å˜èªã§ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:3]

        if top_words:
            return "_".join([word for word, freq in top_words])
        else:
            return "unknown_topic"

    def measure_cluster_coherence(self, cluster: KnowledgeCluster) -> float:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸€è²«æ€§æ¸¬å®š"""
        nodes = cluster.nodes
        if len(nodes) < 2:
            return 1.0

        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºé¡ä¼¼åº¦ã®å¹³å‡
        similarities = []
        for i in range(len(nodes)):
            for j in range(i + 1, len(nodes)):
                similarity = self.calculate_node_similarity(nodes[i], nodes[j])
                similarities.append(similarity)

        if similarities:
            coherence = np.mean(similarities)
        else:
            coherence = 0.0

        return coherence

    async def update_graph_dynamically(
        self, new_information: Dict[str, Any]
    ) -> GraphUpdate:
        """å‹•çš„ã‚°ãƒ©ãƒ•æ›´æ–°"""
        content = new_information["content"]
        source = new_information.get("source", "unknown")
        confidence = new_information.get("confidence", 0.8)

        # æ–°ã—ã„ãƒãƒ¼ãƒ‰ä½œæˆ
        new_node = self.create_node(
            content=content,
            node_type=NodeType.CONCEPT.value,
            metadata={"source": source, "confidence": confidence},
        )

        # æ—¢å­˜ãƒãƒ¼ãƒ‰ã¨ã®é–¢ä¿‚ç™ºè¦‹
        potential_relations = []
        for existing_id, existing_node in self.nodes.items():
            if existing_id != new_node.node_id:
                similarity = self.calculate_node_similarity(new_node, existing_node)

                if similarity > self.similarity_threshold:
                    # é–¢ä¿‚ã‚¨ãƒƒã‚¸ä½œæˆ
                    edge = self.create_edge(
                        source_id=new_node.node_id,
                        target_id=existing_id,
                        relation_type=RelationType.RELATED_TO.value,
                        strength=similarity,
                        metadata={
                            "discovery_type": "similarity",
                            "auto_generated": True,
                        },
                    )
                    potential_relations.append(edge)

        # æ›´æ–°è¨˜éŒ²
        update = GraphUpdate(
            update_type="node_added",
            target=new_node.node_id,
            data={
                "node_content": content,
                "relations_created": len(potential_relations),
                "confidence": confidence,
            },
        )

        self.evolution_history.append(
            {
                "timestamp": datetime.now(),
                "type": "dynamic_update",
                "details": update.data,
            }
        )

        self.stats["concept_updates"] += 1

        return update

    def track_knowledge_evolution(
        self, evolution_events: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """çŸ¥è­˜é€²åŒ–è¿½è·¡"""
        if not evolution_events:
            return {
                "evolution_rate": 0.0,
                "concept_growth": 0.0,
                "relation_growth": 0.0,
            }

        # æ™‚ç³»åˆ—ã§ã‚½ãƒ¼ãƒˆ
        sorted_events = sorted(evolution_events, key=lambda x: x["timestamp"])

        # é€²åŒ–ç‡è¨ˆç®—
        time_span = (
            sorted_events[-1]["timestamp"] - sorted_events[0]["timestamp"]
        ).total_seconds()
        evolution_rate = len(evolution_events) / max(
            1, time_span / 3600
        )  # æ™‚é–“ã‚ãŸã‚Šã®ã‚¤ãƒ™ãƒ³ãƒˆæ•°

        # ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
        event_types = defaultdict(int)
        for event in evolution_events:
            event_types[event["type"]] += 1

        concept_growth = event_types.get("concept_added", 0) + event_types.get(
            "concept_updated", 0
        )
        relation_growth = event_types.get("relation_discovered", 0)

        return {
            "evolution_rate": evolution_rate,
            "concept_growth": concept_growth,
            "relation_growth": relation_growth,
            "total_events": len(evolution_events),
            "time_span_hours": time_span / 3600,
            "event_type_distribution": dict(event_types),
        }

    async def infer_new_knowledge(
        self, existing_relations: List[ConceptRelation]
    ) -> List[ConceptRelation]:
        """çŸ¥è­˜æ¨è«–"""
        self.stats["inference_operations"] += 1
        inferred_relations = []

        # æ¨ç§»çš„é–¢ä¿‚ã®æ¨è«–
        for rel1 in existing_relations:
            for rel2 in existing_relations:
                if (
                    rel1.concept_b == rel2.concept_a
                    and rel1.concept_a != rel2.concept_b
                    and rel1.relation_type == rel2.relation_type == "causes"
                ):
                # è¤‡é›‘ãªæ¡ä»¶åˆ¤å®š
                    # A -> B -> C ã‹ã‚‰ A -> C ã‚’æ¨è«–
                    confidence = (
                        rel1.confidence * rel2.confidence * 0.8
                    )  # æ¨ç§»ã«ã‚ˆã‚‹æ¸›è¡°

                    # æ¨ç§»çš„æ¨è«–ã®é—¾å€¤ã‚’ä¸‹ã’ã¦ç™ºè¦‹ã—ã‚„ã™ãã™ã‚‹
                    if confidence > 0.4:  # 0.5â†’0.4ã«å¤‰æ›´
                        inferred = ConceptRelation(
                            concept_a=rel1.concept_a,
                            concept_b=rel2.concept_b,
                            relation_type="causes",
                            confidence=confidence,
                            context=f"Inferred from {rel1.concept_a} -> {rel1.concept_b} -> {rel2.concept_b}",
                            evidence=[f"Transitive inference via {rel1.concept_b}"],
                        )
                        inferred_relations.append(inferred)

        # å¯¾ç§°é–¢ä¿‚ã®æ¨è«–
        for rel in existing_relations:
            if rel.relation_type == "related_to" and rel.confidence > 0.7:
                # A relates to B => B relates to A
                symmetric = ConceptRelation(
                    concept_a=rel.concept_b,
                    concept_b=rel.concept_a,
                    relation_type="related_to",
                    confidence=rel.confidence * 0.9,
                    context=f"Symmetric relation of {rel.concept_a} -> {rel.concept_b}",
                    evidence=["Symmetric relation inference"],
                )
                inferred_relations.append(symmetric)

        logger.info(f"ğŸ§  æ¨è«–å®Œäº†: {len(inferred_relations)}ä»¶ã®æ–°çŸ¥è­˜")
        return inferred_relations

    def detect_knowledge_anomalies(
        self, knowledge_statements: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """çŸ¥è­˜ç•°å¸¸æ¤œå‡º"""
        anomalies = []

        # ä¿¡é ¼åº¦ã«ã‚ˆã‚‹ç•°å¸¸æ¤œå‡º
        confidences = [stmt["confidence"] for stmt in knowledge_statements]
        mean_confidence = np.mean(confidences)
        std_confidence = np.std(confidences)

        # ã‚ˆã‚Šå³ã—ã„é—¾å€¤è¨­å®šã§ä½ä¿¡é ¼åº¦ã®ã¿ç•°å¸¸åˆ¤å®š
        threshold = 0.3  # çµ±è¨ˆãƒ™ãƒ¼ã‚¹ã§ã¯ãªãå›ºå®šé—¾å€¤

        for stmt in knowledge_statements:
            if stmt["confidence"] < threshold:
                anomaly = {
                    "concept_a": stmt["concept_a"],
                    "concept_b": stmt["concept_b"],
                    "confidence": stmt["confidence"],
                    "anomaly_type": "low_confidence",
                    "severity": "medium" if stmt["confidence"] < 0.2 else "low",
                }
                anomalies.append(anomaly)

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ç•°å¸¸ã®æ¤œå‡ºï¼ˆä½ä¿¡é ¼åº¦ã®å ´åˆã®ã¿ï¼‰
        for stmt in knowledge_statements:
            concept_a = stmt["concept_a"]
            concept_b = stmt["concept_b"]
            confidence = stmt["confidence"]

            # é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆä½ä¿¡é ¼åº¦ã‹ã¤ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è·é›¢ãŒå¤§ãã„å ´åˆã®ã¿ï¼‰
            if confidence < 0.3 and self._are_concepts_semantically_distant(
                concept_a, concept_b
            ):
                anomaly = {
                    "concept_a": concept_a,
                    "concept_b": concept_b,
                    "confidence": confidence,
                    "anomaly_type": "semantic_mismatch",
                    "severity": "high",
                }
                anomalies.append(anomaly)

        return anomalies

    def _are_concepts_semantically_distant(
        self, concept_a: str, concept_b: str
    ) -> bool:
        """ã‚³ãƒ³ã‚»ãƒ—ãƒˆã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è·é›¢åˆ¤å®š"""
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†é¡
        tech_domains = {
            "ai": ["machine learning", "neural network", "algorithm", "model"],
            "database": ["sql", "query", "index", "database"],
            "system": ["performance", "memory", "cpu", "cache"],
            "web": ["api", "service", "http", "endpoint"],
            "other": ["cooking", "recipe", "car", "maintenance", "sports"],
        }

        domain_a = self._classify_concept_domain(concept_a, tech_domains)
        domain_b = self._classify_concept_domain(concept_b, tech_domains)

        # æŠ€è¡“ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨éæŠ€è¡“ãƒ‰ãƒ¡ã‚¤ãƒ³ã®çµ„ã¿åˆã‚ã›ã¯ç•°å¸¸
        return (
            domain_a in ["ai", "database", "system", "web"] and domain_b == "other"
        ) or (domain_b in ["ai", "database", "system", "web"] and domain_a == "other")

    def _classify_concept_domain(
        self, concept: str, domains: Dict[str, List[str]]
    ) -> str:
        """ã‚³ãƒ³ã‚»ãƒ—ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†é¡"""
        concept_lower = concept.lower()

        for domain, keywords in domains.items():
            for keyword in keywords:
                if keyword in concept_lower:
                    return domain
        # ç¹°ã‚Šè¿”ã—å‡¦ç†

        return "other"

    def integrate_multilingual_knowledge(
        self, multilingual_concepts: Dict[str, str]
    ) -> Dict[str, Any]:
        """å¤šè¨€èªçŸ¥è­˜çµ±åˆ"""
        # çµ±ä¸€IDç”Ÿæˆ
        primary_concept = multilingual_concepts.get(
            "en", list(multilingual_concepts.values())[0]
        )
        unified_id = self._generate_node_id(primary_concept)

        # è¨€èªãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜
        for lang, concept in multilingual_concepts.items():
            self.language_mappings[unified_id][lang] = concept

        # ä¸»è¨€èªæ±ºå®šï¼ˆè‹±èªå„ªå…ˆï¼‰
        primary_language = (
            "en"
            if "en" in multilingual_concepts
            else list(multilingual_concepts.keys())[0]
        )

        unified_concept = {
            "unified_id": unified_id,
            "primary_language": primary_language,
            "primary_concept": multilingual_concepts[primary_language],
            "translations": multilingual_concepts,
            "language_count": len(multilingual_concepts),
        }

        return unified_concept

    def discover_cross_lingual_relations(
        self, text_sources: Dict[str, List[str]]
    ) -> List[Dict[str, Any]]:
        """å¤šè¨€èªé–“é–¢ä¿‚ç™ºè¦‹"""
        cross_lingual_relations = []

        # è¨€èªãƒšã‚¢ã®çµ„ã¿åˆã‚ã›
        languages = list(text_sources.keys())

        for i, lang1 in enumerate(languages):
            for lang2 in languages[i + 1 :]:
                # å„è¨€èªã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
                embeddings1 = [
                    self.generate_embedding(text) for text in text_sources[lang1]
                ]
                embeddings2 = [
                    self.generate_embedding(text) for text in text_sources[lang2]
                ]

                # é¡ä¼¼æ€§è¨ˆç®—
                for emb1 in embeddings1:
                    for emb2 in embeddings2:
                        similarity = emb1.cosine_similarity(emb2)

                        if not (similarity > 0.7:  # é«˜ã„é¡ä¼¼åº¦):
                            continue  # Early return to reduce nesting
                        # Reduced nesting - original condition satisfied
                        if similarity > 0.7:  # é«˜ã„é¡ä¼¼åº¦
                            relation = {
                                "source_lang": lang1,
                                "target_lang": lang2,
                                "source_text": emb1.text,
                                "target_text": emb2.text,
                                "semantic_similarity": similarity,
                                "relation_type": "translation_equivalent",
                            }
                            cross_lingual_relations.append(relation)

        return cross_lingual_relations

    async def batch_process_nodes(
        self, batch_nodes: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ãƒãƒ¼ãƒ‰ãƒãƒƒãƒå‡¦ç†"""
        start_time = datetime.now()
        processed_count = 0
        success_count = 0

        for node_data in batch_nodes:
            try:
                node = self.create_node(
                    content=node_data["content"],
                    node_type=node_data["type"],
                    metadata=node_data.get("metadata", {}),
                )
                processed_count += 1
                success_count += 1

            except Exception as e:
                logger.warning(f"ãƒãƒ¼ãƒ‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                processed_count += 1

        processing_time = (datetime.now() - start_time).total_seconds()
        success_rate = success_count / processed_count if processed_count > 0 else 0

        return {
            "processed_count": processed_count,
            "success_count": success_count,
            "success_rate": success_rate,
            "processing_time": processing_time,
        }

    def find_shortest_path(
        self, start_node: str, end_node: str, max_depth: int = 5
    ) -> Optional[List[str]]:
        """æœ€çŸ­ãƒ‘ã‚¹æ¢ç´¢"""
        if start_node == end_node:
            return [start_node]

        # BFSæ¢ç´¢
        queue = deque([(start_node, [start_node])])
        visited = {start_node}

        while queue:
            current_node, path = queue.popleft()

        # ãƒ«ãƒ¼ãƒ—å‡¦ç†
            if len(path) > max_depth:
                continue

            # éš£æ¥ãƒãƒ¼ãƒ‰å–å¾—
            neighbors = self._get_neighbors(current_node)

            for neighbor in neighbors:
                if neighbor == end_node:
                    return path + [neighbor]

                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append((neighbor, path + [neighbor]))

        return None  # ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„

    def _get_neighbors(self, node_id: str) -> List[str]:
        """éš£æ¥ãƒãƒ¼ãƒ‰å–å¾—"""
        neighbors = []

        for edge in self.edges:
            if edge.source_id == node_id:
                neighbors.append(edge.target_id)
            elif edge.target_id == node_id:
                neighbors.append(edge.source_id)

        return neighbors

    async def run_discovery_cycle(
        self, input_documents: List[str], discovery_config: Dict[str, Any]
    ) -> DiscoveryResult:
        """å®Œå…¨çŸ¥è­˜ç™ºè¦‹ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œ"""
        start_time = datetime.now()

        # Step 1: ã‚³ãƒ³ã‚»ãƒ—ãƒˆé–¢ä¿‚ç™ºè¦‹
        relations = await self.discover_concept_relations(input_documents)

        # Step 2: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        clusters = []
        if discovery_config.get("enable_clustering", False):
            all_nodes = list(self.nodes.values())
            if len(all_nodes) >= 3:
                clusters = self.cluster_knowledge(all_nodes, num_clusters=3)

        # Step 3: æ¨è«–ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        inferred_relations = []
        if discovery_config.get("enable_inference", False):
            inferred_relations = await self.infer_new_knowledge(relations)

        # çµæœçµ±åˆ
        total_entities = len(relations) + len(clusters) + len(inferred_relations)

        # ä¿¡é ¼åº¦è¨ˆç®—
        if relations:
            avg_confidence = np.mean([r.confidence for r in relations])
        else:
            avg_confidence = 0.5

        discovery_result = DiscoveryResult(
            discovery_type="comprehensive",
            entities=relations + clusters + inferred_relations,
            confidence=avg_confidence,
            metadata={
                "relations_found": len(relations),
                "clusters_formed": len(clusters),
                "inferences_made": len(inferred_relations),
                "processing_time": (datetime.now() - start_time).total_seconds(),
            },
        )

        return discovery_result

    def get_graph_statistics(self) -> Dict[str, Any]:
        """ã‚°ãƒ©ãƒ•çµ±è¨ˆå–å¾—"""
        total_nodes = len(self.nodes)
        total_edges = len(self.edges)

        # å¹³å‡æ¬¡æ•°
        if total_nodes > 0:
            average_degree = (2 * total_edges) / total_nodes
        else:
            average_degree = 0.0

        # ã‚°ãƒ©ãƒ•å¯†åº¦
        if total_nodes > 1:
            max_edges = total_nodes * (total_nodes - 1) / 2
            graph_density = total_edges / max_edges
        else:
            graph_density = 0.0

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¿‚æ•°ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        clustering_coefficient = self._calculate_clustering_coefficient()

        # é€£çµæˆåˆ†æ•°ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        connected_components = self._count_connected_components()

        return {
            "total_nodes": total_nodes,
            "total_edges": total_edges,
            "average_degree": average_degree,
            "clustering_coefficient": clustering_coefficient,
            "graph_density": graph_density,
            "connected_components": connected_components,
            "clusters_formed": len(self.clusters),
            "languages_supported": len(self.language_mappings),
        }

    def _calculate_clustering_coefficient(self) -> float:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¿‚æ•°è¨ˆç®—"""
        if len(self.nodes) < 3:
            return 0.0

        # ç°¡æ˜“ç‰ˆï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        sample_nodes = list(self.nodes.keys())[: min(10, len(self.nodes))]
        coefficients = []

        for node_id in sample_nodes:
            neighbors = self._get_neighbors(node_id)
        # ç¹°ã‚Šè¿”ã—å‡¦ç†
            if len(neighbors) < 2:
                coefficients.append(0.0)
                continue

            # éš£æ¥ãƒãƒ¼ãƒ‰é–“ã®ã‚¨ãƒƒã‚¸æ•°
            neighbor_edges = 0
            for i, neighbor1 in enumerate(neighbors):
                for neighbor2 in neighbors[i + 1 :]:
                    if self._has_edge(neighbor1, neighbor2):
                        neighbor_edges += 1

            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¿‚æ•°
            max_neighbor_edges = len(neighbors) * (len(neighbors) - 1) / 2
            if max_neighbor_edges > 0:
                coefficient = neighbor_edges / max_neighbor_edges
            else:
                coefficient = 0.0

            coefficients.append(coefficient)

        return np.mean(coefficients) if coefficients else 0.0

    def _has_edge(self, node1: str, node2: str) -> bool:
        """ã‚¨ãƒƒã‚¸å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
        for edge in self.edges:
            if (edge.source_id == node1 and edge.target_id == node2) or (
                edge.source_id == node2 and edge.target_id == node1
            ):
                return True
        return False

    def _count_connected_components(self) -> int:
        """é€£çµæˆåˆ†æ•°è¨ˆç®—"""
        if not self.nodes:
            return 0

        visited = set()
        components = 0

        for node_id in self.nodes.keys():
            if node_id not in visited:
                self._dfs_mark_component(node_id, visited)
                components += 1

        return components

    def _dfs_mark_component(self, node_id: str, visited: Set[str]):
        """DFSã«ã‚ˆã‚‹é€£çµæˆåˆ†ãƒãƒ¼ã‚­ãƒ³ã‚°"""
        visited.add(node_id)

        for neighbor in self._get_neighbors(node_id):
            if neighbor not in visited:
                self._dfs_mark_component(neighbor, visited)

    def assess_knowledge_quality(self) -> Dict[str, float]:
        """çŸ¥è­˜å“è³ªè©•ä¾¡"""
        # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢
        consistency_score = self._calculate_consistency_score()

        # å®Œå…¨æ€§ã‚¹ã‚³ã‚¢
        completeness_score = self._calculate_completeness_score()

        # ç²¾åº¦ã‚¹ã‚³ã‚¢
        accuracy_score = self._calculate_accuracy_score()

        # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
        overall_quality = (
            consistency_score * 0.4 + completeness_score * 0.3 + accuracy_score * 0.3
        )

        return {
            "overall_quality_score": overall_quality,
            "consistency_score": consistency_score,
            "completeness_score": completeness_score,
            "accuracy_score": accuracy_score,
        }

    def _calculate_consistency_score(self) -> float:
        """ä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not self.edges:
            return 1.0

        # çŸ›ç›¾ã™ã‚‹é–¢ä¿‚ã®æ¤œå‡º
        contradictions = 0
        total_checks = 0

        for edge1 in self.edges:
        # ç¹°ã‚Šè¿”ã—å‡¦ç†
            for edge2 in self.edges:
                # è¤‡é›‘ãªæ¡ä»¶åˆ¤å®š
                if (
                    edge1.source_id == edge2.source_id
                    and edge1.target_id == edge2.target_id
                    and edge1 != edge2
                ):
                    total_checks += 1

                    # çŸ›ç›¾ãƒã‚§ãƒƒã‚¯ï¼ˆä¾‹ï¼šA causes B ã¨ A prevents Bï¼‰
                    if self._are_relations_contradictory(
                        edge1.relation_type, edge2.relation_type
                    ):
                        contradictions += 1

        if total_checks == 0:
            return 1.0

        consistency = 1.0 - (contradictions / total_checks)
        return max(0.0, consistency)

    def _are_relations_contradictory(self, rel1: str, rel2: str) -> bool:
        """é–¢ä¿‚ã®çŸ›ç›¾åˆ¤å®š"""
        contradictory_pairs = [
            ("causes", "prevents"),
            ("improves", "degrades"),
            ("increases", "decreases"),
        ]

        for pair in contradictory_pairs:
            if (rel1 == pair[0] and rel2 == pair[1]) or (
                rel1 == pair[1] and rel2 == pair[0]
            ):
                return True

        return False

    def _calculate_completeness_score(self) -> float:
        """å®Œå…¨æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # ãƒãƒ¼ãƒ‰å¯†åº¦ã«ã‚ˆã‚‹å®Œå…¨æ€§è©•ä¾¡
        total_nodes = len(self.nodes)
        total_edges = len(self.edges)

        if total_nodes < 2:
            return 1.0

        # æœŸå¾…ã•ã‚Œã‚‹ã‚¨ãƒƒã‚¸æ•°ï¼ˆå®Œå…¨ã‚°ãƒ©ãƒ•ã®ä¸€å®šå‰²åˆï¼‰
        max_possible_edges = total_nodes * (total_nodes - 1) / 2
        expected_edges = max_possible_edges * 0.1  # 10%ã®æ¥ç¶šã‚’æœŸå¾…

        completeness = (
            min(1.0, total_edges / expected_edges) if expected_edges > 0 else 0.0
        )

        return completeness

    def _calculate_accuracy_score(self) -> float:
        """ç²¾åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not self.edges:
            return 1.0

        # ã‚¨ãƒƒã‚¸ã®ä¿¡é ¼åº¦å¹³å‡
        confidences = [
            edge.confidence for edge in self.edges if hasattr(edge, "confidence")
        ]

        if confidences:
            accuracy = np.mean(confidences)
        else:
            # å¼·åº¦ã‹ã‚‰ä¿¡é ¼åº¦ã‚’æ¨å®š
            strengths = [edge.strength for edge in self.edges]
            accuracy = np.mean(strengths) if strengths else 0.5

        return accuracy


# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
__all__ = [
    "DynamicKnowledgeGraph",
    "KnowledgeNode",
    "KnowledgeEdge",
    "ConceptRelation",
    "SemanticEmbedding",
    "KnowledgeCluster",
    "GraphUpdate",
    "DiscoveryResult",
    "NodeType",
    "RelationType",
]
