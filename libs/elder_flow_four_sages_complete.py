#!/usr/bin/env python3
"""
Elder Flow + 4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ å®Œå…¨å®Ÿè£…
Created: 2025-01-11 23:40
Author: Claude Elder

çœŸã®4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ ã¨Elder Flowã®å®Œå…¨çµ±åˆ
è‡ªå¾‹å­¦ç¿’ãƒ»é€²åŒ–ã™ã‚‹æ¬¡ä¸–ä»£é–‹ç™ºã‚·ã‚¹ãƒ†ãƒ 
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

import asyncio
import json
import logging
import sqlite3
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Set, Tuple
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict

# Elder Flowçµ±åˆ
from elder_flow_parallel_executor import (
    ParallelServantExecutor, ServantTask, ServantType, TaskPriority, TaskStatus
)
from elder_flow_task_decomposer import TaskDecomposer, DecomposedTask, TaskCategory


class SageType(Enum):
    """è³¢è€…ã‚¿ã‚¤ãƒ—"""
    KNOWLEDGE = "knowledge_sage"    # ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…
    TASK = "task_sage"             # ğŸ“‹ ã‚¿ã‚¹ã‚¯è³¢è€…
    INCIDENT = "incident_sage"     # ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…
    RAG = "rag_sage"              # ğŸ” RAGè³¢è€…


@dataclass
class KnowledgeEntry:
    """çŸ¥è­˜ã‚¨ãƒ³ãƒˆãƒª"""
    id: str
    category: str
    title: str
    content: str
    tags: List[str]
    confidence: float
    created_at: datetime
    updated_at: datetime
    usage_count: int = 0


@dataclass
class TaskPattern:
    """ã‚¿ã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    pattern_id: str
    request_type: str
    task_sequence: List[str]
    success_rate: float
    average_time: float
    optimization_tips: List[str]
    created_at: datetime


@dataclass
class IncidentRecord:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè¨˜éŒ²"""
    incident_id: str
    severity: str
    description: str
    context: Dict[str, Any]
    resolution: Optional[str]
    prevention_measures: List[str]
    created_at: datetime


@dataclass
class RAGContext:
    """RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""
    query: str
    relevant_docs: List[Dict[str, Any]]
    similarity_scores: List[float]
    generated_response: str
    confidence: float


class FunctionalKnowledgeSage:
    """ğŸ“š æ©Ÿèƒ½ã™ã‚‹çŸ¥è­˜è³¢è€…"""

    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.KnowledgeSage")
        self.knowledge_db = {}
        self.categories = {
            "patterns": [],
            "best_practices": [],
            "architectures": [],
            "security": [],
            "performance": []
        }
        self._initialize_base_knowledge()

    def _initialize_base_knowledge(self):
        """åŸºç¤çŸ¥è­˜ã®åˆæœŸåŒ–"""
        base_knowledge = [
            KnowledgeEntry(
                id="oauth2_best_practice",
                category="security",
                title="OAuth2.0ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹",
                content="PKCEä½¿ç”¨ã€HTTPSå¿…é ˆã€çŸ­æœŸé–“ãƒˆãƒ¼ã‚¯ãƒ³ã€é©åˆ‡ã‚¹ã‚³ãƒ¼ãƒ—è¨­å®š",
                tags=["oauth", "security", "authentication"],
                confidence=0.95,
                created_at=datetime.now(),
                updated_at=datetime.now()
            ),
            KnowledgeEntry(
                id="api_design_pattern",
                category="patterns",
                title="RESTful APIè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³",
                content="ãƒªã‚½ãƒ¼ã‚¹æŒ‡å‘ã€HTTPãƒ¡ã‚½ãƒƒãƒ‰é©åˆ‡ä½¿ç”¨ã€çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹",
                tags=["api", "rest", "design"],
                confidence=0.9,
                created_at=datetime.now(),
                updated_at=datetime.now()
            ),
            KnowledgeEntry(
                id="parallel_optimization",
                category="performance",
                title="ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–",
                content="ä¾å­˜é–¢ä¿‚æœ€å°åŒ–ã€ãƒãƒƒãƒãƒ³ã‚°ã€é©åˆ‡ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°è¨­å®š",
                tags=["parallel", "performance", "optimization"],
                confidence=0.88,
                created_at=datetime.now(),
                updated_at=datetime.now()
            )
        ]

        for entry in base_knowledge:
            self.knowledge_db[entry.id] = entry
            self.categories[entry.category].append(entry.id)

    async def search_knowledge(self, query: str, category: str = None) -> List[KnowledgeEntry]:
        """çŸ¥è­˜æ¤œç´¢"""
        results = []
        query_lower = query.lower()

        for entry in self.knowledge_db.values():
            if category and entry.category != category:
                continue

            score = 0
            # ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒƒãƒ
            if any(word in entry.title.lower() for word in query_lower.split()):
                score += 0.5
            # ã‚¿ã‚°ãƒãƒƒãƒ
            if any(tag in query_lower for tag in entry.tags):
                score += 0.3
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒãƒ
            if any(word in entry.content.lower() for word in query_lower.split()):
                score += 0.2

            if score > 0:
                results.append((score, entry))

        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x[0], reverse=True)
        return [entry for score, entry in results[:5]]

    async def learn_from_execution(self, request: str, tasks: List[DecomposedTask],
                                 result: Dict[str, Any]):
        """å®Ÿè¡Œçµæœã‹ã‚‰å­¦ç¿’"""
        if result.get('summary', {}).get('failed', 0) == 0:
            # æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’
            pattern_id = hashlib.md5(request.encode()).hexdigest()[:8]

            new_knowledge = KnowledgeEntry(
                id=f"success_pattern_{pattern_id}",
                category="patterns",
                title=f"æˆåŠŸå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³: {self._categorize_request(request)}",
                content=f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {request[:100]}..., åŠ¹ç‡: {result['summary'].get('parallel_efficiency', 0):.1f}%",
                tags=self._extract_tags_from_request(request),
                confidence=min(0.95, 0.7 + result['summary'].get('parallel_efficiency', 0) / 200),
                created_at=datetime.now(),
                updated_at=datetime.now()
            )

            self.knowledge_db[new_knowledge.id] = new_knowledge
            self.categories["patterns"].append(new_knowledge.id)

            self.logger.info(f"ğŸ“š æ–°ã—ã„æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’: {new_knowledge.title}")

    def _categorize_request(self, request: str) -> str:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ†é¡"""
        if "oauth" in request.lower():
            return "èªè¨¼ã‚·ã‚¹ãƒ†ãƒ "
        elif "api" in request.lower():
            return "APIé–‹ç™º"
        elif "database" in request.lower():
            return "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹"
        else:
            return "ä¸€èˆ¬é–‹ç™º"

    def _extract_tags_from_request(self, request: str) -> List[str]:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰ã‚¿ã‚°æŠ½å‡º"""
        tags = []
        keywords = {
            "oauth": "oauth",
            "api": "api",
            "database": "database",
            "user": "user_management",
            "auth": "authentication",
            "security": "security"
        }

        request_lower = request.lower()
        for keyword, tag in keywords.items():
            if keyword in request_lower:
                tags.append(tag)

        return tags


class FunctionalTaskSage:
    """ğŸ“‹ æ©Ÿèƒ½ã™ã‚‹ã‚¿ã‚¹ã‚¯è³¢è€…"""

    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.TaskSage")
        self.task_patterns = {}
        self.execution_history = []
        self.optimization_rules = [
            {"rule": "ç‹¬ç«‹ã‚¿ã‚¹ã‚¯ã®ä¸¦åˆ—åŒ–", "condition": "no_dependencies", "improvement": 0.3},
            {"rule": "ãƒãƒƒãƒå‡¦ç†ã®é©ç”¨", "condition": "similar_tasks", "improvement": 0.2},
            {"rule": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨", "condition": "repeated_operations", "improvement": 0.15}
        ]

    async def analyze_task_optimization(self, tasks: List[DecomposedTask]) -> List[Dict[str, Any]]:
        """ã‚¿ã‚¹ã‚¯æœ€é©åŒ–åˆ†æ"""
        optimizations = []

        # ä¸¦åˆ—åŒ–ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«åˆ†æ
        independent_tasks = [t for t in tasks if not t.dependencies]
        if len(independent_tasks) > 1:
            optimizations.append({
                "type": "parallelization",
                "description": f"{len(independent_tasks)}å€‹ã®ç‹¬ç«‹ã‚¿ã‚¹ã‚¯ã‚’å®Œå…¨ä¸¦åˆ—å®Ÿè¡Œ",
                "expected_improvement": 0.4,
                "confidence": 0.9
            })

        # é¡ä¼¼ã‚¿ã‚¹ã‚¯ã®ãƒãƒƒãƒãƒ³ã‚°
        task_groups = self._group_similar_tasks(tasks)
        for group_name, group_tasks in task_groups.items():
            if len(group_tasks) > 2:
                optimizations.append({
                    "type": "batching",
                    "description": f"{group_name}ã‚¿ã‚¹ã‚¯ã®ãƒãƒƒãƒå‡¦ç† ({len(group_tasks)}å€‹)",
                    "expected_improvement": 0.2,
                    "confidence": 0.8
                })

        # ä¾å­˜é–¢ä¿‚ã®æœ€é©åŒ–
        dep_optimization = self._analyze_dependencies(tasks)
        if dep_optimization:
            optimizations.extend(dep_optimization)

        return optimizations

    def _group_similar_tasks(self, tasks: List[DecomposedTask]) -> Dict[str, List[DecomposedTask]]:
        """é¡ä¼¼ã‚¿ã‚¹ã‚¯ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
        groups = defaultdict(list)

        for task in tasks:
            if task.servant_type == ServantType.CODE_CRAFTSMAN:
                groups["code_creation"].append(task)
            elif task.servant_type == ServantType.TEST_GUARDIAN:
                groups["testing"].append(task)
            elif task.servant_type == ServantType.QUALITY_INSPECTOR:
                groups["quality_check"].append(task)
            else:
                groups["other"].append(task)

        # 2å€‹æœªæº€ã®ã‚°ãƒ«ãƒ¼ãƒ—ã¯é™¤å¤–
        return {k: v for k, v in groups.items() if len(v) >= 2}

    def _analyze_dependencies(self, tasks: List[DecomposedTask]) -> List[Dict[str, Any]]:
        """ä¾å­˜é–¢ä¿‚åˆ†æ"""
        optimizations = []

        # é•·ã„ä¾å­˜ãƒã‚§ãƒ¼ãƒ³ã®æ¤œå‡º
        max_depth = 0
        for task in tasks:
            depth = len(task.dependencies)
            max_depth = max(max_depth, depth)

        if max_depth > 3:
            optimizations.append({
                "type": "dependency_optimization",
                "description": f"æ·±ã„ä¾å­˜é–¢ä¿‚ã‚’æ¤œå‡º (æœ€å¤§{max_depth}å±¤) - ä¸¦åˆ—åŒ–å†æ¤œè¨ã‚’æ¨å¥¨",
                "expected_improvement": 0.15,
                "confidence": 0.75
            })

        return optimizations

    async def record_execution(self, tasks: List[DecomposedTask], result: Dict[str, Any]):
        """å®Ÿè¡Œè¨˜éŒ²"""
        execution_record = {
            "timestamp": datetime.now(),
            "task_count": len(tasks),
            "execution_time": result.get('summary', {}).get('execution_time', 0),
            "parallel_efficiency": result.get('summary', {}).get('parallel_efficiency', 0),
            "success_rate": result.get('summary', {}).get('completed', 0) / max(len(tasks), 1),
            "optimizations_applied": []
        }

        self.execution_history.append(execution_record)
        self.logger.info(f"ğŸ“‹ å®Ÿè¡Œè¨˜éŒ²ä¿å­˜: åŠ¹ç‡{execution_record['parallel_efficiency']:.1f}%")


class FunctionalIncidentSage:
    """ğŸš¨ æ©Ÿèƒ½ã™ã‚‹ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…"""

    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.IncidentSage")
        self.incident_history = []
        self.risk_patterns = [
            {"pattern": "oauth.*implementation", "risk": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§", "severity": "high"},
            {"pattern": "database.*operation", "risk": "ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§", "severity": "medium"},
            {"pattern": "api.*endpoint", "risk": "èªè¨¼ãƒ»èªå¯", "severity": "medium"},
            {"pattern": "file.*path", "risk": "ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«", "severity": "medium"}
        ]
        self.monitoring_active = False

    async def analyze_risks(self, request: str, tasks: List[DecomposedTask]) -> List[Dict[str, Any]]:
        """ãƒªã‚¹ã‚¯åˆ†æ"""
        risks = []
        request_lower = request.lower()

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ãƒªã‚¹ã‚¯æ¤œå‡º
        for pattern_def in self.risk_patterns:
            import re
            if re.search(pattern_def["pattern"], request_lower):
                risks.append({
                    "type": "pattern_risk",
                    "risk": pattern_def["risk"],
                    "severity": pattern_def["severity"],
                    "description": f"ãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern_def['pattern']}' ã«åŸºã¥ããƒªã‚¹ã‚¯æ¤œå‡º",
                    "mitigation": self._get_mitigation_for_risk(pattern_def["risk"])
                })

        # ã‚¿ã‚¹ã‚¯è¤‡é›‘åº¦ãƒªã‚¹ã‚¯
        if len(tasks) > 15:
            risks.append({
                "type": "complexity_risk",
                "risk": "é«˜è¤‡é›‘åº¦å®Ÿè£…",
                "severity": "medium",
                "description": f"å¤§è¦æ¨¡ã‚¿ã‚¹ã‚¯ ({len(tasks)}å€‹) ã®è¤‡é›‘åº¦ãƒªã‚¹ã‚¯",
                "mitigation": ["æ®µéšçš„å®Ÿè£…", "ãƒ†ã‚¹ãƒˆå¼·åŒ–", "ãƒ¬ãƒ“ãƒ¥ãƒ¼å¼·åŒ–"]
            })

        # ä¸¦åˆ—å‡¦ç†ãƒªã‚¹ã‚¯
        parallel_tasks = len([t for t in tasks if not t.dependencies])
        if parallel_tasks > 10:
            risks.append({
                "type": "concurrency_risk",
                "risk": "ä¸¦åˆ—å‡¦ç†ç«¶åˆ",
                "severity": "low",
                "description": f"é«˜ä¸¦åˆ—åº¦ ({parallel_tasks}å€‹) ã«ã‚ˆã‚‹ç«¶åˆãƒªã‚¹ã‚¯",
                "mitigation": ["ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™", "ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡º", "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š"]
            })

        return risks

    def _get_mitigation_for_risk(self, risk: str) -> List[str]:
        """ãƒªã‚¹ã‚¯ã®è»½æ¸›ç­–"""
        mitigations = {
            "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§": ["ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå¼·åŒ–", "ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒšãƒãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"],
            "ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§": ["ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†", "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", "æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"],
            "èªè¨¼ãƒ»èªå¯": ["æ¨©é™ãƒã‚§ãƒƒã‚¯å¼·åŒ–", "JWTãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼", "ãƒ¬ãƒ¼ãƒˆåˆ¶é™"],
            "ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«": ["å…¥åŠ›å€¤æ¤œè¨¼", "ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆ", "ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹åŒ–"]
        }
        return mitigations.get(risk, ["ä¸€èˆ¬çš„ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–"])

    async def start_monitoring(self, session_id: str):
        """ç›£è¦–é–‹å§‹"""
        self.monitoring_active = True
        self.logger.info(f"ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç›£è¦–é–‹å§‹: {session_id}")

    async def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.monitoring_active = False
        self.logger.info("ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç›£è¦–åœæ­¢")

    async def record_incident(self, severity: str, description: str, context: Dict[str, Any]):
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè¨˜éŒ²"""
        incident = IncidentRecord(
            incident_id=f"inc_{int(datetime.now().timestamp())}",
            severity=severity,
            description=description,
            context=context,
            resolution=None,
            prevention_measures=[],
            created_at=datetime.now()
        )

        self.incident_history.append(incident)
        self.logger.warning(f"ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè¨˜éŒ²: {severity} - {description}")


class FunctionalRAGSage:
    """ğŸ” æ©Ÿèƒ½ã™ã‚‹RAGè³¢è€…"""

    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.RAGSage")
        self.document_store = {}
        self.implementation_patterns = [
            {
                "pattern_name": "ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹èªè¨¼",
                "description": "JWT + OAuth2.0 ã‚’ä½¿ç”¨ã—ãŸãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹èªè¨¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£",
                "technologies": ["FastAPI", "PostgreSQL", "Redis", "JWT"],
                "use_cases": ["èªè¨¼ã‚·ã‚¹ãƒ†ãƒ ", "APIèªè¨¼", "ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†"],
                "similarity_keywords": ["oauth", "jwt", "auth", "microservice"],
                "implementation_guide": {
                    "steps": ["JWTç§˜å¯†éµè¨­å®š", "OAuth2ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…", "èªè¨¼ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢", "ãƒ†ã‚¹ãƒˆ"]
                }
            },
            {
                "pattern_name": "é«˜ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ",
                "description": "asyncio + ThreadPoolExecutor ã‚’ä½¿ç”¨ã—ãŸé«˜åŠ¹ç‡ä¸¦åˆ—å‡¦ç†",
                "technologies": ["Python", "asyncio", "concurrent.futures"],
                "use_cases": ["ä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ", "ãƒãƒƒãƒå‡¦ç†", "ãƒ‡ãƒ¼ã‚¿å‡¦ç†"],
                "similarity_keywords": ["parallel", "async", "concurrent", "batch"],
                "implementation_guide": {
                    "steps": ["ä¾å­˜é–¢ä¿‚åˆ†æ", "ä¸¦åˆ—ã‚°ãƒ«ãƒ¼ãƒ—åˆ†å‰²", "ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°", "ç›£è¦–"]
                }
            }
        ]

    async def search_similar_implementations(self, query: str) -> List[Dict[str, Any]]:
        """é¡ä¼¼å®Ÿè£…æ¤œç´¢"""
        results = []
        query_lower = query.lower()

        for pattern in self.implementation_patterns:
            similarity_score = 0

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
            for keyword in pattern["similarity_keywords"]:
                if keyword in query_lower:
                    similarity_score += 0.25

            # ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãƒãƒƒãƒãƒ³ã‚°
            for use_case in pattern["use_cases"]:
                if any(word in query_lower for word in use_case.lower().split()):
                    similarity_score += 0.15

            if similarity_score > 0:
                results.append({
                    "pattern": pattern,
                    "similarity_score": min(similarity_score, 1.0),
                    "recommendation_strength": "high" if similarity_score > 0.5 else "medium"
                })

        # é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        return results[:3]

    async def generate_implementation_suggestions(self, request: str) -> List[Dict[str, Any]]:
        """å®Ÿè£…ææ¡ˆç”Ÿæˆ"""
        suggestions = []
        similar_patterns = await self.search_similar_implementations(request)

        for result in similar_patterns:
            pattern = result["pattern"]
            suggestions.append({
                "approach_name": pattern["pattern_name"],
                "description": pattern["description"],
                "technologies": pattern["technologies"],
                "implementation_steps": pattern["implementation_guide"]["steps"],
                "confidence": result["similarity_score"],
                "recommendation": f"é¡ä¼¼åº¦ {result['similarity_score']:.1f} - {result['recommendation_strength']}"
            })

        return suggestions


class ElderFlowFourSagesComplete:
    """Elder Flow + å®Œå…¨æ©Ÿèƒ½4è³¢è€…çµ±åˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, max_workers: int = 8):
        self.logger = logging.getLogger(__name__)

        # Elder Flow ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.decomposer = TaskDecomposer()
        self.executor = ParallelServantExecutor(max_workers=max_workers)

        # å®Œå…¨æ©Ÿèƒ½4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ 
        self.knowledge_sage = FunctionalKnowledgeSage()
        self.task_sage = FunctionalTaskSage()
        self.incident_sage = FunctionalIncidentSage()
        self.rag_sage = FunctionalRAGSage()

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
        self.sessions = {}
        self.execution_count = 0

        self.logger.info("ğŸŒŠğŸ§™â€â™‚ï¸ Elder Flow + å®Œå…¨æ©Ÿèƒ½4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    async def execute_with_full_sages_wisdom(self, request: str) -> Dict[str, Any]:
        """å®Œå…¨æ©Ÿèƒ½4è³¢è€…ã®è‹±çŸ¥ã‚’çµ±åˆã—ãŸElder Flowå®Ÿè¡Œ"""
        session_id = f"complete_session_{self.execution_count}"
        self.execution_count += 1

        self.logger.info(f"ğŸŒŠğŸ§™â€â™‚ï¸ å®Œå…¨4è³¢è€…çµ±åˆå®Ÿè¡Œé–‹å§‹: {session_id}")

        start_time = datetime.now()

        # Phase 1: ã‚¿ã‚¹ã‚¯åˆ†è§£
        self.logger.info("ğŸ“‹ Phase 1: ã‚¿ã‚¹ã‚¯åˆ†è§£")
        decomposed_tasks = self.decomposer.decompose_request(request)

        # Phase 2: å®Œå…¨4è³¢è€…è©•è­°ä¼š
        self.logger.info("ğŸ§™â€â™‚ï¸ Phase 2: å®Œå…¨4è³¢è€…è©•è­°ä¼šé–‹å‚¬")
        sages_wisdom = await self._conduct_complete_sages_council(request, decomposed_tasks)

        # Phase 3: è³¢è€…æ¨å¥¨ã®é©ç”¨
        self.logger.info("ğŸ”§ Phase 3: è³¢è€…æ¨å¥¨é©ç”¨")
        optimized_tasks = await self._apply_complete_sages_wisdom(decomposed_tasks, sages_wisdom)

        # Phase 4: å®Œå…¨ç›£è¦–ä¸‹ã§ã®å®Ÿè¡Œ
        self.logger.info("âš¡ Phase 4: å®Œå…¨ç›£è¦–ä¸¦åˆ—å®Ÿè¡Œ")
        await self.incident_sage.start_monitoring(session_id)

        try:
            execution_result = await self._execute_with_complete_monitoring(optimized_tasks)
        finally:
            await self.incident_sage.stop_monitoring()

        # Phase 5: å­¦ç¿’ãƒ»çŸ¥è­˜åŒ–
        self.logger.info("ğŸ§  Phase 5: å­¦ç¿’ãƒ»çŸ¥è­˜è“„ç©")
        learning_results = await self._complete_learning_phase(request, optimized_tasks, execution_result)

        end_time = datetime.now()

        # ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        return self._generate_complete_wisdom_report(
            session_id, request, decomposed_tasks, sages_wisdom,
            execution_result, learning_results, start_time, end_time
        )

    async def _conduct_complete_sages_council(self, request: str, tasks: List[DecomposedTask]) -> Dict[str, Any]:
        """å®Œå…¨4è³¢è€…è©•è­°ä¼š"""

        # ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ã®è‹±çŸ¥
        self.logger.info("ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ã«ç›¸è«‡ä¸­...")
        knowledge_results = await self.knowledge_sage.search_knowledge(request)

        # ğŸ“‹ ã‚¿ã‚¹ã‚¯è³¢è€…ã®æœ€é©åŒ–
        self.logger.info("ğŸ“‹ ã‚¿ã‚¹ã‚¯è³¢è€…ã«ç›¸è«‡ä¸­...")
        task_optimizations = await self.task_sage.analyze_task_optimization(tasks)

        # ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ã®ãƒªã‚¹ã‚¯åˆ†æ
        self.logger.info("ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ã«ç›¸è«‡ä¸­...")
        risk_analysis = await self.incident_sage.analyze_risks(request, tasks)

        # ğŸ” RAGè³¢è€…ã®é¡ä¼¼å®Ÿè£…æ¤œç´¢
        self.logger.info("ğŸ” RAGè³¢è€…ã«ç›¸è«‡ä¸­...")
        similar_implementations = await self.rag_sage.search_similar_implementations(request)
        implementation_suggestions = await self.rag_sage.generate_implementation_suggestions(request)

        return {
            "knowledge_wisdom": knowledge_results,
            "task_optimizations": task_optimizations,
            "risk_analysis": risk_analysis,
            "similar_implementations": similar_implementations,
            "implementation_suggestions": implementation_suggestions
        }

    async def _apply_complete_sages_wisdom(self, tasks: List[DecomposedTask],
                                         wisdom: Dict[str, Any]) -> List[DecomposedTask]:
        """å®Œå…¨4è³¢è€…ã®è‹±çŸ¥ã‚’ã‚¿ã‚¹ã‚¯ã«é©ç”¨"""
        optimized_tasks = tasks.copy()

        # ã‚¿ã‚¹ã‚¯æœ€é©åŒ–ã®é©ç”¨
        for optimization in wisdom.get("task_optimizations", []):
            if optimization.get("confidence", 0) > 0.8:
                self.logger.info(f"ğŸ”§ æœ€é©åŒ–é©ç”¨: {optimization['description']}")

        # ãƒªã‚¹ã‚¯è»½æ¸›ç­–ã®é©ç”¨
        high_risks = [r for r in wisdom.get("risk_analysis", []) if r.get("severity") == "high"]
        for risk in high_risks:
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¿ã‚¹ã‚¯ã®è¿½åŠ 
            security_task = DecomposedTask(
                task_id=f"security_mitigation_{len(optimized_tasks)}",
                category=TaskCategory.SECURITY,
                description=f"ãƒªã‚¹ã‚¯è»½æ¸›: {risk['risk']}",
                servant_type=ServantType.QUALITY_INSPECTOR,
                command="security_scan",
                arguments={"risk_focus": risk["risk"]},
                priority=TaskPriority.HIGH
            )
            optimized_tasks.append(security_task)
            self.logger.info(f"ğŸ›¡ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¿ã‚¹ã‚¯è¿½åŠ : {risk['risk']}")

        return optimized_tasks

    async def _execute_with_complete_monitoring(self, tasks: List[DecomposedTask]) -> Dict[str, Any]:
        """å®Œå…¨ç›£è¦–ä¸‹ã§ã®ä¸¦åˆ—å®Ÿè¡Œ"""
        # ã‚µãƒ¼ãƒãƒ³ãƒˆã‚¿ã‚¹ã‚¯ã«å¤‰æ›
        servant_tasks = self.decomposer.convert_to_servant_tasks(tasks)
        self.executor.add_tasks(servant_tasks)

        # ä¸¦åˆ—å®Ÿè¡Œ
        result = await self.executor.execute_all_parallel()

        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè¨˜éŒ²
        if result.get('summary', {}).get('failed', 0) > 0:
            await self.incident_sage.record_incident(
                severity="medium",
                description=f"{result['summary']['failed']}ä»¶ã®ã‚¿ã‚¹ã‚¯å¤±æ•—",
                context={"failed_tasks": result.get('failed_tasks', {})}
            )

        return result

    async def _complete_learning_phase(self, request: str, tasks: List[DecomposedTask],
                                     result: Dict[str, Any]) -> Dict[str, Any]:
        """å®Œå…¨å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º"""

        # ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ã®å­¦ç¿’
        await self.knowledge_sage.learn_from_execution(request, tasks, result)

        # ã‚¿ã‚¹ã‚¯è³¢è€…ã®è¨˜éŒ²
        await self.task_sage.record_execution(tasks, result)

        return {
            "knowledge_entries_added": 1 if result.get('summary', {}).get('failed', 0) == 0 else 0,
            "task_patterns_updated": 1,
            "incident_records": len(self.incident_sage.incident_history),
            "learning_insights": [
                f"å®Ÿè¡ŒåŠ¹ç‡: {result.get('summary', {}).get('parallel_efficiency', 0):.1f}%",
                f"æˆåŠŸç‡: {(result.get('summary', {}).get('completed', 0) / max(len(tasks), 1)) * 100:.1f}%"
            ]
        }

    def _generate_complete_wisdom_report(self, session_id: str, request: str,
                                       original_tasks: List[DecomposedTask],
                                       sages_wisdom: Dict[str, Any],
                                       execution_result: Dict[str, Any],
                                       learning_results: Dict[str, Any],
                                       start_time: datetime, end_time: datetime) -> Dict[str, Any]:
        """å®Œå…¨è‹±çŸ¥ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        total_time = (end_time - start_time).total_seconds()

        return {
            "session_info": {
                "session_id": session_id,
                "request": request,
                "total_time": total_time,
                "timestamp": start_time.isoformat()
            },
            "task_analysis": {
                "original_task_count": len(original_tasks),
                "final_task_count": execution_result.get('summary', {}).get('total_tasks', 0),
                "optimization_applied": len(sages_wisdom.get("task_optimizations", [])),
                "security_enhancements": len([r for r in sages_wisdom.get("risk_analysis", []) if r.get("severity") == "high"])
            },
            "sages_contributions": {
                "knowledge_sage": {
                    "knowledge_entries_found": len(sages_wisdom.get("knowledge_wisdom", [])),
                    "top_knowledge": [k.title for k in sages_wisdom.get("knowledge_wisdom", [])[:3]]
                },
                "task_sage": {
                    "optimizations_suggested": len(sages_wisdom.get("task_optimizations", [])),
                    "top_optimization": sages_wisdom.get("task_optimizations", [{}])[0].get("description", "ãªã—") if sages_wisdom.get("task_optimizations") else "ãªã—"
                },
                "incident_sage": {
                    "risks_identified": len(sages_wisdom.get("risk_analysis", [])),
                    "high_priority_risks": len([r for r in sages_wisdom.get("risk_analysis", []) if r.get("severity") == "high"])
                },
                "rag_sage": {
                    "similar_patterns_found": len(sages_wisdom.get("similar_implementations", [])),
                    "implementation_suggestions": len(sages_wisdom.get("implementation_suggestions", []))
                }
            },
            "execution_results": execution_result.get('summary', {}),
            "learning_outcomes": learning_results,
            "wisdom_evolution": {
                "knowledge_base_growth": learning_results.get("knowledge_entries_added", 0),
                "pattern_database_updates": learning_results.get("task_patterns_updated", 0),
                "total_sessions": self.execution_count,
                "wisdom_level": "é«˜åº¦" if execution_result.get('summary', {}).get('parallel_efficiency', 0) > 85 else "ä¸­ç´š"
            }
        }


# Usage Example & Test
async def main():
    """å®Œå…¨4è³¢è€…çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢"""
    print("ğŸŒŠğŸ§™â€â™‚ï¸ Elder Flow + å®Œå…¨æ©Ÿèƒ½4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ‡ãƒ¢")
    print("=" * 90)

    # å®Œå…¨çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    complete_system = ElderFlowFourSagesComplete(max_workers=6)

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_requests = [
        "OAuth2.0èªè¨¼ã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†APIã‚’å®Ÿè£…ã—ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã‚‚å«ã‚ã¦ãã ã•ã„",
        "é«˜ä¸¦åˆ—å‡¦ç†å¯èƒ½ãªã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„"
    ]

    for i, request in enumerate(test_requests, 1):
        print(f"\n{'='*90}")
        print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ {i}: {request}")
        print(f"{'='*90}")

        # å®Œå…¨4è³¢è€…çµ±åˆå®Ÿè¡Œ
        result = await complete_system.execute_with_full_sages_wisdom(request)

        # çµæœè¡¨ç¤º
        print("\nğŸ“Š å®Œå…¨çµ±åˆçµæœã‚µãƒãƒªãƒ¼:")
        print("-" * 70)

        session_info = result["session_info"]
        task_analysis = result["task_analysis"]
        execution_results = result["execution_results"]
        sages_contributions = result["sages_contributions"]

        print(f"âš¡ ç·å®Ÿè¡Œæ™‚é–“: {session_info['total_time']:.2f}ç§’")
        print(f"ğŸ“‹ ã‚¿ã‚¹ã‚¯æ•°: {task_analysis['original_task_count']} â†’ {task_analysis['final_task_count']}")
        print(f"ğŸ”§ æœ€é©åŒ–é©ç”¨: {task_analysis['optimization_applied']}ä»¶")
        print(f"ğŸ›¡ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–: {task_analysis['security_enhancements']}ä»¶")

        print(f"\nğŸ“Š å®Ÿè¡Œçµæœ:")
        print(f"  ä¸¦åˆ—åŠ¹ç‡: {execution_results.get('parallel_efficiency', 0):.1f}%")
        print(f"  æˆåŠŸç‡: {(execution_results.get('completed', 0) / max(execution_results.get('total_tasks', 1), 1)) * 100:.1f}%")

        print(f"\nğŸ§™â€â™‚ï¸ 4è³¢è€…ã®è²¢çŒ®:")
        print(f"  ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…: {sages_contributions['knowledge_sage']['knowledge_entries_found']}ä»¶ã®çŸ¥è­˜æ´»ç”¨")
        print(f"  ğŸ“‹ ã‚¿ã‚¹ã‚¯è³¢è€…: {sages_contributions['task_sage']['optimizations_suggested']}ä»¶ã®æœ€é©åŒ–")
        print(f"  ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…: {sages_contributions['incident_sage']['risks_identified']}ä»¶ã®ãƒªã‚¹ã‚¯æ¤œå‡º")
        print(f"  ğŸ” RAGè³¢è€…: {sages_contributions['rag_sage']['similar_patterns_found']}ä»¶ã®é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹")

        print(f"\nğŸš€ è‹±çŸ¥é€²åŒ–çŠ¶æ³:")
        wisdom_evolution = result["wisdom_evolution"]
        print(f"  çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æˆé•·: +{wisdom_evolution['knowledge_base_growth']}ã‚¨ãƒ³ãƒˆãƒª")
        print(f"  ç·ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {wisdom_evolution['total_sessions']}")
        print(f"  ç¾åœ¨ã®è‹±çŸ¥ãƒ¬ãƒ™ãƒ«: {wisdom_evolution['wisdom_level']}")

        if i < len(test_requests):
            print(f"\nâ³ æ¬¡ã®ãƒ†ã‚¹ãƒˆã¾ã§å°‘ã—å¾…æ©Ÿ...")
            await asyncio.sleep(1)

    print(f"\nğŸ‰ å®Œå…¨4è³¢è€…çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¢å®Œäº†!")
    print("Elder Flow ã¯çœŸã®è‡ªå¾‹å­¦ç¿’ãƒ»é€²åŒ–ã‚·ã‚¹ãƒ†ãƒ ã«åˆ°é”ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    asyncio.run(main())
