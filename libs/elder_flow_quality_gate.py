"""
Elder Flow Quality Gate - å“è³ªã‚²ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 
Created: 2025-07-12
Author: Claude Elder
Version: 1.0.0
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
from dataclasses import dataclass, field
from libs.elder_flow_quality_gate_optimizer import ElderFlowQualityGateOptimizer


# Quality Gate Status
class QualityGateStatus(Enum):
    """QualityGateStatusã‚¯ãƒ©ã‚¹"""
    PENDING = "pending"
    ANALYZING = "analyzing"
    PASSED = "passed"
    FAILED = "failed"
    WARNING = "warning"
    BLOCKED = "blocked"


# Quality Check Types
class QualityCheckType(Enum):
    """QualityCheckTypeã‚¯ãƒ©ã‚¹"""
    UNIT_TESTS = "unit_tests"
    INTEGRATION_TESTS = "integration_tests"
    CODE_QUALITY = "code_quality"
    PYLINT = "pylint"
    SECURITY_SCAN = "security_scan"
    PERFORMANCE = "performance"
    COVERAGE = "coverage"
    COMPLIANCE = "compliance"
    DOCUMENTATION = "documentation"
    DEPENDENCY = "dependency"
    SAGE_REVIEW = "sage_review"


# Quality Metrics
@dataclass
class QualityMetric:
    """QualityMetricã‚¯ãƒ©ã‚¹"""
    name: str
    value: float
    threshold: float
    unit: str = ""
    passed: bool = False
    message: str = ""

    def __post_init__(self):
        """__post_init__ç‰¹æ®Šãƒ¡ã‚½ãƒƒãƒ‰"""
        self.passed = self.value >= self.threshold


# Quality Check Result
@dataclass
class QualityCheckResult:
    """QualityCheckResultã‚¯ãƒ©ã‚¹"""
    check_type: QualityCheckType
    status: QualityGateStatus
    metrics: List[QualityMetric] = field(default_factory=list)
    issues: List[Dict] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    execution_time: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    details: Dict = field(default_factory=dict)

    @property
    def overall_score(self) -> float:
        """overall_scoreãƒ¡ã‚½ãƒƒãƒ‰"""
        if not self.metrics:
            return 0.0
        return sum(m.value for m in self.metrics) / len(self.metrics)

    @property
    def passed_count(self) -> int:
        """passed_countãƒ¡ã‚½ãƒƒãƒ‰"""
        return sum(1 for m in self.metrics if m.passed)

    @property
    def failed_count(self) -> int:
        """failed_countãƒ¡ã‚½ãƒƒãƒ‰"""
        return sum(1 for m in self.metrics if not m.passed)


# Quality Gate Configuration
@dataclass
class QualityGateConfig:
    """QualityGateConfigã‚¯ãƒ©ã‚¹"""
    # Test thresholds
    unit_test_coverage: float = 80.0
    integration_test_coverage: float = 70.0
    test_pass_rate: float = 100.0

    # Code quality thresholds
    code_quality_score: float = 8.0
    complexity_threshold: float = 10.0
    duplication_threshold: float = 5.0

    # Security thresholds
    security_score: float = 8.5
    vulnerability_tolerance: int = 0

    # Performance thresholds
    performance_score: float = 8.0
    response_time_threshold: float = 2.0
    memory_threshold: float = 100.0

    # Compliance thresholds
    compliance_score: float = 95.0

    # Documentation thresholds
    documentation_coverage: float = 80.0

    # Dependency thresholds
    outdated_dependencies: int = 5
    vulnerable_dependencies: int = 0

    # Sage review
    sage_approval_required: bool = True


# Base Quality Checker
class BaseQualityChecker:
    """BaseQualityCheckerã‚¯ãƒ©ã‚¹"""
    def __init__(self, check_type: QualityCheckType):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.check_type = check_type
        self.logger = logging.getLogger(f"quality.{check_type.value}")

    async def execute_check(self, context: Dict) -> QualityCheckResult:
        """å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        start_time = datetime.now()

        try:
            self.logger.info(f"Starting {self.check_type.value} check")

            result = await self._perform_check(context)
            result.execution_time = (datetime.now() - start_time).total_seconds()

            self.logger.info(
                f"Completed {self.check_type.value} check: {result.status.value}"
            )

            return result

        except Exception as e:
            self.logger.error(f"Quality check failed: {str(e)}")
            return QualityCheckResult(
                check_type=self.check_type,
                status=QualityGateStatus.FAILED,
                execution_time=(datetime.now() - start_time).total_seconds(),
                details={"error": str(e)},
            )

    async def _perform_check(self, context: Dict) -> QualityCheckResult:
        """å…·ä½“çš„ãªãƒã‚§ãƒƒã‚¯å®Ÿè£…ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        raise NotImplementedError("Subclasses must implement _perform_check")


# Unit Test Checker
class UnitTestChecker(BaseQualityChecker):
    """UnitTestCheckerã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        super().__init__(QualityCheckType.UNIT_TESTS)

    async def _perform_check(self, context: Dict) -> QualityCheckResult:
        """ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯"""
        # ãƒ¢ãƒƒã‚¯å®Ÿè£…
        test_results = {
            "passed": 45,
            "failed": 2,
            "skipped": 3,
            "total": 50,
            "coverage": 85.5,
        }

        metrics = [
            QualityMetric(
                "Test Pass Rate",
                (test_results["passed"] / test_results["total"]) * 100,
                100.0,
                "%",
            ),
            QualityMetric("Test Coverage", test_results["coverage"], 80.0, "%"),
            QualityMetric("Test Count", test_results["total"], 30.0, "tests"),
        ]

        issues = []
        if test_results["failed"] > 0:
            issues.append(
                {
                    "type": "failed_tests",
                    "severity": "high",
                    "count": test_results["failed"],
                    "message": f"{test_results['failed']} tests failed",
                }
            )

        status = (
            QualityGateStatus.PASSED
            if all(m.passed for m in metrics)
            else QualityGateStatus.FAILED
        )

        return QualityCheckResult(
            check_type=self.check_type,
            status=status,
            metrics=metrics,
            issues=issues,
            recommendations=["Fix failing tests", "Improve test coverage"],
            details=test_results,
        )


# Code Quality Checker
class CodeQualityChecker(BaseQualityChecker):
    """CodeQualityCheckerã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        super().__init__(QualityCheckType.CODE_QUALITY)

    async def _perform_check(self, context: Dict) -> QualityCheckResult:
        """ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯"""
        # ãƒ¢ãƒƒã‚¯å®Ÿè£…
        quality_data = {
            "overall_score": 8.5,
            "complexity": 7.2,
            "duplication": 3.1,
            "maintainability": 8.8,
            "reliability": 9.0,
            "security": 8.3,
        }

        metrics = [
            QualityMetric("Overall Score", quality_data["overall_score"], 8.0, "/10"),
            QualityMetric("Complexity", quality_data["complexity"], 10.0, "/10"),
            QualityMetric("Duplication", quality_data["duplication"], 5.0, "%"),
            QualityMetric(
                "Maintainability", quality_data["maintainability"], 8.0, "/10"
            ),
            QualityMetric("Reliability", quality_data["reliability"], 8.0, "/10"),
            QualityMetric("Security", quality_data["security"], 8.0, "/10"),
        ]

        issues = [
            {
                "type": "complexity",
                "severity": "medium",
                "file": "main.py",
                "line": 45,
                "message": "Method too complex",
            },
            {
                "type": "duplication",
                "severity": "low",
                "file": "utils.py",
                "line": 23,
                "message": "Duplicate code detected",
            },
        ]

        status = (
            QualityGateStatus.PASSED
            if all(m.passed for m in metrics)
            else QualityGateStatus.WARNING
        )

        return QualityCheckResult(
            check_type=self.check_type,
            status=status,
            metrics=metrics,
            issues=issues,
            recommendations=["Refactor complex methods", "Remove duplicate code"],
            details=quality_data,
        )


# Security Checker
class SecurityChecker(BaseQualityChecker):
    """SecurityCheckerã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        super().__init__(QualityCheckType.SECURITY_SCAN)

    async def _perform_check(self, context: Dict) -> QualityCheckResult:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯"""
        # ãƒ¢ãƒƒã‚¯å®Ÿè£…
        security_data = {
            "vulnerabilities": [
                {
                    "type": "SQL_INJECTION",
                    "severity": "high",
                    "file": "db.py",
                    "line": 123,
                },
                {"type": "XSS", "severity": "medium", "file": "web.py", "line": 56},
            ],
            "security_score": 7.5,
            "risk_level": "medium",
        }

        metrics = [
            QualityMetric(
                "Security Score", security_data["security_score"], 8.5, "/10"
            ),
            QualityMetric(
                "High Vulnerabilities",
                len(
                    [
                        v
                        for v in security_data["vulnerabilities"]
                        if v["severity"] == "high"
                    ]
                ),
                0,
                "issues",
            ),
            QualityMetric(
                "Medium Vulnerabilities",
                len(
                    [
                        v
                        for v in security_data["vulnerabilities"]
                        if v["severity"] == "medium"
                    ]
                ),
                2,
                "issues",
            ),
        ]

        issues = security_data["vulnerabilities"]

        status = (
            QualityGateStatus.FAILED
            if any(v["severity"] == "high" for v in security_data["vulnerabilities"])
            else QualityGateStatus.WARNING
        )

        return QualityCheckResult(
            check_type=self.check_type,
            status=status,
            metrics=metrics,
            issues=issues,
            recommendations=["Fix SQL injection vulnerability", "Sanitize user input"],
            details=security_data,
        )


# Performance Checker
class PerformanceChecker(BaseQualityChecker):
    """PerformanceCheckerã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        super().__init__(QualityCheckType.PERFORMANCE)

    async def _perform_check(self, context: Dict) -> QualityCheckResult:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯"""
        # ãƒ¢ãƒƒã‚¯å®Ÿè£…
        perf_data = {
            "response_time": 1.2,
            "memory_usage": 75.5,
            "cpu_usage": 45.2,
            "throughput": 850.0,
            "performance_score": 8.8,
        }

        metrics = [
            QualityMetric(
                "Performance Score", perf_data["performance_score"], 8.0, "/10"
            ),
            QualityMetric("Response Time", perf_data["response_time"], 2.0, "s"),
            QualityMetric("Memory Usage", perf_data["memory_usage"], 100.0, "MB"),
            QualityMetric("CPU Usage", perf_data["cpu_usage"], 80.0, "%"),
            QualityMetric("Throughput", perf_data["throughput"], 500.0, "req/s"),
        ]

        issues = []
        if perf_data["response_time"] > 1.5:
            issues.append(
                {
                    "type": "slow_response",
                    "severity": "medium",
                    "message": f"Response time {perf_data['response_time']}s exceeds 1.5s",
                }
            )

        status = (
            QualityGateStatus.PASSED
            if all(m.passed for m in metrics)
            else QualityGateStatus.WARNING
        )

        return QualityCheckResult(
            check_type=self.check_type,
            status=status,
            metrics=metrics,
            issues=issues,
            recommendations=["Optimize database queries", "Add caching"],
            details=perf_data,
        )


# Sage Review Checker
class SageReviewChecker(BaseQualityChecker):
    """SageReviewChecker - 4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ é–¢é€£ã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        super().__init__(QualityCheckType.SAGE_REVIEW)

    async def _perform_check(self, context: Dict) -> QualityCheckResult:
        """4è³¢è€…ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒã‚§ãƒƒã‚¯"""
        # ãƒ¢ãƒƒã‚¯å®Ÿè£…
        sage_reviews = {
            "knowledge_sage": {
                "approved": True,
                "score": 9.0,
                "comments": "Good implementation",
            },
            "task_sage": {
                "approved": True,
                "score": 8.5,
                "comments": "Well structured",
            },
            "incident_sage": {
                "approved": False,
                "score": 7.0,
                "comments": "Security concerns",
            },
            "rag_sage": {
                "approved": True,
                "score": 8.8,
                "comments": "Excellent documentation",
            },
        }

        approved_count = sum(
            1 for review in sage_reviews.values() if review["approved"]
        )
        avg_score = sum(review["score"] for review in sage_reviews.values()) / len(
            sage_reviews
        )

        metrics = [
            QualityMetric(
                "Sage Approval Rate",
                (approved_count / len(sage_reviews)) * 100,
                100.0,
                "%",
            ),
            QualityMetric("Average Score", avg_score, 8.0, "/10"),
            QualityMetric("Approved Reviews", approved_count, 4, "reviews"),
        ]

        issues = []
        for sage, review in sage_reviews.items():
            if not review["approved"]:
                issues.append(
                    {
                        "type": "sage_disapproval",
                        "severity": "high",
                        "sage": sage,
                        "message": review["comments"],
                    }
                )

        status = (
            QualityGateStatus.PASSED
            if approved_count == len(sage_reviews)
            else QualityGateStatus.FAILED
        )

        return QualityCheckResult(
            check_type=self.check_type,
            status=status,
            metrics=metrics,
            issues=issues,
            recommendations=[
                "Address incident sage concerns",
                "Improve security measures",
            ],
            details=sage_reviews,
        )


# Quality Gate System
class QualityGateSystem:
    """QualityGateSystemã‚¯ãƒ©ã‚¹"""
    def __init__(self, config: QualityGateConfig = None):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.config = config or QualityGateConfig()
        self.checkers: Dict[QualityCheckType, BaseQualityChecker] = {}
        self.logger = logging.getLogger(__name__)

        # å“è³ªã‚²ãƒ¼ãƒˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.optimizer = ElderFlowQualityGateOptimizer()
        self.failure_count = 0

        # ãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        self._initialize_checkers()

    def _initialize_checkers(self):
        """ãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–"""
        self.checkers[QualityCheckType.UNIT_TESTS] = UnitTestChecker()
        self.checkers[QualityCheckType.CODE_QUALITY] = CodeQualityChecker()
        self.checkers[QualityCheckType.SECURITY_SCAN] = SecurityChecker()
        self.checkers[QualityCheckType.PERFORMANCE] = PerformanceChecker()
        self.checkers[QualityCheckType.SAGE_REVIEW] = SageReviewChecker()

    async def execute_quality_gate(
        self,
        context: Dict,
        check_types: List[QualityCheckType] = None,
        priority: str = "medium",
        phase: str = "development",
    ) -> Dict:
        """å“è³ªã‚²ãƒ¼ãƒˆå®Ÿè¡Œï¼ˆå‹•çš„é–¾å€¤èª¿æ•´ä»˜ãï¼‰"""
        check_types = check_types or list(self.checkers.keys())

        # å„ªå…ˆåº¦ã¨ãƒ•ã‚§ãƒ¼ã‚ºã«åŸºã¥ã„ã¦å“è³ªåŸºæº–ã‚’èª¿æ•´
        adjusted_metrics = self.optimizer.get_adjusted_metrics(
            priority=priority, phase=phase, failure_count=self.failure_count
        )

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«èª¿æ•´æ¸ˆã¿ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½åŠ 
        context["adjusted_metrics"] = adjusted_metrics

        self.logger.info(
            f"Starting quality gate with {len(check_types)} checks (priority: {priority}, " \
                "phase: {phase})"
        )

        # å…¨ãƒã‚§ãƒƒã‚¯ä¸¦åˆ—å®Ÿè¡Œ
        tasks = []
        for check_type in check_types:
            if check_type in self.checkers:
                task = self.checkers[check_type].execute_check(context)
                tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # çµæœé›†è¨ˆ
        check_results = []
        for result in results:
            if isinstance(result, Exception):
                self.logger.error(f"Quality check failed: {str(result)}")
                continue
            check_results.append(result)

        # ç·åˆè©•ä¾¡
        overall_status = self._calculate_overall_status(check_results)

        # å¤±æ•—ã—ãŸå ´åˆã¯å¤±æ•—å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        if overall_status in [QualityGateStatus.FAILED, QualityGateStatus.BLOCKED]:
            self.failure_count += 1
        else:
            self.failure_count = 0  # æˆåŠŸã—ãŸã‚‰ãƒªã‚»ãƒƒãƒˆ

        summary = {
            "overall_status": overall_status.value,
            "total_checks": len(check_results),
            "passed_checks": len(
                [r for r in check_results if r.status == QualityGateStatus.PASSED]
            ),
            "failed_checks": len(
                [r for r in check_results if r.status == QualityGateStatus.FAILED]
            ),
            "warning_checks": len(
                [r for r in check_results if r.status == QualityGateStatus.WARNING]
            ),
            "overall_score": self._calculate_overall_score(check_results),
            "execution_time": sum(r.execution_time for r in check_results),
            "timestamp": datetime.now().isoformat(),
        }

        return {
            "summary": summary,
            "check_results": [self._result_to_dict(r) for r in check_results],
            "recommendations": self._generate_recommendations(check_results),
            "adjusted_metrics": adjusted_metrics,
            "optimization_stats": self.optimizer.get_statistics(),
        }

    def _calculate_overall_status(
        self, results: List[QualityCheckResult]
    ) -> QualityGateStatus:
        """ç·åˆçŠ¶æ…‹è¨ˆç®—"""
        if not results:
            return QualityGateStatus.FAILED

        failed_count = len([r for r in results if r.status == QualityGateStatus.FAILED])
        warning_count = len(
            [r for r in results if r.status == QualityGateStatus.WARNING]
        )

        if failed_count > 0:
            return QualityGateStatus.FAILED
        elif warning_count > 0:
            return QualityGateStatus.WARNING
        else:
            return QualityGateStatus.PASSED

    def _calculate_overall_score(self, results: List[QualityCheckResult]) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not results:
            return 0.0

        total_score = sum(r.overall_score for r in results)
        return total_score / len(results)

    def _generate_recommendations(self, results: List[QualityCheckResult]) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        for result in results:
            if result.status in [QualityGateStatus.FAILED, QualityGateStatus.WARNING]:
                recommendations.extend(result.recommendations)

        # é‡è¤‡é™¤å»
        return list(set(recommendations))

    def _result_to_dict(self, result: QualityCheckResult) -> Dict:
        """çµæœã‚’è¾æ›¸ã«å¤‰æ›"""
        return {
            "check_type": result.check_type.value,
            "status": result.status.value,
            "overall_score": result.overall_score,
            "passed_count": result.passed_count,
            "failed_count": result.failed_count,
            "execution_time": result.execution_time,
            "timestamp": result.timestamp.isoformat(),
            "metrics": [
                {
                    "name": m.name,
                    "value": m.value,
                    "threshold": m.threshold,
                    "unit": m.unit,
                    "passed": m.passed,
                    "message": m.message,
                }
                for m in result.metrics
            ],
            "issues": result.issues,
            "recommendations": result.recommendations,
            "details": result.details,
        }


# Helper Functions
async def run_quality_gate(context: Dict, config: QualityGateConfig = None) -> Dict:
    """å“è³ªã‚²ãƒ¼ãƒˆå®Ÿè¡Œ"""
    gate = QualityGateSystem(config)
    return await gate.execute_quality_gate(context)


def create_quality_config(**kwargs) -> QualityGateConfig:
    """å“è³ªè¨­å®šä½œæˆ"""
    return QualityGateConfig(**kwargs)


# Example Usage
if __name__ == "__main__":
    pass

    async def main():
        """mainãƒ¡ã‚½ãƒƒãƒ‰"""
        print("ğŸ” Elder Flow Quality Gate Test")

        # ãƒ†ã‚¹ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        context = {
            "project_path": "/home/aicompany/ai_co",
            "target_files": ["libs/elder_flow_orchestrator.py"],
            "task_id": "quality_gate_test",
        }

        # å“è³ªã‚²ãƒ¼ãƒˆå®Ÿè¡Œ
        result = await run_quality_gate(context)

        print(f"Overall Status: {result['summary']['overall_status']}")
        print(f"Overall Score: {result['summary']['overall_score']:0.2f}")
        print(f"Passed Checks: {result['summary']['passed_checks']}")
        print(f"Failed Checks: {result['summary']['failed_checks']}")
        print(f"Recommendations: {len(result['recommendations'])}")

    asyncio.run(main())
