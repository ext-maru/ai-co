"""
ğŸ“Š Elder Servantsçµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
Phase 3 Week 1å®Œäº†: 3ã‚·ã‚¹ãƒ†ãƒ çµ±åˆæ€§èƒ½æ¸¬å®š

ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»éåŒæœŸæœ€é©åŒ–ãƒ»è»½é‡ãƒ—ãƒ­ã‚­ã‚·ã®ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
ç›®æ¨™: 175.9%ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ â†’ 50%ä»¥ä¸‹å‰Šæ¸›ã®æ¤œè¨¼
"""

import asyncio
import gc
import json
import logging
import random
import statistics
import time
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

import psutil

from libs.elder_servants.integrations.performance.async_optimizer import (
    AsyncOptimizationRequest,
    AsyncWorkerOptimizer,
    ExecutionMode,
    OptimizationProfile,
    OptimizationStrategy,
    ResourceLimits,
    ResourceType,
)

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from libs.elder_servants.integrations.performance.cache_manager import (
    CacheKeyType,
    CacheRequest,
    CacheStrategy,
    ElderCacheManager,
)
from libs.elder_servants.integrations.performance.lightweight_proxy import (
    LightweightElderProxy,
    ProxyConfig,
    ProxyMode,
    ProxyRequest,
)


@dataclass
class BenchmarkConfig:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š"""

    test_duration_seconds: int = 60
    concurrent_requests: int = 50
    request_rate_per_second: int = 10
    data_size_range: Tuple[int, int] = (100, 10000)  # bytes
    enable_cache_test: bool = True
    enable_async_test: bool = True
    enable_proxy_test: bool = True
    enable_integrated_test: bool = True
    warmup_requests: int = 20


@dataclass
class BenchmarkResults:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""

    test_name: str
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    average_response_time_ms: float = 0.0
    median_response_time_ms: float = 0.0
    p95_response_time_ms: float = 0.0
    p99_response_time_ms: float = 0.0
    throughput_rps: float = 0.0
    error_rate_percent: float = 0.0
    peak_memory_mb: float = 0.0
    average_cpu_percent: float = 0.0
    cache_hit_rate: float = 0.0
    additional_metrics: Dict[str, Any] = field(default_factory=dict)


class IntegratedPerformanceBenchmark:
    """çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""

    def __init__(self, config: BenchmarkConfig = None):
        self.config = config or BenchmarkConfig()
        self.logger = logging.getLogger("elder_servants.integrated_benchmark")

        # ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ 
        self.cache_manager: Optional[ElderCacheManager] = None
        self.async_optimizer: Optional[AsyncWorkerOptimizer] = None
        self.proxy: Optional[LightweightElderProxy] = None

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        self.results: Dict[str, BenchmarkResults] = {}

        # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
        self.memory_usage_history: List[float] = []
        self.cpu_usage_history: List[float] = []

        self.logger.info("Integrated Performance Benchmark initialized")

    async def run_full_benchmark(self) -> Dict[str, BenchmarkResults]:
        """å®Œå…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        self.logger.info("Starting full performance benchmark...")

        try:
            # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            await self._initialize_systems()

            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š
            baseline_result = await self._run_baseline_benchmark()
            self.results["baseline"] = baseline_result

            if self.config.enable_cache_test:
                cache_result = await self._run_cache_benchmark()
                self.results["cache_optimized"] = cache_result

            if self.config.enable_async_test:
                async_result = await self._run_async_benchmark()
                self.results["async_optimized"] = async_result

            if self.config.enable_proxy_test:
                proxy_result = await self._run_proxy_benchmark()
                self.results["proxy_optimized"] = proxy_result

            if self.config.enable_integrated_test:
                integrated_result = await self._run_integrated_benchmark()
                self.results["fully_integrated"] = integrated_result

            # æ¯”è¼ƒåˆ†æ
            comparison = await self._analyze_performance_comparison()
            self.results["comparison"] = comparison

            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = await self._generate_benchmark_report()

            self.logger.info("Full benchmark completed successfully")
            return self.results

        except Exception as e:
            self.logger.error(f"Benchmark failed: {str(e)}")
            raise
        finally:
            await self._cleanup_systems()

    async def _initialize_systems(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        self.cache_manager = ElderCacheManager(strategy=CacheStrategy.BALANCED)

        # éåŒæœŸæœ€é©åŒ–
        profile = OptimizationProfile(
            strategy=OptimizationStrategy.BALANCED,
            mode=ExecutionMode.HYBRID,
            limits=ResourceLimits(max_concurrent_tasks=self.config.concurrent_requests),
        )
        self.async_optimizer = AsyncWorkerOptimizer(profile)

        # è»½é‡ãƒ—ãƒ­ã‚­ã‚·
        proxy_config = ProxyConfig(
            mode=ProxyMode.OPTIMIZED,
            enable_compression=True,
            cache_small_responses=True,
        )
        self.proxy = LightweightElderProxy(proxy_config)

        # ãƒ—ãƒ­ã‚­ã‚·ã«ã‚µãƒ¼ãƒ“ã‚¹ç™»éŒ²
        self.proxy.register_service("cache_manager", self.cache_manager)
        self.proxy.register_service("async_optimizer", self.async_optimizer)

    async def _run_baseline_benchmark(self) -> BenchmarkResults:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½æ¸¬å®š"""
        self.logger.info("Running baseline benchmark...")

        response_times = []
        errors = 0
        start_time = time.time()

        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in range(self.config.warmup_requests):
            try:
                await self._execute_baseline_request()
            except:
                pass

        # å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆ
        tasks = []
        for i in range(self.config.concurrent_requests):
            task = asyncio.create_task(self._baseline_worker(response_times, i))
            tasks.append(task)

        # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é–‹å§‹
        monitor_task = asyncio.create_task(self._monitor_system_resources())

        # å…¨ã‚¿ã‚¹ã‚¯å®Œäº†å¾…æ©Ÿ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        monitor_task.cancel()

        # ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆ
        errors = sum(1 for r in results if isinstance(r, Exception))

        total_time = time.time() - start_time

        return self._calculate_benchmark_results(
            "baseline", response_times, errors, total_time
        )

    async def _baseline_worker(self, response_times: List[float], worker_id: int):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        for request_num in range(self.config.request_rate_per_second):
            try:
                start_time = time.time()
                await self._execute_baseline_request()
                response_time = (time.time() - start_time) * 1000
                response_times.append(response_time)

                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                await asyncio.sleep(1.0 / self.config.request_rate_per_second)

            except Exception as e:
                self.logger.warning(f"Baseline worker {worker_id} error: {str(e)}")
                raise

    async def _execute_baseline_request(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        # ã‚·ãƒ³ãƒ—ãƒ«ãªéåŒæœŸå‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        data_size = random.randint(*self.config.data_size_range)
        test_data = "x" * data_size

        # CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯
        await asyncio.sleep(0.001)  # 1ms

        # I/Oé›†ç´„çš„ã‚¿ã‚¹ã‚¯
        await asyncio.sleep(0.005)  # 5ms

        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        processed = json.dumps({"data": test_data, "size": len(test_data)})

        return {"result": "success", "processed_size": len(processed)}

    async def _run_cache_benchmark(self) -> BenchmarkResults:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        self.logger.info("Running cache optimization benchmark...")

        response_times = []
        errors = 0
        start_time = time.time()

        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æº–å‚™ï¼‰
        for i in range(self.config.warmup_requests):
            try:
                await self._execute_cache_request(f"warmup_{i}")
            except:
                pass

        # å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆ
        tasks = []
        for i in range(self.config.concurrent_requests):
            task = asyncio.create_task(self._cache_worker(response_times, i))
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)
        errors = sum(1 for r in results if isinstance(r, Exception))

        total_time = time.time() - start_time

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡å–å¾—
        cache_stats = await self.cache_manager.get_cache_statistics()
        cache_hit_rate = cache_stats.get("cache_stats", {}).get("hit_rate_percent", 0.0)

        result = self._calculate_benchmark_results(
            "cache_optimized", response_times, errors, total_time
        )
        result.cache_hit_rate = cache_hit_rate

        return result

    async def _cache_worker(self, response_times: List[float], worker_id: int):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        for request_num in range(self.config.request_rate_per_second):
            try:
                start_time = time.time()
                # 50%ã®ç¢ºç‡ã§åŒã˜ã‚­ãƒ¼ã‚’ä½¿ç”¨ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‹™ã„ï¼‰
                cache_key = f"test_key_{random.randint(0, 10) if random.random() < 0.5 else worker_id}"
                await self._execute_cache_request(cache_key)
                response_time = (time.time() - start_time) * 1000
                response_times.append(response_time)

                await asyncio.sleep(1.0 / self.config.request_rate_per_second)

            except Exception as e:
                self.logger.warning(f"Cache worker {worker_id} error: {str(e)}")
                raise

    async def _execute_cache_request(self, cache_key: str):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—è©¦è¡Œ
        cached_data = await self.cache_manager.get_quality_check_cache(
            cache_key, "performance_test"
        )

        if cached_data:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
            return cached_data
        else:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ - ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            data_size = random.randint(*self.config.data_size_range)
            test_data = {"data": "x" * data_size, "timestamp": time.time()}

            # å‡¦ç†æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            await asyncio.sleep(0.01)  # 10ms

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            await self.cache_manager.set_quality_check_cache(
                cache_key, "performance_test", test_data
            )

            return test_data

    async def _run_async_benchmark(self) -> BenchmarkResults:
        """éåŒæœŸæœ€é©åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        self.logger.info("Running async optimization benchmark...")

        response_times = []
        errors = 0
        start_time = time.time()

        # å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆ
        tasks = []
        for i in range(self.config.concurrent_requests):
            task = asyncio.create_task(self._async_worker(response_times, i))
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)
        errors = sum(1 for r in results if isinstance(r, Exception))

        total_time = time.time() - start_time

        return self._calculate_benchmark_results(
            "async_optimized", response_times, errors, total_time
        )

    async def _async_worker(self, response_times: List[float], worker_id: int):
        """éåŒæœŸæœ€é©åŒ–ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        for request_num in range(self.config.request_rate_per_second):
            try:
                start_time = time.time()
                await self._execute_async_request(worker_id, request_num)
                response_time = (time.time() - start_time) * 1000
                response_times.append(response_time)

                await asyncio.sleep(1.0 / self.config.request_rate_per_second)

            except Exception as e:
                self.logger.warning(f"Async worker {worker_id} error: {str(e)}")
                raise

    async def _execute_async_request(self, worker_id: int, request_num: int):
        """éåŒæœŸæœ€é©åŒ–ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""

        # éåŒæœŸæœ€é©åŒ–ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
        async def optimized_task():
            data_size = random.randint(*self.config.data_size_range)

            # ãƒªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã‚’ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ
            resource_types = [
                ResourceType.IO_BOUND,
                ResourceType.CPU_BOUND,
                ResourceType.NETWORK_BOUND,
            ]
            resource_type = random.choice(resource_types)

            if resource_type == ResourceType.CPU_BOUND:
                # CPUé›†ç´„çš„å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                result = sum(i * i for i in range(1000))
            elif resource_type == ResourceType.IO_BOUND:
                # I/Oé›†ç´„çš„å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                await asyncio.sleep(0.005)
                result = {"io_result": "success"}
            else:
                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é›†ç´„çš„å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                await asyncio.sleep(0.003)
                result = {"network_result": "success"}

            return {
                "worker_id": worker_id,
                "request_num": request_num,
                "result": result,
            }

        opt_request = AsyncOptimizationRequest(
            task_id=f"benchmark_{worker_id}_{request_num}",
            coroutine_func=optimized_task,
            resource_type=random.choice(
                [ResourceType.IO_BOUND, ResourceType.CPU_BOUND]
            ),
            timeout_s=30,
        )

        response = await self.async_optimizer.process_request(opt_request)

        if not response.success:
            raise Exception(response.error_message)

        return response.result

    async def _run_proxy_benchmark(self) -> BenchmarkResults:
        """ãƒ—ãƒ­ã‚­ã‚·æœ€é©åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        self.logger.info("Running proxy optimization benchmark...")

        response_times = []
        errors = 0
        start_time = time.time()

        # å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆ
        tasks = []
        for i in range(self.config.concurrent_requests):
            task = asyncio.create_task(self._proxy_worker(response_times, i))
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)
        errors = sum(1 for r in results if isinstance(r, Exception))

        total_time = time.time() - start_time

        return self._calculate_benchmark_results(
            "proxy_optimized", response_times, errors, total_time
        )

    async def _proxy_worker(self, response_times: List[float], worker_id: int):
        """ãƒ—ãƒ­ã‚­ã‚·ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        for request_num in range(self.config.request_rate_per_second):
            try:
                start_time = time.time()
                await self._execute_proxy_request(worker_id, request_num)
                response_time = (time.time() - start_time) * 1000
                response_times.append(response_time)

                await asyncio.sleep(1.0 / self.config.request_rate_per_second)

            except Exception as e:
                self.logger.warning(f"Proxy worker {worker_id} error: {str(e)}")
                raise

    async def _execute_proxy_request(self, worker_id: int, request_num: int):
        """ãƒ—ãƒ­ã‚­ã‚·ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        # ãƒ—ãƒ­ã‚­ã‚·ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
        proxy_request = ProxyRequest(
            request_id=f"proxy_benchmark_{worker_id}_{request_num}",
            target_service="benchmark_service",
            method="process_data",
            payload={
                "worker_id": worker_id,
                "request_num": request_num,
                "data_size": random.randint(*self.config.data_size_range),
            },
        )

        # ãƒ—ãƒ­ã‚­ã‚·ãƒ¢ãƒ¼ãƒ‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ
        modes = [ProxyMode.DIRECT, ProxyMode.CACHED, ProxyMode.OPTIMIZED]
        proxy_request.config.mode = random.choice(modes)

        # å®Ÿéš›ã®å‡¦ç†ã¯ç›´æ¥å‡¦ç†ã¨ã—ã¦å®Ÿè£…
        response = await self._simulate_service_call(proxy_request.payload)

        return response

    async def _simulate_service_call(self, payload: Dict[str, Any]):
        """ã‚µãƒ¼ãƒ“ã‚¹å‘¼ã³å‡ºã—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        data_size = payload.get("data_size", 1000)

        # å‡¦ç†æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        await asyncio.sleep(0.002)  # 2ms

        return {
            "status": "success",
            "processed_data_size": data_size,
            "timestamp": time.time(),
        }

    async def _run_integrated_benchmark(self) -> BenchmarkResults:
        """çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆå…¨ã‚·ã‚¹ãƒ†ãƒ é€£æºï¼‰"""
        self.logger.info("Running fully integrated benchmark...")

        response_times = []
        errors = 0
        start_time = time.time()

        # å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆ
        tasks = []
        for i in range(self.config.concurrent_requests):
            task = asyncio.create_task(self._integrated_worker(response_times, i))
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)
        errors = sum(1 for r in results if isinstance(r, Exception))

        total_time = time.time() - start_time

        return self._calculate_benchmark_results(
            "fully_integrated", response_times, errors, total_time
        )

    async def _integrated_worker(self, response_times: List[float], worker_id: int):
        """çµ±åˆãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥â†’éåŒæœŸæœ€é©åŒ–â†’ãƒ—ãƒ­ã‚­ã‚·ã®é †ã§ä½¿ç”¨ï¼‰"""
        for request_num in range(self.config.request_rate_per_second):
            try:
                start_time = time.time()
                await self._execute_integrated_request(worker_id, request_num)
                response_time = (time.time() - start_time) * 1000
                response_times.append(response_time)

                await asyncio.sleep(1.0 / self.config.request_rate_per_second)

            except Exception as e:
                self.logger.warning(f"Integrated worker {worker_id} error: {str(e)}")
                raise

    async def _execute_integrated_request(self, worker_id: int, request_num: int):
        """çµ±åˆãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        # 1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        cache_key = f"integrated_{worker_id}_{request_num % 10}"  # 10ç¨®é¡ã®ã‚­ãƒ¼ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡å‘ä¸Š

        cached_result = await self.cache_manager.get_quality_check_cache(
            cache_key, "integrated_test"
        )

        if cached_result:
            return cached_result

        # 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ - éåŒæœŸæœ€é©åŒ–ã§å‡¦ç†
        async def integrated_task():
            # 3. ãƒ—ãƒ­ã‚­ã‚·çµŒç”±ã§ã‚µãƒ¼ãƒ“ã‚¹å‘¼ã³å‡ºã—
            proxy_request = ProxyRequest(
                request_id=f"integrated_{worker_id}_{request_num}",
                target_service="integrated_service",
                method="complex_processing",
                payload={
                    "worker_id": worker_id,
                    "request_num": request_num,
                    "complexity": random.choice(["low", "medium", "high"]),
                },
            )

            # è¤‡é›‘ãªå‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            complexity = proxy_request.payload["complexity"]
            if complexity == "low":
                await asyncio.sleep(0.001)
            elif complexity == "medium":
                await asyncio.sleep(0.005)
            else:
                await asyncio.sleep(0.010)

            result = {
                "worker_id": worker_id,
                "request_num": request_num,
                "complexity": complexity,
                "processed_at": time.time(),
            }

            return result

        opt_request = AsyncOptimizationRequest(
            task_id=f"integrated_{worker_id}_{request_num}",
            coroutine_func=integrated_task,
            resource_type=ResourceType.IO_BOUND,
            timeout_s=30,
        )

        opt_response = await self.async_optimizer.process_request(opt_request)

        if opt_response.success:
            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            await self.cache_manager.set_quality_check_cache(
                cache_key, "integrated_test", opt_response.result
            )
            return opt_response.result
        else:
            raise Exception(opt_response.error_message)

    async def _monitor_system_resources(self):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–"""
        try:
            while True:
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
                memory = psutil.virtual_memory()
                self.memory_usage_history.append(memory.used / 1024 / 1024)  # MB

                # CPUä½¿ç”¨ç‡
                cpu_percent = psutil.cpu_percent(interval=0.1)
                self.cpu_usage_history.append(cpu_percent)

                await asyncio.sleep(1.0)
        except asyncio.CancelledError:
            pass

    def _calculate_benchmark_results(
        self,
        test_name: str,
        response_times: List[float],
        errors: int,
        total_time: float,
    ) -> BenchmarkResults:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœè¨ˆç®—"""
        if not response_times:
            return BenchmarkResults(test_name=test_name, error_rate_percent=100.0)

        total_requests = len(response_times) + errors
        successful_requests = len(response_times)

        average_response_time = statistics.mean(response_times)
        median_response_time = statistics.median(response_times)

        sorted_times = sorted(response_times)
        p95_index = int(len(sorted_times) * 0.95)
        p99_index = int(len(sorted_times) * 0.99)

        p95_response_time = (
            sorted_times[p95_index]
            if p95_index < len(sorted_times)
            else sorted_times[-1]
        )
        p99_response_time = (
            sorted_times[p99_index]
            if p99_index < len(sorted_times)
            else sorted_times[-1]
        )

        throughput = successful_requests / total_time if total_time > 0 else 0
        error_rate = (errors / total_requests * 100) if total_requests > 0 else 0

        peak_memory = max(self.memory_usage_history) if self.memory_usage_history else 0
        average_cpu = (
            statistics.mean(self.cpu_usage_history) if self.cpu_usage_history else 0
        )

        return BenchmarkResults(
            test_name=test_name,
            total_requests=total_requests,
            successful_requests=successful_requests,
            failed_requests=errors,
            average_response_time_ms=average_response_time,
            median_response_time_ms=median_response_time,
            p95_response_time_ms=p95_response_time,
            p99_response_time_ms=p99_response_time,
            throughput_rps=throughput,
            error_rate_percent=error_rate,
            peak_memory_mb=peak_memory,
            average_cpu_percent=average_cpu,
        )

    async def _analyze_performance_comparison(self) -> BenchmarkResults:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒåˆ†æ"""
        if "baseline" not in self.results:
            return BenchmarkResults(test_name="comparison")

        baseline = self.results["baseline"]

        # æ”¹å–„ç‡è¨ˆç®—
        improvements = {}

        for test_name, result in self.results.items():
            if test_name == "baseline":
                continue

            improvements[test_name] = {
                "response_time_improvement": (
                    (
                        baseline.average_response_time_ms
                        - result.average_response_time_ms
                    )
                    / baseline.average_response_time_ms
                    * 100
                )
                if baseline.average_response_time_ms > 0
                else 0,
                "throughput_improvement": (
                    (result.throughput_rps - baseline.throughput_rps)
                    / baseline.throughput_rps
                    * 100
                )
                if baseline.throughput_rps > 0
                else 0,
                "memory_change": result.peak_memory_mb - baseline.peak_memory_mb,
                "cpu_change": result.average_cpu_percent - baseline.average_cpu_percent,
            }

        # æœ€è‰¯ã®çµæœã‚’ç‰¹å®š
        best_response_time = min(
            (
                r.average_response_time_ms
                for r in self.results.values()
                if r.average_response_time_ms > 0
            ),
            default=0,
        )
        best_throughput = max(
            (r.throughput_rps for r in self.results.values()), default=0
        )

        return BenchmarkResults(
            test_name="comparison",
            additional_metrics={
                "improvements": improvements,
                "best_response_time_ms": best_response_time,
                "best_throughput_rps": best_throughput,
                "overall_improvement_percent": (
                    (baseline.average_response_time_ms - best_response_time)
                    / baseline.average_response_time_ms
                    * 100
                )
                if baseline.average_response_time_ms > 0
                else 0,
            },
        )

    async def _generate_benchmark_report(self) -> str:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_lines = [
            "# Elder Servantsçµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ",
            f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().isoformat()}",
            f"ãƒ†ã‚¹ãƒˆè¨­å®š: {self.config.concurrent_requests}ä¸¦è¡Œ, {self.config.test_duration_seconds}ç§’é–“",
            "",
            "## ğŸ“Š çµæœã‚µãƒãƒªãƒ¼",
        ]

        for test_name, result in self.results.items():
            if test_name == "comparison":
                continue

            report_lines.extend(
                [
                    f"### {test_name}",
                    f"- å¹³å‡å¿œç­”æ™‚é–“: {result.average_response_time_ms:.2f}ms",
                    f"- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result.throughput_rps:.2f} RPS",
                    f"- ã‚¨ãƒ©ãƒ¼ç‡: {result.error_rate_percent:.2f}%",
                    f"- ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {result.peak_memory_mb:.2f}MB",
                    f"- å¹³å‡CPU: {result.average_cpu_percent:.2f}%",
                    "",
                ]
            )

        # æ¯”è¼ƒåˆ†æ
        if "comparison" in self.results:
            comparison = self.results["comparison"]
            improvements = comparison.additional_metrics.get("improvements", {})

            report_lines.extend(["## ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„åˆ†æ", ""])

            for test_name, improvement in improvements.items():
                report_lines.extend(
                    [
                        f"### {test_name} vs Baseline",
                        f"- å¿œç­”æ™‚é–“æ”¹å–„: {improvement['response_time_improvement']:.1f}%",
                        f"- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ”¹å–„: {improvement['throughput_improvement']:.1f}%",
                        f"- ãƒ¡ãƒ¢ãƒªå¤‰åŒ–: {improvement['memory_change']:+.1f}MB",
                        f"- CPUå¤‰åŒ–: {improvement['cpu_change']:+.1f}%",
                        "",
                    ]
                )

            overall_improvement = comparison.additional_metrics.get(
                "overall_improvement_percent", 0
            )
            report_lines.extend(
                ["## ğŸ¯ ç·åˆè©•ä¾¡", f"**æœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„: {overall_improvement:.1f}%**", ""]
            )

            # Iron WillåŸºæº–åˆ¤å®š
            if overall_improvement >= 50.0:  # 50%ä»¥ä¸Šæ”¹å–„
                report_lines.append("âœ… **Iron WillåŸºæº–é”æˆ**: 50%ä»¥ä¸Šã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã‚’å®Ÿç¾")
            else:
                report_lines.append("âš ï¸ **Iron WillåŸºæº–æœªé”**: ã•ã‚‰ãªã‚‹æœ€é©åŒ–ãŒå¿…è¦")

        report = "\n".join(report_lines)

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_file = f"/tmp/elder_servants_benchmark_{int(time.time())}.md"
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report)

        self.logger.info(f"Benchmark report saved to: {report_file}")
        return report

    async def _cleanup_systems(self):
        """ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if self.cache_manager:
                await self.cache_manager.close()

            if self.async_optimizer:
                await self.async_optimizer.cleanup_resources()

            if self.proxy:
                await self.proxy.cleanup_resources()

            # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            gc.collect()

            self.logger.info("Systems cleaned up successfully")

        except Exception as e:
            self.logger.error(f"Cleanup failed: {str(e)}")


# ä¾¿åˆ©é–¢æ•°
async def run_quick_benchmark(
    duration_seconds: int = 30, concurrent_requests: int = 20
) -> Dict[str, BenchmarkResults]:
    """ã‚¯ã‚¤ãƒƒã‚¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
    config = BenchmarkConfig(
        test_duration_seconds=duration_seconds,
        concurrent_requests=concurrent_requests,
        request_rate_per_second=5,
        warmup_requests=10,
    )

    benchmark = IntegratedPerformanceBenchmark(config)

    try:
        return await benchmark.run_full_benchmark()
    finally:
        await benchmark._cleanup_systems()


if __name__ == "__main__":
    # ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
    async def main():
        results = await run_quick_benchmark(duration_seconds=15, concurrent_requests=10)

        print("ğŸ¯ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†!")
        for test_name, result in results.items():
            if hasattr(result, "average_response_time_ms"):
                print(
                    f"{test_name}: {result.average_response_time_ms:.2f}ms avg, {result.throughput_rps:.2f} RPS"
                )

    asyncio.run(main())
