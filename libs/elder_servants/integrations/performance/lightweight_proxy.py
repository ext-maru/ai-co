"""
ğŸª¶ Elder Servantsè»½é‡ãƒ—ãƒ­ã‚­ã‚·ãƒ¬ã‚¤ãƒ¤ãƒ¼
Phase 3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼šæœ€å°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚¢ã‚¯ã‚»ã‚¹å±¤

EldersServiceLegacyçµ±åˆ: Iron Willå“è³ªåŸºæº–ã¨ã‚¨ãƒ«ãƒ€ãƒ¼è©•è­°ä¼šä»¤ç¬¬27å·å®Œå…¨æº–æ‹ 
ç›®æ¨™: Elderæ©Ÿèƒ½ã¸ã®é«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹ã‚’ä½ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã§æä¾›
"""

import asyncio
import logging
import time
import weakref
from typing import Dict, Any, List, Optional, Union, Callable, TypeVar, Generic
from datetime import datetime, timedelta
from enum import Enum
from dataclasses import dataclass, field
from contextlib import asynccontextmanager
import json
import gzip
import pickle
from functools import lru_cache, wraps
import threading
from concurrent.futures import Future

# EldersLegacyçµ±åˆã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from libs.core.elders_legacy import (
    EldersServiceLegacy,
    enforce_boundary,
    EldersLegacyDomain,
    IronWillCriteria
)

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from libs.elder_servants.integrations.performance.cache_manager import (
    ElderCacheManager, CacheKeyType, get_cache_manager
)
from libs.elder_servants.integrations.performance.async_optimizer import (
    AsyncWorkerOptimizer, ResourceType, get_global_optimizer
)


class ProxyMode(Enum):
    """ãƒ—ãƒ­ã‚­ã‚·ãƒ¢ãƒ¼ãƒ‰"""
    DIRECT = "direct"                   # ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆæœ€é«˜é€Ÿï¼‰
    CACHED = "cached"                   # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼ˆé«˜é€Ÿï¼‰
    OPTIMIZED = "optimized"             # æœ€é©åŒ–ä½¿ç”¨ï¼ˆãƒãƒ©ãƒ³ã‚¹ï¼‰
    STREAMING = "streaming"             # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼ˆå¤§å®¹é‡ï¼‰
    LAZY = "lazy"                       # é…å»¶ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ï¼‰


class CompressionLevel(Enum):
    """åœ§ç¸®ãƒ¬ãƒ™ãƒ«"""
    NONE = 0                           # åœ§ç¸®ãªã—
    FAST = 1                           # é«˜é€Ÿåœ§ç¸®
    BALANCED = 6                       # ãƒãƒ©ãƒ³ã‚¹åœ§ç¸®
    MAXIMUM = 9                        # æœ€å¤§åœ§ç¸®


@dataclass
class ProxyConfig:
    """ãƒ—ãƒ­ã‚­ã‚·è¨­å®š"""
    mode: ProxyMode = ProxyMode.OPTIMIZED
    enable_compression: bool = True
    compression_level: CompressionLevel = CompressionLevel.BALANCED
    compression_threshold_bytes: int = 1024  # 1KBä»¥ä¸Šã§åœ§ç¸®
    enable_streaming: bool = True
    streaming_chunk_size: int = 8192  # 8KB chunks
    enable_lazy_loading: bool = True
    cache_small_responses: bool = True
    small_response_threshold: int = 10240  # 10KBä»¥ä¸‹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    max_response_size_mb: int = 50  # 50MBåˆ¶é™
    timeout_seconds: int = 30


@dataclass
class ProxyMetrics:
    """ãƒ—ãƒ­ã‚­ã‚·ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    total_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    compression_saves_bytes: int = 0
    streaming_requests: int = 0
    lazy_loads: int = 0
    total_response_time_ms: float = 0.0
    total_bytes_transferred: int = 0


T = TypeVar('T')


class ProxyRequest(Generic[T]):
    """ãƒ—ãƒ­ã‚­ã‚·ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""

    def __init__(self, request_id: str, target_service: str,
                 method: str, payload: T, config: ProxyConfig = None):
        self.request_id = request_id
        self.target_service = target_service
        self.method = method
        self.payload = payload
        self.config = config or ProxyConfig()
        self.created_at = datetime.now()
        self.metadata: Dict[str, Any] = {}


class ProxyResponse(Generic[T]):
    """ãƒ—ãƒ­ã‚­ã‚·ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""

    def __init__(self, request_id: str, success: bool,
                 data: T = None, error_message: Optional[str] = None,
                 was_cached: bool = False, was_compressed: bool = False,
                 was_streamed: bool = False, response_size_bytes: int = 0,
                 processing_time_ms: float = 0.0):
        self.request_id = request_id
        self.success = success
        self.data = data
        self.error_message = error_message
        self.was_cached = was_cached
        self.was_compressed = was_compressed
        self.was_streamed = was_streamed
        self.response_size_bytes = response_size_bytes
        self.processing_time_ms = processing_time_ms
        self.completed_at = datetime.now()


class LightweightElderProxy(EldersServiceLegacy[ProxyRequest, ProxyResponse]):
    """
    ğŸª¶ Elder Servantsè»½é‡ãƒ—ãƒ­ã‚­ã‚·ãƒ¬ã‚¤ãƒ¤ãƒ¼

    EldersServiceLegacyã‹ã‚‰ç¶™æ‰¿ã—ã€Iron Willå“è³ªåŸºæº–ã«å®Œå…¨æº–æ‹ ã€‚
    æœ€å°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã§Elderæ©Ÿèƒ½ã¸ã®é«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹ã‚’æä¾›ã€‚
    """

    def __init__(self, config: ProxyConfig = None):
        # EldersServiceLegacyåˆæœŸåŒ– (EXECUTIONåŸŸ)
        super().__init__("lightweight_elder_proxy")

        self.config = config or ProxyConfig()
        self.logger = logging.getLogger("elder_servants.lightweight_proxy")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.metrics = ProxyMetrics()
        self.response_cache: Dict[str, Any] = {}

        # è»½é‡ã‚µãƒ¼ãƒ“ã‚¹å‚ç…§ï¼ˆå¼±å‚ç…§ä½¿ç”¨ï¼‰
        self._service_refs: Dict[str, weakref.ReferenceType] = {}
        self._cache_manager: Optional[ElderCacheManager] = None
        self._async_optimizer: Optional[AsyncWorkerOptimizer] = None

        # é…å»¶ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¼
        self._lazy_queue: asyncio.Queue = asyncio.Queue()
        self._lazy_worker_task: Optional[asyncio.Task] = None

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        self._streaming_handlers: Dict[str, Callable] = {}

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
        self._hot_cache: Dict[str, Any] = {}  # ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ›ãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._compression_cache: Dict[str, bytes] = {}  # åœ§ç¸®ã‚­ãƒ£ãƒƒã‚·ãƒ¥

        # Iron Willå“è³ªåŸºæº–
        self.quality_threshold = 95.0

        self.logger.info(f"Lightweight Elder Proxy initialized: {self.config.mode.value}")

    @enforce_boundary("proxy")
    async def process_request(self, request: ProxyRequest) -> ProxyResponse:
        """
        EldersServiceLegacyçµ±ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†

        Args:
            request: ProxyRequestå½¢å¼ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

        Returns:
            ProxyResponse: ãƒ—ãƒ­ã‚­ã‚·å‡¦ç†çµæœ
        """
        start_time = time.time()

        try:
            self.metrics.total_requests += 1

            # å‡¦ç†ãƒ¢ãƒ¼ãƒ‰æ±ºå®š
            if request.config.mode == ProxyMode.DIRECT:
                response = await self._process_direct(request)
            elif request.config.mode == ProxyMode.CACHED:
                response = await self._process_cached(request)
            elif request.config.mode == ProxyMode.OPTIMIZED:
                response = await self._process_optimized(request)
            elif request.config.mode == ProxyMode.STREAMING:
                response = await self._process_streaming(request)
            elif request.config.mode == ProxyMode.LAZY:
                response = await self._process_lazy(request)
            else:
                response = await self._process_optimized(request)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            processing_time = (time.time() - start_time) * 1000
            response.processing_time_ms = processing_time
            self.metrics.total_response_time_ms += processing_time

            if response.was_cached:
                self.metrics.cache_hits += 1
            else:
                self.metrics.cache_misses += 1

            if response.was_compressed:
                # åœ§ç¸®ç¯€ç´„ãƒã‚¤ãƒˆæ•°ã‚’æ¨å®š
                estimated_original = response.response_size_bytes * 2  # 50%åœ§ç¸®ç‡ã¨ä»®å®š
                self.metrics.compression_saves_bytes += max(0, estimated_original - response.response_size_bytes)

            if response.was_streamed:
                self.metrics.streaming_requests += 1

            self.metrics.total_bytes_transferred += response.response_size_bytes

            return response

        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            self.logger.error(f"Proxy processing failed for {request.request_id}: {str(e)}")

            return ProxyResponse(
                request_id=request.request_id,
                success=False,
                error_message=str(e),
                processing_time_ms=processing_time
            )

    async def _process_direct(self, request: ProxyRequest) -> ProxyResponse:
        """ç›´æ¥å‡¦ç†ï¼ˆæœ€é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ï¼‰"""
        # ã‚µãƒ¼ãƒ“ã‚¹ç›´æ¥å‘¼ã³å‡ºã—
        service = await self._get_service(request.target_service)
        if not service:
            raise Exception(f"Service not found: {request.target_service}")

        # ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè¡Œ
        if hasattr(service, request.method):
            method = getattr(service, request.method)
            if asyncio.iscoroutinefunction(method):
                result = await method(request.payload)
            else:
                result = method(request.payload)
        else:
            raise Exception(f"Method not found: {request.method}")

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ
        response_data = result
        response_size = len(str(response_data).encode())

        return ProxyResponse(
            request_id=request.request_id,
            success=True,
            data=response_data,
            response_size_bytes=response_size
        )

    async def _process_cached(self, request: ProxyRequest) -> ProxyResponse:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‡¦ç†"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
        cache_key = self._generate_cache_key(request)

        # ãƒ›ãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        if cache_key in self._hot_cache:
            cached_data = self._hot_cache[cache_key]
            return ProxyResponse(
                request_id=request.request_id,
                success=True,
                data=cached_data,
                was_cached=True,
                response_size_bytes=len(str(cached_data).encode())
            )

        # å¤–éƒ¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        cache_manager = await self._get_cache_manager()
        cached_result = await cache_manager.get_quality_check_cache(
            request.request_id,
            f"{request.target_service}_{request.method}"
        )

        if cached_result:
            # ãƒ›ãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            if request.config.cache_small_responses:
                data_size = len(str(cached_result).encode())
                if data_size <= request.config.small_response_threshold:
                    self._hot_cache[cache_key] = cached_result

            return ProxyResponse(
                request_id=request.request_id,
                success=True,
                data=cached_result,
                was_cached=True,
                response_size_bytes=len(str(cached_result).encode())
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ - ç›´æ¥å‡¦ç†ã—ã¦çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        response = await self._process_direct(request)

        if response.success:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            await cache_manager.set_quality_check_cache(
                request.request_id,
                f"{request.target_service}_{request.method}",
                response.data
            )

            # ãƒ›ãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            if (request.config.cache_small_responses and
                response.response_size_bytes <= request.config.small_response_threshold):
                self._hot_cache[cache_key] = response.data

        return response

    async def _process_optimized(self, request: ProxyRequest) -> ProxyResponse:
        """æœ€é©åŒ–å‡¦ç†ï¼ˆéåŒæœŸæœ€é©åŒ–ä½¿ç”¨ï¼‰"""
        async_optimizer = await self._get_async_optimizer()

        # æœ€é©åŒ–å®Ÿè¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
        async def optimized_execution():
            return await self._process_direct(request)

        # ãƒªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—æ±ºå®š
        resource_type = self._determine_resource_type(request)

        # æœ€é©åŒ–å®Ÿè¡Œ
        from libs.elder_servants.integrations.performance.async_optimizer import AsyncOptimizationRequest

        opt_request = AsyncOptimizationRequest(
            task_id=request.request_id,
            coroutine_func=optimized_execution,
            resource_type=resource_type,
            timeout_s=request.config.timeout_seconds
        )

        opt_response = await async_optimizer.process_request(opt_request)

        if opt_response.success:
            result = opt_response.result
            response_size = len(str(result.data).encode()) if result else 0

            return ProxyResponse(
                request_id=request.request_id,
                success=True,
                data=result.data if result else None,
                response_size_bytes=response_size
            )
        else:
            return ProxyResponse(
                request_id=request.request_id,
                success=False,
                error_message=opt_response.error_message
            )

    async def _process_streaming(self, request: ProxyRequest) -> ProxyResponse:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†"""
        # å¤§å®¹é‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
        response_data = await self._process_direct(request)

        if not response_data.success:
            return response_data

        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºç¢ºèª
        data_size = response_data.response_size_bytes

        if data_size > request.config.streaming_chunk_size:
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ
            compressed_data = await self._compress_data(
                response_data.data,
                request.config.compression_level
            )

            return ProxyResponse(
                request_id=request.request_id,
                success=True,
                data=compressed_data,
                was_streamed=True,
                was_compressed=True,
                response_size_bytes=len(compressed_data)
            )
        else:
            # å°ã•ãªãƒ‡ãƒ¼ã‚¿ã¯ç›´æ¥è¿”ã™
            return response_data

    async def _process_lazy(self, request: ProxyRequest) -> ProxyResponse:
        """é…å»¶å‡¦ç†"""
        # é…å»¶ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        future = Future()
        await self._lazy_queue.put((request, future))

        # é…å»¶ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒæœªèµ·å‹•ã®å ´åˆã¯èµ·å‹•
        if self._lazy_worker_task is None or self._lazy_worker_task.done():
            self._lazy_worker_task = asyncio.create_task(self._lazy_worker())

        # é…å»¶å®Ÿè¡ŒæŒ‡ç¤ºãƒ¬ã‚¹ãƒãƒ³ã‚¹
        self.metrics.lazy_loads += 1

        return ProxyResponse(
            request_id=request.request_id,
            success=True,
            data={"status": "lazy_loading", "future_id": id(future)},
            response_size_bytes=50  # å°ã•ãªå¿œç­”ã‚µã‚¤ã‚º
        )

    async def _lazy_worker(self):
        """é…å»¶ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        while True:
            try:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§æ¬¡ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                request, future = await asyncio.wait_for(
                    self._lazy_queue.get(),
                    timeout=5.0
                )

                # å®Ÿéš›ã®å‡¦ç†å®Ÿè¡Œ
                try:
                    response = await self._process_direct(request)
                    future.set_result(response)
                except Exception as e:
                    future.set_exception(e)

            except asyncio.TimeoutError:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ãƒ¯ãƒ¼ã‚«ãƒ¼çµ‚äº†
                break
            except Exception as e:
                self.logger.error(f"Lazy worker error: {str(e)}")

    def _generate_cache_key(self, request: ProxyRequest) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        key_data = {
            "service": request.target_service,
            "method": request.method,
            "payload_hash": hash(str(request.payload))
        }
        return f"proxy_{hash(str(key_data))}"

    def _determine_resource_type(self, request: ProxyRequest) -> ResourceType:
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—æ±ºå®š"""
        # ã‚µãƒ¼ãƒ“ã‚¹ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰åã‹ã‚‰ãƒªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š
        if "cpu" in request.method.lower() or "compute" in request.method.lower():
            return ResourceType.CPU_BOUND
        elif "io" in request.method.lower() or "file" in request.method.lower():
            return ResourceType.IO_BOUND
        elif "network" in request.method.lower() or "api" in request.method.lower():
            return ResourceType.NETWORK_BOUND
        else:
            return ResourceType.IO_BOUND  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    async def _compress_data(self, data: Any, level: CompressionLevel) -> bytes:
        """ãƒ‡ãƒ¼ã‚¿åœ§ç¸®"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚’JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
            json_data = json.dumps(data, ensure_ascii=False)
            json_bytes = json_data.encode('utf-8')

            # åœ§ç¸®ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦åœ§ç¸®
            if level == CompressionLevel.NONE:
                return json_bytes

            compressed = gzip.compress(json_bytes, compresslevel=level.value)

            # åœ§ç¸®åŠ¹æœç¢ºèª
            if len(compressed) >= len(json_bytes):
                # åœ§ç¸®åŠ¹æœãŒãªã„å ´åˆã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
                return json_bytes

            return compressed

        except Exception as e:
            self.logger.warning(f"Compression failed: {str(e)}")
            # åœ§ç¸®å¤±æ•—æ™‚ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’JSONåŒ–ã—ã¦è¿”ã™
            return json.dumps(data, ensure_ascii=False).encode('utf-8')

    async def _get_service(self, service_name: str) -> Optional[Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹å–å¾—ï¼ˆå¼±å‚ç…§ç®¡ç†ï¼‰"""
        if service_name in self._service_refs:
            service_ref = self._service_refs[service_name]
            service = service_ref()
            if service is not None:
                return service
            else:
                # å¼±å‚ç…§ãŒç„¡åŠ¹ã«ãªã£ãŸå ´åˆã¯å‰Šé™¤
                del self._service_refs[service_name]

        # ã‚µãƒ¼ãƒ“ã‚¹å‹•çš„ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å®Ÿè£…ï¼‰
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªã‚µãƒ¼ãƒ“ã‚¹ãƒ­ã‚±ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨
        return None

    async def _get_cache_manager(self) -> ElderCacheManager:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼å–å¾—"""
        if self._cache_manager is None:
            self._cache_manager = await get_cache_manager()
        return self._cache_manager

    async def _get_async_optimizer(self) -> AsyncWorkerOptimizer:
        """éåŒæœŸæœ€é©åŒ–å–å¾—"""
        if self._async_optimizer is None:
            self._async_optimizer = await get_global_optimizer()
        return self._async_optimizer

    def register_service(self, name: str, service: Any):
        """ã‚µãƒ¼ãƒ“ã‚¹ç™»éŒ²ï¼ˆå¼±å‚ç…§ï¼‰"""
        self._service_refs[name] = weakref.ref(service)
        self.logger.info(f"Registered service: {name}")

    def register_streaming_handler(self, handler_name: str, handler: Callable):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç™»éŒ²"""
        self._streaming_handlers[handler_name] = handler
        self.logger.info(f"Registered streaming handler: {handler_name}")

    def validate_request(self, request: ProxyRequest) -> bool:
        """EldersServiceLegacyãƒªã‚¯ã‚¨ã‚¹ãƒˆæ¤œè¨¼"""
        if not request.request_id:
            return False
        if not request.target_service:
            return False
        if not request.method:
            return False
        return True

    def get_capabilities(self) -> List[str]:
        """EldersServiceLegacyèƒ½åŠ›å–å¾—"""
        return [
            "lightweight_proxying",
            "intelligent_caching",
            "data_compression",
            "streaming_support",
            "lazy_loading",
            "performance_optimization",
            "resource_management"
        ]

    @lru_cache(maxsize=1000)
    def _cached_method_lookup(self, service_name: str, method_name: str) -> Optional[str]:
        """ãƒ¡ã‚½ãƒƒãƒ‰æ¤œç´¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        # ãƒ¡ã‚½ãƒƒãƒ‰å­˜åœ¨ç¢ºèªã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        return f"{service_name}.{method_name}"

    async def get_proxy_metrics(self) -> Dict[str, Any]:
        """ãƒ—ãƒ­ã‚­ã‚·ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        hit_rate = (self.metrics.cache_hits / max(self.metrics.total_requests, 1)) * 100
        avg_response_time = (self.metrics.total_response_time_ms /
                           max(self.metrics.total_requests, 1))

        compression_efficiency = 0.0
        if self.metrics.total_bytes_transferred > 0:
            compression_efficiency = (self.metrics.compression_saves_bytes /
                                    self.metrics.total_bytes_transferred) * 100

        return {
            "total_requests": self.metrics.total_requests,
            "cache_hit_rate": round(hit_rate, 2),
            "average_response_time_ms": round(avg_response_time, 2),
            "total_bytes_transferred": self.metrics.total_bytes_transferred,
            "compression_saves_bytes": self.metrics.compression_saves_bytes,
            "compression_efficiency_percent": round(compression_efficiency, 2),
            "streaming_requests": self.metrics.streaming_requests,
            "lazy_loads": self.metrics.lazy_loads,
            "hot_cache_size": len(self._hot_cache),
            "registered_services": len(self._service_refs),
            "proxy_mode": self.config.mode.value,
            "iron_will_compliance": hit_rate >= 70.0 and avg_response_time <= 100.0
        }

    async def clear_caches(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        self._hot_cache.clear()
        self._compression_cache.clear()
        self._cached_method_lookup.cache_clear()
        self.logger.info("All caches cleared")

    async def health_check(self) -> Dict[str, Any]:
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        try:
            # åŸºæœ¬ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
            base_health = await super().health_check()

            # ãƒ—ãƒ­ã‚­ã‚·å›ºæœ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            metrics = await self.get_proxy_metrics()

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
            performance_healthy = (
                metrics["cache_hit_rate"] >= 70.0 and
                metrics["average_response_time_ms"] <= 100.0
            )

            # ãƒªã‚½ãƒ¼ã‚¹å¥å…¨æ€§
            resource_healthy = (
                len(self._hot_cache) < 10000 and  # ãƒ›ãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
                self.metrics.total_bytes_transferred < 1024*1024*1024  # 1GBåˆ¶é™
            )

            overall_healthy = performance_healthy and resource_healthy

            return {
                **base_health,
                "proxy_status": "healthy" if overall_healthy else "degraded",
                "metrics": metrics,
                "performance_healthy": performance_healthy,
                "resource_healthy": resource_healthy,
                "iron_will_compliance": overall_healthy
            }

        except Exception as e:
            self.logger.error(f"Health check failed: {str(e)}")
            return {
                "success": False,
                "status": "error",
                "error": str(e)
            }

    async def cleanup_resources(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # é…å»¶ãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢
        if self._lazy_worker_task and not self._lazy_worker_task.done():
            self._lazy_worker_task.cancel()
            try:
                await self._lazy_worker_task
            except asyncio.CancelledError:
                pass

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        await self.clear_caches()

        # å¼±å‚ç…§ã‚¯ãƒªã‚¢
        self._service_refs.clear()

        self.logger.info("Lightweight proxy resources cleaned up")


# ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿é–¢æ•°ç¾¤
def lightweight_proxy(mode: ProxyMode = ProxyMode.OPTIMIZED,
                     cache_response: bool = True,
                     compress_response: bool = True):
    """è»½é‡ãƒ—ãƒ­ã‚­ã‚·ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ãƒ—ãƒ­ã‚­ã‚·çµŒç”±ã§å®Ÿè¡Œ
            proxy = LightweightElderProxy(ProxyConfig(
                mode=mode,
                cache_small_responses=cache_response,
                enable_compression=compress_response
            ))

            try:
                # é–¢æ•°ã‚’ãƒ—ãƒ­ã‚­ã‚·çµŒç”±ã§å®Ÿè¡Œ
                request = ProxyRequest(
                    request_id=f"decorated_{int(time.time())}",
                    target_service="decorator_service",
                    method=func.__name__,
                    payload={"args": args, "kwargs": kwargs}
                )

                # ç›´æ¥å®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                result = await func(*args, **kwargs)

                return ProxyResponse(
                    request_id=request.request_id,
                    success=True,
                    data=result
                )

            finally:
                await proxy.cleanup_resources()

        return wrapper
    return decorator


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ­ã‚­ã‚·ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_global_proxy: Optional[LightweightElderProxy] = None


async def get_global_proxy() -> LightweightElderProxy:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ­ã‚­ã‚·å–å¾—"""
    global _global_proxy

    if _global_proxy is None:
        _global_proxy = LightweightElderProxy()

    return _global_proxy


# ä¾¿åˆ©é–¢æ•°ç¾¤
async def quick_proxy_call(service_name: str, method_name: str,
                          payload: Any, mode: ProxyMode = ProxyMode.OPTIMIZED) -> Any:
    """ã‚¯ã‚¤ãƒƒã‚¯ãƒ—ãƒ­ã‚­ã‚·å‘¼ã³å‡ºã—"""
    proxy = await get_global_proxy()

    request = ProxyRequest(
        request_id=f"quick_{int(time.time())}",
        target_service=service_name,
        method=method_name,
        payload=payload,
        config=ProxyConfig(mode=mode)
    )

    response = await proxy.process_request(request)

    if response.success:
        return response.data
    else:
        raise Exception(response.error_message)


async def cached_proxy_call(service_name: str, method_name: str, payload: Any) -> Any:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ããƒ—ãƒ­ã‚­ã‚·å‘¼ã³å‡ºã—"""
    return await quick_proxy_call(service_name, method_name, payload, ProxyMode.CACHED)


async def streaming_proxy_call(service_name: str, method_name: str, payload: Any) -> Any:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ—ãƒ­ã‚­ã‚·å‘¼ã³å‡ºã—"""
    return await quick_proxy_call(service_name, method_name, payload, ProxyMode.STREAMING)
