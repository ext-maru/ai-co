#!/usr/bin/env python3
"""
Enhanced Autonomous Learning System - å¼·åŒ–ç‰ˆè‡ªå¾‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ AIè‡ªå‹•åŒ–ã‚¿ã‚¹ã‚¯ - è‡ªå¾‹å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ”¹å–„å®Ÿè£…

æ©Ÿèƒ½å¼·åŒ–:
- é©å¿œå‹å­¦ç¿’ç‡èª¿æ•´
- å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³é‡è¦åº¦è©•ä¾¡
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½æœ€é©åŒ–
- äºˆæ¸¬çš„ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå›é¿
- ãƒ¡ã‚¿å­¦ç¿’ã«ã‚ˆã‚‹è‡ªå·±æ”¹å–„
"""

import asyncio
import secrets
import json
import logging
import sqlite3
import statistics
from collections import defaultdict
from collections import deque
from dataclasses import dataclass
from dataclasses import field
from datetime import datetime
from pathlib import Path
from typing import Any
from typing import Dict
from typing import List
from typing import Optional
from typing import Tuple

import numpy as np

from .four_sages_integration import FourSagesIntegration

logger = logging.getLogger(__name__)


@dataclass
class EnhancedLearningPattern:
    """å¼·åŒ–ç‰ˆå­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³"""

    pattern_id: str
    pattern_type: str
    context: Dict[str, Any]
    success_rate: float
    usage_count: int
    importance_score: float = 0.5
    adaptation_rate: float = 0.1
    confidence_interval: Tuple[float, float] = (0.0, 1.0)
    last_performance: Optional[float] = None
    trend_direction: str = "stable"  # improving, declining, stable
    meta_features: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    last_updated: datetime = field(default_factory=datetime.now)

    def update_performance(self, success: bool, context_similarity: float = 1.0):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ›´æ–° - æ–‡è„ˆé¡ä¼¼åº¦ã‚’è€ƒæ…®"""
        # é©å¿œå‹å­¦ç¿’ç‡ï¼ˆé¡ä¼¼åº¦ã«åŸºã¥ãï¼‰
        effective_learning_rate = self.adaptation_rate * context_similarity

        # æˆåŠŸç‡æ›´æ–°ï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰
        old_success_rate = self.success_rate
        self.success_rate = (
            1 - effective_learning_rate
        ) * self.success_rate + effective_learning_rate * (1.0 if success else 0.0)

        # ä½¿ç”¨å›æ•°ã¨æœ€çµ‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ›´æ–°
        self.usage_count += 1
        self.last_performance = 1.0 if success else 0.0

        # ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘ã®åˆ¤å®š
        if self.last_performance is not None:
            if self.success_rate > old_success_rate:
                self.trend_direction = "improving"
            elif self.success_rate < old_success_rate:
                self.trend_direction = "declining"
            else:
                self.trend_direction = "stable"

        # ä¿¡é ¼åŒºé–“ã®æ›´æ–°ï¼ˆãƒ™ã‚¤ã‚ºæ¨å®šï¼‰
        self._update_confidence_interval()

        # é‡è¦åº¦ã‚¹ã‚³ã‚¢ã®å‹•çš„èª¿æ•´
        self._update_importance_score()

        self.last_updated = datetime.now()

    def _update_confidence_interval(self):
        """ä¿¡é ¼åŒºé–“ã®æ›´æ–°ï¼ˆãƒ™ãƒ¼ã‚¿åˆ†å¸ƒãƒ™ãƒ¼ã‚¹ï¼‰"""
        if self.usage_count > 10:
            # ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            alpha = self.success_rate * self.usage_count + 1
            beta = (1 - self.success_rate) * self.usage_count + 1

            # 95%ä¿¡é ¼åŒºé–“ã®è¿‘ä¼¼è¨ˆç®—
            mean = alpha / (alpha + beta)
            variance = (alpha * beta) / ((alpha + beta) ** 2 * (alpha + beta + 1))
            std_dev = np.sqrt(variance)

            # ä¿¡é ¼åŒºé–“ï¼ˆæ­£è¦è¿‘ä¼¼ï¼‰
            margin = 1.96 * std_dev
            self.confidence_interval = (
                max(0.0, mean - margin),
                min(1.0, mean + margin),
            )

    def _update_importance_score(self):
        """é‡è¦åº¦ã‚¹ã‚³ã‚¢ã®å‹•çš„æ›´æ–°"""
        # åŸºæœ¬é‡è¦åº¦ï¼ˆæˆåŠŸç‡ãƒ™ãƒ¼ã‚¹ï¼‰
        base_importance = self.success_rate

        # ä½¿ç”¨é »åº¦ãƒœãƒ¼ãƒŠã‚¹
        frequency_bonus = min(0.2, self.usage_count / 100.0)

        # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒœãƒ¼ãƒŠã‚¹/ãƒšãƒŠãƒ«ãƒ†ã‚£
        trend_modifier = {"improving": 0.1, "stable": 0.0, "declining": -0.1}.get(
            self.trend_direction, 0.0
        )

        # ä¿¡é ¼åº¦ãƒœãƒ¼ãƒŠã‚¹ï¼ˆä¿¡é ¼åŒºé–“ã®ç‹­ã•ï¼‰
        interval_width = self.confidence_interval[1] - self.confidence_interval[0]
        confidence_bonus = max(0.0, 0.1 * (1.0 - interval_width))

        self.importance_score = min(
            1.0,
            max(
                0.0,
                base_importance + frequency_bonus + trend_modifier + confidence_bonus,
            ),
        )

    def predict_success_probability(self, context_features: Dict[str, Any]) -> float:
        """æ–‡è„ˆç‰¹å¾´é‡ã«åŸºã¥ãæˆåŠŸç¢ºç‡äºˆæ¸¬"""
        # åŸºæœ¬æˆåŠŸç¢ºç‡
        base_prob = self.success_rate

        # æ–‡è„ˆé¡ä¼¼åº¦ã®è¨ˆç®—
        context_similarity = self._calculate_context_similarity(context_features)

        # é¡ä¼¼åº¦ã«åŸºã¥ãèª¿æ•´
        adjusted_prob = base_prob * (0.5 + 0.5 * context_similarity)

        return min(1.0, max(0.0, adjusted_prob))

    def _calculate_context_similarity(self, new_context: Dict[str, Any]) -> float:
        """æ–‡è„ˆé¡ä¼¼åº¦ã®è¨ˆç®—"""
        if not self.context or not new_context:
            return 0.5

        # å…±é€šã‚­ãƒ¼ã®å‰²åˆ
        common_keys = set(self.context.keys()) & set(new_context.keys())
        all_keys = set(self.context.keys()) | set(new_context.keys())

        if not all_keys:
            return 1.0

        key_similarity = len(common_keys) / len(all_keys)

        # å€¤ã®é¡ä¼¼åº¦ï¼ˆå…±é€šã‚­ãƒ¼ã«ã¤ã„ã¦ï¼‰
        value_similarities = []
        for key in common_keys:
            if self.context[key] == new_context[key]:
                value_similarities.append(1.0)
            elif isinstance(self.context[key], (int, float)) and isinstance(
                new_context[key], (int, float)
            ):
                # æ•°å€¤ã®å ´åˆã¯ç›¸å¯¾å·®ã§è¨ˆç®—
                diff = abs(self.context[key] - new_context[key])
                max_val = max(abs(self.context[key]), abs(new_context[key]), 1)
                value_similarities.append(1.0 - min(1.0, diff / max_val))
            else:
                value_similarities.append(0.0)

        value_similarity = (
            statistics.mean(value_similarities) if value_similarities else 0.5
        )

        # ç·åˆé¡ä¼¼åº¦
        return 0.6 * key_similarity + 0.4 * value_similarity


@dataclass
class LearningMetrics:
    """å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½è·¡"""

    total_patterns: int = 0
    active_patterns: int = 0
    successful_predictions: int = 0
    failed_predictions: int = 0
    average_confidence: float = 0.0
    learning_velocity: float = 0.0  # ãƒ‘ã‚¿ãƒ¼ãƒ³/æ™‚é–“
    adaptation_efficiency: float = 0.0
    meta_learning_score: float = 0.0

    def update_from_predictions(self, predictions: List[Tuple[bool, float]]):
        """äºˆæ¸¬çµæœã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°"""
        if predictions:
            successes = sum(1 for success, _ in predictions if success)
            self.successful_predictions += successes
            self.failed_predictions += len(predictions) - successes

            confidences = [conf for _, conf in predictions]
            self.average_confidence = statistics.mean(confidences)

    def calculate_accuracy(self) -> float:
        """äºˆæ¸¬ç²¾åº¦è¨ˆç®—"""
        total = self.successful_predictions + self.failed_predictions
        if total == 0:
            return 0.0
        return self.successful_predictions / total


class EnhancedAutonomousLearningSystem:
    """å¼·åŒ–ç‰ˆè‡ªå¾‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, learning_config: Optional[Dict[str, Any]] = None):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.four_sages = FourSagesIntegration()

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.db_path = Path("data/enhanced_learning.db")
        self.knowledge_base_path = Path("knowledge_base/enhanced_learning")
        self.knowledge_base_path.mkdir(parents=True, exist_ok=True)

        # å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ç®¡ç†
        self.learning_patterns: Dict[str, EnhancedLearningPattern] = {}
        self.pattern_clusters: Dict[str, List[str]] = defaultdict(list)
        self.performance_history: deque = deque(maxlen=1000)

        # å­¦ç¿’è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        default_config = {
            "min_pattern_confidence": 0.6,
            "adaptation_learning_rate": 0.1,
            "importance_threshold": 0.3,
            "meta_learning_enabled": True,
            "real_time_optimization": True,
            "predictive_mode": True,
            "clustering_enabled": True,
            "performance_target": 0.85,
        }

        self.config = {**default_config, **(learning_config or {})}

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½è·¡
        self.metrics = LearningMetrics()
        self.meta_learning_state = {
            "learning_rate_history": deque(maxlen=100),
            "performance_trends": deque(maxlen=50),
            "optimal_parameters": {},
            "adaptation_success_rate": 0.5,
        }

        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–
        self.optimization_queue = deque(maxlen=100)
        self.performance_targets = {
            "accuracy": self.config["performance_target"],
            "learning_speed": 0.1,  # ãƒ‘ã‚¿ãƒ¼ãƒ³/ç§’
            "adaptation_rate": 0.05,
        }

        self._init_database()
        logger.info("Enhanced Autonomous Learning System initialized")

    def _init_database(self):
        """å¼·åŒ–ç‰ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        self.db_path.parent.mkdir(exist_ok=True)
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()

        # å¼·åŒ–ç‰ˆå­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS enhanced_patterns (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            pattern_id TEXT UNIQUE,
            pattern_type TEXT,
            context TEXT,
            success_rate REAL,
            usage_count INTEGER,
            importance_score REAL,
            adaptation_rate REAL,
            confidence_interval_low REAL,
            confidence_interval_high REAL,
            trend_direction TEXT,
            meta_features TEXT,
            created_at TIMESTAMP,
            last_updated TIMESTAMP
        )
        """
        )

        # ãƒ¡ã‚¿å­¦ç¿’å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS meta_learning_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TIMESTAMP,
            learning_rate REAL,
            performance_score REAL,
            adaptation_efficiency REAL,
            active_patterns INTEGER,
            meta_parameters TEXT
        )
        """
        )

        # äºˆæ¸¬å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS prediction_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            pattern_id TEXT,
            predicted_success_prob REAL,
            actual_success BOOLEAN,
            context_features TEXT,
            prediction_accuracy REAL,
            timestamp TIMESTAMP
        )
        """
        )

        conn.commit()
        conn.close()

    async def start_enhanced_learning(self):
        """å¼·åŒ–ç‰ˆè‡ªå¾‹å­¦ç¿’é–‹å§‹"""
        logger.info("ğŸš€ Starting Enhanced Autonomous Learning System")

        # ä¸¦è¡Œå­¦ç¿’ã‚¿ã‚¹ã‚¯
        await asyncio.gather(
            self._adaptive_pattern_learning_loop(),
            self._real_time_optimization_loop(),
            self._meta_learning_loop(),
            self._predictive_analysis_loop(),
            self._performance_monitoring_loop(),
        )

    async def _adaptive_pattern_learning_loop(self):
        """é©å¿œå‹ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                # å‹•çš„å­¦ç¿’ç‡èª¿æ•´
                self._adjust_learning_rates()

                # æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç™ºè¦‹
                await self._discover_new_patterns()

                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é‡è¦åº¦è©•ä¾¡ã¨æ›´æ–°
                await self._evaluate_pattern_importance()

                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
                if self.config["clustering_enabled"]:
                    self._cluster_similar_patterns()

                # ä½é‡è¦åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‰Šé™¤
                self._prune_low_importance_patterns()

                await asyncio.sleep(60)  # 1åˆ†é–“éš”

            except Exception as e:
                logger.error(f"Adaptive pattern learning error: {e}")
                await asyncio.sleep(30)

    async def _real_time_optimization_loop(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                if not self.config["real_time_optimization"]:
                    await asyncio.sleep(60)
                    continue

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
                current_performance = self._collect_current_performance()

                # æœ€é©åŒ–æ©Ÿä¼šã®ç‰¹å®š
                optimization_opportunities = self._identify_optimization_opportunities(
                    current_performance
                )

                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èª¿æ•´å®Ÿè¡Œ
                for opportunity in optimization_opportunities:
                    await self._apply_real_time_optimization(opportunity)

                await asyncio.sleep(30)  # 30ç§’é–“éš”

            except Exception as e:
                logger.error(f"Real-time optimization error: {e}")
                await asyncio.sleep(30)

    async def _meta_learning_loop(self):
        """ãƒ¡ã‚¿å­¦ç¿’ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                if not self.config["meta_learning_enabled"]:
                    await asyncio.sleep(300)
                    continue

                # å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹è‡ªä½“ã®åˆ†æ
                learning_analysis = self._analyze_learning_process()

                # æœ€é©å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç™ºè¦‹
                optimal_params = self._discover_optimal_parameters(learning_analysis)

                # å­¦ç¿’æˆ¦ç•¥ã®é©å¿œ
                if self._validate_parameter_improvement(optimal_params):
                    self._apply_meta_learning_insights(optimal_params)

                # ãƒ¡ã‚¿å­¦ç¿’å±¥æ­´ã®è¨˜éŒ²
                await self._record_meta_learning_session(
                    learning_analysis, optimal_params
                )

                await asyncio.sleep(300)  # 5åˆ†é–“éš”

            except Exception as e:
                logger.error(f"Meta-learning error: {e}")
                await asyncio.sleep(60)

    async def _predictive_analysis_loop(self):
        """äºˆæ¸¬åˆ†æãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                if not self.config["predictive_mode"]:
                    await asyncio.sleep(120)
                    continue

                # å°†æ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬
                performance_predictions = self._predict_future_performance()

                # æ½œåœ¨çš„å•é¡Œã®æ—©æœŸç™ºè¦‹
                potential_issues = self._detect_potential_issues(
                    performance_predictions
                )

                # äºˆé˜²çš„å¯¾ç­–ã®ææ¡ˆãƒ»å®Ÿè¡Œ
                for issue in potential_issues:
                    await self._apply_preventive_measures(issue)

                # äºˆæ¸¬ç²¾åº¦ã®è¿½è·¡
                self._track_prediction_accuracy()

                await asyncio.sleep(120)  # 2åˆ†é–“éš”

            except Exception as e:
                logger.error(f"Predictive analysis error: {e}")
                await asyncio.sleep(60)

    async def _performance_monitoring_loop(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                # å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ è‡ªä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
                system_metrics = self._measure_system_performance()

                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
                self._update_learning_metrics(system_metrics)

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´è¨˜éŒ²
                self.performance_history.append(
                    {
                        "timestamp": datetime.now(),
                        "metrics": system_metrics,
                        "active_patterns": len(self.learning_patterns),
                        "accuracy": self.metrics.calculate_accuracy(),
                    }
                )

                # ã‚¢ãƒ©ãƒ¼ãƒˆæ¤œå‡º
                alerts = self._detect_performance_alerts(system_metrics)
                if alerts:
                    await self._handle_performance_alerts(alerts)

                await asyncio.sleep(60)  # 1åˆ†é–“éš”

            except Exception as e:
                logger.error(f"Performance monitoring error: {e}")
                await asyncio.sleep(60)

    # å­¦ç¿’ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£…

    def _adjust_learning_rates(self):
        """å‹•çš„å­¦ç¿’ç‡èª¿æ•´"""
        # æœ€è¿‘ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã«åŸºã¥ãèª¿æ•´
        recent_performance = list(self.performance_history)[-10:]

        if len(recent_performance) >= 5:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            accuracies = [p["accuracy"] for p in recent_performance]
            trend = self._calculate_trend(accuracies)

            # å­¦ç¿’ç‡èª¿æ•´
            for pattern in self.learning_patterns.values():
                if trend > 0.05:  # æ”¹å–„å‚¾å‘
                    pattern.adaptation_rate = min(0.3, pattern.adaptation_rate * 1.1)
                elif trend < -0.05:  # æ‚ªåŒ–å‚¾å‘
                    pattern.adaptation_rate = max(0.01, pattern.adaptation_rate * 0.9)
                # else: å®‰å®šå‚¾å‘ã¯ç¾çŠ¶ç¶­æŒ

    def _calculate_trend(self, values: List[float]) -> float:
        """å€¤ã®å‚¾å‘è¨ˆç®—ï¼ˆç·šå½¢å›å¸°ã®å‚¾ãï¼‰"""
        if len(values) < 2:
            return 0.0

        n = len(values)
        x = list(range(n))

        # ç·šå½¢å›å¸°ã®å‚¾ãã‚’è¨ˆç®—
        x_mean = statistics.mean(x)
        y_mean = statistics.mean(values)

        numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
        denominator = sum((x[i] - x_mean) ** 2 for i in range(n))

        if denominator == 0:
            return 0.0

        return numerator / denominator

    async def _discover_new_patterns(self) -> List[EnhancedLearningPattern]:
        """æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç™ºè¦‹"""
        # Four Sagesã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        learning_data = await self._collect_sage_learning_data()

        new_patterns = []
        for data in learning_data:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³å€™è£œã®ç”Ÿæˆ
            pattern_candidate = self._generate_pattern_candidate(data)

            # æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if not self._is_duplicate_pattern(pattern_candidate):
                new_pattern = EnhancedLearningPattern(
                    pattern_id=f"pattern_{datetime.now().timestamp()}_{len(new_patterns)}",
                    pattern_type=data.get("type", "general"),
                    context=data.get("context", {}),
                    success_rate=data.get("initial_success_rate", 0.5),
                    usage_count=1,
                    adaptation_rate=self.config["adaptation_learning_rate"],
                )

                self.learning_patterns[new_pattern.pattern_id] = new_pattern
                new_patterns.append(new_pattern)

        return new_patterns

    async def _collect_sage_learning_data(self) -> List[Dict[str, Any]]:
        """è³¢è€…ã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åé›†"""
        # å„è³¢è€…ã‹ã‚‰æœ€è¿‘ã®æ´»å‹•ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        sage_data = []

        for sage_name in ["knowledge_sage", "task_sage", "incident_sage", "rag_sage"]:
            try:
                # è³¢è€…å›ºæœ‰ã®ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰
                sage_specific_data = self._collect_sage_specific_data(sage_name)
                sage_data.extend(sage_specific_data)
            except Exception as e:
                logger.warning(f"Failed to collect data from {sage_name}: {e}")

        return sage_data

    def _collect_sage_specific_data(self, sage_name: str) -> List[Dict[str, Any]]:
        """è³¢è€…å›ºæœ‰ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰"""
        import random

        # è³¢è€…åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³
        sage_patterns = {
            "knowledge_sage": [
                {
                    "type": "knowledge_pattern",
                    "context": {"domain": "api_optimization"},
                    "initial_success_rate": 0.8,
                },
                {
                    "type": "learning_pattern",
                    "context": {"method": "incremental"},
                    "initial_success_rate": 0.75,
                },
            ],
            "task_sage": [
                {
                    "type": "task_optimization",
                    "context": {"priority": "high"},
                    "initial_success_rate": 0.85,
                },
                {
                    "type": "workflow_pattern",
                    "context": {"parallel": True},
                    "initial_success_rate": 0.7,
                },
            ],
            "incident_sage": [
                {
                    "type": "error_prevention",
                    "context": {"severity": "medium"},
                    "initial_success_rate": 0.9,
                },
                {
                    "type": "recovery_pattern",
                    "context": {"auto_heal": True},
                    "initial_success_rate": 0.65,
                },
            ],
            "rag_sage": [
                {
                    "type": "search_optimization",
                    "context": {"semantic": True},
                    "initial_success_rate": 0.8,
                },
                {
                    "type": "context_enhancement",
                    "context": {"relevance": 0.9},
                    "initial_success_rate": 0.75,
                },
            ],
        }

        patterns = sage_patterns.get(sage_name, [])
        return random.sample(patterns, min(2, len(patterns)))

    def _generate_pattern_candidate(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³å€™è£œç”Ÿæˆ"""
        return {
            "type": data.get("type", "unknown"),
            "context_hash": hash(str(sorted(data.get("context", {}).items()))),
            "features": self._extract_pattern_features(data),
        }

    def _extract_pattern_features(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡æŠ½å‡º"""
        features = {}

        context = data.get("context", {})

        # åŸºæœ¬ç‰¹å¾´é‡
        features["complexity"] = len(str(context))
        features["has_numeric_values"] = any(
            isinstance(v, (int, float)) for v in context.values()
        )
        features["context_size"] = len(context)

        # ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ç‰¹å¾´é‡
        if "optimization" in data.get("type", ""):
            features["is_optimization"] = True
        if "error" in data.get("type", "") or "incident" in data.get("type", ""):
            features["is_error_related"] = True

        return features

    def _is_duplicate_pattern(self, candidate: Dict[str, Any]) -> bool:
        """é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯"""
        candidate_hash = candidate.get("context_hash")
        candidate_type = candidate.get("type")

        for pattern in self.learning_patterns.values():
            existing_hash = hash(str(sorted(pattern.context.items())))
            if (
                existing_hash == candidate_hash
                and pattern.pattern_type == candidate_type
            ):
                return True

        return False

    async def _evaluate_pattern_importance(self):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é‡è¦åº¦è©•ä¾¡"""
        for pattern in self.learning_patterns.values():
            # é‡è¦åº¦ã®å†è¨ˆç®—
            pattern._update_importance_score()

            # è³¢è€…ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å–å¾—
            sage_feedback = await self._get_sage_feedback_for_pattern(pattern)

            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ãèª¿æ•´
            if sage_feedback:
                pattern.importance_score = (
                    0.7 * pattern.importance_score
                    + 0.3 * sage_feedback.get("importance", 0.5)
                )

    async def _get_sage_feedback_for_pattern(
        self, pattern: EnhancedLearningPattern
    ) -> Optional[Dict[str, Any]]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾ã™ã‚‹è³¢è€…ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å–å¾—"""
        # å®Ÿè£…ç°¡ç•¥åŒ–ï¼šãƒ¢ãƒƒã‚¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        import random

        if secrets.token_hex(16) < 0.3:  # 30%ã®ç¢ºç‡ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
            return {
                "importance": random.uniform(0.3, 0.9),
                "confidence": random.uniform(0.6, 0.95),
                "relevance": random.uniform(0.5, 1.0),
            }
        return None

    def _cluster_similar_patterns(self):
        """é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        if len(self.learning_patterns) < 3:
            return

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        type_clusters = defaultdict(list)

        for pattern_id, pattern in self.learning_patterns.items():
            # åŸºæœ¬çš„ãªã‚¿ã‚¤ãƒ—ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            base_type = pattern.pattern_type.split("_")[0]
            type_clusters[base_type].append(pattern_id)

        # é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®è©³ç´°ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        for cluster_type, pattern_ids in type_clusters.items():
            if len(pattern_ids) > 1:
                detailed_clusters = self._detailed_similarity_clustering(pattern_ids)
                self.pattern_clusters[cluster_type] = detailed_clusters

    def _detailed_similarity_clustering(
        self, pattern_ids: List[str]
    ) -> List[List[str]]:
        """è©³ç´°é¡ä¼¼åº¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        clusters = []
        remaining_patterns = pattern_ids.copy()

        while remaining_patterns:
            current_cluster = [remaining_patterns.pop(0)]
            current_pattern = self.learning_patterns[current_cluster[0]]

            # é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŒã˜ã‚¯ãƒ©ã‚¹ã‚¿ã«è¿½åŠ 
            for pattern_id in remaining_patterns.copy():
                pattern = self.learning_patterns[pattern_id]
                similarity = current_pattern._calculate_context_similarity(
                    pattern.context
                )

                if similarity > 0.7:  # 70%ä»¥ä¸Šã®é¡ä¼¼åº¦
                    current_cluster.append(pattern_id)
                    remaining_patterns.remove(pattern_id)

            clusters.append(current_cluster)

        return clusters

    def _prune_low_importance_patterns(self):
        """ä½é‡è¦åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‰Šé™¤"""
        threshold = self.config["importance_threshold"]

        patterns_to_remove = []
        for pattern_id, pattern in self.learning_patterns.items():
            if (
                pattern.importance_score < threshold
                and pattern.usage_count < 5
                and (datetime.now() - pattern.last_updated).days > 7
            ):
                patterns_to_remove.append(pattern_id)

        for pattern_id in patterns_to_remove:
            del self.learning_patterns[pattern_id]
            logger.info(f"Pruned low-importance pattern: {pattern_id}")

    def _collect_current_performance(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åé›†"""
        active_patterns = len(
            [
                p
                for p in self.learning_patterns.values()
                if p.importance_score >= self.config["importance_threshold"]
            ]
        )

        return {
            "total_patterns": len(self.learning_patterns),
            "active_patterns": active_patterns,
            "average_success_rate": (
                statistics.mean(
                    [p.success_rate for p in self.learning_patterns.values()]
                )
                if self.learning_patterns
                else 0.0
            ),
            "average_importance": (
                statistics.mean(
                    [p.importance_score for p in self.learning_patterns.values()]
                )
                if self.learning_patterns
                else 0.0
            ),
            "learning_velocity": (
                len(self.learning_patterns)
                / max(
                    1,
                    (
                        datetime.now()
                        - min(p.created_at for p in self.learning_patterns.values())
                    ).total_seconds()
                    / 3600,
                )
                if self.learning_patterns
                else 0.0
            ),
        }

    def _identify_optimization_opportunities(
        self, performance: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æœ€é©åŒ–æ©Ÿä¼šã®ç‰¹å®š"""
        opportunities = []

        # æˆåŠŸç‡ãŒä½ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ”¹å–„
        if performance["average_success_rate"] < 0.7:
            opportunities.append(
                {
                    "type": "success_rate_improvement",
                    "target": "low_performing_patterns",
                    "action": "increase_learning_rate",
                }
            )

        # å­¦ç¿’é€Ÿåº¦ã®æ”¹å–„
        if performance["learning_velocity"] < 0.1:
            opportunities.append(
                {
                    "type": "learning_velocity_improvement",
                    "target": "pattern_discovery",
                    "action": "enhance_data_collection",
                }
            )

        # é‡è¦åº¦åˆ†å¸ƒã®æœ€é©åŒ–
        if performance["average_importance"] < 0.5:
            opportunities.append(
                {
                    "type": "importance_optimization",
                    "target": "pattern_relevance",
                    "action": "refine_importance_scoring",
                }
            )

        return opportunities

    async def _apply_real_time_optimization(self, opportunity: Dict[str, Any]):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–ã®é©ç”¨"""
        action = opportunity.get("action")

        if action == "increase_learning_rate":
            # ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’ç‡å¢—åŠ 
            for pattern in self.learning_patterns.values():
                if pattern.success_rate < 0.7:
                    pattern.adaptation_rate = min(0.3, pattern.adaptation_rate * 1.2)

        elif action == "enhance_data_collection":
            # ãƒ‡ãƒ¼ã‚¿åé›†é »åº¦ã®å¢—åŠ ï¼ˆå®Ÿè£…ç°¡ç•¥åŒ–ï¼‰
            pass

        elif action == "refine_importance_scoring":
            # é‡è¦åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã®èª¿æ•´
            for pattern in self.learning_patterns.values():
                pattern._update_importance_score()

    def _analyze_learning_process(self) -> Dict[str, Any]:
        """å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹åˆ†æ"""
        if not self.performance_history:
            return {}

        recent_history = list(self.performance_history)[-20:]

        return {
            "performance_trend": self._calculate_trend(
                [h["accuracy"] for h in recent_history]
            ),
            "learning_stability": (
                statistics.stdev([h["accuracy"] for h in recent_history])
                if len(recent_history) > 1
                else 0.0
            ),
            "pattern_growth_rate": (
                len(recent_history[-1]["metrics"]) / max(1, len(recent_history))
                if recent_history
                else 0.0
            ),
            "adaptation_effectiveness": self._calculate_adaptation_effectiveness(),
        }

    def _calculate_adaptation_effectiveness(self) -> float:
        """é©å¿œåŠ¹æœæ€§è¨ˆç®—"""
        if not self.learning_patterns:
            return 0.0

        # æœ€è¿‘æ›´æ–°ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ”¹å–„åº¦
        recent_patterns = [
            p
            for p in self.learning_patterns.values()
            if (datetime.now() - p.last_updated).hours < 24
        ]

        if not recent_patterns:
            return 0.5

        improvement_scores = []
        for pattern in recent_patterns:
            if pattern.trend_direction == "improving":
                improvement_scores.append(1.0)
            elif pattern.trend_direction == "stable":
                improvement_scores.append(0.5)
            else:
                improvement_scores.append(0.0)

        return statistics.mean(improvement_scores)

    def _discover_optimal_parameters(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç™ºè¦‹"""
        current_performance = analysis.get("performance_trend", 0.0)
        stability = analysis.get("learning_stability", 1.0)

        optimal_params = {}

        # å­¦ç¿’ç‡ã®æœ€é©åŒ–
        if current_performance > 0.05 and stability < 0.1:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šä¸”ã¤å®‰å®š
            optimal_params["adaptation_learning_rate"] = min(
                0.2, self.config["adaptation_learning_rate"] * 1.1
            )
        elif current_performance < -0.05 or stability > 0.2:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä½ä¸‹ã¾ãŸã¯ä¸å®‰å®š
            optimal_params["adaptation_learning_rate"] = max(
                0.05, self.config["adaptation_learning_rate"] * 0.9
            )

        # é‡è¦åº¦é–¾å€¤ã®æœ€é©åŒ–
        active_ratio = len(
            [
                p
                for p in self.learning_patterns.values()
                if p.importance_score >= self.config["importance_threshold"]
            ]
        ) / max(1, len(self.learning_patterns))

        if active_ratio < 0.3:  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå°‘ãªã™ãã‚‹
            optimal_params["importance_threshold"] = max(
                0.1, self.config["importance_threshold"] - 0.05
            )
        elif active_ratio > 0.8:  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¤šã™ãã‚‹
            optimal_params["importance_threshold"] = min(
                0.8, self.config["importance_threshold"] + 0.05
            )

        return optimal_params

    def _validate_parameter_improvement(self, optimal_params: Dict[str, Any]) -> bool:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ”¹å–„ã®æ¤œè¨¼"""
        # ç°¡å˜ãªæ¤œè¨¼ï¼šå¤§ããªå¤‰æ›´ã§ãªã‘ã‚Œã°é©ç”¨
        for key, new_value in optimal_params.items():
            current_value = self.config.get(key, 0.5)
            change_ratio = abs(new_value - current_value) / max(current_value, 0.1)

            if change_ratio > 0.5:  # 50%ä»¥ä¸Šã®å¤‰æ›´ã¯æ…é‡ã«
                return False

        return True

    def _apply_meta_learning_insights(self, optimal_params: Dict[str, Any]):
        """ãƒ¡ã‚¿å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®é©ç”¨"""
        for key, value in optimal_params.items():
            if key in self.config:
                old_value = self.config[key]
                self.config[key] = value
                logger.info(f"Meta-learning update: {key} {old_value} -> {value}")

    async def _record_meta_learning_session(
        self, analysis: Dict[str, Any], optimal_params: Dict[str, Any]
    ):
        """ãƒ¡ã‚¿å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨˜éŒ²"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()

        try:
            cursor.execute(
                """
                INSERT INTO meta_learning_history
                (timestamp, learning_rate, performance_score, adaptation_efficiency,
                 active_patterns, meta_parameters)
                VALUES (?, ?, ?, ?, ?, ?)
            """,
                (
                    datetime.now(),
                    self.config.get("adaptation_learning_rate", 0.1),
                    analysis.get("performance_trend", 0.0),
                    analysis.get("adaptation_effectiveness", 0.5),
                    len(self.learning_patterns),
                    json.dumps(optimal_params),
                ),
            )

            conn.commit()
        finally:
            conn.close()

    def _predict_future_performance(self) -> Dict[str, Any]:
        """å°†æ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬"""
        if len(self.performance_history) < 5:
            return {
                "predicted_accuracy": 0.5,
                "trend": 0.0,
                "confidence": 0.3,
                "prediction_horizon": 5,
            }

        recent_accuracies = [
            h["accuracy"] for h in list(self.performance_history)[-10:]
        ]
        trend = self._calculate_trend(recent_accuracies)

        # ç·šå½¢äºˆæ¸¬ï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
        current_accuracy = recent_accuracies[-1]
        predicted_accuracy = max(
            0.0, min(1.0, current_accuracy + trend * 5)
        )  # 5æœŸé–“å…ˆäºˆæ¸¬

        # äºˆæ¸¬ä¿¡é ¼åº¦ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ã®å®‰å®šæ€§ã«åŸºã¥ãï¼‰
        accuracy_variance = (
            statistics.variance(recent_accuracies)
            if len(recent_accuracies) > 1
            else 0.5
        )
        confidence = max(0.1, 1.0 - accuracy_variance)

        return {
            "predicted_accuracy": predicted_accuracy,
            "trend": trend,
            "confidence": confidence,
            "prediction_horizon": 5,
        }

    def _detect_potential_issues(
        self, predictions: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æ½œåœ¨çš„å•é¡Œã®æ¤œå‡º"""
        issues = []

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä½ä¸‹äºˆæ¸¬
        if predictions["predicted_accuracy"] < 0.6:
            issues.append(
                {
                    "type": "performance_degradation",
                    "severity": (
                        "high" if predictions["predicted_accuracy"] < 0.4 else "medium"
                    ),
                    "predicted_impact": predictions["predicted_accuracy"],
                    "confidence": predictions["confidence"],
                }
            )

        # å­¦ç¿’åœæ»ã®æ¤œå‡º
        if abs(predictions["trend"]) < 0.01 and predictions["confidence"] > 0.8:
            issues.append(
                {
                    "type": "learning_stagnation",
                    "severity": "medium",
                    "trend": predictions["trend"],
                    "confidence": predictions["confidence"],
                }
            )

        return issues

    async def _apply_preventive_measures(self, issue: Dict[str, Any]):
        """äºˆé˜²çš„å¯¾ç­–ã®é©ç”¨"""
        issue_type = issue["type"]

        if issue_type == "performance_degradation":
            # å­¦ç¿’ç‡ã®å‹•çš„èª¿æ•´
            for pattern in self.learning_patterns.values():
                if pattern.success_rate < 0.7:
                    pattern.adaptation_rate = min(0.3, pattern.adaptation_rate * 1.3)

            logger.warning(f"Applied preventive measures for {issue_type}")

        elif issue_type == "learning_stagnation":
            # æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹ã®ä¿ƒé€²
            self.config["importance_threshold"] = max(
                0.1, self.config["importance_threshold"] - 0.1
            )

            logger.info(f"Applied preventive measures for {issue_type}")

    def _track_prediction_accuracy(self):
        """äºˆæ¸¬ç²¾åº¦ã®è¿½è·¡"""
        # å®Ÿè£…ç°¡ç•¥åŒ–ï¼šåŸºæœ¬çš„ãªç²¾åº¦è¿½è·¡ã®ã¿
        if hasattr(self, "_previous_predictions"):
            # å‰å›ã®äºˆæ¸¬ã¨å®Ÿéš›ã®çµæœã‚’æ¯”è¼ƒ
            pass

    def _measure_system_performance(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š"""
        return {
            "patterns_per_minute": (
                len(self.learning_patterns)
                / max(
                    1,
                    (
                        datetime.now()
                        - min(p.created_at for p in self.learning_patterns.values())
                    ).total_seconds()
                    / 60,
                )
                if self.learning_patterns
                else 0.0
            ),
            "average_adaptation_rate": (
                statistics.mean(
                    [p.adaptation_rate for p in self.learning_patterns.values()]
                )
                if self.learning_patterns
                else 0.1
            ),
            "success_rate_variance": (
                statistics.variance(
                    [p.success_rate for p in self.learning_patterns.values()]
                )
                if len(self.learning_patterns) > 1
                else 0.0
            ),
            "system_efficiency": len(self.learning_patterns)
            / max(1, len(self.optimization_queue)),
        }

    def _update_learning_metrics(self, system_metrics: Dict[str, Any]):
        """å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°"""
        self.metrics.total_patterns = len(self.learning_patterns)
        self.metrics.active_patterns = len(
            [
                p
                for p in self.learning_patterns.values()
                if p.importance_score >= self.config["importance_threshold"]
            ]
        )
        self.metrics.learning_velocity = system_metrics.get("patterns_per_minute", 0.0)
        self.metrics.adaptation_efficiency = self._calculate_adaptation_effectiveness()

    def _detect_performance_alerts(self, metrics: Dict[str, Any]) -> List[str]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆæ¤œå‡º"""
        alerts = []

        if metrics.get("patterns_per_minute", 0) < 0.01:
            alerts.append("Low learning velocity detected")

        if metrics.get("success_rate_variance", 0) > 0.3:
            alerts.append("High performance variance detected")

        if metrics.get("system_efficiency", 1) < 0.5:
            alerts.append("Low system efficiency detected")

        return alerts

    async def _handle_performance_alerts(self, alerts: List[str]):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆå‡¦ç†"""
        for alert in alerts:
            logger.warning(f"Performance Alert: {alert}")

            # Four Sagesã«å ±å‘Š
            if self.four_sages:
                try:
                    await self.four_sages.report_to_claude_elder(
                        sage_type="meta_learning_system",
                        report_type="performance_alert",
                        content={
                            "alert": alert,
                            "timestamp": datetime.now().isoformat(),
                        },
                    )
                except Exception as e:
                    logger.error(f"Failed to report alert to Claude Elder: {e}")

    def get_enhanced_learning_report(self) -> Dict[str, Any]:
        """å¼·åŒ–ç‰ˆå­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        return {
            "timestamp": datetime.now().isoformat(),
            "system_config": self.config.copy(),
            "learning_metrics": {
                "total_patterns": self.metrics.total_patterns,
                "active_patterns": self.metrics.active_patterns,
                "prediction_accuracy": self.metrics.calculate_accuracy(),
                "learning_velocity": self.metrics.learning_velocity,
                "adaptation_efficiency": self.metrics.adaptation_efficiency,
            },
            "pattern_analytics": {
                "pattern_types": list(
                    set(p.pattern_type for p in self.learning_patterns.values())
                ),
                "success_rate_distribution": self._calculate_success_rate_distribution(),
                "importance_distribution": self._calculate_importance_distribution(),
                "trend_analysis": self._analyze_pattern_trends(),
            },
            "meta_learning_insights": {
                "optimal_learning_rate": self._get_optimal_learning_rate(),
                "performance_stability": self._calculate_performance_stability(),
                "adaptation_success_rate": self.meta_learning_state[
                    "adaptation_success_rate"
                ],
            },
            "predictive_analysis": {
                "future_performance": self._predict_future_performance(),
                "potential_issues": len(
                    self._detect_potential_issues(self._predict_future_performance())
                ),
                "system_health": self._assess_system_health(),
            },
            "recommendations": self._generate_improvement_recommendations(),
        }

    def _calculate_success_rate_distribution(self) -> Dict[str, int]:
        """æˆåŠŸç‡åˆ†å¸ƒè¨ˆç®—"""
        distribution = {
            "0.0-0.2": 0,
            "0.2-0.4": 0,
            "0.4-0.6": 0,
            "0.6-0.8": 0,
            "0.8-1.0": 0,
        }

        for pattern in self.learning_patterns.values():
            rate = pattern.success_rate
            if rate < 0.2:
                distribution["0.0-0.2"] += 1
            elif rate < 0.4:
                distribution["0.2-0.4"] += 1
            elif rate < 0.6:
                distribution["0.4-0.6"] += 1
            elif rate < 0.8:
                distribution["0.6-0.8"] += 1
            else:
                distribution["0.8-1.0"] += 1

        return distribution

    def _calculate_importance_distribution(self) -> Dict[str, int]:
        """é‡è¦åº¦åˆ†å¸ƒè¨ˆç®—"""
        distribution = {"low": 0, "medium": 0, "high": 0}

        for pattern in self.learning_patterns.values():
            importance = pattern.importance_score
            if importance < 0.3:
                distribution["low"] += 1
            elif importance < 0.7:
                distribution["medium"] += 1
            else:
                distribution["high"] += 1

        return distribution

    def _analyze_pattern_trends(self) -> Dict[str, int]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        trends = {"improving": 0, "stable": 0, "declining": 0}

        for pattern in self.learning_patterns.values():
            trends[pattern.trend_direction] += 1

        return trends

    def _get_optimal_learning_rate(self) -> float:
        """æœ€é©å­¦ç¿’ç‡å–å¾—"""
        if self.learning_patterns:
            # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¹³å‡å­¦ç¿’ç‡
            high_performers = [
                p for p in self.learning_patterns.values() if p.success_rate > 0.8
            ]
            if high_performers:
                return statistics.mean([p.adaptation_rate for p in high_performers])

        return self.config["adaptation_learning_rate"]

    def _calculate_performance_stability(self) -> float:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å®‰å®šæ€§è¨ˆç®—"""
        if len(self.performance_history) < 5:
            return 0.5

        recent_accuracies = [
            h["accuracy"] for h in list(self.performance_history)[-10:]
        ]
        variance = (
            statistics.variance(recent_accuracies)
            if len(recent_accuracies) > 1
            else 0.0
        )

        # å®‰å®šæ€§ã¯åˆ†æ•£ã®é€†æ•°ï¼ˆæ­£è¦åŒ–ï¼‰
        return max(0.0, min(1.0, 1.0 - variance))

    def _assess_system_health(self) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ å¥åº·åº¦è©•ä¾¡"""
        accuracy = self.metrics.calculate_accuracy()
        stability = self._calculate_performance_stability()
        efficiency = self.metrics.adaptation_efficiency

        health_score = (accuracy + stability + efficiency) / 3

        if health_score >= 0.8:
            return "excellent"
        elif health_score >= 0.6:
            return "good"
        elif health_score >= 0.4:
            return "fair"
        else:
            return "poor"

    def _generate_improvement_recommendations(self) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        accuracy = self.metrics.calculate_accuracy()
        if accuracy < 0.7:
            recommendations.append(
                "Increase adaptation rates for low-performing patterns"
            )

        if self.metrics.learning_velocity < 0.1:
            recommendations.append("Enhance pattern discovery mechanisms")

        if self._calculate_performance_stability() < 0.6:
            recommendations.append("Implement stability improvement measures")

        active_ratio = self.metrics.active_patterns / max(
            1, self.metrics.total_patterns
        )
        if active_ratio < 0.3:
            recommendations.append(
                "Lower importance threshold to activate more patterns"
            )
        elif active_ratio > 0.8:
            recommendations.append(
                "Raise importance threshold to focus on high-value patterns"
            )

        return recommendations


# ãƒ‡ãƒ¢å®Ÿè¡Œ
if __name__ == "__main__":

    async def demo():
        print("ğŸš€ Enhanced Autonomous Learning System Demo")
        print("=" * 50)

        learning_system = EnhancedAutonomousLearningSystem()

        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        print("1. Initializing enhanced learning system...")
        await learning_system.four_sages.initialize()

        # å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        print("2. Generating enhanced learning report...")
        report = learning_system.get_enhanced_learning_report()

        print("\nğŸ“Š Enhanced Learning Report:")
        print(f"  ğŸ§  Total Patterns: {report['learning_metrics']['total_patterns']}")
        print(
            f"  âš¡ Learning Velocity: {report['learning_metrics']['learning_velocity']:.3f}"
        )
        print(
            f"  ğŸ¯ Adaptation Efficiency: {report['learning_metrics']['adaptation_efficiency']:.3f}"
        )
        print(f"  ğŸ¥ System Health: {report['predictive_analysis']['system_health']}")

        print("\nğŸ”® Predictive Analysis:")
        future_perf = report["predictive_analysis"]["future_performance"]
        print(f"  ğŸ“ˆ Predicted Accuracy: {future_perf['predicted_accuracy']:.3f}")
        print(f"  ğŸ“Š Trend: {future_perf['trend']:.3f}")
        print(f"  ğŸ¯ Confidence: {future_perf['confidence']:.3f}")

        print("\nğŸ’¡ Recommendations:")
        for i, rec in enumerate(report["recommendations"], 1):
            print(f"  {i}. {rec}")

        print("\nâœ¨ Enhanced Autonomous Learning Capabilities:")
        print("  âœ… Adaptive learning rate adjustment")
        print("  âœ… Dynamic pattern importance evaluation")
        print("  âœ… Real-time performance optimization")
        print("  âœ… Predictive issue detection")
        print("  âœ… Meta-learning for self-improvement")

        await learning_system.four_sages.cleanup()
        print("\nğŸ¯ Enhanced Autonomous Learning Demo - COMPLETED")

    asyncio.run(demo())