#!/usr/bin/env python3
"""
ETLãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…
ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»å¤‰æ›ãƒ»ãƒ­ãƒ¼ãƒ‰å‡¦ç†

è¨­è¨ˆ: RAGã‚¨ãƒ«ãƒ€ãƒ¼ Ã— ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼
æ‰¿èª: ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºè©•è­°ä¼š
å®Ÿè£…æ—¥: 2025å¹´7æœˆ10æ—¥
"""

import asyncio
import hashlib
import json
import logging
import sqlite3
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, AsyncIterator, Dict, List, Optional

import aiofiles
import numpy as np
import pandas as pd

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DataSource(Enum):
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç¨®åˆ¥"""

    LOG_FILES = "log_files"
    DATABASE = "database"
    API_ENDPOINTS = "api_endpoints"
    KNOWLEDGE_BASE = "knowledge_base"
    METRICS_STREAM = "metrics_stream"


@dataclass
class ETLJob:
    """ETLã‚¸ãƒ§ãƒ–å®šç¾©"""

    job_id: str
    source: DataSource
    target: str
    transform_rules: Dict[str, Any]
    schedule: Optional[str] = None
    status: str = "pending"
    created_at: datetime = None

    def __post_init__(self):
        """__post_init__ç‰¹æ®Šãƒ¡ã‚½ãƒƒãƒ‰"""
        if self.created_at is None:
            self.created_at = datetime.now()


class DataExtractor:
    """ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, project_root:
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
    Path):
        self.project_root = Path(project_root)
        self.cache_dir = self.project_root / ".etl_cache"
        self.cache_dir.mkdir(exist_ok=True)

    async def extract_from_logs(
        self, log_pattern: str = "*.log"
    ) -> AsyncIterator[Dict]:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        logs_dir = self.project_root / "logs"

        async for log_file in self._async_glob(logs_dir, log_pattern):
            async with aiofiles.open(log_file, "r", encoding="utf-8") as f:
                content = await f.read()

            for line in content.splitlines():
                if not line.strip():
                    continue

                parsed = self._parse_log_line(line)
                if parsed:
                    yield {
                        "source": str(log_file),
                        "timestamp": parsed.get("timestamp", datetime.now()),
                        "data": parsed,
                    }

    async def extract_from_database(self, query: str, db_path: Path) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        try:
            conn = sqlite3.connect(str(db_path))
            df = pd.read_sql_query(query, conn)
            conn.close()

            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰{len(df)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º")
            return df

        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()

    async def extract_from_api(self, endpoint: str, params: Dict = None) -> Dict:
        """APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        # å®Ÿéš›ã®APIå®Ÿè£…ã§ã¯aiohttpç­‰ã‚’ä½¿ç”¨
        # ã“ã“ã§ã¯ãƒ¢ãƒƒã‚¯å®Ÿè£…
        mock_data = {
            "endpoint": endpoint,
            "timestamp": datetime.now().isoformat(),
            "data": {
                "metrics": {
                    "requests_per_second": 45.2,
                    "error_rate": 0.02,
                    "average_response_time": 120,
                }
            },
        }
        return mock_data

    async def extract_from_knowledge_base(self, pattern: str = "*.md") -> List[Dict]:
        """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        kb_dir = self.project_root / "knowledge_base"
        documents = []

        async for doc_file in self._async_glob(kb_dir, pattern):
            async with aiofiles.open(doc_file, "r", encoding="utf-8") as f:
                content = await f.read()

            metadata = self._extract_document_metadata(content)
            documents.append(
                {
                    "file": str(doc_file),
                    "metadata": metadata,
                    "content_hash": hashlib.md5(content.encode()).hexdigest(),
                    "size": len(content),
                    "extracted_at": datetime.now(),
                }
            )

        return documents

    async def _async_glob(self, directory: Path, pattern: str) -> AsyncIterator[Path]:
        """éåŒæœŸãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢"""
        for file_path in directory.glob(pattern):
            yield file_path

    def _parse_log_line(self, line: str) -> Optional[Dict]:
        """ãƒ­ã‚°è¡Œã®ãƒ‘ãƒ¼ã‚¹"""
        # ç°¡æ˜“ãƒ‘ãƒ¼ã‚µãƒ¼å®Ÿè£…
        import re

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨ãƒ¬ãƒ™ãƒ«ã‚’æŠ½å‡º
        pattern = r"\[(\w+)\]\s+(.+)"
        match = re.match(pattern, line)

        if match:
            return {"level": match.group(1), "message": match.group(2), "raw": line}
        return None

    def _extract_document_metadata(self, content: str) -> Dict:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        metadata = {"title": None, "author": None, "date": None, "tags": []}

        lines = content.split("\n")
        for line in lines[:10]:  # æœ€åˆã®10è¡Œã‚’ãƒã‚§ãƒƒã‚¯
            if line.startswith("# "):
                metadata["title"] = line[2:].strip()
            elif "ä½œæˆè€…:" in line or "Author:" in line:
                metadata["author"] = line.split(":")[1].strip()
            elif "æ—¥ä»˜:" in line or "Date:" in line:
                metadata["date"] = line.split(":")[1].strip()

        return metadata


class DataTransformer:
    """ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.transformations = {
            "normalize": self._normalize,
            "aggregate": self._aggregate,
            "filter": self._filter,
            "enrich": self._enrich,
            "pivot": self._pivot,
        }

    async def transform(
        self, data: pd.DataFrame, rules: Dict[str, Any]
    ) -> pd.DataFrame:
        """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å¤‰æ›"""
        result = data.copy()

        for transform_type, params in rules.items():
            if transform_type in self.transformations:
                transform_func = self.transformations[transform_type]
                result = await transform_func(result, params)
                logger.info(f"âœ… {transform_type}å¤‰æ›ã‚’é©ç”¨")

        return result

    async def _normalize(self, df: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–"""
        columns = params.get("columns", [])
        method = params.get("method", "minmax")

        for col in columns:
            if col in df.columns and df[col].dtype in ["float64", "int64"]:
                if method == "minmax":
                    df[col] = (df[col] - df[col].min()) / (
                        df[col].max() - df[col].min()
                    )
                elif method == "zscore":
                    df[col] = (df[col] - df[col].mean()) / df[col].std()

        return df

    async def _aggregate(self, df: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿é›†ç´„"""
        group_by = params.get("group_by", [])
        agg_funcs = params.get("functions", {"count": "size"})

        if group_by:
            return df.groupby(group_by).agg(agg_funcs).reset_index()
        return df

    async def _filter(self, df: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        conditions = params.get("conditions", {})

        for column, condition in conditions.items():
            if column in df.columns:
                operator = condition.get("operator", "==")
                value = condition.get("value")

                if operator == "==":
                    df = df[df[column] == value]
                elif operator == ">":
                    df = df[df[column] > value]
                elif operator == "<":
                    df = df[df[column] < value]
                elif operator == "in":
                    df = df[df[column].isin(value)]

        return df

    async def _enrich(self, df: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆ"""
        # æ´¾ç”Ÿãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¿½åŠ 
        derived_fields = params.get("derived_fields", {})

        for field_name, expression in derived_fields.items():
            try:
                # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¿®æ­£: evalã‚’ä½¿ã‚ãªã„å®‰å…¨ãªå®Ÿè£…
                # ã‚µãƒãƒ¼ãƒˆã™ã‚‹æ¼”ç®—ã‚’åˆ¶é™
                if expression.startswith("df[") and "]" in expression:
                    # å˜ç´”ãªã‚«ãƒ©ãƒ å‚ç…§
                    col_name = expression[3:expression.index("]")].strip("'\"")
                    df[field_name] = df[col_name]
                else:
                    logger.warning(f"âš ï¸ æ´¾ç”Ÿãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰{field_name}ã®å¼ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“: {expression}")
            except Exception as e:
                logger.warning(f"âš ï¸ æ´¾ç”Ÿãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰{field_name}ã®ä½œæˆå¤±æ•—: {e}")

        return df

    async def _pivot(self, df: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """ãƒ”ãƒœãƒƒãƒˆå¤‰æ›"""
        index = params.get("index")
        columns = params.get("columns")
        values = params.get("values")

        if all([index, columns, values]):
            return df.pivot_table(index=index, columns=columns, values=values)
        return df


class DataLoader:
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, project_root:
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
    Path):
        self.project_root = Path(project_root)
        self.output_dir = self.project_root / "etl_output"
        self.output_dir.mkdir(exist_ok=True)

    async def load_to_database(self, df: pd.DataFrame, table_name: str, db_path: Path):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ãƒ­ãƒ¼ãƒ‰"""
        try:
            conn = sqlite3.connect(str(db_path))
            df.to_sql(table_name, conn, if_exists="append", index=False)
            conn.close()

            logger.info(f"âœ… {len(df)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’{table_name}ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ­ãƒ¼ãƒ‰")

        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")

    async def load_to_file(self, data: Any, filename: str, format: str = "json"):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ­ãƒ¼ãƒ‰"""
        output_path = self.output_dir / filename

        try:
            if format == "json":
                async with aiofiles.open(output_path, "w", encoding="utf-8") as f:
                    await f.write(json.dumps(data, indent=2, default=str))

            elif format == "csv" and isinstance(data, pd.DataFrame):
                data.to_csv(output_path, index=False)

            elif format == "parquet" and isinstance(data, pd.DataFrame):
                data.to_parquet(output_path, index=False)

            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚’{output_path}ã«ä¿å­˜")

        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")

    async def load_to_cache(self, data: Any, cache_key: str, ttl: int = 3600):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¸ã®ãƒ­ãƒ¼ãƒ‰"""
        cache_file = self.project_root / ".etl_cache" / f"{cache_key}.cache"

        cache_data = {"data": data, "timestamp": datetime.now().isoformat(), "ttl": ttl}

        async with aiofiles.open(cache_file, "w", encoding="utf-8") as f:
            await f.write(json.dumps(cache_data, default=str))


class ETLPipeline:
    """ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ"""

    def __init__(self, project_root:
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
    Path):
        self.project_root = Path(project_root)
        self.extractor = DataExtractor(project_root)
        self.transformer = DataTransformer()
        self.loader = DataLoader(project_root)
        self.jobs: List[ETLJob] = []

    async def create_job(self, job_config: Dict) -> ETLJob:
        """ETLã‚¸ãƒ§ãƒ–ä½œæˆ"""
        job = ETLJob(
            job_id=f"etl_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            source=DataSource(job_config["source"]),
            target=job_config["target"],
            transform_rules=job_config.get("transform_rules", {}),
            schedule=job_config.get("schedule"),
        )

        self.jobs.append(job)
        logger.info(f"ğŸš€ ETLã‚¸ãƒ§ãƒ–ä½œæˆ: {job.job_id}")

        return job

    async def execute_job(self, job: ETLJob) -> Dict[str, Any]:
        """ETLã‚¸ãƒ§ãƒ–å®Ÿè¡Œ"""
        logger.info(f"â–¶ï¸ ã‚¸ãƒ§ãƒ–å®Ÿè¡Œé–‹å§‹: {job.job_id}")
        job.status = "running"

        try:
            # Extract
            data = await self._extract_phase(job)

            # Transform
            if isinstance(data, pd.DataFrame) and job.transform_rules:
                data = await self.transformer.transform(data, job.transform_rules)

            # Load
            await self._load_phase(job, data)

            job.status = "completed"
            logger.info(f"âœ… ã‚¸ãƒ§ãƒ–å®Œäº†: {job.job_id}")

            return {
                "job_id": job.job_id,
                "status": "success",
                "records_processed": len(data) if hasattr(data, "__len__") else 1,
                "completed_at": datetime.now(),
            }

        except Exception as e:
            job.status = "failed"
            logger.error(f"âŒ ã‚¸ãƒ§ãƒ–å¤±æ•—: {job.job_id} - {e}")

            return {
                "job_id": job.job_id,
                "status": "failed",
                "error": str(e),
                "failed_at": datetime.now(),
            }

    async def _extract_phase(self, job: ETLJob) -> Any:
        """æŠ½å‡ºãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œ"""
        if job.source == DataSource.LOG_FILES:
            data = []
            async for record in self.extractor.extract_from_logs():
                data.append(record)
            return pd.DataFrame(data)

        elif job.source == DataSource.DATABASE:
            query = job.transform_rules.get("query", "SELECT * FROM commits")
            db_path = self.project_root / "elder_dashboard.db"
            return await self.extractor.extract_from_database(query, db_path)

        elif job.source == DataSource.API_ENDPOINTS:
            endpoint = job.transform_rules.get("endpoint", "/metrics")
            return await self.extractor.extract_from_api(endpoint)

        elif job.source == DataSource.KNOWLEDGE_BASE:
            return await self.extractor.extract_from_knowledge_base()

        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {job.source}")

    async def _load_phase(self, job: ETLJob, data: Any):
        """ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œ"""
        target_parts = job.target.split(":")
        target_type = target_parts[0]

        if target_type == "database":
            table_name = target_parts[1] if len(target_parts) > 1 else "etl_output"
            db_path = self.project_root / "elder_dashboard.db"
            await self.loader.load_to_database(data, table_name, db_path)

        elif target_type == "file":
            filename = (
                target_parts[1] if len(target_parts) > 1 else f"{job.job_id}.json"
            )
            format = target_parts[2] if len(target_parts) > 2 else "json"
            await self.loader.load_to_file(data, filename, format)

        elif target_type == "cache":
            cache_key = target_parts[1] if len(target_parts) > 1 else job.job_id
            await self.loader.load_to_cache(data, cache_key)

        else:
            raise ValueError(f"æœªå¯¾å¿œã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {target_type}")

    async def run_scheduled_jobs(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¸ãƒ§ãƒ–å®Ÿè¡Œ"""
        # ç°¡æ˜“ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼å®Ÿè£…
        # å®Ÿéš›ã¯APSchedulerç­‰ã‚’ä½¿ç”¨
        while True:
            for job in self.jobs:
                if job.schedule and job.status == "pending":
                    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯ãƒ­ã‚¸ãƒƒã‚¯
                    await self.execute_job(job)

            await asyncio.sleep(60)  # 1åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯


# ã‚µãƒ³ãƒ—ãƒ«ä½¿ç”¨ä¾‹
async def main():
    """ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ"""
    pipeline = ETLPipeline(Path("/home/aicompany/ai_co"))

    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    job_config = {
        "source": "log_files",
        "target": "database:etl_metrics",
        "transform_rules": {
            "filter": {
                "conditions": {
                    "level": {"operator": "in", "value": ["ERROR", "WARNING"]}
                }
            },
            "aggregate": {
                "group_by": ["level"],
                "functions": {"count": "size", "timestamp": "max"},
            },
        },
    }

    job = await pipeline.create_job(job_config)
    result = await pipeline.execute_job(job)
    print(f"ETLã‚¸ãƒ§ãƒ–çµæœ: {result}")


if __name__ == "__main__":
    asyncio.run(main())
