#!/usr/bin/env python3
"""
Feedback Loop System - ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã‚·ã‚¹ãƒ†ãƒ 
å®Ÿè¡Œçµæœã‚’å³åº§ã«å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å–ã‚Šè¾¼ã¿ã€ç¶™ç¶šçš„ãªæ”¹å–„ã‚µã‚¤ã‚¯ãƒ«ã‚’è‡ªå‹•åŒ–

4è³¢è€…ã¨ã®é€£æº:
ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…: çŸ­ã„ã‚µã‚¤ã‚¯ãƒ«ã§é«˜é »åº¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®å®Ÿè£…
ğŸ” RAGè³¢è€…: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®æ„å‘³çš„æ¤œç´¢ã¨é–¢é€£ä»˜ã‘
ğŸ“‹ ã‚¿ã‚¹ã‚¯è³¢è€…: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†ã®å„ªå…ˆé †ä½ä»˜ã‘ã¨åŠ¹ç‡åŒ–
ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—æš´èµ°ã®æ¤œçŸ¥ã¨é˜²æ­¢
"""

import sys
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import json
import logging
import math
import statistics
import threading
import time
import uuid
from collections import defaultdict, deque
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple, Union

logger = logging.getLogger(__name__)


class FeedbackLoopSystem:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """FeedbackLoopSystem åˆæœŸåŒ–"""
        self.system_id = f"feedback_loop_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.metrics_collector = MetricsCollector()
        self.evaluator = PerformanceEvaluator()
        self.data_pipeline = LearningDataPipeline()

        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç®¡ç†
        self.feedback_buffer = deque(maxlen=10000)
        self.processed_feedback = defaultdict(list)
        self.learning_history = deque(maxlen=5000)

        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†
        self.real_time_processor = RealTimeProcessor()
        self.anomaly_detector = FeedbackAnomalyDetector()

        # è¨­å®š
        self.loop_config = {
            "collection_interval": 1.0,  # ç§’
            "processing_batch_size": 50,
            "quality_threshold": 0.7,
            "anomaly_sensitivity": 0.8,
        }

        # 4è³¢è€…çµ±åˆãƒ•ãƒ©ã‚°
        self.knowledge_sage_integration = True
        self.rag_sage_integration = True
        self.task_sage_integration = True
        self.incident_sage_integration = True

        # ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        self.knowledge_base_path = PROJECT_ROOT / "knowledge_base" / "feedback_learning"
        self.knowledge_base_path.mkdir(parents=True, exist_ok=True)

        logger.info(f"FeedbackLoopSystem initialized: {self.system_id}")

    def collect_feedback(self, execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """å®Ÿè¡Œçµæœã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åé›†"""
        try:
            feedback_id = f"fb_{uuid.uuid4().hex[:8]}"

            # æ“ä½œãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            operation_metadata = {
                "operation_id": execution_result.get("operation_id"),
                "operation_type": execution_result.get("operation_type"),
                "parameters": execution_result.get("parameters", {}),
                "execution_duration": self._calculate_duration(execution_result),
            }

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º
            performance_metrics = {}
            if "metrics" in execution_result:
                metrics = execution_result["metrics"]
                for metric_name, metric_data in metrics.items():
                    if isinstance(metric_data, dict):
                        improvement = metric_data.get("improvement", 0)
                        performance_metrics[f"{metric_name}_improvement"] = improvement

            # æ”¹å–„æŒ‡æ¨™è¨ˆç®—
            improvement_indicators = self._calculate_improvement_indicators(
                performance_metrics
            )

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
            context_data = self._extract_context_data(execution_result)

            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            quality_score = self._calculate_feedback_quality(
                execution_result, performance_metrics
            )

            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ§‹ç¯‰
            feedback = {
                "feedback_id": feedback_id,
                "operation_metadata": operation_metadata,
                "performance_metrics": performance_metrics,
                "improvement_indicators": improvement_indicators,
                "context_data": context_data,
                "quality_score": quality_score,
                "collection_timestamp": datetime.now(),
                "raw_execution_result": execution_result,
            }

            # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
            self.feedback_buffer.append(feedback)

            return feedback

        except Exception as e:
            logger.error(f"Error collecting feedback: {str(e)}")
            return {"feedback_id": None, "error": str(e)}

    def process_feedback(self, feedback_data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å‡¦ç†"""
        try:
            processed_id = f"proc_{uuid.uuid4().hex[:8]}"

            # ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
            patterns = self._extract_patterns(feedback_data)

            # å®Ÿè¡Œå¯èƒ½ãªæ´å¯Ÿç”Ÿæˆ
            insights = self._generate_actionable_insights(feedback_data, patterns)

            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            confidence_score = self._calculate_processing_confidence(
                feedback_data, patterns, insights
            )

            # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
            recommended_actions = self._determine_recommended_actions(insights)

            # å­¦ç¿’å„ªå…ˆåº¦è¨­å®š
            learning_priority = self._assign_learning_priority(
                feedback_data, patterns, confidence_score
            )

            # å‡¦ç†çµæœæ§‹ç¯‰
            processing_result = {
                "processed_feedback_id": processed_id,
                "original_feedback_id": feedback_data.get("feedback_id"),
                "extracted_patterns": patterns,
                "actionable_insights": insights,
                "confidence_score": confidence_score,
                "recommended_actions": recommended_actions,
                "learning_priority": learning_priority,
                "processing_timestamp": datetime.now(),
            }

            # å‡¦ç†æ¸ˆã¿ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«è¿½åŠ 
            feedback_type = feedback_data.get("operation_metadata", {}).get(
                "operation_type", "unknown"
            )
            self.processed_feedback[feedback_type].append(processing_result)

            return processing_result

        except Exception as e:
            logger.error(f"Error processing feedback: {str(e)}")
            return {"processed_feedback_id": None, "error": str(e)}

    def evaluate_performance(
        self, metrics_history: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡"""
        try:
            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            performance_trend = self._analyze_performance_trend(metrics_history)

            # æ”¹å–„ç‡è¨ˆç®—
            improvement_rate = self._calculate_improvement_rates(metrics_history)

            # å®‰å®šæ€§è©•ä¾¡
            stability_assessment = self._assess_stability(metrics_history)

            # ç•°å¸¸æ¤œå‡º
            anomaly_detection = self._detect_performance_anomalies(metrics_history)

            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ
            benchmark_comparison = self._compare_with_benchmarks(metrics_history)

            # è©•ä¾¡ã‚¹ã‚³ã‚¢è¨ˆç®—
            evaluation_score = self._calculate_evaluation_score(
                performance_trend, improvement_rate, stability_assessment
            )

            return {
                "performance_trend": performance_trend,
                "improvement_rate": improvement_rate,
                "stability_assessment": stability_assessment,
                "anomaly_detection": anomaly_detection,
                "benchmark_comparison": benchmark_comparison,
                "evaluation_score": evaluation_score,
                "evaluation_timestamp": datetime.now(),
            }

        except Exception as e:
            logger.error(f"Error evaluating performance: {str(e)}")
            return {"evaluation_score": 0, "error": str(e)}

    def generate_learning_data(
        self, processed_feedback: Dict[str, Any]
    ) -> Dict[str, Any]:
        """å‡¦ç†æ¸ˆã¿ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        try:
            # å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ
            learning_samples = self._create_learning_samples(processed_feedback)

            # ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«æ§‹ç¯‰
            feature_vectors = self._build_feature_vectors(processed_feedback)

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆçµæœæŠ½å‡º
            target_outcomes = self._extract_target_outcomes(processed_feedback)

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
            metadata = self._build_learning_metadata(processed_feedback)

            # å“è³ªæŒ‡æ¨™è¨ˆç®—
            quality_indicators = self._calculate_learning_quality(
                learning_samples, feature_vectors
            )

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹
            patterns_discovered = self._discover_new_patterns(processed_feedback)

            learning_data = {
                "learning_samples": learning_samples,
                "feature_vectors": feature_vectors,
                "target_outcomes": target_outcomes,
                "metadata": metadata,
                "quality_indicators": quality_indicators,
                "patterns_discovered": patterns_discovered,
                "generation_timestamp": datetime.now(),
            }

            return learning_data

        except Exception as e:
            logger.error(f"Error generating learning data: {str(e)}")
            return {"learning_samples": [], "error": str(e)}

    def update_knowledge_base(self, learning_data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ï¼ˆãƒŠãƒ¬ãƒƒã‚¸è³¢è€…é€£æºï¼‰"""
        try:
            # ãƒŠãƒ¬ãƒƒã‚¸æ›´æ–°ID
            update_id = f"kb_update_{uuid.uuid4().hex[:8]}"

            # ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±åˆ
            patterns_added = self._integrate_patterns(
                learning_data.get("patterns_discovered", [])
            )

            # å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«çµ±åˆ
            samples_integrated = self._integrate_learning_samples(
                learning_data.get("learning_samples", [])
            )

            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
            knowledge_version = self._increment_knowledge_version()

            # ä¿å­˜å‡¦ç†
            save_path = self.knowledge_base_path / f"{update_id}_learning.json"
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(learning_data, f, indent=2, default=str)

            # å­¦ç¿’å±¥æ­´ã«è¿½åŠ 
            self.learning_history.append(
                {
                    "update_id": update_id,
                    "learning_data": learning_data,
                    "updated_at": datetime.now(),
                }
            )

            # ãƒŠãƒ¬ãƒƒã‚¸çµ±åˆå“è³ªè©•ä¾¡
            integration_quality = self._assess_integration_quality(
                learning_data, patterns_added, samples_integrated
            )

            # ãƒŠãƒ¬ãƒƒã‚¸çµ±åˆçµæœ
            knowledge_integration = {
                "status": "success",
                "integration_quality": integration_quality,
                "patterns_merged": patterns_added,
                "samples_added": samples_integrated,
            }

            return {
                "updated": True,
                "update_id": update_id,
                "knowledge_base_path": str(save_path),
                "patterns_added": patterns_added,
                "samples_integrated": samples_integrated,
                "knowledge_version": knowledge_version,
                "knowledge_integration": knowledge_integration,
            }

        except Exception as e:
            logger.error(f"Error updating knowledge base: {str(e)}")
            return {"updated": False, "error": str(e)}

    def detect_feedback_anomalies(
        self, feedback_stream: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç•°å¸¸ã‚’æ¤œçŸ¥ï¼ˆã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…é€£æºï¼‰"""
        try:
            anomalies = []
            severity_levels = defaultdict(int)

            # å„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ç•°å¸¸ãƒã‚§ãƒƒã‚¯
            for feedback in feedback_stream:
                anomaly_results = self._check_feedback_anomalies(feedback)
                if anomaly_results:
                    anomalies.extend(anomaly_results)
                    for anomaly in anomaly_results:
                        severity_levels[anomaly["severity"]] += 1

            # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
            recommended_actions = self._determine_anomaly_actions(anomalies)

            # ãƒ«ãƒ¼ãƒ—å¥å…¨æ€§è©•ä¾¡
            loop_health_status = self._assess_loop_health(anomalies, feedback_stream)

            return {
                "anomalies_detected": anomalies,
                "anomaly_count": len(anomalies),
                "severity_levels": dict(severity_levels),
                "recommended_actions": recommended_actions,
                "loop_health_status": loop_health_status,
                "detection_timestamp": datetime.now(),
            }

        except Exception as e:
            logger.error(f"Error detecting feedback anomalies: {str(e)}")
            return {"anomalies_detected": [], "anomaly_count": 0, "error": str(e)}

    def optimize_feedback_cycle(
        self, cycle_performance: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚µã‚¤ã‚¯ãƒ«ã‚’æœ€é©åŒ–"""
        try:
            # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
            bottleneck_analysis = self._analyze_cycle_bottlenecks(cycle_performance)

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ææ¡ˆ
            performance_improvements = self._propose_performance_improvements(
                cycle_performance, bottleneck_analysis
            )

            # èª¿æ•´æ¨å¥¨
            recommended_adjustments = self._recommend_cycle_adjustments(
                bottleneck_analysis
            )

            # æœŸå¾…ã•ã‚Œã‚‹å½±éŸ¿è©•ä¾¡
            expected_impact = self._estimate_optimization_impact(
                recommended_adjustments
            )

            # æœ€é©åŒ–é©ç”¨
            optimization_applied = self._apply_cycle_optimizations(
                recommended_adjustments
            )

            return {
                "optimization_applied": optimization_applied,
                "performance_improvements": performance_improvements,
                "bottleneck_analysis": bottleneck_analysis,
                "recommended_adjustments": recommended_adjustments,
                "expected_impact": expected_impact,
                "optimization_timestamp": datetime.now(),
            }

        except Exception as e:
            logger.error(f"Error optimizing feedback cycle: {str(e)}")
            return {"optimization_applied": False, "error": str(e)}

    def create_improvement_suggestions(
        self, analyzed_feedback: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ”¹å–„ææ¡ˆã‚’ä½œæˆ"""
        try:
            # å³åº§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            immediate_actions = self._identify_immediate_actions(analyzed_feedback)

            # ä¸­æœŸæœ€é©åŒ–
            medium_term_optimizations = self._identify_medium_term_optimizations(
                analyzed_feedback
            )

            # é•·æœŸæˆ¦ç•¥
            long_term_strategies = self._identify_long_term_strategies(
                analyzed_feedback
            )

            # å„ªå…ˆé †ä½ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            priority_ranking = self._rank_suggestions_by_priority(
                immediate_actions, medium_term_optimizations, long_term_strategies
            )

            # ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆæ¨å®š
            impact_estimations = self._estimate_suggestion_impacts(
                immediate_actions, medium_term_optimizations, long_term_strategies
            )

            return {
                "immediate_actions": immediate_actions,
                "medium_term_optimizations": medium_term_optimizations,
                "long_term_strategies": long_term_strategies,
                "priority_ranking": priority_ranking,
                "impact_estimations": impact_estimations,
                "creation_timestamp": datetime.now(),
            }

        except Exception as e:
            logger.error(f"Error creating improvement suggestions: {str(e)}")
            return {"immediate_actions": [], "error": str(e)}

    def process_real_time_events(
        self, real_time_events: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†"""
        try:
            start_time = time.time()

            # ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†
            processed_events = []
            immediate_insights = []
            alerts_generated = []

            for event in real_time_events:
                # å€‹åˆ¥ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†
                processed_event = self._process_single_event(event)
                processed_events.append(processed_event)

                # å³åº§ã®æ´å¯ŸæŠ½å‡º
                insights = self._extract_immediate_insights(event)
                immediate_insights.extend(insights)

                # ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆãƒã‚§ãƒƒã‚¯
                alerts = self._check_alert_conditions(event)
                alerts_generated.extend(alerts)

            # å­¦ç¿’ãƒˆãƒªã‚¬ãƒ¼åˆ¤å®š
            learning_triggered = self._should_trigger_learning(processed_events)

            # å‡¦ç†é…å»¶è¨ˆç®—
            processing_latency = time.time() - start_time

            return {
                "events_processed": len(processed_events),
                "immediate_insights": immediate_insights,
                "alerts_generated": alerts_generated,
                "learning_triggered": learning_triggered,
                "processing_latency": processing_latency,
                "processing_timestamp": datetime.now(),
            }

        except Exception as e:
            logger.error(f"Error processing real-time events: {str(e)}")
            return {
                "events_processed": 0,
                "processing_latency": float("inf"),
                "error": str(e),
            }

    def assess_feedback_quality(
        self, feedback_samples: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å“è³ªã‚’è©•ä¾¡"""
        try:
            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            quality_scores = []
            high_quality_count = 0
            medium_quality_count = 0
            low_quality_count = 0

            for sample in feedback_samples:
                # å€‹åˆ¥å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
                individual_score = self._calculate_individual_quality_score(sample)
                quality_scores.append(individual_score)

                # å“è³ªåˆ†é¡
                if individual_score >= 0.8:
                    high_quality_count += 1
                elif individual_score >= 0.6:
                    medium_quality_count += 1
                else:
                    low_quality_count += 1

            # å…¨ä½“å“è³ªã‚¹ã‚³ã‚¢
            overall_quality_score = (
                statistics.mean(quality_scores) if quality_scores else 0
            )

            # å“è³ªåˆ†å¸ƒ
            quality_distribution = {
                "high_quality_count": high_quality_count,
                "medium_quality_count": medium_quality_count,
                "low_quality_count": low_quality_count,
                "total_samples": len(feedback_samples),
            }

            # æ”¹å–„æ¨å¥¨
            improvement_recommendations = self._generate_quality_improvements(
                feedback_samples, quality_scores
            )

            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ææ¡ˆ
            filtering_suggestions = self._suggest_quality_filtering(
                quality_distribution
            )

            return {
                "overall_quality_score": overall_quality_score,
                "quality_distribution": quality_distribution,
                "improvement_recommendations": improvement_recommendations,
                "filtering_suggestions": filtering_suggestions,
                "assessment_timestamp": datetime.now(),
            }

        except Exception as e:
            logger.error(f"Error assessing feedback quality: {str(e)}")
            return {"overall_quality_score": 0, "error": str(e)}

    def process_with_sage_collaboration(
        self, complex_feedback: Dict[str, Any]
    ) -> Dict[str, Any]:
        """4è³¢è€…å”èª¿ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†"""
        try:
            sage_contributions = {}

            # ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            if self.knowledge_sage_integration:
                sage_contributions["knowledge_sage"] = {
                    "historical_patterns": self._get_historical_patterns(),
                    "pattern_evolution": self._analyze_pattern_evolution(),
                    "knowledge_gaps": self._identify_knowledge_gaps(),
                }

            # RAGè³¢è€…: é–¢é€£æƒ…å ±æ¤œç´¢
            if self.rag_sage_integration:
                sage_contributions["rag_sage"] = {
                    "related_feedback": self._search_related_feedback(complex_feedback),
                    "similar_cases": self._find_similar_cases(complex_feedback),
                    "contextual_insights": self._extract_contextual_insights(),
                }

            # ã‚¿ã‚¹ã‚¯è³¢è€…: å‡¦ç†å„ªå…ˆé †ä½
            if self.task_sage_integration:
                sage_contributions["task_sage"] = {
                    "processing_priority": self._determine_processing_priority(
                        complex_feedback
                    ),
                    "resource_allocation": self._optimize_resource_allocation(),
                    "scheduling_recommendations": self._recommend_processing_schedule(),
                }

            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…: ç•°å¸¸å¯¾å¿œ
            if self.incident_sage_integration:
                sage_contributions["incident_sage"] = {
                    "anomaly_analysis": self._analyze_feedback_anomalies(
                        complex_feedback
                    ),
                    "risk_assessment": self._assess_feedback_risks(complex_feedback),
                    "containment_strategy": self._develop_containment_strategy(),
                }

            # çµ±åˆåˆ†æ
            integrated_analysis = self._integrate_sage_analysis(
                sage_contributions, complex_feedback
            )

            # ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹æ´å¯Ÿ
            consensus_insights = self._form_sage_consensus(
                sage_contributions, integrated_analysis
            )

            # å”èª¿ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            coordinated_actions = self._coordinate_sage_actions(
                sage_contributions, consensus_insights
            )

            return {
                "sage_contributions": sage_contributions,
                "integrated_analysis": integrated_analysis,
                "consensus_insights": consensus_insights,
                "coordinated_actions": coordinated_actions,
                "collaboration_timestamp": datetime.now(),
            }

        except Exception as e:
            logger.error(f"Error in sage collaboration: {str(e)}")
            return {"sage_contributions": {}, "error": str(e)}

    # ===== Private Helper Methods =====

    def _calculate_duration(self, execution_result: Dict[str, Any]) -> float:
        """å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—"""
        exec_time = execution_result.get("execution_time", {})
        if isinstance(exec_time, dict) and "duration" in exec_time:
            duration = exec_time["duration"]
            if isinstance(duration, timedelta):
                return duration.total_seconds()
        return 0.0

    def _calculate_improvement_indicators(
        self, metrics: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ”¹å–„æŒ‡æ¨™ã‚’è¨ˆç®—"""
        improvements = [v for v in metrics.values() if isinstance(v, (int, float))]

        if improvements:
            overall_improvement = statistics.mean(improvements)
            stability_maintained = all(
                imp >= -0.05 for imp in improvements
            )  # 5%ä»¥ä¸‹ã®åŠ£åŒ–è¨±å®¹
        else:
            overall_improvement = 0.0
            stability_maintained = True

        return {
            "overall_improvement": overall_improvement,
            "stability_maintained": stability_maintained,
            "improvement_count": len([i for i in improvements if i > 0]),
        }

    def _extract_context_data(self, execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        context = {}

        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        if "system_state" in execution_result:
            sys_state = execution_result["system_state"]
            context["system_load"] = (
                "high" if sys_state.get("cpu_usage", 0) > 70 else "medium"
            )
            context["memory_pressure"] = sys_state.get("memory_usage", 0) > 80

        # æ™‚é–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        now = datetime.now()
        if 9 <= now.hour <= 17:
            context["time_of_day"] = "business_hours"
        # è¤‡é›‘ãªæ¡ä»¶åˆ¤å®š
        elif 18 <= now.hour <= 22:
            context["time_of_day"] = "peak_hours"
        else:
            context["time_of_day"] = "off_peak"

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        if "user_feedback" in execution_result:
            user_fb = execution_result["user_feedback"]
            context["user_satisfaction"] = user_fb.get("satisfaction_score", 5.0)

        return context

    def _calculate_feedback_quality(
        self, execution_result: Dict[str, Any], metrics: Dict[str, Any]
    ) -> float:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å“è³ªã‚’è¨ˆç®—"""
        quality_score = 0.5  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Œå…¨æ€§
        if metrics:
            quality_score += 0.2

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­˜åœ¨
        if "user_feedback" in execution_result:
            quality_score += 0.2

        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹æƒ…å ±
        if "system_state" in execution_result:
            quality_score += 0.1

        return min(quality_score, 1.0)

    def _extract_patterns(self, feedback_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º"""
        patterns = []

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ãƒ‘ã‚¿ãƒ¼ãƒ³
        perf_metrics = feedback_data.get("performance_metrics", {})
        for metric, improvement in perf_metrics.items():
            if isinstance(improvement, (int, float)) and improvement > 0.2:
                patterns.append(
                    {
                        "pattern_type": "performance_improvement",
                        "description": f"{metric} shows significant improvement",
                        "confidence": min(improvement, 1.0),
                    }
                )

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç›¸é–¢ãƒ‘ã‚¿ãƒ¼ãƒ³
        context = feedback_data.get("context_data", {})
        if context.get("time_of_day") == "peak_hours":
            patterns.append(
                {
                    "pattern_type": "context_correlation",
                    "description": "Peak hour optimization pattern detected",
                    "confidence": 0.75,
                }
            )

        return patterns

    def _generate_actionable_insights(
        self, feedback_data: Dict[str, Any], patterns: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """å®Ÿè¡Œå¯èƒ½ãªæ´å¯Ÿã‚’ç”Ÿæˆ"""
        insights = []

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®æ´å¯Ÿ
        for pattern in patterns:
            if pattern["pattern_type"] == "performance_improvement":
                insights.append(
                    {
                        "insight_type": "optimization_opportunity",
                        "description": "Similar optimization may work in other contexts",
                        "potential_impact": pattern["confidence"] * 0.3,
                    }
                )

        # å“è³ªãƒ™ãƒ¼ã‚¹ã®æ´å¯Ÿ
        quality = feedback_data.get("quality_score", 0)
        if quality > 0.8:
            insights.append(
                {
                    "insight_type": "high_quality_feedback",
                    "description": "Reliable feedback for learning",
                    "potential_impact": 0.2,
                }
            )

        return insights

    def _calculate_processing_confidence(
        self,
        feedback_data: Dict[str, Any],
        patterns: List[Dict[str, Any]],
        insights: List[Dict[str, Any]],
    ) -> float:
        """å‡¦ç†ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        base_confidence = feedback_data.get("quality_score", 0.5)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿¡é ¼åº¦ã«ã‚ˆã‚‹èª¿æ•´
        if patterns:
            pattern_confidence = statistics.mean([p["confidence"] for p in patterns])
            base_confidence = (base_confidence + pattern_confidence) / 2

        # æ´å¯Ÿæ•°ã«ã‚ˆã‚‹èª¿æ•´
        insight_bonus = min(len(insights) * 0.1, 0.3)

        return min(base_confidence + insight_bonus, 1.0)

    def _determine_recommended_actions(
        self, insights: List[Dict[str, Any]]
    ) -> List[str]:
        """æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š"""
        actions = []

        for insight in insights:
            if insight["insight_type"] == "optimization_opportunity":
                actions.append("Replicate optimization in similar contexts")
            elif insight["insight_type"] == "high_quality_feedback":
                actions.append("Prioritize this feedback for learning")

        return actions

    def _assign_learning_priority(
        self,
        feedback_data: Dict[str, Any],
        patterns: List[Dict[str, Any]],
        confidence: float,
    ) -> str:
        """å­¦ç¿’å„ªå…ˆåº¦ã‚’å‰²ã‚Šå½“ã¦"""
        if confidence > 0.8 and len(patterns) > 1:
            return "high"
        elif confidence > 0.6:
            return "medium"
        else:
            return "low"

    def _analyze_performance_trend(
        self, metrics_history: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ"""
        if len(metrics_history) < 2:
            return {"direction": "unknown", "confidence": 0.0}

        # response_timeã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        response_times = [m.get("response_time", 0) for m in metrics_history]

        if len(response_times) >= 2:
            # å˜ç´”ãªå‚¾å‘åˆ†æ
            if response_times[-1] < response_times[0]:
                direction = "improving"
            elif response_times[-1] > response_times[0]:
                direction = "degrading"
            else:
                direction = "stable"

            # ä¿¡é ¼åº¦ã¯å¤‰åŒ–ã®ä¸€è²«æ€§ã«åŸºã¥ã
            confidence = (
                0.8 if abs(response_times[-1] - response_times[0]) > 10 else 0.6
            )
        else:
            direction = "unknown"
            confidence = 0.0

        return {
            "direction": direction,
            "confidence": confidence,
            "metric_analyzed": "response_time",
        }

    def _calculate_improvement_rates(
        self, metrics_history: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """æ”¹å–„ç‡ã‚’è¨ˆç®—"""
        improvement_rates = {}

        if len(metrics_history) >= 2:
            first = metrics_history[0]
            last = metrics_history[-1]

            for metric in ["response_time", "throughput"]:
                if metric in first and metric in last:
                    first_val = first[metric]
                    last_val = last[metric]

                    if first_val > 0:
                        if not (metric == "response_time"):
                            continue  # Early return to reduce nesting
                        # Reduced nesting - original condition satisfied
                        if metric == "response_time":
                            # ä½ã„æ–¹ãŒè‰¯ã„
                            rate = (first_val - last_val) / first_val
                        else:
                            # é«˜ã„æ–¹ãŒè‰¯ã„
                            rate = (last_val - first_val) / first_val

                        improvement_rates[metric] = rate

        return improvement_rates

    def _assess_stability(
        self, metrics_history: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """å®‰å®šæ€§ã‚’è©•ä¾¡"""
        if len(metrics_history) < 3:
            return {"stability_score": 0.5, "variance": "unknown"}

        # response_timeã®åˆ†æ•£åˆ†æ
        response_times = [m.get("response_time", 0) for m in metrics_history]

        if response_times:
            variance = (
                statistics.variance(response_times) if len(response_times) > 1 else 0
            )
            mean_val = statistics.mean(response_times)

            # å¤‰å‹•ä¿‚æ•°
            cv = (variance**0.5) / mean_val if mean_val > 0 else 0

            # å®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼ˆä½ã„å¤‰å‹•ä¿‚æ•°ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
            stability_score = max(0, 1 - cv)
        else:
            stability_score = 0.5
            variance = 0

        return {
            "stability_score": stability_score,
            "variance": variance,
            "assessment": "stable" if stability_score > 0.8 else "unstable",
        }

    def _detect_performance_anomalies(
        self, metrics_history: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç•°å¸¸ã‚’æ¤œå‡º"""
        anomalies = []

        if len(metrics_history) >= 3:
            response_times = [m.get("response_time", 0) for m in metrics_history]

            if response_times:
                mean_val = statistics.mean(response_times[:-1])  # æœ€æ–°ã‚’é™¤ã
                std_val = (
                    statistics.stdev(response_times[:-1])
                    if len(response_times) > 2
                    else 0
                )

                latest = response_times[-1]

                # Z-scoreç•°å¸¸æ¤œå‡º
                if std_val > 0:
                    z_score = abs(latest - mean_val) / std_val
                    if z_score > 2:  # 2Ïƒã‚’è¶…ãˆã‚‹
                        anomalies.append(
                            {
                                "type": "performance_spike",
                                "metric": "response_time",
                                "severity": "high" if z_score > 3 else "medium",
                                "z_score": z_score,
                            }
                        )

        return anomalies

    def _compare_with_benchmarks(
        self, metrics_history: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®æ¯”è¼ƒ"""
        if not metrics_history:
            return {"comparison_available": False}

        latest = metrics_history[-1]

        # ä»®ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å€¤
        benchmarks = {
            "response_time": 200.0,
            "throughput": 1500,
            "user_satisfaction": 8.0,
        }

        comparison = {}
        for metric, benchmark in benchmarks.items():
            if metric in latest:
                current = latest[metric]
                if metric == "response_time":
                    # ä½ã„æ–¹ãŒè‰¯ã„
                    performance = "better" if current < benchmark else "worse"
                else:
                    # é«˜ã„æ–¹ãŒè‰¯ã„
                    performance = "better" if current > benchmark else "worse"

                comparison[metric] = {
                    "current": current,
                    "benchmark": benchmark,
                    "performance": performance,
                }

        return {"comparison_available": True, "comparisons": comparison}

    def _calculate_evaluation_score(
        self,
        trend: Dict[str, Any],
        improvement: Dict[str, float],
        stability: Dict[str, Any],
    ) -> float:
        """è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 50.0  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢

        # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢
        if trend["direction"] == "improving":
            score += 30
        elif trend["direction"] == "stable":
            score += 10

        # æ”¹å–„ç‡ã‚¹ã‚³ã‚¢
        if improvement:
            avg_improvement = statistics.mean(improvement.values())
            score += min(avg_improvement * 50, 20)

        # å®‰å®šæ€§ã‚¹ã‚³ã‚¢
        score += stability.get("stability_score", 0.5) * 30

        return min(score, 100)

    def _create_learning_samples(
        self, processed_feedback: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ"""
        samples = []

        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ
        patterns = processed_feedback.get("extracted_patterns", [])

        for pattern in patterns:
            sample = {
                "input_features": {
                    "pattern_type": pattern["pattern_type"],
                    "confidence": pattern["confidence"],
                },
                "expected_output": {
                    "success_prediction": pattern["confidence"],
                    "improvement_estimate": pattern["confidence"] * 0.3,
                },
                "confidence_weight": pattern["confidence"],
            }
            samples.append(sample)

        return samples

    def _build_feature_vectors(
        self, processed_feedback: Dict[str, Any]
    ) -> Dict[str, List[str]]:
        """ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ§‹ç¯‰"""
        return {
            "parameter_features": ["thread_pool_size", "cache_size", "timeout"],
            "context_features": ["system_load", "time_of_day", "user_count"],
            "performance_features": ["response_time", "throughput", "error_rate"],
        }

    def _extract_target_outcomes(
        self, processed_feedback: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆçµæœã‚’æŠ½å‡º"""
        insights = processed_feedback.get("actionable_insights", [])

        outcomes = {
            "success_indicators": [],
            "improvement_metrics": [],
            "risk_factors": [],
        }

        for insight in insights:
            if insight.get("potential_impact", 0) > 0.2:
                outcomes["success_indicators"].append(insight["insight_type"])

        return outcomes

    def _build_learning_metadata(
        self, processed_feedback: Dict[str, Any]
    ) -> Dict[str, Any]:
        """å­¦ç¿’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰"""
        return {
            "feedback_id": processed_feedback.get("processed_feedback_id"),
            "confidence_score": processed_feedback.get("confidence_score", 0.5),
            "learning_priority": processed_feedback.get("learning_priority", "medium"),
            "extraction_timestamp": datetime.now(),
        }

    def _calculate_learning_quality(
        self, samples: List[Dict[str, Any]], features: Dict[str, List[str]]
    ) -> Dict[str, float]:
        """å­¦ç¿’å“è³ªã‚’è¨ˆç®—"""
        data_completeness = 1.0 if samples else 0.0
        pattern_strength = (
            statistics.mean([s["confidence_weight"] for s in samples])
            if samples
            else 0.0
        )
        noise_level = 0.1  # ä»®ã®å€¤

        return {
            "data_completeness": data_completeness,
            "pattern_strength": pattern_strength,
            "noise_level": noise_level,
        }

    def _discover_new_patterns(
        self, processed_feedback: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹"""
        patterns = []

        # é«˜ä¿¡é ¼åº¦ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        confidence = processed_feedback.get("confidence_score", 0)
        if confidence > 0.8:
            patterns.append(
                {
                    "pattern_id": f"pat_{uuid.uuid4().hex[:8]}",
                    "type": "high_confidence_success",
                    "description": "High confidence optimization pattern",
                    "applicability": confidence,
                }
            )

        return patterns

    def _integrate_patterns(self, patterns: List[Dict[str, Any]]) -> int:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çµ±åˆ"""
        # ç°¡ç•¥åŒ–: ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’è¿”ã™
        return len(patterns)

    def _integrate_learning_samples(self, samples: List[Dict[str, Any]]) -> int:
        """å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«ã‚’çµ±åˆ"""
        # ç°¡ç•¥åŒ–: ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è¿”ã™
        return len(samples)

    def _increment_knowledge_version(self) -> str:
        """ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å¢—åŠ """
        return f"v_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    def _assess_integration_quality(
        self, learning_data: Dict[str, Any], patterns_added: int, samples_added: int
    ) -> float:
        """çµ±åˆå“è³ªã‚’è©•ä¾¡"""
        quality_indicators = learning_data.get("quality_indicators", {})
        base_quality = quality_indicators.get("data_completeness", 0.5)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã‚µãƒ³ãƒ—ãƒ«æ•°ã«ã‚ˆã‚‹èª¿æ•´
        integration_bonus = min((patterns_added + samples_added) * 0.1, 0.3)

        return min(base_quality + integration_bonus, 1.0)

    def _check_feedback_anomalies(
        self, feedback: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """å€‹åˆ¥ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ç•°å¸¸ã‚’ãƒã‚§ãƒƒã‚¯"""
        anomalies = []

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç•°å¸¸
        perf_metrics = feedback.get("performance_metrics", {})
        for metric, value in perf_metrics.items():
            if isinstance(value, (int, float)):
                if value < -0.3:  # 30%ä»¥ä¸Šã®åŠ£åŒ–
                    anomalies.append(
                        {
                            "feedback_id": feedback.get("feedback_id"),
                            "anomaly_type": "performance_degradation",
                            "severity": "high",
                            "description": f"{metric} shows severe degradation: {value}",
                        }
                    )
                elif value > 1.0:  # 100%ä»¥ä¸Šã®æ”¹å–„ï¼ˆéç¾å®Ÿçš„ï¼‰
                    anomalies.append(
                        {
                            "feedback_id": feedback.get("feedback_id"),
                            "anomaly_type": "unrealistic_improvement",
                            "severity": "medium",
                            "description": f"{metric} shows unrealistic improvement: {value}",
                        }
                    )

        return anomalies

    def _determine_anomaly_actions(self, anomalies: List[Dict[str, Any]]) -> List[str]:
        """ç•°å¸¸å¯¾å¿œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š"""
        actions = []

        high_severity_count = len([a for a in anomalies if a["severity"] == "high"])

        if high_severity_count > 0:
            actions.append("Investigate high severity anomalies immediately")

        if len(anomalies) > 5:
            actions.append("Review feedback collection process")

        if any("unrealistic" in a["anomaly_type"] for a in anomalies):
            actions.append("Validate measurement accuracy")

        return actions

    def _assess_loop_health(
        self, anomalies: List[Dict[str, Any]], feedback_stream: List[Dict[str, Any]]
    ) -> str:
        """ãƒ«ãƒ¼ãƒ—å¥å…¨æ€§ã‚’è©•ä¾¡"""
        anomaly_rate = len(anomalies) / len(feedback_stream) if feedback_stream else 0

        if anomaly_rate > 0.3:
            return "unhealthy"
        elif anomaly_rate > 0.1:
            return "degraded"
        else:
            return "healthy"

    def _analyze_cycle_bottlenecks(self, performance: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚µã‚¤ã‚¯ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’åˆ†æ"""
        current_metrics = performance.get("current_metrics", {})

        # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹å‡¦ç†ã‚’ç‰¹å®š
        processing_times = {
            "collection": current_metrics.get("collection_latency", 0),
            "processing": current_metrics.get("processing_time", 0),
            "integration": current_metrics.get("learning_integration_time", 0),
        }

        primary_bottleneck = max(processing_times.items(), key=lambda x: x[1])
        secondary_bottlenecks = sorted(
            processing_times.items(), key=lambda x: x[1], reverse=True
        )[1:]

        return {
            "primary_bottleneck": {
                "component": primary_bottleneck[0],
                "latency": primary_bottleneck[1],
            },
            "secondary_bottlenecks": [
                {"component": comp, "latency": lat}
                for comp, lat in secondary_bottlenecks
            ],
        }

    def _propose_performance_improvements(
        self, performance: Dict[str, Any], bottlenecks: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã‚’ææ¡ˆ"""
        improvements = []

        primary = bottlenecks["primary_bottleneck"]
        if primary["component"] == "processing" and primary["latency"] > 2.0:
            improvements.append(
                {
                    "component": "processing",
                    "improvement": "Implement parallel processing",
                    "expected_reduction": 0.5,
                }
            )

        return improvements

    def _recommend_cycle_adjustments(
        self, bottlenecks: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ã‚µã‚¤ã‚¯ãƒ«èª¿æ•´ã‚’æ¨å¥¨"""
        adjustments = []

        primary = bottlenecks["primary_bottleneck"]
        if primary["component"] == "processing":
            adjustments.append(
                {
                    "component": "processing_batch_size",
                    "current_value": 50,
                    "recommended_value": 100,
                    "expected_improvement": 0.3,
                }
            )

        return adjustments

    def _estimate_optimization_impact(
        self, adjustments: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """æœ€é©åŒ–å½±éŸ¿ã‚’æ¨å®š"""
        total_improvement = sum(
            adj.get("expected_improvement", 0) for adj in adjustments
        )

        return {
            "total_latency_reduction": total_improvement,
            "throughput_increase": total_improvement * 0.5,
            "resource_efficiency": total_improvement * 0.3,
        }

    def _apply_cycle_optimizations(self, adjustments: List[Dict[str, Any]]) -> bool:
        """ã‚µã‚¤ã‚¯ãƒ«æœ€é©åŒ–ã‚’é©ç”¨"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯è¨­å®šã‚’æ›´æ–°
        for adjustment in adjustments:
            component = adjustment["component"]
            new_value = adjustment["recommended_value"]
            logger.info(f"Applying optimization: {component} = {new_value}")

        return True

    def _identify_immediate_actions(
        self, analyzed_feedback: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """å³åº§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç‰¹å®š"""
        actions = []

        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã«åŸºã¥ã
        trends = analyzed_feedback.get("performance_trends", {})
        for metric, trend_data in trends.items():
            # æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰ã¾ãŸã¯å®‰å®šãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«åŸºã¥ãå³åº§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            trend = trend_data.get("trend", "")
            rate = trend_data.get("rate", 0.1)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ”¹å–„ç‡ã‚’è¨­å®š

            if trend == "improving":
                actions.append(
                    {
                        "action_type": "scaling_opportunity",
                        "description": f"Scale up {metric} optimization",
                        "implementation_effort": "low",
                        "expected_impact": max(rate, 0.1),  # æœ€å°æ”¹å–„ç‡ä¿è¨¼
                    }
                )
            elif trend == "stable" and rate >= 0:
                actions.append(
                    {
                        "action_type": "maintain_stability",
                        "description": f"Maintain stable {metric} performance",
                        "implementation_effort": "low",
                        "expected_impact": 0.05,
                    }
                )

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãå³åº§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        patterns = analyzed_feedback.get("identified_patterns", [])
        for pattern in patterns:
            if pattern.get("applicability", 0) > 0.8:
                actions.append(
                    {
                        "action_type": "pattern_optimization",
                        "description": f'Apply {pattern.get("pattern", "optimization")} pattern',
                        "implementation_effort": "medium",
                        "expected_impact": pattern.get("strength", 0.5),
                    }
                )

        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è§£æ±º
        bottlenecks = analyzed_feedback.get("bottleneck_insights", [])
        for bottleneck in bottlenecks:
            if (
                bottleneck.get("severity") == "medium"
                or bottleneck.get("severity") == "high"
            ):
                actions.append(
                    {
                        "action_type": "bottleneck_resolution",
                        "description": f'Address {bottleneck.get(
                            "component",
                            "system"
                        )} bottleneck',
                        "implementation_effort": "medium",
                        "expected_impact": bottleneck.get("impact", 0.2),
                    }
                )

        return actions

    def _identify_medium_term_optimizations(
        self, analyzed_feedback: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ä¸­æœŸæœ€é©åŒ–ã‚’ç‰¹å®š"""
        optimizations = []

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ã
        patterns = analyzed_feedback.get("identified_patterns", [])
        for pattern in patterns:
            if pattern.get("strength", 0) > 0.8:
                optimizations.append(
                    {
                        "optimization_type": "pattern_application",
                        "description": f'Apply {pattern["pattern"]} more broadly',
                        "timeline": "2-4 weeks",
                        "expected_impact": pattern.get("applicability", 0.5),
                    }
                )

        return optimizations

    def _identify_long_term_strategies(
        self, analyzed_feedback: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """é•·æœŸæˆ¦ç•¥ã‚’ç‰¹å®š"""
        strategies = []

        # æˆåŠŸè¦å› ã«åŸºã¥ã
        success_factors = analyzed_feedback.get("success_factors", [])
        if "comprehensive_monitoring" in success_factors:
            strategies.append(
                {
                    "strategy_type": "infrastructure_enhancement",
                    "description": "Implement advanced monitoring infrastructure",
                    "timeline": "3-6 months",
                    "expected_impact": 0.4,
                }
            )

        return strategies

    def _rank_suggestions_by_priority(
        self,
        immediate: List[Dict[str, Any]],
        medium: List[Dict[str, Any]],
        long_term: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """ææ¡ˆã‚’å„ªå…ˆé †ä½ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        all_suggestions = []

        # å³åº§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆé«˜å„ªå…ˆåº¦ï¼‰
        for i, action in enumerate(immediate):
            all_suggestions.append(
                {
                    "rank": i + 1,
                    "suggestion_id": f"immediate_{i}",
                    "priority_score": 0.9,
                    "category": "immediate",
                }
            )

        # ä¸­æœŸï¼ˆä¸­å„ªå…ˆåº¦ï¼‰
        for i, opt in enumerate(medium):
            all_suggestions.append(
                {
                    "rank": len(immediate) + i + 1,
                    "suggestion_id": f"medium_{i}",
                    "priority_score": 0.6,
                    "category": "medium_term",
                }
            )

        return all_suggestions

    def _estimate_suggestion_impacts(
        self,
        immediate: List[Dict[str, Any]],
        medium: List[Dict[str, Any]],
        long_term: List[Dict[str, Any]],
    ) -> Dict[str, float]:
        """ææ¡ˆå½±éŸ¿ã‚’æ¨å®š"""
        immediate_impact = sum(a.get("expected_impact", 0) for a in immediate)
        medium_impact = sum(o.get("expected_impact", 0) for o in medium)
        long_term_impact = sum(s.get("expected_impact", 0) for s in long_term)

        return {
            "immediate_total_impact": immediate_impact,
            "medium_term_total_impact": medium_impact,
            "long_term_total_impact": long_term_impact,
        }

    def _process_single_event(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """å˜ä¸€ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†"""
        return {
            "event_id": event.get("event_id"),
            "processed_at": datetime.now(),
            "processing_status": "success",
        }

    def _extract_immediate_insights(
        self, event: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """å³åº§ã®æ´å¯Ÿã‚’æŠ½å‡º"""
        insights = []

        if event.get("event_type") == "performance_metric":
            data = event.get("data", {})
            if "previous_value" in data and "value" in data:
                improvement = (data["previous_value"] - data["value"]) / data[
                    "previous_value"
                ]
                if improvement > 0.1:
                    insights.append(
                        {
                            "type": "performance_improvement",
                            "description": f'{data.get("metric_name")} improved by {improvement:.1%}',
                        }
                    )

        return insights

    def _check_alert_conditions(self, event: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯"""
        alerts = []

        if event.get("event_type") == "system_state":
            data = event.get("data", {})
            if data.get("cpu_usage", 0) > 90:
                alerts.append(
                    {
                        "type": "high_cpu_usage",
                        "severity": "warning",
                        "value": data["cpu_usage"],
                    }
                )

        return alerts

    def _should_trigger_learning(self, processed_events: List[Dict[str, Any]]) -> bool:
        """å­¦ç¿’ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã¹ãã‹ãƒã‚§ãƒƒã‚¯"""
        return len(processed_events) >= 10  # 10ã‚¤ãƒ™ãƒ³ãƒˆä»¥ä¸Šã§å­¦ç¿’ãƒˆãƒªã‚¬ãƒ¼

    def _calculate_individual_quality_score(self, sample: Dict[str, Any]) -> float:
        """å€‹åˆ¥å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        quality_factors = [
            "completeness",
            "accuracy",
            "relevance",
            "timeliness",
            "consistency",
        ]

        scores = [sample.get(factor, 0.5) for factor in quality_factors]
        return statistics.mean(scores)

    def _generate_quality_improvements(
        self, samples: List[Dict[str, Any]], scores: List[float]
    ) -> List[str]:
        """å“è³ªæ”¹å–„ã‚’ç”Ÿæˆ"""
        improvements = []

        avg_score = statistics.mean(scores) if scores else 0

        if avg_score < 0.7:
            improvements.append("Improve data collection completeness")

        if any(s < 0.5 for s in scores):
            improvements.append("Filter out low-quality feedback")

        return improvements

    def _suggest_quality_filtering(self, distribution: Dict[str, int]) -> List[str]:
        """å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ææ¡ˆ"""
        suggestions = []

        total = distribution.get("total_samples", 1)
        low_ratio = distribution.get("low_quality_count", 0) / total

        if low_ratio > 0.3:
            suggestions.append("Implement quality threshold filtering")

        return suggestions

    # 4è³¢è€…å”èª¿ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _get_historical_patterns(self) -> List[Dict[str, Any]]:
        """éå»ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—"""
        return [
            {"pattern": "optimization_success", "frequency": 0.85},
            {"pattern": "peak_hour_effectiveness", "frequency": 0.78},
        ]

    def _analyze_pattern_evolution(self) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é€²åŒ–ã‚’åˆ†æ"""
        return {"trend": "improving", "confidence": 0.82, "evolution_rate": 0.05}

    def _identify_knowledge_gaps(self) -> List[str]:
        """çŸ¥è­˜ã‚®ãƒ£ãƒƒãƒ—ã‚’ç‰¹å®š"""
        return ["Long-term impact patterns", "Cross-system correlations"]

    def _search_related_feedback(
        self, feedback: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """é–¢é€£ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ¤œç´¢"""
        return [
            {"feedback_id": "related_001", "similarity": 0.85},
            {"feedback_id": "related_002", "similarity": 0.72},
        ]

    def _find_similar_cases(self, feedback: Dict[str, Any]) -> List[Dict[str, Any]]:
        """é¡ä¼¼ã‚±ãƒ¼ã‚¹ã‚’æ¤œç´¢"""
        return [{"case_id": "case_001", "similarity": 0.80, "outcome": "success"}]

    def _extract_contextual_insights(self) -> List[Dict[str, Any]]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ´å¯Ÿã‚’æŠ½å‡º"""
        return [
            {
                "insight": "Peak hour optimizations are 40% more effective",
                "confidence": 0.85,
            }
        ]

    def _determine_processing_priority(self, feedback: Dict[str, Any]) -> str:
        """å‡¦ç†å„ªå…ˆåº¦ã‚’æ±ºå®š"""
        return "high" if feedback.get("multiple_operations") else "medium"

    def _optimize_resource_allocation(self) -> Dict[str, Any]:
        """ãƒªã‚½ãƒ¼ã‚¹å‰²ã‚Šå½“ã¦ã‚’æœ€é©åŒ–"""
        return {
            "cpu_allocation": "increased",
            "memory_allocation": "optimized",
            "parallel_processing": "enabled",
        }

    def _recommend_processing_schedule(self) -> Dict[str, Any]:
        """å‡¦ç†ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ¨å¥¨"""
        return {
            "schedule_type": "adaptive",
            "peak_hours_handling": "priority_mode",
            "batch_processing": "off_peak",
        }

    def _analyze_feedback_anomalies(self, feedback: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç•°å¸¸ã‚’åˆ†æ"""
        return {
            "anomaly_detected": feedback.get("anomalous_patterns", False),
            "severity": "medium",
            "type": "data_inconsistency",
        }

    def _assess_feedback_risks(self, feedback: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡"""
        return {
            "risk_level": "medium",
            "risk_factors": ["high_volume", "conflicting_signals"],
            "mitigation_required": True,
        }

    def _develop_containment_strategy(self) -> Dict[str, Any]:
        """å°ã˜è¾¼ã‚æˆ¦ç•¥ã‚’é–‹ç™º"""
        return {
            "strategy": "gradual_processing",
            "monitoring": "enhanced",
            "rollback_ready": True,
        }

    def _integrate_sage_analysis(
        self, contributions: Dict[str, Any], feedback: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è³¢è€…åˆ†æã‚’çµ±åˆ"""
        return {
            "confidence_level": 0.85,
            "action_priority": "high",
            "consensus_reached": True,
        }

    def _form_sage_consensus(
        self, contributions: Dict[str, Any], analysis: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """è³¢è€…ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ã‚’å½¢æˆ"""
        return [
            {
                "insight": "Process high-priority feedback immediately",
                "agreement_level": 0.9,
                "supporting_sages": ["task_sage", "incident_sage"],
            }
        ]

    def _coordinate_sage_actions(
        self, contributions: Dict[str, Any], insights: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """è³¢è€…ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’èª¿æ•´"""
        return [
            {
                "action": "Enhanced monitoring activation",
                "responsible_sage": "incident_sage",
                "timeline": "immediate",
            },
            {
                "action": "Pattern analysis acceleration",
                "responsible_sage": "knowledge_sage",
                "timeline": "1_hour",
            },
        ]


# ===== Supporting Classes =====


class MetricsCollector:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""

    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.collection_history = deque(maxlen=1000)

    def collect_metrics(self, source: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
        collected = {
            "timestamp": datetime.now(),
            "source": source,
            "metrics": self._extract_metrics(source),
        }
        self.collection_history.append(collected)
        return collected

    def _extract_metrics(self, source: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º"""
        return source.get("metrics", {})


class PerformanceEvaluator:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡"""

    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.evaluation_history = deque(maxlen=500)

    def evaluate(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡"""
        evaluation = {
            "timestamp": datetime.now(),
            "metrics": metrics,
            "score": self._calculate_score(metrics),
        }
        self.evaluation_history.append(evaluation)
        return evaluation

    def _calculate_score(self, metrics: Dict[str, Any]) -> float:
        """ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        # ç°¡ç•¥åŒ–: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°ã«åŸºã¥ã
        return min(len(metrics) * 0.2, 1.0)


class LearningDataPipeline:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.pipeline_history = deque(maxlen=200)

    def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†"""
        processed = {
            "timestamp": datetime.now(),
            "input_data": data,
            "processed_data": self._transform_data(data),
        }
        self.pipeline_history.append(processed)
        return processed

    def _transform_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›"""
        return {"features": list(data.keys()), "target": "optimization_success"}


class RealTimeProcessor:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†"""

    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.processing_queue = deque(maxlen=1000)

    def process_event(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†"""
        processed = {
            "event": event,
            "processed_at": datetime.now(),
            "processing_latency": 0.05,  # ä»®ã®å€¤
        }
        self.processing_queue.append(processed)
        return processed


class FeedbackAnomalyDetector:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç•°å¸¸æ¤œå‡º"""

    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.detection_history = deque(maxlen=500)
        self.thresholds = {
            "performance_degradation": -0.3,
            "unrealistic_improvement": 1.0,
        }

    def detect(self, feedback: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç•°å¸¸ã‚’æ¤œå‡º"""
        anomalies = []

        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å“è³ªãƒã‚§ãƒƒã‚¯
        quality = feedback.get("quality_score", 0.5)
        if quality < 0.3:
            anomalies.append(
                {
                    "type": "low_quality_feedback",
                    "severity": "medium",
                    "description": f"Quality score too low: {quality}",
                }
            )

        self.detection_history.append(
            {
                "feedback_id": feedback.get("feedback_id"),
                "anomalies": anomalies,
                "detected_at": datetime.now(),
            }
        )

        return anomalies
