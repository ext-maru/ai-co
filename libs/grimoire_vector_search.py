#!/usr/bin/env python3
"""
Magic Grimoire Vector Search Engine
é­”æ³•æ›¸ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ - OpenAI embeddings + pgvector
"""

import os
import sys
import json
import asyncio
import logging
import hashlib
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import numpy as np
from enum import Enum

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from libs.grimoire_database import GrimoireDatabase, SpellType, MagicSchool

logger = logging.getLogger(__name__)

# OpenAI APIç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯å¿…è¦ï¼‰
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("OpenAI not available - using mock embeddings")

@dataclass
class SearchQuery:
    """æ¤œç´¢ã‚¯ã‚¨ãƒª"""
    query_text: str
    filters: Optional[Dict[str, Any]] = None
    limit: int = 10
    similarity_threshold: float = 0.8
    include_eternal_only: bool = False
    magic_schools: Optional[List[MagicSchool]] = None
    spell_types: Optional[List[SpellType]] = None
    min_power_level: Optional[int] = None

@dataclass
class SearchResult:
    """æ¤œç´¢çµæœ"""
    spell_id: str
    spell_name: str
    content: str
    similarity_score: float
    spell_type: str
    magic_school: str
    tags: List[str]
    power_level: int
    is_eternal: bool
    casting_frequency: int
    created_at: datetime
    snippet: str  # æ¤œç´¢çµæœã®ãƒã‚¤ãƒ©ã‚¤ãƒˆéƒ¨åˆ†

class EmbeddingGenerator:
    """åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆå™¨"""
    
    def __init__(self, api_key: Optional[str] = None):
        """åˆæœŸåŒ–"""
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.model_name = "text-embedding-3-small"  # 1536æ¬¡å…ƒ
        self.cache = {}  # å˜ç´”ãªã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
        if OPENAI_AVAILABLE and self.api_key:
            openai.api_key = self.api_key
            self.use_real_embeddings = True
        else:
            self.use_real_embeddings = False
            logger.warning("Using mock embeddings - set OPENAI_API_KEY for real embeddings")
    
    async def generate_embedding(self, text: str) -> List[float]:
        """ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        if self.use_real_embeddings:
            try:
                response = await openai.Embedding.acreate(
                    model=self.model_name,
                    input=text
                )
                embedding = response['data'][0]['embedding']
            except Exception as e:
                logger.error(f"OpenAI embedding failed: {e}")
                embedding = self._generate_mock_embedding(text)
        else:
            embedding = self._generate_mock_embedding(text)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.cache[text_hash] = embedding
        return embedding
    
    def _generate_mock_embedding(self, text: str) -> List[float]:
        """ãƒ¢ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆãƒ»é–‹ç™ºç”¨ï¼‰"""
        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã§ä¸€è²«æ€§ã®ã‚ã‚‹åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
        text_hash = hashlib.md5(text.encode()).digest()
        np.random.seed(int.from_bytes(text_hash[:4], 'big'))
        
        # æ­£è¦åŒ–ã•ã‚ŒãŸ1536æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«
        embedding = np.random.normal(0, 1, 1536)
        embedding = embedding / np.linalg.norm(embedding)
        
        return embedding.tolist()
    
    async def generate_batch_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        if self.use_real_embeddings and len(texts) > 1:
            try:
                response = await openai.Embedding.acreate(
                    model=self.model_name,
                    input=texts
                )
                return [item['embedding'] for item in response['data']]
            except Exception as e:
                logger.error(f"Batch embedding failed: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå€‹åˆ¥ç”Ÿæˆ
        embeddings = []
        for text in texts:
            embedding = await self.generate_embedding(text)
            embeddings.append(embedding)
        
        return embeddings

class SemanticProcessor:
    """æ„å‘³å‡¦ç†å™¨"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.magic_keywords = {
            MagicSchool.KNOWLEDGE_SAGE: [
                'knowledge', 'learn', 'document', 'guide', 'tutorial', 
                'explanation', 'concept', 'theory', 'understand'
            ],
            MagicSchool.TASK_ORACLE: [
                'task', 'project', 'workflow', 'process', 'step', 
                'procedure', 'plan', 'execute', 'manage'
            ],
            MagicSchool.CRISIS_SAGE: [
                'error', 'bug', 'fix', 'debug', 'incident', 'crisis', 
                'emergency', 'troubleshoot', 'resolve'
            ],
            MagicSchool.SEARCH_MYSTIC: [
                'search', 'find', 'query', 'analyze', 'discover', 
                'explore', 'investigate', 'research'
            ]
        }
    
    def preprocess_query(self, query: str) -> str:
        """ã‚¯ã‚¨ãƒªå‰å‡¦ç†"""
        # å°æ–‡å­—åŒ–
        processed = query.lower().strip()
        
        # æ—¥æœ¬èªãƒ»è‹±èªã®æ­£è¦åŒ–ï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
        replacements = {
            'ã‚¨ãƒ©ãƒ¼': 'error',
            'ãƒã‚°': 'bug',
            'ã‚¿ã‚¹ã‚¯': 'task',
            'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ': 'project',
            'ãƒŠãƒ¬ãƒƒã‚¸': 'knowledge',
            'æ¤œç´¢': 'search',
            'claude': 'claude cli development'
        }
        
        for jp, en in replacements.items():
            processed = processed.replace(jp, en)
        
        return processed
    
    def infer_magic_school(self, text: str) -> Optional[MagicSchool]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é­”æ³•å­¦æ´¾ã‚’æ¨å®š"""
        text_lower = text.lower()
        school_scores = {}
        
        for school, keywords in self.magic_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                school_scores[school] = score
        
        if school_scores:
            return max(school_scores, key=school_scores.get)
        
        return None
    
    def extract_keywords(self, text: str) -> List[str]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        # ç°¡å˜ãªå®Ÿè£…ï¼šã‚ˆãä½¿ã‚ã‚Œã‚‹æŠ€è¡“ç”¨èª
        keywords = [
            'claude', 'tdd', 'test', 'database', 'api', 'web', 'ui',
            'postgresql', 'vector', 'search', 'embedding', 'ai',
            'worker', 'task', 'project', 'system', 'error', 'bug'
        ]
        
        found_keywords = []
        text_lower = text.lower()
        
        for keyword in keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def generate_snippet(self, content: str, query: str, max_length: int = 200) -> str:
        """æ¤œç´¢çµæœã‚¹ãƒ‹ãƒšãƒƒãƒˆç”Ÿæˆ"""
        query_words = query.lower().split()
        content_lower = content.lower()
        
        # ã‚¯ã‚¨ãƒªãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ä½ç½®ã‚’ç‰¹å®š
        best_pos = 0
        best_score = 0
        
        for i in range(0, len(content_lower) - max_length + 1, 50):
            section = content_lower[i:i + max_length]
            score = sum(1 for word in query_words if word in section)
            if score > best_score:
                best_score = score
                best_pos = i
        
        # ã‚¹ãƒ‹ãƒšãƒƒãƒˆæŠ½å‡º
        snippet = content[best_pos:best_pos + max_length]
        
        # æ–‡ã®å¢ƒç•Œã§åˆ‡ã‚‹
        if best_pos > 0:
            # æœ€åˆã®æ–‡ã®å¢ƒç•Œã‚’æ¢ã™
            first_period = snippet.find('.')
            if first_period > 50:
                snippet = '...' + snippet[first_period + 1:]
        
        if len(snippet) == max_length:
            # æœ€å¾Œã®æ–‡ã®å¢ƒç•Œã‚’æ¢ã™
            last_period = snippet.rfind('.')
            if last_period > max_length - 50:
                snippet = snippet[:last_period + 1] + '...'
            else:
                snippet += '...'
        
        return snippet.strip()

class GrimoireVectorSearch:
    """é­”æ³•æ›¸ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, database: Optional[GrimoireDatabase] = None):
        """åˆæœŸåŒ–"""
        self.database = database or GrimoireDatabase()
        self.embedding_generator = EmbeddingGenerator()
        self.semantic_processor = SemanticProcessor()
        
        logger.info("ğŸ” Grimoire Vector Search Engine initialized")
    
    async def initialize(self) -> bool:
        """åˆæœŸåŒ–"""
        if not await self.database.initialize():
            return False
        
        logger.info("âœ… Vector Search Engine ready")
        return True
    
    async def index_spell(self, spell_id: str, spell_data: Dict[str, Any]) -> bool:
        """å‘ªæ–‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–"""
        try:
            # æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™
            searchable_text = self._prepare_searchable_text(spell_data)
            
            # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
            embedding = await self.embedding_generator.generate_embedding(searchable_text)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            await self.database.create_spell(spell_data, embedding)
            
            logger.info(f"ğŸ”® Spell indexed: {spell_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to index spell {spell_id}: {e}")
            return False
    
    def _prepare_searchable_text(self, spell_data: Dict[str, Any]) -> str:
        """æ¤œç´¢å¯èƒ½ãƒ†ã‚­ã‚¹ãƒˆã®æº–å‚™"""
        components = [
            spell_data.get('spell_name', ''),
            spell_data.get('content', ''),
            ' '.join(spell_data.get('tags', [])),
            spell_data.get('spell_type', ''),
            spell_data.get('magic_school', '')
        ]
        
        return ' '.join(filter(None, components))
    
    async def search(self, query: SearchQuery) -> List[SearchResult]:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢"""
        try:
            # ã‚¯ã‚¨ãƒªå‰å‡¦ç†
            processed_query = self.semantic_processor.preprocess_query(query.query_text)
            
            # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
            query_embedding = await self.embedding_generator.generate_embedding(processed_query)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æº–å‚™
            filters = self._prepare_filters(query)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
            raw_results = await self.database.search_spells_by_vector(
                query_embedding,
                limit=query.limit * 2,  # å¾Œå‡¦ç†ã®ãŸã‚å¤šã‚ã«å–å¾—
                filters=filters
            )
            
            # çµæœå‡¦ç†
            search_results = []
            for raw_result in raw_results:
                # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆè·é›¢ã‚’é¡ä¼¼åº¦ã«å¤‰æ›ï¼‰
                similarity_score = 1.0 - raw_result['similarity_distance']
                
                if similarity_score >= query.similarity_threshold:
                    # ã‚¹ãƒ‹ãƒšãƒƒãƒˆç”Ÿæˆ
                    snippet = self.semantic_processor.generate_snippet(
                        raw_result['content'], 
                        query.query_text
                    )
                    
                    result = SearchResult(
                        spell_id=raw_result['id'],
                        spell_name=raw_result['spell_name'],
                        content=raw_result['content'],
                        similarity_score=similarity_score,
                        spell_type=raw_result['spell_type'],
                        magic_school=raw_result['magic_school'],
                        tags=raw_result['tags'],
                        power_level=raw_result['power_level'],
                        is_eternal=raw_result['is_eternal'],
                        casting_frequency=raw_result['casting_frequency'],
                        created_at=raw_result['created_at'],
                        snippet=snippet
                    )
                    
                    search_results.append(result)
                    
                    # è© å”±å›æ•°æ›´æ–°
                    await self.database.update_casting_frequency(raw_result['id'])
            
            # é¡ä¼¼åº¦ã¨ãƒ‘ãƒ¯ãƒ¼ãƒ¬ãƒ™ãƒ«ã§ã‚½ãƒ¼ãƒˆ
            search_results.sort(
                key=lambda x: (x.similarity_score, x.power_level), 
                reverse=True
            )
            
            return search_results[:query.limit]
            
        except Exception as e:
            logger.error(f"âŒ Search failed: {e}")
            return []
    
    def _prepare_filters(self, query: SearchQuery) -> Dict[str, Any]:
        """æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æº–å‚™"""
        filters = query.filters or {}
        
        if query.include_eternal_only:
            filters['is_eternal'] = True
        
        if query.magic_schools:
            filters['magic_school'] = [school.value for school in query.magic_schools]
        
        if query.spell_types:
            filters['spell_type'] = [spell_type.value for spell_type in query.spell_types]
        
        if query.min_power_level:
            filters['min_power_level'] = query.min_power_level
        
        return filters
    
    async def find_related_spells(self, spell_id: str, limit: int = 5) -> List[SearchResult]:
        """é–¢é€£å‘ªæ–‡æ¤œç´¢"""
        try:
            # å…ƒå‘ªæ–‡å–å¾—
            spell_data = await self.database.get_spell_by_id(spell_id)
            if not spell_data:
                return []
            
            # å…ƒå‘ªæ–‡ã®ãƒ†ã‚­ã‚¹ãƒˆã§æ¤œç´¢
            searchable_text = self._prepare_searchable_text(spell_data)
            
            query = SearchQuery(
                query_text=searchable_text,
                limit=limit + 1,  # è‡ªåˆ†è‡ªèº«ã‚’é™¤å¤–ã™ã‚‹ãŸã‚+1
                similarity_threshold=0.3
            )
            
            results = await self.search(query)
            
            # è‡ªåˆ†è‡ªèº«ã‚’é™¤å¤–
            related_results = [r for r in results if r.spell_id != spell_id]
            
            return related_results[:limit]
            
        except Exception as e:
            logger.error(f"âŒ Related search failed: {e}")
            return []
    
    async def suggest_tags(self, content: str) -> List[str]:
        """è‡ªå‹•ã‚¿ã‚°ææ¡ˆ"""
        try:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            keywords = self.semantic_processor.extract_keywords(content)
            
            # é­”æ³•å­¦æ´¾æ¨å®š
            inferred_school = self.semantic_processor.infer_magic_school(content)
            if inferred_school:
                keywords.append(inferred_school.value)
            
            # é‡è¤‡é™¤å»
            return list(set(keywords))
            
        except Exception as e:
            logger.error(f"âŒ Tag suggestion failed: {e}")
            return []
    
    async def search_by_similarity(self, reference_text: str, limit: int = 10) -> List[SearchResult]:
        """é¡ä¼¼åº¦ã«ã‚ˆã‚‹æ¤œç´¢"""
        query = SearchQuery(
            query_text=reference_text,
            limit=limit,
            similarity_threshold=0.5
        )
        
        return await self.search(query)
    
    async def get_search_statistics(self) -> Dict[str, Any]:
        """æ¤œç´¢çµ±è¨ˆå–å¾—"""
        # ç°¡å˜ãªçµ±è¨ˆå®Ÿè£…
        return {
            'total_indexed_spells': 0,  # å®Ÿè£…æ™‚ã«å®Ÿéš›ã®æ•°ã‚’å–å¾—
            'cache_size': len(self.embedding_generator.cache),
            'search_engine_status': 'active'
        }
    
    async def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒ­ãƒ¼ã‚º"""
        await self.database.close()
        logger.info("ğŸ” Vector Search Engine closed")

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
async def test_vector_search():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    search_engine = GrimoireVectorSearch()
    
    try:
        await search_engine.initialize()
        
        # ã‚µãƒ³ãƒ—ãƒ«å‘ªæ–‡ä½œæˆãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
        sample_spell = {
            'spell_name': 'Claude CLI Vector Search Guide',
            'content': 'Claudeã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã®å®Œå…¨ã‚¬ã‚¤ãƒ‰ã€‚PostgreSQLã®pgvectorã‚’ä½¿ç”¨ã—ã¦ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚',
            'spell_type': SpellType.KNOWLEDGE.value,
            'magic_school': MagicSchool.SEARCH_MYSTIC.value,
            'tags': ['claude', 'vector', 'search', 'postgresql'],
            'power_level': 8
        }
        
        await search_engine.index_spell("test-vector-spell", sample_spell)
        print("âœ… Test spell indexed")
        
        # æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        query = SearchQuery(
            query_text="ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å®Ÿè£…æ–¹æ³•",
            limit=5,
            similarity_threshold=0.7
        )
        
        results = await search_engine.search(query)
        print(f"âœ… Search completed: {len(results)} results")
        
        for result in results:
            print(f"- {result.spell_name} (similarity: {result.similarity_score:.3f})")
        
        # ã‚¿ã‚°ææ¡ˆãƒ†ã‚¹ãƒˆ
        suggested_tags = await search_engine.suggest_tags(sample_spell['content'])
        print(f"âœ… Suggested tags: {suggested_tags}")
        
    finally:
        await search_engine.close()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(test_vector_search())