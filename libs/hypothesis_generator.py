#!/usr/bin/env python3
"""
Hypothesis Generator - ä»®èª¬ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
è¦³å¯Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿé¨“å¯èƒ½ãªä»®èª¬ã‚’è‡ªå‹•ç”Ÿæˆã—ã€æ¤œè¨¼è¨ˆç”»ã‚’ç«‹æ¡ˆ

4è³¢è€…ã¨ã®é€£æº:
ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…: éå»ã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ä»®èª¬ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
"ğŸ”" RAGè³¢è€…: é¡ä¼¼ä»®èª¬ã®æ¤œç´¢ã¨æ¤œè¨¼çµæœã®å‚ç…§
ğŸ“‹ ã‚¿ã‚¹ã‚¯è³¢è€…: ä»®èª¬æ¤œè¨¼ã®å„ªå…ˆé †ä½ä»˜ã‘ã¨å®Ÿè¡Œè¨ˆç”»
ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…: ãƒªã‚¹ã‚¯è¦å› ã‚’è€ƒæ…®ã—ãŸä»®èª¬ã®å®‰å…¨æ€§è©•ä¾¡
"""

import sys
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import hashlib
import json
import logging
import random
import statistics
import threading
import time
import uuid
from collections import defaultdict, deque
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple, Union

logger = logging.getLogger(__name__)


class HypothesisGenerator:
    """ä»®èª¬ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """HypothesisGenerator åˆæœŸåŒ–"""
        self.generator_id = f"hyp_gen_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # ä»®èª¬ç”Ÿæˆè¨­å®š
        self.generation_config = {
            "min_confidence": 0.6,
            "max_hypotheses_per_observation": 5,
            "hypothesis_types": [
                "performance",
                "reliability",
                "scalability",
                "resource",
                "configuration",
                "behavioral",
            ],
            "template_reuse_threshold": 0.7,
        }

        # ä»®èª¬ç®¡ç†
        self.hypothesis_history = deque(maxlen=1000)
        self.hypothesis_templates = defaultdict(list)
        self.experiment_results = {}

        # 4è³¢è€…çµ±åˆãƒ•ãƒ©ã‚°
        self.knowledge_sage_integration = True
        self.rag_sage_integration = True
        self.task_sage_integration = True
        self.incident_sage_integration = True

        # ç¶™ç¶šå­¦ç¿’
        self.continuous_learning_enabled = False
        self.learning_pipeline = None
        self.pattern_library = defaultdict(list)

        # ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        self.knowledge_base_path = (
            PROJECT_ROOT / "knowledge_base" / "hypothesis_templates"
        )
        self.knowledge_base_path.mkdir(parents=True, exist_ok=True)

        logger.info(f"HypothesisGenerator initialized: {self.generator_id}")

    def generate_hypotheses(self, observation_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¦³å¯Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä»®èª¬ã‚’ç”Ÿæˆ"""
        try:
            hypotheses = []

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ã®ä»®èª¬ç”Ÿæˆ
            if "performance_metrics" in observation_data:
                perf_hypotheses = self._generate_performance_hypotheses(
                    observation_data["performance_metrics"]
                )
                hypotheses.extend(perf_hypotheses)

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã®ä»®èª¬ç”Ÿæˆ
            if "patterns" in observation_data:
                pattern_hypotheses = self._generate_pattern_based_hypotheses(
                    observation_data["patterns"]
                )
                hypotheses.extend(pattern_hypotheses)

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®ä»®èª¬ç”Ÿæˆ
            if "context" in observation_data:
                context_hypotheses = self._generate_context_based_hypotheses(
                    observation_data["context"]
                )
                hypotheses.extend(context_hypotheses)

            # ä»®èª¬ã®å„ªå…ˆé †ä½ä»˜ã‘ã¨é¸åˆ¥
            selected_hypotheses = self._select_best_hypotheses(
                hypotheses, self.generation_config["max_hypotheses_per_observation"]
            )

            # ä»®èª¬ã®è©³ç´°åŒ–
            for hyp in selected_hypotheses:
                hyp["id"] = f"hyp_{uuid.uuid4().hex[:8]}"
                hyp["testable_predictions"] = self._generate_predictions(hyp)
                hyp["supporting_evidence"] = self._gather_evidence(
                    hyp, observation_data
                )

            # å±¥æ­´ã«è¿½åŠ 
            self.hypothesis_history.extend(selected_hypotheses)

            return {
                "generated_hypotheses": selected_hypotheses,
                "hypothesis_count": len(selected_hypotheses),
                "generation_metadata": {
                    "observation_summary": self._summarize_observation(
                        observation_data
                    ),
                    "generation_time": datetime.now(),
                    "generator_id": self.generator_id,
                },
            }

        except Exception as e:
            logger.error(f"Error generating hypotheses: {str(e)}")
            return {"generated_hypotheses": [], "hypothesis_count": 0, "error": str(e)}

    def validate_hypothesis(
        self, hypothesis: Dict[str, Any], validation_criteria: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä»®èª¬ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
        try:
            # çµ±è¨ˆçš„æœ‰æ„æ€§ãƒã‚§ãƒƒã‚¯
            statistical_validity = self._check_statistical_validity(
                hypothesis, validation_criteria
            )

            # å®Ÿç¾å¯èƒ½æ€§è©•ä¾¡
            feasibility = self._assess_feasibility(hypothesis)

            # ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶è©•ä¾¡
            resource_requirements = self._estimate_resources(hypothesis)

            # ãƒªã‚¹ã‚¯è©•ä¾¡
            risks = self._identify_risks(hypothesis)

            # æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ±ºå®š
            approach = self._recommend_approach(hypothesis, feasibility, risks)

            # ç·åˆæ¤œè¨¼ã‚¹ã‚³ã‚¢è¨ˆç®—
            validation_score = self._calculate_validation_score(
                statistical_validity, feasibility, risks
            )

            return {
                "is_valid": validation_score >= 0.7,
                "validation_score": validation_score,
                "feasibility_assessment": feasibility,
                "required_resources": resource_requirements,
                "potential_risks": risks,
                "recommended_approach": approach,
                "statistical_validity": statistical_validity,
            }

        except Exception as e:
            logger.error(f"Error validating hypothesis: {str(e)}")
            return {"is_valid": False, "validation_score": 0.0, "error": str(e)}

    def rank_hypotheses(
        self, hypotheses: List[Dict[str, Any]], ranking_criteria: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä»®èª¬ã‚’ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        try:
            weights = ranking_criteria.get(
                "weights",
                {"confidence": 0.3, "impact": 0.4, "cost_efficiency": 0.2, "risk": 0.1},
            )

            # å„ä»®èª¬ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
            scored_hypotheses = []
            for hyp in hypotheses:
                scores = self._calculate_hypothesis_scores(hyp, weights)
                hyp_copy = hyp.copy()
                hyp_copy["composite_score"] = scores["composite"]
                hyp_copy["score_breakdown"] = scores
                scored_hypotheses.append(hyp_copy)

            # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
            scored_hypotheses.sort(key=lambda x: x["composite_score"], reverse=True)

            # ãƒ©ãƒ³ã‚¯ä»˜ã‘
            for i, hyp in enumerate(scored_hypotheses):
                hyp["rank"] = i + 1

            # ãƒˆãƒƒãƒ—æ¨å¥¨ã®é¸å®š
            preferences = ranking_criteria.get("preferences", {})
            top_recommendations = self._select_top_recommendations(
                scored_hypotheses, preferences
            )

            return {
                "ranked_list": scored_hypotheses,
                "ranking_rationale": self._explain_ranking(scored_hypotheses, weights),
                "top_recommendations": top_recommendations,
            }

        except Exception as e:
            logger.error(f"Error ranking hypotheses: {str(e)}")
            return {"ranked_list": hypotheses, "error": str(e)}

    def create_experiment_plan(
        self, hypothesis: Dict[str, Any], experiment_constraints: Dict[str, Any]
    ) -> Dict[str, Any]:
        """å®Ÿé¨“è¨ˆç”»ã‚’ä½œæˆ"""
        try:
            experiment_id = f"exp_{uuid.uuid4().hex[:8]}"

            # ãƒ•ã‚§ãƒ¼ã‚ºå®šç¾©
            phases = self._define_experiment_phases(hypothesis, experiment_constraints)

            # æˆåŠŸåŸºæº–è¨­å®š
            success_criteria = self._define_success_criteria(hypothesis)

            # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨ˆç”»
            monitoring_plan = self._create_monitoring_plan(
                hypothesis, experiment_constraints
            )

            # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨ˆç”»
            rollback_plan = self._create_rollback_plan(
                hypothesis, experiment_constraints
            )

            # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä½œæˆ
            timeline = self._create_timeline(phases, experiment_constraints)

            return {
                "experiment_id": experiment_id,
                "hypothesis_id": hypothesis.get("id", "unknown"),
                "phases": phases,
                "success_criteria": success_criteria,
                "monitoring_plan": monitoring_plan,
                "rollback_plan": rollback_plan,
                "timeline": timeline,
                "constraints": experiment_constraints,
                "created_at": datetime.now(),
            }

        except Exception as e:
            logger.error(f"Error creating experiment plan: {str(e)}")
            return {"experiment_id": None, "error": str(e)}

    def save_hypothesis_template(
        self, successful_hypothesis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æˆåŠŸã—ãŸä»®èª¬ã‚’ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ã—ã¦ä¿å­˜ï¼ˆãƒŠãƒ¬ãƒƒã‚¸è³¢è€…é€£æºï¼‰"""
        try:
            template_id = f"template_{uuid.uuid4().hex[:8]}"

            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåŒ–
            template = self._create_template_from_hypothesis(successful_hypothesis)
            template["template_id"] = template_id
            template["created_at"] = datetime.now()

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚«ãƒ†ã‚´ãƒªãƒ¼æ±ºå®š
            category = self._categorize_hypothesis(successful_hypothesis)

            # ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            save_path = self.knowledge_base_path / f"{category}_{template_id}.json"
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(template, f, indent=2, default=str)

            # ãƒ¡ãƒ¢ãƒªå†…ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«è¿½åŠ 
            self.hypothesis_templates[category].append(template)

            # ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…çµ±åˆæƒ…å ±
            knowledge_integration = {
                "status": "success",
                "categorization": category,
                "related_patterns": self._find_related_patterns(template),
            }

            return {
                "saved": True,
                "template_id": template_id,
                "knowledge_base_path": str(save_path),
                "template_metadata": {
                    "category": category,
                    "reusability_score": template.get("reusability_score", 0.85),
                },
                "knowledge_integration": knowledge_integration,
            }

        except Exception as e:
            logger.error(f"Error saving hypothesis template: {str(e)}")
            return {"saved": False, "error": str(e)}

    def search_similar_hypotheses(
        self, search_query: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """é¡ä¼¼ä»®èª¬ã‚’æ¤œç´¢ï¼ˆRAGè³¢è€…é€£æºï¼‰"""
        try:
            similar_hypotheses = []

            # æ¤œç´¢ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
            query_vector = self._create_search_vector(search_query)

            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰æ¤œç´¢
            for category, templates in self.hypothesis_templates.items():
                for template in templates:
                    similarity = self._calculate_similarity(query_vector, template)
                    if similarity >= self.generation_config["template_reuse_threshold"]:
                        adapted = self._adapt_template_to_context(
                            template, search_query
                        )
                        adapted["similarity_score"] = similarity
                        adapted["rag_integration"] = {
                            "retrieval_confidence": similarity,
                            "search_quality": self._assess_search_quality(similarity),
                        }
                        similar_hypotheses.append(adapted)

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚‚æ¤œç´¢
            file_results = self._search_file_templates(search_query)
            similar_hypotheses.extend(file_results)

            # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
            similar_hypotheses.sort(key=lambda x: x["similarity_score"], reverse=True)

            return similar_hypotheses[:5]  # Top 5ã‚’è¿”ã™

        except Exception as e:
            logger.error(f"Error searching similar hypotheses: {str(e)}")
            return []

    def evaluate_hypothesis_risk(
        self, hypothesis: Dict[str, Any], system_state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä»®èª¬ã®ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡ï¼ˆã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…é€£æºï¼‰"""
        try:
            # ãƒªã‚¹ã‚¯è¦å› åˆ†æ
            risk_factors = self._analyze_risk_factors(hypothesis, system_state)

            # æ½œåœ¨çš„ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬
            potential_incidents = self._predict_incidents(hypothesis, system_state)

            # ç·©å’Œæˆ¦ç•¥ç”Ÿæˆ
            mitigation_strategies = self._generate_mitigation_strategies(
                risk_factors, potential_incidents
            )

            # ç·åˆãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«æ±ºå®š
            overall_risk = self._calculate_overall_risk(
                risk_factors, potential_incidents
            )

            # å®‰å…¨æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            safety_score = 1.0 - (overall_risk / 4.0)  # 4æ®µéšã‚’0-1ã«æ­£è¦åŒ–

            return {
                "overall_risk_level": self._risk_level_to_string(overall_risk),
                "risk_factors": risk_factors,
                "potential_incidents": potential_incidents,
                "mitigation_strategies": mitigation_strategies,
                "safety_score": safety_score,
                "incident_sage_assessment": {
                    "critical_operations_impact": self._assess_critical_impact(
                        hypothesis, system_state
                    ),
                    "recovery_complexity": self._assess_recovery_complexity(hypothesis),
                },
            }

        except Exception as e:
            logger.error(f"Error evaluating hypothesis risk: {str(e)}")
            return {
                "overall_risk_level": "unknown",
                "safety_score": 0.0,
                "error": str(e),
            }

    def track_hypothesis_results(
        self, experiment_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """å®Ÿé¨“çµæœã‚’è¿½è·¡ã—å­¦ç¿’"""
        try:
            result_id = f"result_{uuid.uuid4().hex[:8]}"

            # çµæœä¿å­˜
            self.experiment_results[experiment_results["hypothesis_id"]] = (
                experiment_results
            )

            # å­¦ç¿’å†…å®¹æŠ½å‡º
            learning = self._extract_learning(experiment_results)

            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆåˆ¤å®š
            template_generated = False
            if experiment_results.get("conclusion") == "hypothesis_confirmed":
                # æˆåŠŸã—ãŸä»®èª¬ã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåŒ–
                hypothesis = self._find_hypothesis(experiment_results["hypothesis_id"])
                if hypothesis:
                    template_result = self.save_hypothesis_template(hypothesis)
                    template_generated = template_result["saved"]

            # ãƒŠãƒ¬ãƒƒã‚¸æ›´æ–°
            knowledge_updated = self._update_knowledge_base(learning)

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæ›´æ–°
            self._update_pattern_library(experiment_results)

            return {
                "tracked": True,
                "result_id": result_id,
                "learning_extracted": learning,
                "template_generated": template_generated,
                "knowledge_updated": knowledge_updated,
                "tracking_metadata": {
                    "tracked_at": datetime.now(),
                    "hypothesis_id": experiment_results["hypothesis_id"],
                },
            }

        except Exception as e:
            logger.error(f"Error tracking hypothesis results: {str(e)}")
            return {"tracked": False, "error": str(e)}

    def prioritize_experiments_with_task_sage(
        self, hypothesis_experiments: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ã‚¿ã‚¹ã‚¯è³¢è€…ã¨é€£æºã—ã¦å®Ÿé¨“ã®å„ªå…ˆé †ä½ã‚’æ±ºå®š"""
        try:
            prioritized = []

            for exp in hypothesis_experiments:
                # å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
                priority_score = self._calculate_priority_score(exp)

                exp_copy = exp.copy()
                exp_copy["priority_score"] = priority_score
                exp_copy["scheduling_recommendation"] = self._recommend_scheduling(exp)
                prioritized.append(exp_copy)

            # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
            prioritized.sort(key=lambda x: x["priority_score"], reverse=True)

            # å®Ÿè¡Œé †åºä»˜ä¸
            for i, exp in enumerate(prioritized):
                exp["execution_order"] = i + 1

            return prioritized

        except Exception as e:
            logger.error(f"Error prioritizing experiments: {str(e)}")
            return hypothesis_experiments

    def generate_error_prevention_optimizations(
        self, error_patterns: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰äºˆé˜²çš„æœ€é©åŒ–ã‚’ç”Ÿæˆï¼ˆã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…é€£æºï¼‰"""
        try:
            preventive_strategies = []

            # é »ç™ºã‚¨ãƒ©ãƒ¼ã®åˆ†æ
            for error in error_patterns.get("frequent_errors", []):
                strategy = self._create_prevention_strategy(error)
                preventive_strategies.append(strategy)

            # å„ªå…ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
            priority_actions = self._determine_priority_actions(preventive_strategies)

            # ã‚¨ãƒ©ãƒ¼å‰Šæ¸›æœŸå¾…å€¤è¨ˆç®—
            expected_reduction = self._calculate_error_reduction(
                preventive_strategies, error_patterns
            )

            return {
                "preventive_strategies": preventive_strategies,
                "priority_actions": priority_actions,
                "expected_error_reduction": expected_reduction,
                "incident_prevention_plan": self._create_prevention_plan(
                    preventive_strategies
                ),
            }

        except Exception as e:
            logger.error(f"Error generating error prevention optimizations: {str(e)}")
            return {"preventive_strategies": [], "error": str(e)}

    def enable_continuous_learning(
        self, learning_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç¶™ç¶šçš„å­¦ç¿’ã‚’æœ‰åŠ¹åŒ–"""
        try:
            self.continuous_learning_enabled = learning_config.get("enabled", True)

            # å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
            self.learning_pipeline = {
                "feedback_collection": learning_config.get("feedback_collection", True),
                "pattern_extraction": learning_config.get("pattern_extraction", True),
                "template_generation": learning_config.get("template_generation", True),
                "success_threshold": learning_config.get("success_threshold", 0.75),
            }

            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿæ§‹è¨­å®š
            feedback_mechanism = {
                "collection_method": "automated",
                "processing_interval": "real_time",
                "learning_rate": learning_config.get("learning_rate", 0.1),
            }

            return {
                "learning_enabled": self.continuous_learning_enabled,
                "learning_pipeline": self.learning_pipeline,
                "feedback_mechanism": feedback_mechanism,
                "configuration": learning_config,
            }

        except Exception as e:
            logger.error(f"Error enabling continuous learning: {str(e)}")
            return {"learning_enabled": False, "error": str(e)}

    def learn_from_feedback(self, feedback: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å­¦ç¿’"""
        try:
            if not self.continuous_learning_enabled:
                return {"feedback_processed": False, "reason": "Learning disabled"}

            # ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°
            patterns_updated = self._update_patterns_from_feedback(feedback)

            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ”¹å–„
            templates_refined = self._refine_templates(feedback)

            # ç”Ÿæˆç²¾åº¦å‘ä¸Š
            accuracy_improvement = self._improve_generation_accuracy(feedback)

            # å­¦ç¿’å±¥æ­´è¨˜éŒ²
            self._record_learning_history(feedback)

            return {
                "feedback_processed": True,
                "patterns_updated": patterns_updated,
                "templates_refined": templates_refined,
                "generation_accuracy_improved": accuracy_improvement,
                "learning_metadata": {
                    "feedback_id": feedback.get("hypothesis_id"),
                    "learned_at": datetime.now(),
                },
            }

        except Exception as e:
            logger.error(f"Error learning from feedback: {str(e)}")
            return {"feedback_processed": False, "error": str(e)}

    def generate_collaborative_hypotheses(
        self, complex_problem: Dict[str, Any]
    ) -> Dict[str, Any]:
        """4è³¢è€…å”èª¿ã«ã‚ˆã‚‹ä»®èª¬ç”Ÿæˆ"""
        try:
            sage_contributions = {}

            # ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…: éå»ã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³
            if self.knowledge_sage_integration:
                sage_contributions["knowledge_sage"] = self._get_historical_patterns(
                    complex_problem
                )

            # RAGè³¢è€…: é¡ä¼¼äº‹ä¾‹æ¤œç´¢
            if self.rag_sage_integration:
                sage_contributions["rag_sage"] = self._search_similar_cases(
                    complex_problem
                )

            # ã‚¿ã‚¹ã‚¯è³¢è€…: å®Ÿè¡Œå„ªå…ˆé †ä½
            if self.task_sage_integration:
                sage_contributions["task_sage"] = self._get_execution_priorities(
                    complex_problem
                )

            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…: ãƒªã‚¹ã‚¯è©•ä¾¡
            if self.incident_sage_integration:
                sage_contributions["incident_sage"] = self._assess_problem_risks(
                    complex_problem
                )

            # çµ±åˆä»®èª¬ç”Ÿæˆ
            integrated_hypotheses = self._integrate_sage_inputs(
                sage_contributions, complex_problem
            )

            # ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹æ¨å¥¨
            consensus = self._form_sage_consensus(
                integrated_hypotheses, sage_contributions
            )

            # å®Ÿè¡Œè¨ˆç”»
            execution_plan = self._create_collaborative_execution_plan(
                integrated_hypotheses, consensus
            )

            return {
                "sage_contributions": sage_contributions,
                "integrated_hypotheses": integrated_hypotheses,
                "consensus_recommendations": consensus,
                "execution_plan": execution_plan,
                "collaboration_metadata": {
                    "sages_involved": list(sage_contributions.keys()),
                    "collaboration_time": datetime.now(),
                },
            }

        except Exception as e:
            logger.error(f"Error in collaborative hypothesis generation: {str(e)}")
            return {
                "sage_contributions": {},
                "integrated_hypotheses": [],
                "error": str(e),
            }

    # ===== Private Helper Methods =====

    def _generate_performance_hypotheses(
        self, metrics: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ä»®èª¬ç”Ÿæˆ"""
        hypotheses = []

        for metric_name, metric_data in metrics.items():
            if isinstance(metric_data, dict) and "trend" in metric_data:
                if metric_data["trend"] == "increasing" and metric_name in [
                    "response_time",
                    "error_rate",
                ]:
                    hypothesis = {
                        "statement": f"Optimizing {metric_name} will improve system performance",
                        "hypothesis_type": "performance",
                        "confidence_score": 0.8,
                        "rationale": f"{metric_name} is trending upward",
                    }
                    hypotheses.append(hypothesis)

        return hypotheses

    def _generate_pattern_based_hypotheses(
        self, patterns: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®ä»®èª¬ç”Ÿæˆ"""
        hypotheses = []

        for pattern in patterns:
            if pattern.get("confidence", 0) > 0.7:
                hypothesis = {
                    "statement": f"Addressing {pattern['type']} pattern will resolve issues",
                    "hypothesis_type": "behavioral",
                    "confidence_score": pattern["confidence"],
                    "rationale": pattern["description"],
                }
                hypotheses.append(hypothesis)

        return hypotheses

    def _generate_context_based_hypotheses(
        self, context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ä»®èª¬ç”Ÿæˆ"""
        hypotheses = []

        if "system_changes" in context:
            for change in context["system_changes"]:
                hypothesis = {
                    "statement": f"Recent {change} may be causing performance issues",
                    "hypothesis_type": "configuration",
                    "confidence_score": 0.75,
                    "rationale": f"Temporal correlation with {change}",
                }
                hypotheses.append(hypothesis)

        return hypotheses

    def _select_best_hypotheses(
        self, hypotheses: List[Dict[str, Any]], max_count: int
    ) -> List[Dict[str, Any]]:
        """æœ€è‰¯ã®ä»®èª¬ã‚’é¸æŠ"""
        # ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        hypotheses.sort(key=lambda x: x.get("confidence_score", 0), reverse=True)
        return hypotheses[:max_count]

    def _generate_predictions(self, hypothesis: Dict[str, Any]) -> List[str]:
        """æ¤œè¨¼å¯èƒ½ãªäºˆæ¸¬ã‚’ç”Ÿæˆ"""
        predictions = []

        if hypothesis.get("hypothesis_type") == "performance":
            predictions.append("Performance metrics will improve by at least 20%")
            predictions.append("System resource usage will remain stable")
        elif hypothesis.get("hypothesis_type") == "reliability":
            predictions.append("Error rate will decrease by at least 50%")
            predictions.append("System uptime will increase")
        elif hypothesis.get("hypothesis_type") == "behavioral":
            predictions.append("Behavior patterns will normalize")
            predictions.append("Anomaly frequency will decrease")
        elif hypothesis.get("hypothesis_type") == "configuration":
            predictions.append("System stability will improve")
            predictions.append("Configuration conflicts will be resolved")
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®äºˆæ¸¬
            predictions.append("Target metrics will show measurable improvement")
            predictions.append("No negative side effects will occur")

        return predictions

    def _gather_evidence(
        self, hypothesis: Dict[str, Any], observation_data: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æ”¯æŒè¨¼æ‹ ã‚’åé›†"""
        evidence = []

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ã®è¨¼æ‹ 
        if "performance_metrics" in observation_data:
            for metric, data in observation_data["performance_metrics"].items():
                if isinstance(data, dict) and "trend" in data:
                    evidence.append(
                        {
                            "type": "metric_trend",
                            "description": f"{metric} shows {data['trend']} trend",
                            "strength": 0.7,
                        }
                    )

        return evidence

    def _summarize_observation(
        self, observation_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è¦³å¯Ÿãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒãƒªãƒ¼"""
        return {
            "metrics_count": len(observation_data.get("performance_metrics", {})),
            "patterns_found": len(observation_data.get("patterns", [])),
            "context_factors": len(observation_data.get("context", {})),
        }

    def _check_statistical_validity(
        self, hypothesis: Dict[str, Any], criteria: Dict[str, Any]
    ) -> Dict[str, Any]:
        """çµ±è¨ˆçš„å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        return {
            "significance_level": criteria.get("statistical_significance", 0.95),
            "sample_size_adequate": True,
            "control_variables_defined": True,
        }

    def _assess_feasibility(self, hypothesis: Dict[str, Any]) -> Dict[str, Any]:
        """å®Ÿç¾å¯èƒ½æ€§è©•ä¾¡"""
        return {
            "technical_feasibility": 0.85,
            "resource_availability": 0.90,
            "implementation_complexity": "medium",
        }

    def _estimate_resources(self, hypothesis: Dict[str, Any]) -> Dict[str, Any]:
        """å¿…è¦ãƒªã‚½ãƒ¼ã‚¹è¦‹ç©ã‚‚ã‚Š"""
        return {
            "time_required": "2-4 hours",
            "human_resources": 1,
            "computational_resources": "moderate",
            "cost_estimate": "low",
        }

    def _identify_risks(self, hypothesis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ãƒªã‚¹ã‚¯ç‰¹å®š"""
        risks = []

        if hypothesis.get("hypothesis_type") == "performance":
            risks.append(
                {
                    "type": "performance_degradation",
                    "severity": "medium",
                    "mitigation": "Gradual rollout with monitoring",
                }
            )

        return risks

    def _recommend_approach(
        self,
        hypothesis: Dict[str, Any],
        feasibility: Dict[str, Any],
        risks: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ±ºå®š"""
        if feasibility["technical_feasibility"] > 0.8 and len(risks) < 3:
            approach = "direct_implementation"
        else:
            approach = "phased_rollout"

        return {
            "approach_type": approach,
            "rationale": "Based on feasibility and risk assessment",
        }

    def _calculate_validation_score(
        self,
        statistical_validity: Dict[str, Any],
        feasibility: Dict[str, Any],
        risks: List[Dict[str, Any]],
    ) -> float:
        """æ¤œè¨¼ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        stat_score = statistical_validity.get("significance_level", 0.5)
        feas_score = feasibility.get("technical_feasibility", 0.5)
        risk_score = 1.0 - (len(risks) * 0.1)  # ãƒªã‚¹ã‚¯ãŒå¤šã„ã»ã©ä½ã‚¹ã‚³ã‚¢

        return (stat_score + feas_score + risk_score) / 3.0

    def _calculate_hypothesis_scores(
        self, hypothesis: Dict[str, Any], weights: Dict[str, float]
    ) -> Dict[str, float]:
        """ä»®èª¬ã®ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        scores = {
            "confidence_score": hypothesis.get("confidence_score", 0.5),
            "impact_score": hypothesis.get("expected_impact", 0.5),
            "cost_efficiency_score": self._calculate_cost_efficiency(hypothesis),
            "risk_score": 1.0 - (len(hypothesis.get("risks", [])) * 0.2),
        }

        # é‡ã¿ä»˜ãåˆè¨ˆ
        composite = sum(
            scores[key] * weights.get(key.replace("_score", ""), 0.25) for key in scores
        )
        scores["composite"] = composite

        return scores

    def _calculate_cost_efficiency(self, hypothesis: Dict[str, Any]) -> float:
        """ã‚³ã‚¹ãƒˆåŠ¹ç‡è¨ˆç®—"""
        cost_map = {"low": 1.0, "medium": 0.5, "high": 0.2}
        return cost_map.get(hypothesis.get("implementation_cost", "medium"), 0.5)

    def _select_top_recommendations(
        self, scored_hypotheses: List[Dict[str, Any]], preferences: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ãƒˆãƒƒãƒ—æ¨å¥¨é¸å®š"""
        if preferences.get("prefer_quick_wins"):
            # ã‚³ã‚¹ãƒˆåŠ¹ç‡é‡è¦–ã§ãƒ•ã‚£ãƒ«ã‚¿
            return [
                h
                for h in scored_hypotheses
                if h["score_breakdown"]["cost_efficiency_score"] > 0.7
            ][:3]

        return scored_hypotheses[:3]

    def _explain_ranking(
        self, ranked_hypotheses: List[Dict[str, Any]], weights: Dict[str, float]
    ) -> str:
        """ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®èª¬æ˜"""
        return f"Ranking based on weights: {weights}"

    def _define_experiment_phases(
        self, hypothesis: Dict[str, Any], constraints: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """å®Ÿé¨“ãƒ•ã‚§ãƒ¼ã‚ºå®šç¾©"""
        return [
            {
                "name": "preparation",
                "duration": "30 minutes",
                "tasks": ["Setup monitoring", "Backup current state"],
                "checkpoints": ["Environment ready", "Baseline captured"],
            },
            {
                "name": "baseline",
                "duration": "30 minutes",
                "tasks": ["Collect baseline metrics", "Verify stability"],
                "checkpoints": ["Baseline established", "System stable"],
            },
            {
                "name": "experiment",
                "duration": constraints.get("duration", "2_hours").replace("_", " "),
                "tasks": ["Apply changes", "Monitor impact"],
                "checkpoints": ["Changes applied", "Data collection ongoing"],
            },
            {
                "name": "analysis",
                "duration": "30 minutes",
                "tasks": ["Analyze results", "Statistical validation"],
                "checkpoints": ["Analysis complete", "Results validated"],
            },
            {
                "name": "decision",
                "duration": "15 minutes",
                "tasks": ["Make decision", "Document results"],
                "checkpoints": ["Decision made", "Documentation complete"],
            },
        ]

    def _define_success_criteria(self, hypothesis: Dict[str, Any]) -> Dict[str, Any]:
        """æˆåŠŸåŸºæº–å®šç¾©"""
        return {
            "metrics": hypothesis.get("testable_predictions", []),
            "thresholds": {"minimum_improvement": 0.15, "maximum_degradation": 0.05},
            "statistical_tests": ["t-test", "chi-square"],
        }

    def _create_monitoring_plan(
        self, hypothesis: Dict[str, Any], constraints: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨ˆç”»ä½œæˆ"""
        return {
            "metrics_to_track": ["response_time", "error_rate", "resource_usage"],
            "sampling_rate": "1_minute",
            "alert_conditions": [
                {"metric": "error_rate", "threshold": 0.05, "action": "alert"},
                {"metric": "response_time", "threshold": 500, "action": "rollback"},
            ],
        }

    def _create_rollback_plan(
        self, hypothesis: Dict[str, Any], constraints: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨ˆç”»ä½œæˆ"""
        return {
            "trigger_conditions": [
                "Error rate exceeds 5%",
                "Response time exceeds baseline by 50%",
                "System instability detected",
            ],
            "rollback_steps": [
                "Revert configuration changes",
                "Restart affected services",
                "Verify system stability",
            ],
            "estimated_time": constraints.get("rollback_time", "5_minutes"),
        }

    def _create_timeline(
        self, phases: List[Dict[str, Any]], constraints: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä½œæˆ"""
        start_time = datetime.now()
        timeline = {"start": start_time, "phases": []}

        current_time = start_time
        for phase in phases:
            # ç°¡ç•¥åŒ–: durationæ–‡å­—åˆ—ã‹ã‚‰æ™‚é–“ã‚’æŠ½å‡º
            duration_hours = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30åˆ†
            duration_str = phase.get("duration", "30 minutes")

            try:
                if "hour" in duration_str:
                    # "2 hours" -> 2.0
                    duration_hours = float(duration_str.split()[0])
                elif "minute" in duration_str:
                    # "30 minutes" -> 0.5
                    minutes = float(duration_str.split()[0])
                    duration_hours = minutes / 60.0
            except (ValueError, IndexError):
                # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                duration_hours = 0.5

            phase_timeline = {
                "phase": phase["name"],
                "start": current_time,
                "end": current_time + timedelta(hours=duration_hours),
            }
            timeline["phases"].append(phase_timeline)
            current_time = phase_timeline["end"]

        timeline["end"] = current_time
        return timeline

    def _create_template_from_hypothesis(
        self, hypothesis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä»®èª¬ã‹ã‚‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ"""
        return {
            "pattern": hypothesis.get("pattern", "general_optimization"),
            "statement_template": self._generalize_statement(hypothesis["statement"]),
            "applicable_contexts": self._extract_contexts(hypothesis),
            "success_parameters": hypothesis.get("successful_parameters", {}),
            "reusability_score": hypothesis.get("reusability_score", 0.85),
            "validation_results": hypothesis.get("validation_results", {}),
        }

    def _generalize_statement(self, statement: str) -> str:
        """æ–‡ç« ã‚’ä¸€èˆ¬åŒ–"""
        # ç°¡å˜ãªå¤‰æ•°åŒ–
        generalized = statement
        generalized = generalized.replace("caching", "{optimization_type}")
        generalized = generalized.replace("response time", "{metric}")
        return generalized

    def _extract_contexts(self, hypothesis: Dict[str, Any]) -> List[str]:
        """é©ç”¨å¯èƒ½ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        contexts = []
        if "context" in hypothesis:
            context = hypothesis["context"]
            if "problem_type" in context:
                contexts.append(context["problem_type"])
            if "system_state" in context:
                contexts.append(context["system_state"])
        return contexts

    def _categorize_hypothesis(self, hypothesis: Dict[str, Any]) -> str:
        """ä»®èª¬ã‚’ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡"""
        return hypothesis.get("hypothesis_type", "general")

    def _find_related_patterns(self, template: Dict[str, Any]) -> List[str]:
        """é–¢é€£ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢"""
        # ç°¡ç•¥åŒ–: ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãé–¢é€£ãƒ‘ã‚¿ãƒ¼ãƒ³
        pattern_relations = {
            "performance_optimization": ["caching", "pooling", "batching"],
            "resource_optimization": ["memory_management", "cpu_optimization"],
            "reliability_improvement": ["retry_logic", "circuit_breaker"],
        }
        return pattern_relations.get(template.get("pattern", ""), [])

    def _create_search_vector(self, search_query: Dict[str, Any]) -> Dict[str, Any]:
        """æ¤œç´¢ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ"""
        # ç°¡ç•¥åŒ–: è¾æ›¸ãƒ™ãƒ¼ã‚¹ã®ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾
        vector = {
            "problem": search_query.get("problem_description", ""),
            "symptoms": search_query.get("symptoms", []),
            "context": search_query.get("context", {}),
            "outcome": search_query.get("desired_outcome", ""),
        }
        return vector

    def _calculate_similarity(
        self, query_vector: Dict[str, Any], template: Dict[str, Any]
    ) -> float:
        """é¡ä¼¼åº¦è¨ˆç®—"""
        # ç°¡ç•¥åŒ–: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        score = 0.0

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸€è‡´ãƒã‚§ãƒƒã‚¯
        template_contexts = template.get("applicable_contexts", [])
        query_contexts = []
        if isinstance(query_vector.get("context"), dict):
            query_contexts = list(query_vector["context"].values())

        for ctx in template_contexts:
            if any(str(ctx).lower() in str(qc).lower() for qc in query_contexts):
                score += 0.3

        # ç—‡çŠ¶ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        if "symptoms" in query_vector:
            for symptom in query_vector["symptoms"]:
                if symptom in str(template):
                    score += 0.2

        return min(score, 1.0)

    def _adapt_template_to_context(
        self, template: Dict[str, Any], search_query: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«é©å¿œ"""
        adapted = template.copy()
        adapted["hypothesis_id"] = f"adapted_{uuid.uuid4().hex[:8]}"
        adapted["original_statement"] = template.get("statement_template", "")

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«é©å¿œ
        if "context" in search_query:
            adapted["adapted_statement"] = self._fill_template_variables(
                template.get("statement_template", ""), search_query["context"]
            )
        else:
            adapted["adapted_statement"] = adapted["original_statement"]

        adapted["historical_success_rate"] = 0.75  # ä»®ã®å€¤
        adapted["applicable_parameters"] = template.get("success_parameters", {})

        return adapted

    def _fill_template_variables(self, template: str, context: Dict[str, Any]) -> str:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ•°ã‚’åŸ‹ã‚ã‚‹"""
        filled = template
        # ç°¡å˜ãªå¤‰æ•°ç½®æ›
        if "{optimization_type}" in filled and "database_type" in context:
            filled = filled.replace(
                "{optimization_type}", f"{context['database_type']} optimization"
            )
        if "{metric}" in filled:
            filled = filled.replace("{metric}", "query execution time")
        return filled

    def _assess_search_quality(self, similarity: float) -> str:
        """æ¤œç´¢å“è³ªè©•ä¾¡"""
        if similarity > 0.8:
            return "high"
        elif similarity > 0.6:
            return "medium"
        else:
            return "low"

    def _search_file_templates(
        self, search_query: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ¤œç´¢"""
        results = []

        try:
            for template_file in self.knowledge_base_path.glob("*.json"):
                with open(template_file, "r", encoding="utf-8") as f:
                    template = json.load(f)

                similarity = self._calculate_similarity(
                    self._create_search_vector(search_query), template
                )
                if similarity >= self.generation_config["template_reuse_threshold"]:
                    adapted = self._adapt_template_to_context(template, search_query)
                    adapted["similarity_score"] = similarity
                    adapted["rag_integration"] = {
                        "retrieval_confidence": similarity,
                        "search_quality": self._assess_search_quality(similarity),
                    }
                    results.append(adapted)
        except Exception as e:
            logger.error(f"Error searching file templates: {str(e)}")

        return results

    def _analyze_risk_factors(
        self, hypothesis: Dict[str, Any], system_state: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ãƒªã‚¹ã‚¯è¦å› åˆ†æ"""
        risk_factors = []

        # é«˜è² è·æ™‚ã®ãƒªã‚¹ã‚¯
        if system_state.get("current_load") == "high":
            risk_factors.append(
                {
                    "category": "performance",
                    "description": "Changes during high load may cause instability",
                    "severity": "high",
                    "probability": 0.7,
                }
            )

        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«æ“ä½œã¸ã®å½±éŸ¿
        if "critical_operations" in system_state:
            risk_factors.append(
                {
                    "category": "operational",
                    "description": "May impact critical operations",
                    "severity": "critical",
                    "probability": 0.5,
                }
            )

        return risk_factors

    def _predict_incidents(
        self, hypothesis: Dict[str, Any], system_state: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æ½œåœ¨çš„ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬"""
        incidents = []

        if (
            hypothesis.get("impact_areas")
            and "performance" in hypothesis["impact_areas"]
        ):
            incidents.append(
                {
                    "type": "performance_degradation",
                    "likelihood": "medium",
                    "impact": "user_experience",
                    "estimated_duration": "30 minutes",
                }
            )

        return incidents

    def _generate_mitigation_strategies(
        self, risk_factors: List[Dict[str, Any]], incidents: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ç·©å’Œæˆ¦ç•¥ç”Ÿæˆ"""
        strategies = []

        # ãƒªã‚¹ã‚¯è¦å› ã«åŸºã¥ãæˆ¦ç•¥
        for risk in risk_factors:
            if risk["severity"] in ["high", "critical"]:
                strategies.append(
                    {
                        "strategy": "Gradual rollout with canary deployment",
                        "targets": risk["category"],
                        "effectiveness": 0.8,
                    }
                )

        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆé˜²æˆ¦ç•¥
        if incidents:
            strategies.append(
                {
                    "strategy": "Enhanced monitoring and automatic rollback",
                    "targets": "all",
                    "effectiveness": 0.9,
                }
            )

        return strategies

    def _calculate_overall_risk(
        self, risk_factors: List[Dict[str, Any]], incidents: List[Dict[str, Any]]
    ) -> int:
        """ç·åˆãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«è¨ˆç®—"""
        # ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢è¨ˆç®—
        risk_score = 0

        for risk in risk_factors:
            severity_scores = {"low": 1, "medium": 2, "high": 3, "critical": 4}
            risk_score += severity_scores.get(risk["severity"], 2) * risk["probability"]

        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå½±éŸ¿åŠ ç®—
        risk_score += len(incidents) * 2

        # 4æ®µéšè©•ä¾¡
        if risk_score < 3:
            return 1  # low
        elif risk_score < 6:
            return 2  # medium
        elif risk_score < 10:
            return 3  # high
        else:
            return 4  # critical

    def _risk_level_to_string(self, risk_level: int) -> str:
        """ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã‚’æ–‡å­—åˆ—ã«å¤‰æ›"""
        mapping = {1: "low", 2: "medium", 3: "high", 4: "critical"}
        return mapping.get(risk_level, "unknown")

    def _assess_critical_impact(
        self, hypothesis: Dict[str, Any], system_state: Dict[str, Any]
    ) -> str:
        """ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«æ“ä½œã¸ã®å½±éŸ¿è©•ä¾¡"""
        if "critical_operations" in system_state and hypothesis.get("impact_areas"):
            return (
                "high"
                if any(
                    area in ["performance", "reliability"]
                    for area in hypothesis["impact_areas"]
                )
                else "low"
            )
        return "unknown"

    def _assess_recovery_complexity(self, hypothesis: Dict[str, Any]) -> str:
        """å¾©æ—§è¤‡é›‘åº¦è©•ä¾¡"""
        if hypothesis.get("proposed_changes"):
            changes_count = len(hypothesis["proposed_changes"])
            if changes_count > 3:
                return "high"
            elif changes_count > 1:
                return "medium"
        return "low"

    def _find_hypothesis(self, hypothesis_id: str) -> Optional[Dict[str, Any]]:
        """ä»®èª¬IDã‹ã‚‰ä»®èª¬ã‚’æ¤œç´¢"""
        for hyp in self.hypothesis_history:
            if hyp.get("id") == hypothesis_id:
                return hyp
        return None

    def _extract_learning(self, experiment_results: Dict[str, Any]) -> Dict[str, Any]:
        """å®Ÿé¨“çµæœã‹ã‚‰å­¦ç¿’å†…å®¹æŠ½å‡º"""
        return {
            "success_factors": self._identify_success_factors(experiment_results),
            "applicable_contexts": self._identify_applicable_contexts(
                experiment_results
            ),
            "reusability_assessment": self._assess_reusability(experiment_results),
        }

    def _identify_success_factors(self, results: Dict[str, Any]) -> List[str]:
        """æˆåŠŸè¦å› ç‰¹å®š"""
        factors = []

        if results.get("predictions_validated"):
            for pred, data in results["predictions_validated"].items():
                if data.get("validated"):
                    factors.append(f"{pred} achieved as predicted")

        return factors

    def _identify_applicable_contexts(self, results: Dict[str, Any]) -> List[str]:
        """é©ç”¨å¯èƒ½ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç‰¹å®š"""
        # ç°¡ç•¥åŒ–: åŸºæœ¬çš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        return ["high_load", "batch_processing", "real_time_operations"]

    def _assess_reusability(self, results: Dict[str, Any]) -> float:
        """å†åˆ©ç”¨å¯èƒ½æ€§è©•ä¾¡"""
        # äºˆæ¸¬ç²¾åº¦ã«åŸºã¥ãè©•ä¾¡
        if results.get("predictions_validated"):
            validated_count = sum(
                1
                for v in results["predictions_validated"].values()
                if v.get("validated")
            )
            total_count = len(results["predictions_validated"])
            return validated_count / total_count if total_count > 0 else 0.5
        return 0.5

    def _update_knowledge_base(self, learning: Dict[str, Any]) -> bool:
        """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ›´æ–°"""
        try:
            # å­¦ç¿’å†…å®¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            learning_file = (
                self.knowledge_base_path
                / f"learning_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            with open(learning_file, "w", encoding="utf-8") as f:
                json.dump(learning, f, indent=2, default=str)
            return True
        except Exception as e:
            logger.error(f"Error updating knowledge base: {str(e)}")
            return False

    def _update_pattern_library(self, experiment_results: Dict[str, Any]) -> None:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæ›´æ–°"""
        if experiment_results.get("conclusion") == "hypothesis_confirmed":
            pattern_type = "success_pattern"
        else:
            pattern_type = "failure_pattern"

        self.pattern_library[pattern_type].append(
            {
                "hypothesis_id": experiment_results["hypothesis_id"],
                "metrics": experiment_results.get("metrics_collected", {}),
                "timestamp": datetime.now(),
            }
        )

    def _calculate_priority_score(self, experiment: Dict[str, Any]) -> float:
        """å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        urgency_scores = {"critical": 1.0, "high": 0.8, "medium": 0.5, "low": 0.2}
        resource_scores = {"low": 1.0, "medium": 0.7, "high": 0.4}

        urgency = urgency_scores.get(experiment.get("urgency", "medium"), 0.5)
        resource = resource_scores.get(
            experiment.get("resource_requirement", "medium"), 0.5
        )

        return (urgency * 0.7) + (resource * 0.3)

    def _recommend_scheduling(self, experiment: Dict[str, Any]) -> str:
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°æ¨å¥¨"""
        if experiment.get("urgency") == "critical":
            return "immediate"
        elif experiment.get("urgency") == "high":
            return "within_2_hours"
        else:
            return "next_maintenance_window"

    def _create_prevention_strategy(self, error: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼äºˆé˜²æˆ¦ç•¥ä½œæˆ"""
        strategy = {"targets": [error["error_type"]], "optimization_focus": []}

        # ç›¸é–¢é–¢ä¿‚ã«åŸºã¥ãæœ€é©åŒ–ãƒ•ã‚©ãƒ¼ã‚«ã‚¹
        if "correlation" in error:
            for factor, correlation in error["correlation"].items():
                if correlation > 0.7:
                    strategy["optimization_focus"].append(factor.replace("_", " "))

        strategy["expected_effectiveness"] = 0.8
        strategy["implementation_complexity"] = "medium"

        return strategy

    def _determine_priority_actions(
        self, strategies: List[Dict[str, Any]]
    ) -> List[str]:
        """å„ªå…ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š"""
        # é »åº¦ã®é«˜ã„ã‚‚ã®ã‹ã‚‰å„ªå…ˆ
        return [s["targets"][0] for s in strategies[:3]]

    def _calculate_error_reduction(
        self, strategies: List[Dict[str, Any]], error_patterns: Dict[str, Any]
    ) -> float:
        """ã‚¨ãƒ©ãƒ¼å‰Šæ¸›æœŸå¾…å€¤è¨ˆç®—"""
        # ç°¡ç•¥åŒ–: æˆ¦ç•¥æ•°ã«åŸºã¥ãå‰Šæ¸›ç‡
        reduction_per_strategy = 0.15
        total_reduction = len(strategies) * reduction_per_strategy
        return min(total_reduction, 0.75)  # æœ€å¤§75%å‰Šæ¸›

    def _create_prevention_plan(
        self, strategies: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """äºˆé˜²è¨ˆç”»ä½œæˆ"""
        return {
            "phases": ["analysis", "implementation", "monitoring"],
            "timeline": "1_week",
            "success_metrics": ["error_rate_reduction", "mtbf_improvement"],
        }

    def _update_patterns_from_feedback(self, feedback: Dict[str, Any]) -> bool:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°"""
        if feedback.get("outcome") == "success":
            self.pattern_library["success_patterns"].append(feedback)
        return True

    def _refine_templates(self, feedback: Dict[str, Any]) -> bool:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ”¹å–„"""
        # é«˜ç²¾åº¦ã®äºˆæ¸¬ãŒã‚ã£ãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å¼·åŒ–
        if feedback.get("accuracy", {}).get("predictions", 0) > 0.8:
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹
            return True
        return False

    def _improve_generation_accuracy(self, feedback: Dict[str, Any]) -> float:
        """ç”Ÿæˆç²¾åº¦å‘ä¸Š"""
        # å­¦ç¿’ã«ã‚ˆã‚‹æ”¹å–„ç‡ï¼ˆä»®ï¼‰
        improvement = feedback.get("accuracy", {}).get("predictions", 0.5) * 0.1
        return improvement

    def _record_learning_history(self, feedback: Dict[str, Any]) -> None:
        """å­¦ç¿’å±¥æ­´è¨˜éŒ²"""
        # ãƒ¡ãƒ¢ãƒªå†…å±¥æ­´ã«è¿½åŠ 
        self.hypothesis_history.append(
            {
                "type": "learning_feedback",
                "feedback": feedback,
                "timestamp": datetime.now(),
            }
        )

    def _get_historical_patterns(self, problem: Dict[str, Any]) -> Dict[str, Any]:
        """éå»ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å–å¾—ï¼ˆãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ï¼‰"""
        return {
            "successful_patterns": list(self.pattern_library["success_patterns"][-5:]),
            "failure_patterns": list(self.pattern_library["failure_patterns"][-3:]),
            "recommendation": "Apply pattern from similar past incident",
        }

    def _search_similar_cases(self, problem: Dict[str, Any]) -> Dict[str, Any]:
        """é¡ä¼¼äº‹ä¾‹æ¤œç´¢ï¼ˆRAGè³¢è€…ï¼‰"""
        # ç°¡ç•¥åŒ–: ç—‡çŠ¶ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢
        similar_cases = []
        if problem.get("symptoms"):
            similar_cases = [
                {
                    "case_id": "case_001",
                    "similarity": 0.85,
                    "solution": "Optimization through caching",
                }
            ]

        return {
            "similar_cases": similar_cases,
            "best_match_confidence": 0.85 if similar_cases else 0.0,
        }

    def _get_execution_priorities(self, problem: Dict[str, Any]) -> Dict[str, Any]:
        """å®Ÿè¡Œå„ªå…ˆé †ä½å–å¾—ï¼ˆã‚¿ã‚¹ã‚¯è³¢è€…ï¼‰"""
        return {
            "priority_level": (
                "high"
                if problem.get("risk_factors", {}).get("production_impact") == "high"
                else "medium"
            ),
            "recommended_sequence": ["quick_fixes", "comprehensive_solutions"],
            "resource_allocation": "parallel_execution_recommended",
        }

    def _assess_problem_risks(self, problem: Dict[str, Any]) -> Dict[str, Any]:
        """å•é¡Œãƒªã‚¹ã‚¯è©•ä¾¡ï¼ˆã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ï¼‰"""
        risk_level = (
            "high"
            if problem.get("risk_factors", {}).get("user_complaints", 0) > 10
            else "medium"
        )

        return {
            "overall_risk": risk_level,
            "critical_factors": ["user_impact", "data_integrity"],
            "mitigation_required": risk_level == "high",
        }

    def _integrate_sage_inputs(
        self, contributions: Dict[str, Any], problem: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """è³¢è€…å…¥åŠ›çµ±åˆ"""
        integrated = []

        # å„è³¢è€…ã®å…¥åŠ›ã‚’çµ±åˆã—ãŸä»®èª¬ç”Ÿæˆ
        base_hypothesis = {
            "statement": "Implement comprehensive optimization based on sage analysis",
            "hypothesis_type": "integrated",
            "multi_sage_confidence": 0.85,
            "sage_inputs": contributions,
        }

        integrated.append(base_hypothesis)

        # è¿½åŠ ã®å…·ä½“çš„ä»®èª¬
        if contributions.get("rag_sage", {}).get("similar_cases"):
            integrated.append(
                {
                    "statement": "Apply caching solution from similar case",
                    "hypothesis_type": "performance",
                    "multi_sage_confidence": 0.80,
                    "based_on": "rag_sage_recommendation",
                }
            )

        return integrated

    def _form_sage_consensus(
        self, hypotheses: List[Dict[str, Any]], contributions: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è³¢è€…ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹å½¢æˆ"""
        return {
            "consensus_reached": True,
            "primary_recommendation": hypotheses[0] if hypotheses else None,
            "confidence_level": 0.85,
            "dissenting_opinions": [],
        }

    def _create_collaborative_execution_plan(
        self, hypotheses: List[Dict[str, Any]], consensus: Dict[str, Any]
    ) -> Dict[str, Any]:
        """å”èª¿å®Ÿè¡Œè¨ˆç”»ä½œæˆ"""
        return {
            "execution_phases": [
                {"phase": "preparation", "duration": "1_hour", "lead_sage": "task"},
                {
                    "phase": "implementation",
                    "duration": "2_hours",
                    "lead_sage": "knowledge",
                },
                {"phase": "monitoring", "duration": "1_hour", "lead_sage": "incident"},
                {"phase": "evaluation", "duration": "30_minutes", "lead_sage": "rag"},
            ],
            "resource_allocation": "distributed",
            "rollback_authority": "incident_sage",
        }
