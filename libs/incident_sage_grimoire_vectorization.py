#!/usr/bin/env python3
"""
Incident Sage Grimoire Vectorization System
ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…é­”æ³•æ›¸ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚¹ãƒ†ãƒ 

ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é¡ä¼¼æ¤œç´¢ã€äºˆå…†æ¤œçŸ¥ã€è‡ªå‹•è§£æ±ºç­–ææ¡ˆã‚’å®Ÿç¾
ã‚°ãƒ©ãƒ³ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼maruç›´å±ã®ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼ã«ã‚ˆã‚‹æ…é‡ãªå®Ÿè£…
"""

import sys
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import asyncio
import json
import logging
import re
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np

from libs.four_sages_integration import FourSagesIntegration
from libs.grimoire_database import (
    GrimoireDatabase,
    MagicSchool,
    SpellMetadata,
    SpellType,
)
from libs.grimoire_spell_evolution import EvolutionEngine, EvolutionType
from libs.grimoire_vector_search import GrimoireVectorSearch, SearchQuery, SearchResult

logger = logging.getLogger(__name__)


class IncidentSeverity(Enum):
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆé‡è¦åº¦"""

    CRITICAL = "critical"  # ğŸ‰ å¤é¾ã®è¦šé†’ (ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“éšœå®³)
    HIGH = "high"  # ğŸ’€ ã‚¹ã‚±ãƒ«ãƒˆãƒ³è»å›£ (é‡è¦ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢)
    MEDIUM = "medium"  # âš”ï¸ ã‚ªãƒ¼ã‚¯ã®å¤§è» (è¤‡æ•°éšœå®³)
    LOW = "low"  # ğŸ‘¹ ã‚´ãƒ–ãƒªãƒ³ã®å°ç´°å·¥ (è¨­å®šãƒŸã‚¹)
    MINOR = "minor"  # ğŸ§šâ€â™€ï¸ å¦–ç²¾ã®æ‚ªæˆ¯ (è»½å¾®ãƒã‚°)


class IncidentCategory(Enum):
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆåˆ†é¡"""

    MEMORY_LEAK = "memory_leak"  # ğŸŒŠ ã‚¹ãƒ©ã‚¤ãƒ ã®å¢—æ®–
    INFINITE_LOOP = "infinite_loop"  # ğŸ—¿ ã‚´ãƒ¼ãƒ¬ãƒ ã®æš´èµ°
    DEADLOCK = "deadlock"  # ğŸ•·ï¸ ã‚¯ãƒ¢ã®å·£
    PROCESS_CRASH = "process_crash"  # ğŸ§Ÿâ€â™‚ï¸ ã‚¾ãƒ³ãƒ“ã®ä¾µå…¥
    CONFIG_ERROR = "config_error"  # ğŸ‘¹ ã‚´ãƒ–ãƒªãƒ³ã®å°ç´°å·¥
    NETWORK_ISSUE = "network_issue"  # ğŸŒ©ï¸ åµã®å¦¨å®³
    DATABASE_ERROR = "database_error"  # ğŸ’ ã‚¯ãƒªã‚¹ã‚¿ãƒ«ã®ç ´æ
    PERMISSION_DENIED = "permission_denied"  # ğŸ›¡ï¸ çµç•Œã®æ‹’çµ¶


@dataclass
class IncidentVectorMetadata:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""

    incident_id: str
    incident_type: IncidentCategory
    severity: IncidentSeverity
    error_patterns: List[str] = field(default_factory=list)
    stack_trace_patterns: List[str] = field(default_factory=list)
    resolution_patterns: List[str] = field(default_factory=list)
    affected_systems: List[str] = field(default_factory=list)
    resolution_time_minutes: Optional[float] = None
    recurrence_count: int = 0
    related_incidents: List[str] = field(default_factory=list)


@dataclass
class IncidentVectorDimensions:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒå®šç¾©"""

    error_message: int = 512  # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®åŸ‹ã‚è¾¼ã¿
    stack_trace: int = 384  # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹æƒ…å ±
    system_context: int = 256  # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³ãƒ»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    resolution_steps: int = 384  # è§£æ±ºæ‰‹é †ãƒ»å¯¾ç­–
    prevention_measures: int = 256  # äºˆé˜²ç­–ãƒ»æ”¹å–„ç‚¹


class IncidentSageGrimoireVectorization:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…é­”æ³•æ›¸ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(
        self,
        database_url: str = "postgresql://aicompany@localhost:5432/ai_company_grimoire",
    ):
        self.database_url = database_url
        self.logger = logging.getLogger(__name__)

        # ã‚¨ãƒ«ãƒ€ãƒ¼éšå±¤ã¸ã®æ•¬æ„
        self.logger.info("ğŸ›¡ï¸ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ã«ã‚ˆã‚‹å±æ©Ÿå¯¾å¿œã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        self.logger.info("ğŸ›ï¸ ã‚°ãƒ©ãƒ³ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼maru â†’ ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼ã®æŒ‡ç¤ºä¸‹ã§å®Ÿè¡Œ")

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.grimoire_db = None
        self.vector_search = None
        self.evolution_engine = None
        self.four_sages = None

        # ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ
        self.dimensions = IncidentVectorDimensions()
        self.total_dimensions = (
            self.dimensions.error_message
            + self.dimensions.stack_trace
            + self.dimensions.system_context
            + self.dimensions.resolution_steps
            + self.dimensions.prevention_measures
        )

        # æ¤œç´¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.pattern_cache = {}
        self.similarity_cache = {}

        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆçµ±è¨ˆ
        self.stats = {
            "incidents_vectorized": 0,
            "pattern_searches": 0,
            "resolutions_provided": 0,
            "preventions_applied": 0,
            "crisis_averted": 0,
        }

        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æ­£è¦è¡¨ç¾
        self.error_patterns = {
            IncidentCategory.MEMORY_LEAK: [
                r"out of memory",
                r"memory leak",
                r"oom killed",
                r"heap.*full",
            ],
            IncidentCategory.INFINITE_LOOP: [
                r"infinite loop",
                r"stack overflow",
                r"recursion limit",
                r"timeout",
            ],
            IncidentCategory.DEADLOCK: [
                r"deadlock",
                r"lock.*timeout",
                r"circular dependency",
                r"blocked",
            ],
            IncidentCategory.PROCESS_CRASH: [
                r"segmentation fault",
                r"core dumped",
                r"crash",
                r"terminated",
            ],
            IncidentCategory.CONFIG_ERROR: [
                r"config.*error",
                r"invalid.*setting",
                r"missing.*parameter",
            ],
            IncidentCategory.NETWORK_ISSUE: [
                r"connection.*refused",
                r"timeout",
                r"network.*unreachable",
            ],
            IncidentCategory.DATABASE_ERROR: [
                r"database.*error",
                r"sql.*error",
                r"connection.*failed",
            ],
            IncidentCategory.PERMISSION_DENIED: [
                r"permission.*denied",
                r"access.*forbidden",
                r"unauthorized",
            ],
        }

    async def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆã‚¨ãƒ«ãƒ€ãƒ¼æ‰¿èªå¿…é ˆï¼‰"""
        try:
            self.logger.info("ğŸ›ï¸ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")

            # é­”æ³•æ›¸ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
            self.grimoire_db = GrimoireDatabase(self.database_url)
            await self.grimoire_db.initialize()

            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
            self.vector_search = GrimoireVectorSearch(database=self.grimoire_db)
            await self.vector_search.initialize()

            # é€²åŒ–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
            self.evolution_engine = EvolutionEngine(database=self.grimoire_db)
            await self.evolution_engine.initialize()

            # 4è³¢è€…çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
            self.four_sages = FourSagesIntegration()

            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…å°‚ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
            await self._create_incident_sage_tables()

            self.logger.info("âœ… ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…é­”æ³•æ›¸ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            self.logger.info("ğŸ›¡ï¸ å±æ©Ÿå¯¾å¿œæº–å‚™å®Œäº† - ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã®æŒ‡ç¤ºã‚’å¾…æ©Ÿä¸­")

        except Exception as e:
            self.logger.error(f"âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…åˆæœŸåŒ–å¤±æ•—: {e}")
            self.logger.error("ğŸ›ï¸ ã‚°ãƒ©ãƒ³ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼maruã¸ã®ç·Šæ€¥å ±å‘ŠãŒå¿…è¦")
            raise

    async def _create_incident_sage_tables(self):
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…å°‚ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ"""
        try:
            async with self.grimoire_db.connection_pool.acquire() as conn:
                # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«
                await conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS incident_patterns (
                        id SERIAL PRIMARY KEY,
                        incident_id TEXT NOT NULL,
                        pattern_type TEXT NOT NULL,
                        pattern_text TEXT NOT NULL,
                        severity VARCHAR(20),
                        category VARCHAR(50),
                        occurrence_count INTEGER DEFAULT 1,
                        first_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(incident_id, pattern_type, pattern_text)
                    )
                """
                )

                # è§£æ±ºç­–å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
                await conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS incident_resolutions (
                        id SERIAL PRIMARY KEY,
                        incident_id TEXT NOT NULL,
                        resolution_text TEXT NOT NULL,
                        resolution_steps JSONB,
                        success_rate FLOAT DEFAULT 0.0,
                        resolution_time_minutes FLOAT,
                        applied_by VARCHAR(100),
                        applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        effectiveness_score FLOAT DEFAULT 0.0
                    )
                """
                )

                # äºˆé˜²ç­–ãƒ†ãƒ¼ãƒ–ãƒ«
                await conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS incident_preventions (
                        id SERIAL PRIMARY KEY,
                        incident_category VARCHAR(50) NOT NULL,
                        prevention_measure TEXT NOT NULL,
                        implementation_difficulty VARCHAR(20),
                        effectiveness_rating FLOAT DEFAULT 0.0,
                        implementation_count INTEGER DEFAULT 0,
                        success_count INTEGER DEFAULT 0,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """
                )

                # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆé–¢é€£æ€§ãƒ†ãƒ¼ãƒ–ãƒ«
                await conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS incident_relationships (
                        id SERIAL PRIMARY KEY,
                        incident_id_1 TEXT NOT NULL,
                        incident_id_2 TEXT NOT NULL,
                        relationship_type VARCHAR(50),
                        similarity_score FLOAT,
                        relationship_strength FLOAT DEFAULT 0.0,
                        identified_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(incident_id_1, incident_id_2)
                    )
                """
                )

            self.logger.info("ğŸ›¡ï¸ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…å°‚ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†")

        except Exception as e:
            self.logger.error(f"âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå¤±æ•—: {e}")
            raise

    async def vectorize_incident(self, incident_data: Dict[str, Any]) -> str:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        try:
            self.logger.info(
                f"ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆåˆ†æé–‹å§‹: {incident_data.get('title', 'Unknown')}"
            )

            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æº–å‚™
            incident_metadata = await self._analyze_incident_metadata(incident_data)

            # è¤‡åˆãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
            incident_vector = await self._generate_incident_vector(
                incident_data, incident_metadata
            )

            # é­”æ³•æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            spell_metadata = SpellMetadata(
                id=str(uuid.uuid4()),
                spell_name=f"incident_{incident_metadata.incident_id}",
                content=json.dumps(incident_data),
                spell_type=SpellType.REFERENCE,  # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã¯å‚ç…§ç³»
                magic_school=MagicSchool.CRISIS_SAGE,
                tags=[
                    "incident_sage",
                    incident_metadata.incident_type.value,
                    incident_metadata.severity.value,
                ],
                power_level=self._severity_to_power_level(incident_metadata.severity),
                casting_frequency=incident_metadata.recurrence_count,
                last_cast_at=None,
                is_eternal=incident_metadata.severity
                in [IncidentSeverity.CRITICAL, IncidentSeverity.HIGH],
                evolution_history=[],
                created_at=datetime.now(timezone.utc),
                updated_at=datetime.now(timezone.utc),
                version=1,
            )

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            spell_id = await self.grimoire_db.create_spell(
                {
                    "spell_name": spell_metadata.spell_name,
                    "content": spell_metadata.content,
                    "spell_type": spell_metadata.spell_type.value,
                    "magic_school": spell_metadata.magic_school.value,
                    "tags": spell_metadata.tags,
                    "power_level": spell_metadata.power_level,
                    "is_eternal": spell_metadata.is_eternal,
                }
            )

            # ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ 
            await self.vector_search.index_spell(spell_id, spell_metadata.__dict__)

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜
            await self._save_incident_patterns(incident_metadata)

            # è§£æ±ºç­–ä¿å­˜
            if incident_data.get("resolution"):
                await self._save_incident_resolution(
                    incident_metadata, incident_data["resolution"]
                )

            # çµ±è¨ˆæ›´æ–°
            self.stats["incidents_vectorized"] += 1

            self.logger.info(f"âœ… ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆé­”æ³•æ›¸åŒ–å®Œäº†: {spell_id}")
            return spell_id

        except Exception as e:
            self.logger.error(f"âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«åŒ–å¤±æ•—: {e}")
            raise

    async def _analyze_incident_metadata(
        self, incident_data: Dict[str, Any]
    ) -> IncidentVectorMetadata:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ"""
        incident_id = incident_data.get("id", f"incident_{datetime.now().timestamp()}")

        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªæ¨å®š
        error_message = incident_data.get("error_message", "").lower()
        incident_type = await self._classify_incident_type(error_message)

        # é‡è¦åº¦æ¨å®š
        severity = await self._assess_incident_severity(incident_data)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
        error_patterns = await self._extract_error_patterns(incident_data)
        stack_patterns = await self._extract_stack_trace_patterns(incident_data)
        resolution_patterns = await self._extract_resolution_patterns(incident_data)

        return IncidentVectorMetadata(
            incident_id=incident_id,
            incident_type=incident_type,
            severity=severity,
            error_patterns=error_patterns,
            stack_trace_patterns=stack_patterns,
            resolution_patterns=resolution_patterns,
            affected_systems=incident_data.get("affected_systems", []),
            resolution_time_minutes=incident_data.get("resolution_time_minutes"),
            recurrence_count=incident_data.get("recurrence_count", 0),
        )

    async def _classify_incident_type(self, error_message: str) -> IncidentCategory:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ†é¡"""
        for category, patterns in self.error_patterns.items():
            for pattern in patterns:
                if re.search(pattern, error_message, re.IGNORECASE):
                    return category

        return IncidentCategory.CONFIG_ERROR  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    async def _assess_incident_severity(
        self, incident_data: Dict[str, Any]
    ) -> IncidentSeverity:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆé‡è¦åº¦è©•ä¾¡"""
        severity_indicators = {
            IncidentSeverity.CRITICAL: [
                "system down",
                "complete failure",
                "total outage",
                "critical",
                "production halt",
                "data loss",
                "security breach",
            ],
            IncidentSeverity.HIGH: [
                "service unavailable",
                "major functionality",
                "multiple users",
                "performance degradation",
                "important feature",
            ],
            IncidentSeverity.MEDIUM: [
                "some users affected",
                "partial functionality",
                "moderate impact",
                "workaround available",
            ],
            IncidentSeverity.LOW: [
                "minor issue",
                "cosmetic",
                "single user",
                "low priority",
            ],
        }

        description = (
            incident_data.get("description", "")
            + " "
            + incident_data.get("error_message", "")
        ).lower()

        for severity, indicators in severity_indicators.items():
            for indicator in indicators:
                if indicator in description:
                    return severity

        return IncidentSeverity.MINOR

    def _severity_to_power_level(self, severity: IncidentSeverity) -> int:
        """é‡è¦åº¦ã‚’ãƒ‘ãƒ¯ãƒ¼ãƒ¬ãƒ™ãƒ«ã«å¤‰æ›"""
        severity_map = {
            IncidentSeverity.CRITICAL: 10,
            IncidentSeverity.HIGH: 8,
            IncidentSeverity.MEDIUM: 6,
            IncidentSeverity.LOW: 4,
            IncidentSeverity.MINOR: 2,
        }
        return severity_map.get(severity, 1)

    async def _extract_error_patterns(self, incident_data: Dict[str, Any]) -> List[str]:
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        patterns = []

        error_message = incident_data.get("error_message", "")
        if error_message:
            # ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
            patterns.extend(re.findall(r"\w+Error", error_message))
            patterns.extend(re.findall(r"\w+Exception", error_message))
            patterns.extend(re.findall(r"HTTP \d{3}", error_message))

        return list(set(patterns))

    async def _extract_stack_trace_patterns(
        self, incident_data: Dict[str, Any]
    ) -> List[str]:
        """ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        patterns = []

        stack_trace = incident_data.get("stack_trace", "")
        if stack_trace:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ¡ã‚½ãƒƒãƒ‰åã‚’æŠ½å‡º
            patterns.extend(re.findall(r"at\s+(\S+)", stack_trace))
            patterns.extend(re.findall(r'File\s+"([^"]+)"', stack_trace))

        return list(set(patterns))

    async def _extract_resolution_patterns(
        self, incident_data: Dict[str, Any]
    ) -> List[str]:
        """è§£æ±ºãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        patterns = []

        resolution = incident_data.get("resolution", {})
        if isinstance(resolution, dict):
            steps = resolution.get("steps", [])
            for step in steps:
                if isinstance(step, str):
                    # è§£æ±ºæ‰‹é †ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                    patterns.extend(
                        re.findall(
                            r"\b(restart|reload|reset|update|fix|patch)\b", step.lower()
                        )
                    )

        return list(set(patterns))

    async def _generate_incident_vector(
        self, incident_data: Dict[str, Any], metadata: IncidentVectorMetadata
    ) -> np.ndarray:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        try:
            # å„æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
            error_vec = await self._generate_error_message_vector(
                incident_data.get("error_message", "")
            )

            stack_vec = await self._generate_stack_trace_vector(
                incident_data.get("stack_trace", "")
            )

            context_vec = await self._generate_system_context_vector(
                incident_data, metadata
            )

            resolution_vec = await self._generate_resolution_vector(
                incident_data.get("resolution", {})
            )

            prevention_vec = await self._generate_prevention_vector(
                incident_data.get("prevention_measures", [])
            )

            # è¤‡åˆãƒ™ã‚¯ãƒˆãƒ«çµåˆ
            combined_vector = np.concatenate(
                [error_vec, stack_vec, context_vec, resolution_vec, prevention_vec]
            )

            # æ­£è¦åŒ–
            norm = np.linalg.norm(combined_vector)
            if norm > 0:
                combined_vector = combined_vector / norm

            return combined_vector

        except Exception as e:
            self.logger.error(f"âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆå¤±æ•—: {e}")
            raise

    async def _generate_error_message_vector(self, error_message: str) -> np.ndarray:
        """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯OpenAI APIã‚’ä½¿ç”¨
        # ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼å®Ÿè£…
        vector = np.random.randn(self.dimensions.error_message)

        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç‰¹å¾´ã‚’åæ˜ 
        if "memory" in error_message.lower():
            vector[0] = 1.0
        if "connection" in error_message.lower():
            vector[1] = 1.0
        if "timeout" in error_message.lower():
            vector[2] = 1.0

        return vector / np.linalg.norm(vector) if np.linalg.norm(vector) > 0 else vector

    async def _generate_stack_trace_vector(self, stack_trace: str) -> np.ndarray:
        """ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        vector = np.zeros(self.dimensions.stack_trace)

        if stack_trace:
            # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã®æ·±ã•
            lines = stack_trace.count("\n")
            vector[0] = min(lines / 20.0, 1.0)

            # ãƒ•ã‚¡ã‚¤ãƒ«æ•°
            files = len(re.findall(r'File\s+"[^"]+"', stack_trace))
            vector[1] = min(files / 10.0, 1.0)

            # ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³
            if "recursion" in stack_trace.lower():
                vector[2] = 1.0

        return vector

    async def _generate_system_context_vector(
        self, incident_data: Dict[str, Any], metadata: IncidentVectorMetadata
    ) -> np.ndarray:
        """ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        vector = np.zeros(self.dimensions.system_context)

        # é‡è¦åº¦
        vector[0] = self._severity_to_power_level(metadata.severity) / 10.0

        # å½±éŸ¿ã‚·ã‚¹ãƒ†ãƒ æ•°
        affected_count = len(metadata.affected_systems)
        vector[1] = min(affected_count / 5.0, 1.0)

        # å†ç™ºå›æ•°
        vector[2] = min(metadata.recurrence_count / 10.0, 1.0)

        # è§£æ±ºæ™‚é–“ï¼ˆæ™‚é–“å˜ä½ï¼‰
        if metadata.resolution_time_minutes:
            vector[3] = min(
                metadata.resolution_time_minutes / 1440.0, 1.0
            )  # 24æ™‚é–“ã§æ­£è¦åŒ–

        return vector

    async def _generate_resolution_vector(
        self, resolution: Dict[str, Any]
    ) -> np.ndarray:
        """è§£æ±ºæ‰‹é †ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        vector = np.zeros(self.dimensions.resolution_steps)

        if resolution:
            steps = resolution.get("steps", [])
            vector[0] = min(len(steps) / 10.0, 1.0)  # ã‚¹ãƒ†ãƒƒãƒ—æ•°

            # è§£æ±ºæ‰‹é †ã®ç¨®é¡
            if any("restart" in str(step).lower() for step in steps):
                vector[1] = 1.0
            if any("config" in str(step).lower() for step in steps):
                vector[2] = 1.0
            if any("update" in str(step).lower() for step in steps):
                vector[3] = 1.0

        return vector

    async def _generate_prevention_vector(self, preventions: List[str]) -> np.ndarray:
        """äºˆé˜²ç­–ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        vector = np.zeros(self.dimensions.prevention_measures)

        if preventions:
            vector[0] = min(len(preventions) / 5.0, 1.0)  # äºˆé˜²ç­–æ•°

            # äºˆé˜²ç­–ã®ç¨®é¡
            prevention_text = " ".join(preventions).lower()
            if "monitoring" in prevention_text:
                vector[1] = 1.0
            if "backup" in prevention_text:
                vector[2] = 1.0
            if "testing" in prevention_text:
                vector[3] = 1.0

        return vector

    async def search_similar_incidents(
        self,
        query: Union[str, Dict[str, Any]],
        limit: int = 10,
        filters: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """é¡ä¼¼ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œç´¢"""
        try:
            self.stats["pattern_searches"] += 1

            # æ¤œç´¢ã‚¯ã‚¨ãƒªä½œæˆ
            search_query = SearchQuery(
                query_text=query if isinstance(query, str) else json.dumps(query),
                limit=limit,
                filters=filters,
            )

            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
            results = await self.vector_search.search(search_query)

            # çµæœã‚’æ‹¡å¼µæƒ…å ±ä»˜ãã§è¿”ã™
            enhanced_results = []
            for result in results:
                incident_data = json.loads(result.content)

                enhanced_results.append(
                    {
                        "incident_id": incident_data.get("id"),
                        "similarity_score": result.similarity_score,
                        "incident_data": incident_data,
                        "severity": (
                            result.tags[2] if len(result.tags) > 2 else "unknown"
                        ),
                        "category": (
                            result.tags[1] if len(result.tags) > 1 else "unknown"
                        ),
                        "spell_id": result.spell_id,
                        "resolution_available": bool(incident_data.get("resolution")),
                    }
                )

            return enhanced_results

        except Exception as e:
            self.logger.error(f"âŒ é¡ä¼¼ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œç´¢å¤±æ•—: {e}")
            return []

    async def predict_incident_prevention(
        self, system_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆå…†æ¤œçŸ¥ãƒ»äºˆé˜²ç­–ææ¡ˆ"""
        try:
            self.logger.info("ğŸ”® ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆå…†åˆ†æé–‹å§‹")

            # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³åˆ†æ
            risk_analysis = await self._analyze_system_risk(system_context)

            # é¡ä¼¼éå»ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œç´¢
            similar_incidents = await self.search_similar_incidents(
                json.dumps(system_context), limit=5
            )

            # äºˆé˜²ç­–ç”Ÿæˆ
            prevention_measures = await self._generate_prevention_measures(
                risk_analysis, similar_incidents
            )

            # 4è³¢è€…çµ±åˆç›¸è«‡
            sage_consultation = await self._consult_other_sages_for_prevention(
                system_context, risk_analysis
            )

            result = {
                "risk_level": risk_analysis["overall_risk"],
                "predicted_incidents": risk_analysis["potential_incidents"],
                "prevention_measures": prevention_measures,
                "similar_past_incidents": similar_incidents,
                "sage_consultation": sage_consultation,
                "monitoring_recommendations": await self._generate_monitoring_recommendations(
                    risk_analysis
                ),
                "immediate_actions": await self._generate_immediate_actions(
                    risk_analysis
                ),
            }

            self.stats["preventions_applied"] += 1

            if risk_analysis["overall_risk"] < 0.3:
                self.stats["crisis_averted"] += 1
                self.logger.info("âœ… å±æ©Ÿå›é¿æˆåŠŸ - ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã¸ã®å ±å‘Šå®Œäº†")

            return result

        except Exception as e:
            self.logger.error(f"âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆå…†æ¤œçŸ¥å¤±æ•—: {e}")
            raise

    async def provide_incident_resolution(
        self, incident_description: str, context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè§£æ±ºç­–æä¾›"""
        try:
            self.logger.info("ğŸš‘ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè§£æ±ºç­–åˆ†æé–‹å§‹")

            # é¡ä¼¼ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œç´¢
            similar_incidents = await self.search_similar_incidents(
                incident_description, limit=3
            )

            # è§£æ±ºç­–çµ±åˆ
            resolution_strategies = []
            for incident in similar_incidents:
                if incident["resolution_available"]:
                    strategy = await self._extract_resolution_strategy(incident)
                    if strategy:
                        resolution_strategies.append(strategy)

            # æœ€é©è§£æ±ºç­–é¸æŠ
            best_resolution = await self._select_best_resolution(
                resolution_strategies, context
            )

            # 4è³¢è€…çµ±åˆç›¸è«‡
            sage_guidance = await self._consult_sages_for_resolution(
                incident_description, context, best_resolution
            )

            result = {
                "recommended_resolution": best_resolution,
                "alternative_resolutions": resolution_strategies,
                "confidence_score": await self._calculate_resolution_confidence(
                    best_resolution
                ),
                "estimated_resolution_time": await self._estimate_resolution_time(
                    best_resolution
                ),
                "risk_assessment": await self._assess_resolution_risk(best_resolution),
                "sage_guidance": sage_guidance,
                "follow_up_actions": await self._generate_follow_up_actions(
                    best_resolution
                ),
            }

            self.stats["resolutions_provided"] += 1

            return result

        except Exception as e:
            self.logger.error(f"âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè§£æ±ºç­–æä¾›å¤±æ•—: {e}")
            raise

    async def _save_incident_patterns(self, metadata: IncidentVectorMetadata):
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜"""
        async with self.grimoire_db.connection_pool.acquire() as conn:
            # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜
            for pattern in metadata.error_patterns:
                await conn.execute(
                    """
                    INSERT INTO incident_patterns
                    (incident_id, pattern_type, pattern_text, severity, category)
                    VALUES ($1, $2, $3, $4, $5)
                    ON CONFLICT (incident_id, pattern_type, pattern_text)
                    DO UPDATE SET occurrence_count = incident_patterns.occurrence_count + 1,
                                  last_seen = CURRENT_TIMESTAMP
                """,
                    metadata.incident_id,
                    "error",
                    pattern,
                    metadata.severity.value,
                    metadata.incident_type.value,
                )

    async def _save_incident_resolution(
        self, metadata: IncidentVectorMetadata, resolution: Dict[str, Any]
    ):
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè§£æ±ºç­–ä¿å­˜"""
        async with self.grimoire_db.connection_pool.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO incident_resolutions
                (incident_id, resolution_text, resolution_steps, resolution_time_minutes)
                VALUES ($1, $2, $3, $4)
            """,
                metadata.incident_id,
                json.dumps(resolution),
                json.dumps(resolution.get("steps", [])),
                metadata.resolution_time_minutes,
            )

    # äºˆå…†æ¤œçŸ¥ãƒ»äºˆé˜²ç­–é–¢é€£ãƒ¡ã‚½ãƒƒãƒ‰
    async def _analyze_system_risk(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚¹ã‚¯åˆ†æ"""
        risk_factors = []
        risk_score = 0.0

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
        memory_usage = context.get("memory_usage_percent", 0)
        if memory_usage > 90:
            risk_factors.append("Critical memory usage")
            risk_score += 0.4
        elif memory_usage > 80:
            risk_factors.append("High memory usage")
            risk_score += 0.2

        # CPUä½¿ç”¨ç‡
        cpu_usage = context.get("cpu_usage_percent", 0)
        if cpu_usage > 95:
            risk_factors.append("Critical CPU usage")
            risk_score += 0.3

        # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡
        disk_usage = context.get("disk_usage_percent", 0)
        if disk_usage > 95:
            risk_factors.append("Critical disk space")
            risk_score += 0.3

        return {
            "overall_risk": min(risk_score, 1.0),
            "risk_factors": risk_factors,
            "potential_incidents": await self._predict_potential_incidents(context),
        }

    async def _predict_potential_incidents(
        self, context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æ½œåœ¨çš„ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬"""
        predictions = []

        memory_usage = context.get("memory_usage_percent", 0)
        if memory_usage > 85:
            predictions.append(
                {
                    "type": IncidentCategory.MEMORY_LEAK.value,
                    "probability": min((memory_usage - 85) / 15, 1.0),
                    "estimated_time_to_incident": f"{int((100 - memory_usage) * 2)} minutes",
                }
            )

        return predictions

    async def _generate_prevention_measures(
        self, risk_analysis: Dict[str, Any], similar_incidents: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """äºˆé˜²ç­–ç”Ÿæˆ"""
        measures = []

        for risk_factor in risk_analysis["risk_factors"]:
            if "memory" in risk_factor.lower():
                measures.append(
                    {
                        "measure": "Implement memory monitoring and cleanup",
                        "priority": "high",
                        "effort": "medium",
                    }
                )
            elif "cpu" in risk_factor.lower():
                measures.append(
                    {
                        "measure": "Scale resources or optimize processes",
                        "priority": "high",
                        "effort": "high",
                    }
                )

        return measures

    async def _consult_other_sages_for_prevention(
        self, context: Dict[str, Any], risk_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä»–ã®è³¢è€…ã¸ã®äºˆé˜²ç­–ç›¸è«‡"""
        if self.four_sages:
            request = {
                "type": "prevention_consultation",
                "data": {
                    "system_context": context,
                    "risk_analysis": risk_analysis,
                    "requester": "incident_sage",
                },
            }

            response = await self.four_sages.coordinate_learning_session(request)

            return {
                "consultation_successful": response.get("consensus_reached", False),
                "prevention_insights": response.get("learning_outcome", {}),
                "sage_recommendations": response.get("individual_responses", {}),
            }

        return {"consultation_successful": False, "reason": "Four sages not available"}

    # ãã®ä»–ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆç°¡ç•¥åŒ–å®Ÿè£…ï¼‰
    async def _generate_monitoring_recommendations(
        self, risk_analysis: Dict[str, Any]
    ) -> List[str]:
        """ç›£è¦–æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        return [
            "Set up memory usage alerts at 85% threshold",
            "Monitor CPU utilization trends",
            "Implement automated health checks",
        ]

    async def _generate_immediate_actions(
        self, risk_analysis: Dict[str, Any]
    ) -> List[str]:
        """å³åº§å¯¾å¿œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        actions = []

        if risk_analysis["overall_risk"] > 0.7:
            actions.append("Alert operations team immediately")
            actions.append("Prepare rollback procedures")

        return actions

    async def _extract_resolution_strategy(
        self, incident: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """è§£æ±ºç­–æˆ¦ç•¥æŠ½å‡º"""
        incident_data = incident.get("incident_data", {})
        resolution = incident_data.get("resolution")

        if resolution:
            return {
                "strategy": resolution,
                "success_rate": 0.9,  # ç°¡ç•¥åŒ–
                "estimated_time": incident_data.get("resolution_time_minutes", 60),
            }

        return None

    async def _select_best_resolution(
        self, strategies: List[Dict[str, Any]], context: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """æœ€é©è§£æ±ºç­–é¸æŠ"""
        if not strategies:
            return None

        # æˆåŠŸç‡ã§é¸æŠï¼ˆç°¡ç•¥åŒ–ï¼‰
        return max(strategies, key=lambda s: s.get("success_rate", 0))

    async def _consult_sages_for_resolution(
        self, incident: str, context: Dict[str, Any], resolution: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è§£æ±ºç­–ã«ã¤ã„ã¦ä»–ã®è³¢è€…ã«ç›¸è«‡"""
        if self.four_sages:
            request = {
                "type": "resolution_consultation",
                "data": {
                    "incident_description": incident,
                    "context": context,
                    "proposed_resolution": resolution,
                    "requester": "incident_sage",
                },
            }

            response = await self.four_sages.coordinate_learning_session(request)

            return {
                "consultation_successful": response.get("consensus_reached", False),
                "resolution_validation": response.get("learning_outcome", {}),
                "sage_feedback": response.get("individual_responses", {}),
            }

        return {"consultation_successful": False}

    async def _calculate_resolution_confidence(
        self, resolution: Dict[str, Any]
    ) -> float:
        """è§£æ±ºç­–ä¿¡é ¼åº¦è¨ˆç®—"""
        return resolution.get("success_rate", 0.5) if resolution else 0.0

    async def _estimate_resolution_time(self, resolution: Dict[str, Any]) -> int:
        """è§£æ±ºæ™‚é–“æ¨å®š"""
        return resolution.get("estimated_time", 60) if resolution else 60

    async def _assess_resolution_risk(
        self, resolution: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è§£æ±ºç­–ãƒªã‚¹ã‚¯è©•ä¾¡"""
        return {
            "risk_level": "low",
            "potential_side_effects": [],
            "rollback_required": False,
        }

    async def _generate_follow_up_actions(
        self, resolution: Dict[str, Any]
    ) -> List[str]:
        """ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        return [
            "Monitor system stability after resolution",
            "Document lessons learned",
            "Update prevention measures",
        ]

    async def get_statistics(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            "system_stats": self.stats.copy(),
            "cache_stats": {
                "pattern_cache_size": len(self.pattern_cache),
                "similarity_cache_size": len(self.similarity_cache),
            },
            "vector_dimensions": {
                "total": self.total_dimensions,
                "breakdown": {
                    "error_message": self.dimensions.error_message,
                    "stack_trace": self.dimensions.stack_trace,
                    "system_context": self.dimensions.system_context,
                    "resolution_steps": self.dimensions.resolution_steps,
                    "prevention_measures": self.dimensions.prevention_measures,
                },
            },
            "incident_categories": [category.value for category in IncidentCategory],
            "severity_levels": [severity.value for severity in IncidentSeverity],
        }

    async def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if self.grimoire_db:
                await self.grimoire_db.close()

            self.logger.info("ğŸ›¡ï¸ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
            self.logger.info("ğŸ›ï¸ ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã¸ã®æœ€çµ‚å ±å‘Šå®Œäº†")

        except Exception as e:
            self.logger.error(f"âŒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
async def test_incident_sage_system():
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    incident_sage = IncidentSageGrimoireVectorization()

    try:
        await incident_sage.initialize()

        # ãƒ†ã‚¹ãƒˆã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ
        test_incident = {
            "id": "test_incident_001",
            "title": "Memory leak in worker process",
            "description": "Worker process consuming excessive memory",
            "error_message": "OutOfMemoryError: Java heap space",
            "stack_trace": "at java.util.ArrayList.grow(ArrayList.java:267)",
            "affected_systems": ["worker-service", "api-gateway"],
            "severity": "high",
            "resolution": {
                "steps": [
                    "Restart worker service",
                    "Increase heap size",
                    "Add memory monitoring",
                ]
            },
            "resolution_time_minutes": 45,
        }

        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«åŒ–
        spell_id = await incident_sage.vectorize_incident(test_incident)
        print(f"âœ… Test incident vectorized: {spell_id}")

        # é¡ä¼¼ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œç´¢
        similar = await incident_sage.search_similar_incidents(
            "OutOfMemoryError in worker", limit=3
        )
        print(f"âœ… Found {len(similar)} similar incidents")

        # çµ±è¨ˆæƒ…å ±
        stats = await incident_sage.get_statistics()
        print(f"âœ… System stats: {stats['system_stats']}")

    finally:
        await incident_sage.cleanup()


if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s [%(name)s] %(levelname)s: %(message)s"
    )

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(test_incident_sage_system())
