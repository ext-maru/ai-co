#!/usr/bin/env python3
"""
Knowledge Sage Grimoire Vectorization System
ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…é­”æ³•æ›¸ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚¹ãƒ†ãƒ 

çŸ¥è­˜ã®é¡ä¼¼æ€§æ¤œç´¢ã€çŸ¥æµã®ç¶™æ‰¿ã€çŸ¥è­˜é€²åŒ–ã®è¿½è·¡ã‚’å®Ÿç¾
ã‚°ãƒ©ãƒ³ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼maruç›´å±ã®ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼ã«ã‚ˆã‚‹æ…é‡ãªå®Ÿè£…
"""

import sys
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import asyncio
import json
import logging
import re
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np

from libs.four_sages_integration import FourSagesIntegration
from libs.grimoire_database import (
    GrimoireDatabase,
    MagicSchool,
    SpellMetadata,
    SpellType,
)
from libs.grimoire_spell_evolution import EvolutionEngine, EvolutionType
from libs.grimoire_vector_search import GrimoireVectorSearch, SearchQuery, SearchResult
from libs.knowledge_base_manager import KnowledgeBaseManager

logger = logging.getLogger(__name__)


class KnowledgeType(Enum):
    """çŸ¥è­˜ã‚¿ã‚¤ãƒ—åˆ†é¡"""

    TECHNICAL = "technical"  # ğŸ”§ æŠ€è¡“çš„çŸ¥è­˜ (å®Ÿè£…æ–¹æ³•ã€APIä»•æ§˜)
    PROCEDURAL = "procedural"  # ğŸ“‹ æ‰‹é †ãƒ»ãƒ—ãƒ­ã‚»ã‚¹çŸ¥è­˜ (TDDã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼)
    CONCEPTUAL = "conceptual"  # ğŸ’¡ æ¦‚å¿µçš„çŸ¥è­˜ (è¨­è¨ˆæ€æƒ³ã€åŸç†)
    HISTORICAL = "historical"  # ğŸ“œ æ­´å²çš„çŸ¥è­˜ (éå»ã®æ±ºå®šã€çµŒç·¯)
    EXPERIENTIAL = "experiential"  # ğŸ¯ çµŒé¨“çš„çŸ¥è­˜ (å¤±æ•—è«‡ã€æ•™è¨“)


class KnowledgeDepth(Enum):
    """çŸ¥è­˜ã®æ·±ã•"""

    SURFACE = "surface"  # ğŸŒŠ è¡¨å±¤çš„çŸ¥è­˜ (æ¦‚è¦ã€ã‚µãƒãƒªãƒ¼)
    INTERMEDIATE = "intermediate"  # ğŸ”ï¸ ä¸­é–“çš„çŸ¥è­˜ (è©³ç´°èª¬æ˜)
    DEEP = "deep"  # ğŸ—ï¸ æ·±å±¤çš„çŸ¥è­˜ (å®Ÿè£…è©³ç´°ã€è¨­è¨ˆæ€æƒ³)
    EXPERT = "expert"  # ğŸ§™â€â™‚ï¸ å°‚é–€çš„çŸ¥è­˜ (ã‚¨ãƒ«ãƒ€ãƒ¼ç´šã®è‹±æ™º)


@dataclass
class KnowledgeVectorMetadata:
    """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""

    knowledge_id: str
    knowledge_type: KnowledgeType
    knowledge_depth: KnowledgeDepth
    source_file: str
    tags: List[str] = field(default_factory=list)
    related_concepts: List[str] = field(default_factory=list)
    prerequisite_knowledge: List[str] = field(default_factory=list)
    wisdom_level: float = 0.0  # 0.0-1.0 ã‚¨ãƒ«ãƒ€ãƒ¼è©•ä¾¡ã«ã‚ˆã‚‹çŸ¥æµãƒ¬ãƒ™ãƒ«
    creation_context: Optional[str] = None
    last_evolution: Optional[datetime] = None
    evolution_count: int = 0


@dataclass
class KnowledgeVectorDimensions:
    """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒå®šç¾©"""

    content_semantic: int = 768  # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿
    concept_relations: int = 256  # æ¦‚å¿µé–“é–¢ä¿‚æ€§
    procedural_steps: int = 256  # æ‰‹é †ãƒ»ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
    contextual_embedding: int = 384  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
    wisdom_evolution: int = 128  # çŸ¥æµã®é€²åŒ–å±¥æ­´


class KnowledgeSageGrimoireVectorization:
    """ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…é­”æ³•æ›¸ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(
        self,
        database_url: str = "postgresql://aicompany@localhost:5432/ai_company_grimoire",
    ):
        self.database_url = database_url
        self.logger = logging.getLogger(__name__)

        # ã‚¨ãƒ«ãƒ€ãƒ¼éšå±¤ã¸ã®æ•¬æ„
        self.logger.info("ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ã«ã‚ˆã‚‹çŸ¥è­˜ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        self.logger.info("ğŸ›ï¸ ã‚°ãƒ©ãƒ³ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼maru â†’ ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼ã®æŒ‡ç¤ºä¸‹ã§å®Ÿè¡Œ")

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.grimoire_db = None
        self.vector_search = None
        self.evolution_engine = None
        self.four_sages = None
        self.knowledge_manager = KnowledgeBaseManager()

        # ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ
        self.dimensions = KnowledgeVectorDimensions()
        self.total_dimensions = (
            self.dimensions.content_semantic
            + self.dimensions.concept_relations
            + self.dimensions.procedural_steps
            + self.dimensions.contextual_embedding
            + self.dimensions.wisdom_evolution
        )

        # çŸ¥è­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.knowledge_cache = {}
        self.wisdom_cache = {}

        # ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…çµ±è¨ˆ
        self.stats = {
            "knowledge_vectorized": 0,
            "wisdom_searches": 0,
            "concept_mappings": 0,
            "knowledge_evolutions": 0,
            "elder_consultations": 0,
        }

        # çŸ¥è­˜ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
        self.knowledge_patterns = {
            KnowledgeType.TECHNICAL: [
                r"def\s+\w+",
                r"class\s+\w+",
                r"import\s+\w+",
                r"pip install",
                r"npm\s+install",
                r"docker\s+run",
                r"git\s+\w+",
            ],
            KnowledgeType.PROCEDURAL: [
                r"step\s+\d+",
                r"phase\s+\d+",
                r"ãƒ—ãƒ­ã‚»ã‚¹",
                r"æ‰‹é †",
                r"workflow",
            ],
            KnowledgeType.CONCEPTUAL: [
                r"concept",
                r"principle",
                r"theory",
                r"paradigm",
                r"philosophy",
            ],
            KnowledgeType.HISTORICAL: [
                r"history",
                r"legacy",
                r"deprecated",
                r"migration",
                r"version",
            ],
            KnowledgeType.EXPERIENTIAL: [
                r"lesson learned",
                r"best practice",
                r"pitfall",
                r"gotcha",
                r"experience",
            ],
        }

    async def initialize(self):
        """ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ– - ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºæ¨™æº–API"""
        try:
            self.logger.info("ğŸ›ï¸ ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")

            # é­”æ³•æ›¸ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
            self.grimoire_db = GrimoireDatabase(self.database_url)
            await self.grimoire_db.initialize()

            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
            self.vector_search = GrimoireVectorSearch(database=self.grimoire_db)
            await self.vector_search.initialize()

            # é€²åŒ–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
            self.evolution_engine = EvolutionEngine(database=self.grimoire_db)
            await self.evolution_engine.initialize()

            # 4è³¢è€…çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
            self.four_sages = FourSagesIntegration()
            await self.four_sages.initialize()

            # ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…å°‚ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
            await self._create_knowledge_sage_tables()

            self.logger.info("âœ… ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            self.logger.info("ğŸ“š çŸ¥è­˜ã®ç¶™æ‰¿ã¨é€²åŒ–æº–å‚™å®Œäº† - ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã®è‹±æ™ºã‚’å¾…æ©Ÿä¸­")

        except Exception as e:
            self.logger.error(f"âŒ ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…åˆæœŸåŒ–å¤±æ•—: {e}")
            self.logger.error("ğŸ›ï¸ ã‚°ãƒ©ãƒ³ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼maruã¸ã®ç·Šæ€¥å ±å‘ŠãŒå¿…è¦")
            raise

    async def _create_knowledge_sage_tables(self):
        """ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…å°‚ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ"""
        try:
            async with self.grimoire_db.connection_pool.acquire() as conn:
                # çŸ¥è­˜é–¢é€£æ€§ãƒ†ãƒ¼ãƒ–ãƒ«
                await conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS knowledge_relations (
                        id SERIAL PRIMARY KEY,
                        source_knowledge_id TEXT NOT NULL,
                        target_knowledge_id TEXT NOT NULL,
                        relation_type VARCHAR(50),
                        strength FLOAT DEFAULT 0.0,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(source_knowledge_id, target_knowledge_id)
                    )
                """
                )

                # çŸ¥è­˜é€²åŒ–å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
                await conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS knowledge_evolution_history (
                        id SERIAL PRIMARY KEY,
                        knowledge_id TEXT NOT NULL,
                        evolution_type VARCHAR(50),
                        old_content TEXT,
                        new_content TEXT,
                        wisdom_delta FLOAT DEFAULT 0.0,
                        elder_approval BOOLEAN DEFAULT FALSE,
                        evolution_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """
                )

                # æ¦‚å¿µãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«
                await conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS concept_mappings (
                        id SERIAL PRIMARY KEY,
                        concept_name TEXT NOT NULL,
                        knowledge_ids TEXT[], -- Array of knowledge IDs
                        concept_vector FLOAT[], -- Vector representation
                        usage_frequency INTEGER DEFAULT 1,
                        last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """
                )

                # ã‚¨ãƒ«ãƒ€ãƒ¼çŸ¥æµè©•ä¾¡ãƒ†ãƒ¼ãƒ–ãƒ«
                await conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS elder_wisdom_evaluations (
                        id SERIAL PRIMARY KEY,
                        knowledge_id TEXT NOT NULL,
                        evaluator VARCHAR(100),
                        wisdom_score FLOAT,
                        evaluation_criteria TEXT,
                        comments TEXT,
                        evaluation_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """
                )

            self.logger.info("ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…å°‚ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†")

        except Exception as e:
            self.logger.error(f"âŒ ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå¤±æ•—: {e}")
            raise

    async def vectorize_knowledge(self, knowledge_data: Dict[str, Any]) -> str:
        """çŸ¥è­˜ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        try:
            self.logger.info(f"ğŸ“š çŸ¥è­˜åˆ†æé–‹å§‹: {knowledge_data.get('title', 'Unknown')}")

            # çŸ¥è­˜ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ
            knowledge_metadata = await self._analyze_knowledge_metadata(knowledge_data)

            # è¤‡åˆãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
            knowledge_vector = await self._generate_knowledge_vector(
                knowledge_data, knowledge_metadata
            )

            # é­”æ³•æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            spell_metadata = SpellMetadata(
                id=str(uuid.uuid4()),
                spell_name=f"knowledge_{knowledge_metadata.knowledge_id}",
                content=json.dumps(knowledge_data),
                spell_type=SpellType.REFERENCE,
                magic_school=MagicSchool.KNOWLEDGE_SAGE,
                tags=[
                    "knowledge_sage",
                    knowledge_metadata.knowledge_type.value,
                    knowledge_metadata.knowledge_depth.value,
                ]
                + knowledge_metadata.tags,
                power_level=self._wisdom_to_power_level(
                    knowledge_metadata.wisdom_level
                ),
                casting_frequency=0,
                last_cast_at=None,
                is_eternal=knowledge_metadata.knowledge_depth
                in [KnowledgeDepth.EXPERT, KnowledgeDepth.DEEP],
                evolution_history=[],
                created_at=datetime.now(timezone.utc),
                updated_at=datetime.now(timezone.utc),
                version=1,
            )

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            spell_id = await self.grimoire_db.create_spell(
                {
                    "metadata": spell_metadata,
                    "vector": knowledge_vector,
                    "knowledge_metadata": knowledge_metadata,
                }
            )

            # æ¦‚å¿µãƒãƒƒãƒ”ãƒ³ã‚°æ›´æ–°
            await self._update_concept_mappings(knowledge_metadata, spell_id)

            # çµ±è¨ˆæ›´æ–°
            self.stats["knowledge_vectorized"] += 1

            self.logger.info(f"âœ… çŸ¥è­˜ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†: {spell_id}")
            return spell_id

        except Exception as e:
            self.logger.error(f"âŒ çŸ¥è­˜ãƒ™ã‚¯ãƒˆãƒ«åŒ–å¤±æ•—: {e}")
            raise

    async def _analyze_knowledge_metadata(
        self, knowledge_data: Dict[str, Any]
    ) -> KnowledgeVectorMetadata:
        """çŸ¥è­˜ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ"""
        knowledge_id = knowledge_data.get("id", str(uuid.uuid4()))

        # çŸ¥è­˜ã‚¿ã‚¤ãƒ—åˆ†é¡
        knowledge_type = await self._classify_knowledge_type(
            knowledge_data.get("content", "")
        )

        # çŸ¥è­˜ã®æ·±ã•è©•ä¾¡
        knowledge_depth = await self._assess_knowledge_depth(knowledge_data)

        # ã‚¿ã‚°æŠ½å‡º
        tags = self._extract_knowledge_tags(knowledge_data)

        # é–¢é€£æ¦‚å¿µæŠ½å‡º
        related_concepts = await self._extract_related_concepts(knowledge_data)

        # çŸ¥æµãƒ¬ãƒ™ãƒ«è©•ä¾¡
        wisdom_level = await self._evaluate_wisdom_level(
            knowledge_data, knowledge_type, knowledge_depth
        )

        return KnowledgeVectorMetadata(
            knowledge_id=knowledge_id,
            knowledge_type=knowledge_type,
            knowledge_depth=knowledge_depth,
            source_file=knowledge_data.get("source_file", "unknown"),
            tags=tags,
            related_concepts=related_concepts,
            wisdom_level=wisdom_level,
            creation_context=knowledge_data.get("context", None),
            last_evolution=None,
            evolution_count=0,
        )

    async def _classify_knowledge_type(self, content: str) -> KnowledgeType:
        """çŸ¥è­˜ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        content_lower = content.lower()

        type_scores = {}
        for knowledge_type, patterns in self.knowledge_patterns.items():
            score = 0
            for pattern in patterns:
                matches = len(re.findall(pattern, content_lower, re.IGNORECASE))
                score += matches
            type_scores[knowledge_type] = score

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¿ã‚¤ãƒ—ã‚’è¿”ã™
        if type_scores:
            return max(type_scores, key=type_scores.get)
        return KnowledgeType.CONCEPTUAL

    async def _assess_knowledge_depth(
        self, knowledge_data: Dict[str, Any]
    ) -> KnowledgeDepth:
        """çŸ¥è­˜ã®æ·±ã•è©•ä¾¡"""
        content = knowledge_data.get("content", "")
        content_length = len(content)

        # ã‚³ãƒ¼ãƒ‰ä¾‹ã®æ•°
        code_blocks = len(re.findall(r"```.*?```", content, re.DOTALL))

        # è©³ç´°åº¦ã®æŒ‡æ¨™
        technical_terms = len(
            re.findall(
                r"\b(implementation|architecture|algorithm|optimization)\b",
                content,
                re.IGNORECASE,
            )
        )

        if code_blocks >= 3 or technical_terms >= 5 or content_length > 3000:
            return KnowledgeDepth.EXPERT
        elif code_blocks >= 2 or technical_terms >= 3 or content_length > 1500:
            return KnowledgeDepth.DEEP
        elif code_blocks >= 1 or technical_terms >= 1 or content_length > 500:
            return KnowledgeDepth.INTERMEDIATE
        else:
            return KnowledgeDepth.SURFACE

    def _extract_knowledge_tags(self, knowledge_data: Dict[str, Any]) -> List[str]:
        """çŸ¥è­˜ã‚¿ã‚°æŠ½å‡º"""
        tags = []
        content = knowledge_data.get("content", "").lower()

        # æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯æ¤œå‡º
        tech_keywords = [
            "python",
            "javascript",
            "docker",
            "postgresql",
            "react",
            "fastapi",
            "pytest",
        ]
        for keyword in tech_keywords:
            if keyword in content:
                tags.append(keyword)

        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰æ¨æ¸¬
        source_file = knowledge_data.get("source_file", "")
        if source_file.endswith(".py"):
            tags.append("python")
        elif source_file.endswith(".md"):
            tags.append("documentation")
        elif source_file.endswith(".json"):
            tags.append("configuration")

        return tags

    async def _extract_related_concepts(
        self, knowledge_data: Dict[str, Any]
    ) -> List[str]:
        """é–¢é€£æ¦‚å¿µæŠ½å‡º"""
        content = knowledge_data.get("content", "")

        # æ¦‚å¿µã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        concept_patterns = [
            r"\b([A-Z][a-z]+(?:[A-Z][a-z]+)*)\b",  # PascalCase
            r"\b(design pattern|best practice|anti-pattern)\b",
            r"\b(algorithm|data structure|architecture)\b",
        ]

        concepts = set()
        for pattern in concept_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            concepts.update(matches)

        return list(concepts)[:10]  # æœ€å¤§10å€‹

    async def _evaluate_wisdom_level(
        self,
        knowledge_data: Dict[str, Any],
        knowledge_type: KnowledgeType,
        knowledge_depth: KnowledgeDepth,
    ) -> float:
        """çŸ¥æµãƒ¬ãƒ™ãƒ«è©•ä¾¡"""
        base_score = 0.3

        # æ·±ã•ã«ã‚ˆã‚‹é‡ã¿
        depth_weights = {
            KnowledgeDepth.SURFACE: 0.1,
            KnowledgeDepth.INTERMEDIATE: 0.3,
            KnowledgeDepth.DEEP: 0.6,
            KnowledgeDepth.EXPERT: 1.0,
        }

        # ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹é‡ã¿
        type_weights = {
            KnowledgeType.EXPERIENTIAL: 0.9,  # çµŒé¨“çš„çŸ¥è­˜ã¯é«˜ä¾¡å€¤
            KnowledgeType.HISTORICAL: 0.8,
            KnowledgeType.CONCEPTUAL: 0.7,
            KnowledgeType.TECHNICAL: 0.6,
            KnowledgeType.PROCEDURAL: 0.5,
        }

        wisdom_score = (
            base_score
            + depth_weights.get(knowledge_depth, 0.3) * 0.4
            + type_weights.get(knowledge_type, 0.5) * 0.3
        )

        return min(wisdom_score, 1.0)

    def _wisdom_to_power_level(self, wisdom_level: float) -> int:
        """çŸ¥æµãƒ¬ãƒ™ãƒ«ã‚’ãƒ‘ãƒ¯ãƒ¼ãƒ¬ãƒ™ãƒ«ã«å¤‰æ›"""
        return int(wisdom_level * 100)

    async def _generate_knowledge_vector(
        self, knowledge_data: Dict[str, Any], metadata: KnowledgeVectorMetadata
    ) -> np.ndarray:
        """çŸ¥è­˜ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        # ç·æ¬¡å…ƒæ•°ã®ãƒ™ã‚¯ãƒˆãƒ«åˆæœŸåŒ–
        vector = np.zeros(self.total_dimensions)

        current_idx = 0

        # 1. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿
        content_vector = await self._generate_content_semantic_vector(
            knowledge_data.get("content", "")
        )
        vector[
            current_idx : current_idx + self.dimensions.content_semantic
        ] = content_vector
        current_idx += self.dimensions.content_semantic

        # 2. æ¦‚å¿µé–“é–¢ä¿‚æ€§
        concept_vector = await self._generate_concept_relations_vector(
            metadata.related_concepts
        )
        vector[
            current_idx : current_idx + self.dimensions.concept_relations
        ] = concept_vector
        current_idx += self.dimensions.concept_relations

        # 3. æ‰‹é †ãƒ»ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
        procedural_vector = await self._generate_procedural_vector(knowledge_data)
        vector[
            current_idx : current_idx + self.dimensions.procedural_steps
        ] = procedural_vector
        current_idx += self.dimensions.procedural_steps

        # 4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
        contextual_vector = await self._generate_contextual_vector(
            knowledge_data, metadata
        )
        vector[
            current_idx : current_idx + self.dimensions.contextual_embedding
        ] = contextual_vector
        current_idx += self.dimensions.contextual_embedding

        # 5. çŸ¥æµã®é€²åŒ–å±¥æ­´
        wisdom_vector = await self._generate_wisdom_evolution_vector(metadata)
        vector[
            current_idx : current_idx + self.dimensions.wisdom_evolution
        ] = wisdom_vector

        return vector

    async def _generate_content_semantic_vector(self, content: str) -> np.ndarray:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        # OpenAI Embeddings APIã‚’ä½¿ç”¨ï¼ˆãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯: openai.Embedding.create(model="text-embedding-ada-002", input=content)
        return np.random.random(self.dimensions.content_semantic)

    async def _generate_concept_relations_vector(
        self, concepts: List[str]
    ) -> np.ndarray:
        """æ¦‚å¿µé–“é–¢ä¿‚æ€§ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        vector = np.zeros(self.dimensions.concept_relations)

        # æ¦‚å¿µã®é »åº¦ã¨é–¢ä¿‚æ€§ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        for i, concept in enumerate(concepts[:32]):  # æœ€å¤§32æ¦‚å¿µ
            if i * 8 + 8 <= len(vector):
                concept_hash = hash(concept) % 256
                vector[i * 8 : (i + 1) * 8] = [
                    concept_hash / 256.0,  # æ¦‚å¿µID
                    1.0,  # å­˜åœ¨ãƒ•ãƒ©ã‚°
                    0.8,  # é‡è¦åº¦ï¼ˆå›ºå®šå€¤ï¼‰
                    0.5,  # é–¢é€£åº¦ï¼ˆå›ºå®šå€¤ï¼‰
                    0.0,
                    0.0,
                    0.0,
                    0.0,  # å°†æ¥æ‹¡å¼µç”¨
                ]

        return vector

    async def _generate_procedural_vector(
        self, knowledge_data: Dict[str, Any]
    ) -> np.ndarray:
        """æ‰‹é †ãƒ»ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        vector = np.zeros(self.dimensions.procedural_steps)

        content = knowledge_data.get("content", "")

        # ã‚¹ãƒ†ãƒƒãƒ—æ¤œå‡º
        step_patterns = [r"step\s+(\d+)", r"phase\s+(\d+)", r"(\d+)\."]
        step_count = 0

        for pattern in step_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            step_count += len(matches)

        # æ‰‹é †æ•°ã‚’æ­£è¦åŒ–ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        vector[0] = min(step_count / 10.0, 1.0)

        # ãƒ—ãƒ­ã‚»ã‚¹ã‚¿ã‚¤ãƒ—åˆ†é¡
        if "workflow" in content.lower():
            vector[1] = 1.0
        if "procedure" in content.lower():
            vector[2] = 1.0
        if "guide" in content.lower():
            vector[3] = 1.0

        return vector

    async def _generate_contextual_vector(
        self, knowledge_data: Dict[str, Any], metadata: KnowledgeVectorMetadata
    ) -> np.ndarray:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        vector = np.zeros(self.dimensions.contextual_embedding)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        vector[0] = list(KnowledgeType).index(metadata.knowledge_type) / len(
            KnowledgeType
        )
        vector[1] = list(KnowledgeDepth).index(metadata.knowledge_depth) / len(
            KnowledgeDepth
        )
        vector[2] = metadata.wisdom_level
        vector[3] = len(metadata.tags) / 10.0

        return vector

    async def _generate_wisdom_evolution_vector(
        self, metadata: KnowledgeVectorMetadata
    ) -> np.ndarray:
        """çŸ¥æµã®é€²åŒ–å±¥æ­´ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        vector = np.zeros(self.dimensions.wisdom_evolution)

        # é€²åŒ–å±¥æ­´ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        vector[0] = metadata.evolution_count / 10.0
        vector[1] = metadata.wisdom_level

        # æœ€çµ‚é€²åŒ–ã‹ã‚‰ã®çµŒéæ™‚é–“
        if metadata.last_evolution:
            days_since = (datetime.now(timezone.utc) - metadata.last_evolution).days
            vector[2] = min(days_since / 365.0, 1.0)

        return vector

    async def _update_concept_mappings(
        self, metadata: KnowledgeVectorMetadata, spell_id: str
    ):
        """æ¦‚å¿µãƒãƒƒãƒ”ãƒ³ã‚°æ›´æ–°"""
        try:
            async with self.grimoire_db.connection_pool.acquire() as conn:
                for concept in metadata.related_concepts:
                    await conn.execute(
                        """
                        INSERT INTO concept_mappings (concept_name, knowledge_ids, usage_frequency)
                        VALUES ($1, $2, 1)
                        ON CONFLICT (concept_name) DO UPDATE SET
                            knowledge_ids = array_append(concept_mappings.knowledge_ids, $2),
                            usage_frequency = concept_mappings.usage_frequency + 1,
                            last_accessed = CURRENT_TIMESTAMP
                    """,
                        concept,
                        spell_id,
                    )

            self.stats["concept_mappings"] += len(metadata.related_concepts)

        except Exception as e:
            self.logger.warning(f"âš ï¸ æ¦‚å¿µãƒãƒƒãƒ”ãƒ³ã‚°æ›´æ–°å¤±æ•—: {e}")

    async def search_wisdom(self, query: str, limit: int = 10) -> List[SearchResult]:
        """çŸ¥æµæ¤œç´¢"""
        try:
            self.logger.info(f"ğŸ” çŸ¥æµæ¤œç´¢é–‹å§‹: {query}")

            search_query = SearchQuery(
                query_text=query,
                magic_schools=[MagicSchool.KNOWLEDGE_SAGE],
                limit=limit,
                similarity_threshold=0.7,
            )

            results = await self.vector_search.search_similar(search_query)

            self.stats["wisdom_searches"] += 1

            self.logger.info(f"ğŸ“š çŸ¥æµæ¤œç´¢å®Œäº†: {len(results)}ä»¶ã®çŸ¥è­˜ã‚’ç™ºè¦‹")
            return results

        except Exception as e:
            self.logger.error(f"âŒ çŸ¥æµæ¤œç´¢å¤±æ•—: {e}")
            return []

    async def evolve_knowledge(
        self,
        knowledge_id: str,
        new_content: str,
        evolution_type: str = "content_update",
    ) -> Dict[str, Any]:
        """çŸ¥è­˜é€²åŒ–"""
        try:
            self.logger.info(f"ğŸŒ± çŸ¥è­˜é€²åŒ–é–‹å§‹: {knowledge_id}")

            # æ—¢å­˜çŸ¥è­˜å–å¾—
            old_knowledge = await self._get_knowledge_by_id(knowledge_id)
            if not old_knowledge:
                raise ValueError(f"Knowledge {knowledge_id} not found")

            # é€²åŒ–ã®åˆ†æ
            evolution_analysis = await self._analyze_knowledge_evolution(
                old_knowledge, new_content, evolution_type
            )

            # 4è³¢è€…ã¸ã®ç›¸è«‡
            if evolution_analysis["significance"] > 0.7:
                elder_consultation = await self._consult_elders_for_evolution(
                    knowledge_id, evolution_analysis
                )
                evolution_analysis["elder_approval"] = elder_consultation.get(
                    "approved", False
                )

            # é€²åŒ–å±¥æ­´è¨˜éŒ²
            await self._record_knowledge_evolution(knowledge_id, evolution_analysis)

            self.stats["knowledge_evolutions"] += 1

            self.logger.info(f"âœ… çŸ¥è­˜é€²åŒ–å®Œäº†: {knowledge_id}")
            return evolution_analysis

        except Exception as e:
            self.logger.error(f"âŒ çŸ¥è­˜é€²åŒ–å¤±æ•—: {e}")
            raise

    async def _get_knowledge_by_id(self, knowledge_id: str) -> Optional[Dict[str, Any]]:
        """ID ã«ã‚ˆã‚‹çŸ¥è­˜å–å¾—"""
        # ç°¡æ˜“å®Ÿè£…
        return {"id": knowledge_id, "content": "existing content"}

    async def _analyze_knowledge_evolution(
        self, old_knowledge: Dict[str, Any], new_content: str, evolution_type: str
    ) -> Dict[str, Any]:
        """çŸ¥è­˜é€²åŒ–åˆ†æ"""
        return {
            "evolution_type": evolution_type,
            "significance": 0.8,  # ç°¡æ˜“å®Ÿè£…
            "improvements": ["More detailed examples", "Updated best practices"],
            "wisdom_delta": 0.1,
        }

    async def _consult_elders_for_evolution(
        self, knowledge_id: str, evolution_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """çŸ¥è­˜é€²åŒ–ã«ã¤ã„ã¦ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã«ç›¸è«‡"""
        if self.four_sages:
            consultation_request = {
                "type": "knowledge_evolution_review",
                "data": {
                    "knowledge_id": knowledge_id,
                    "evolution_analysis": evolution_analysis,
                    "requester": "knowledge_sage",
                },
            }

            response = await self.four_sages.coordinate_learning_session(
                consultation_request
            )
            self.stats["elder_consultations"] += 1

            return {
                "approved": response.get("consensus_reached", False),
                "elder_feedback": response.get("learning_outcome", {}),
                "sage_votes": response.get("individual_responses", {}),
            }

        return {"approved": True, "reason": "Elders not available"}

    async def _record_knowledge_evolution(
        self, knowledge_id: str, evolution_analysis: Dict[str, Any]
    ):
        """çŸ¥è­˜é€²åŒ–å±¥æ­´è¨˜éŒ²"""
        try:
            async with self.grimoire_db.connection_pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO knowledge_evolution_history
                    (knowledge_id, evolution_type, wisdom_delta, elder_approval)
                    VALUES ($1, $2, $3, $4)
                """,
                    knowledge_id,
                    evolution_analysis["evolution_type"],
                    evolution_analysis.get("wisdom_delta", 0.0),
                    evolution_analysis.get("elder_approval", False),
                )

        except Exception as e:
            self.logger.warning(f"âš ï¸ çŸ¥è­˜é€²åŒ–å±¥æ­´è¨˜éŒ²å¤±æ•—: {e}")

    async def get_statistics(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±å–å¾— - ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºæ¨™æº–API"""
        return {
            "system_stats": self.stats.copy(),
            "cache_stats": {
                "knowledge_cache_size": len(self.knowledge_cache),
                "wisdom_cache_size": len(self.wisdom_cache),
            },
            "vector_dimensions": {
                "total": self.total_dimensions,
                "breakdown": {
                    "content_semantic": self.dimensions.content_semantic,
                    "concept_relations": self.dimensions.concept_relations,
                    "procedural_steps": self.dimensions.procedural_steps,
                    "contextual_embedding": self.dimensions.contextual_embedding,
                    "wisdom_evolution": self.dimensions.wisdom_evolution,
                },
            },
            "knowledge_types": [ktype.value for ktype in KnowledgeType],
            "knowledge_depths": [depth.value for depth in KnowledgeDepth],
        }

    async def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— - ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºæ¨™æº–API"""
        try:
            if self.four_sages:
                await self.four_sages.cleanup()

            if self.grimoire_db:
                await self.grimoire_db.close()

            self.logger.info("ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
            self.logger.info("ğŸ›ï¸ ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã¸ã®æœ€çµ‚å ±å‘Šå®Œäº†")

        except Exception as e:
            self.logger.error(f"âŒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
async def test_knowledge_sage_system():
    """ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    knowledge_sage = KnowledgeSageGrimoireVectorization()

    try:
        await knowledge_sage.initialize()

        # ãƒ†ã‚¹ãƒˆçŸ¥è­˜
        test_knowledge = {
            "id": "test_knowledge_001",
            "title": "Test Driven Development Best Practices",
            "content": """
            # Test Driven Development (TDD) Best Practices

            TDD is a software development methodology where tests are written before code.

            ## Steps:
            1. Write a failing test (Red)
            2. Write minimal code to pass (Green)
            3. Refactor and improve (Refactor)

            ## Best Practices:
            - Keep tests simple and focused
            - Use descriptive test names
            - Follow the AAA pattern (Arrange, Act, Assert)

            ```python
            def test_user_creation():
                # Arrange
                user_data = {"name": "John", "email": "john@example.com"}

                # Act
                user = create_user(user_data)

                # Assert
                assert user.name == "John"
                assert user.email == "john@example.com"
            ```
            """,
            "source_file": "knowledge_base/tdd_best_practices.md",
            "context": "Development methodology documentation",
        }

        # çŸ¥è­˜ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        spell_id = await knowledge_sage.vectorize_knowledge(test_knowledge)
        print(f"âœ… Test knowledge vectorized: {spell_id}")

        # çŸ¥æµæ¤œç´¢
        results = await knowledge_sage.search_wisdom("TDD best practices", limit=3)
        print(f"âœ… Found {len(results)} related wisdom entries")

        # çµ±è¨ˆæƒ…å ±
        stats = await knowledge_sage.get_statistics()
        print(f"âœ… System stats: {stats['system_stats']}")

    finally:
        await knowledge_sage.cleanup()


if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s [%(name)s] %(levelname)s: %(message)s"
    )

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(test_knowledge_sage_system())
