#!/usr/bin/env python3
"""
Log Cleanup System - è‡ªå‹•ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å®¹é‡æœ€é©åŒ–
731MBã®ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŠ¹ç‡çš„ã«ç®¡ç†
"""

import gzip
import json
import logging
import os
import shutil
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)


@dataclass
class LogFileInfo:
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±"""

    path: Path
    size_mb: float
    age_days: int
    last_modified: datetime
    type: str


@dataclass
class CleanupStats:
    """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµ±è¨ˆ"""

    files_processed: int
    files_compressed: int
    files_deleted: int
    space_saved_mb: float
    space_compressed_mb: float


class LogCleanupSystem:
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, logs_dir: str = "/home/aicompany/ai_co/logs"):
        self.logs_dir = Path(logs_dir)
        self.cleanup_rules = self._initialize_cleanup_rules()

    def _initialize_cleanup_rules(self) -> Dict[str, Dict]:
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ«ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–"""
        return {
            "worker_logs": {
                "pattern": "*worker*.log",
                "max_age_days": 7,
                "max_size_mb": 10,
                "compress_after_days": 3,
                "action": "compress_and_rotate",
            },
            "error_logs": {
                "pattern": "*error*.log",
                "max_age_days": 14,
                "max_size_mb": 50,
                "compress_after_days": 7,
                "action": "archive",
            },
            "debug_logs": {
                "pattern": "*debug*.log",
                "max_age_days": 3,
                "max_size_mb": 5,
                "compress_after_days": 1,
                "action": "delete",
            },
            "old_logs": {
                "pattern": "*.log",
                "max_age_days": 30,
                "max_size_mb": 100,
                "compress_after_days": 14,
                "action": "archive",
            },
        }

    def scan_log_files(self) -> List[LogFileInfo]:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³"""
        log_files = []

        if not self.logs_dir.exists():
            logger.warning(f"Log directory not found: {self.logs_dir}")
            return log_files

        for log_file in self.logs_dir.rglob("*.log"):
            try:
                stat = log_file.stat()
                size_mb = stat.st_size / (1024 * 1024)
                last_modified = datetime.fromtimestamp(stat.st_mtime)
                age_days = (datetime.now() - last_modified).days

                # ãƒ­ã‚°ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
                log_type = self._classify_log_type(log_file.name)

                log_info = LogFileInfo(
                    path=log_file,
                    size_mb=size_mb,
                    age_days=age_days,
                    last_modified=last_modified,
                    type=log_type,
                )

                log_files.append(log_info)

            except (OSError, IOError) as e:
                logger.warning(f"Could not process {log_file}: {e}")

        return sorted(log_files, key=lambda x: x.size_mb, reverse=True)

    def _classify_log_type(self, filename: str) -> str:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
        filename_lower = filename.lower()

        if "worker" in filename_lower:
            return "worker_logs"
        elif "error" in filename_lower:
            return "error_logs"
        elif "debug" in filename_lower:
            return "debug_logs"
        else:
            return "old_logs"

    def compress_log_file(self, log_file: LogFileInfo) -> bool:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’gzipåœ§ç¸®"""
        try:
            compressed_path = log_file.path.with_suffix(log_file.path.suffix + ".gz")

            with open(log_file.path, "rb") as f_in:
                with gzip.open(compressed_path, "wb") as f_out:
                    shutil.copyfileobj(f_in, f_out)

            # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            log_file.path.unlink()

            logger.info(f"Compressed {log_file.path} -> {compressed_path}")
            return True

        except Exception as e:
            logger.error(f"Failed to compress {log_file.path}: {e}")
            return False

    def rotate_log_file(self, log_file: LogFileInfo, max_rotations: int = 5) -> bool:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
        try:
            base_path = log_file.path

            # æ—¢å­˜ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚·ãƒ•ãƒˆ
            for i in range(max_rotations - 1, 0, -1):
                old_file = base_path.with_suffix(f".{i}")
                new_file = base_path.with_suffix(f".{i + 1}")

                if old_file.exists():
                    if new_file.exists():
                        new_file.unlink()
                    old_file.rename(new_file)

            # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’.1ã«ãƒªãƒãƒ¼ãƒ 
            if base_path.exists():
                rotated_file = base_path.with_suffix(".1")
                if rotated_file.exists():
                    rotated_file.unlink()
                base_path.rename(rotated_file)

                # æ–°ã—ã„ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                base_path.touch()

            logger.info(f"Rotated {log_file.path}")
            return True

        except Exception as e:
            logger.error(f"Failed to rotate {log_file.path}: {e}")
            return False

    def delete_old_compressed_files(self, max_age_days: int = 30) -> int:
        """å¤ã„åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        deleted_count = 0
        cutoff_date = datetime.now() - timedelta(days=max_age_days)

        for gz_file in self.logs_dir.rglob("*.gz"):
            try:
                stat = gz_file.stat()
                last_modified = datetime.fromtimestamp(stat.st_mtime)

                if last_modified < cutoff_date:
                    gz_file.unlink()
                    deleted_count += 1
                    logger.info(f"Deleted old compressed file: {gz_file}")

            except Exception as e:
                logger.warning(f"Could not delete {gz_file}: {e}")

        return deleted_count

    def cleanup_by_size_threshold(self, target_size_mb: float = 500) -> CleanupStats:
        """ã‚µã‚¤ã‚ºé–¾å€¤ã«ã‚ˆã‚‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        log_files = self.scan_log_files()
        current_size = sum(f.size_mb for f in log_files)

        stats = CleanupStats(0, 0, 0, 0.0, 0.0)

        if current_size <= target_size_mb:
            logger.info(
                f"Current size {current_size:.1f}MB is within target {target_size_mb}MB"
            )
            return stats

        # ã‚µã‚¤ã‚ºã®å¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‡¦ç†
        size_to_reduce = current_size - target_size_mb
        size_reduced = 0.0

        for log_file in log_files:
            if size_reduced >= size_to_reduce:
                break

            stats.files_processed += 1
            original_size = log_file.size_mb

            # ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ã„ã¦å‡¦ç†
            rule = self.cleanup_rules.get(log_file.type, self.cleanup_rules["old_logs"])

            if (
                log_file.age_days > rule["max_age_days"]
                or log_file.size_mb > rule["max_size_mb"]
            ):
                if rule["action"] == "delete":
                    log_file.path.unlink()
                    stats.files_deleted += 1
                    stats.space_saved_mb += original_size
                    size_reduced += original_size

                elif rule["action"] == "compress_and_rotate":
                    if self.compress_log_file(log_file):
                        stats.files_compressed += 1
                        compressed_size = original_size * 0.1  # æƒ³å®šåœ§ç¸®ç‡
                        stats.space_compressed_mb += original_size - compressed_size
                        size_reduced += original_size - compressed_size

                elif rule["action"] == "archive":
                    if log_file.age_days > rule["compress_after_days"]:
                        if self.compress_log_file(log_file):
                            stats.files_compressed += 1
                            compressed_size = original_size * 0.1
                            stats.space_compressed_mb += original_size - compressed_size
                            size_reduced += original_size - compressed_size

        return stats

    def execute_scheduled_cleanup(self) -> Dict[str, any]:
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã•ã‚ŒãŸã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
        start_time = datetime.now()
        log_files = self.scan_log_files()

        cleanup_result = {
            "timestamp": start_time.isoformat(),
            "before": {
                "total_files": len(log_files),
                "total_size_mb": sum(f.size_mb for f in log_files),
            },
            "actions_taken": [],
            "stats": CleanupStats(0, 0, 0, 0.0, 0.0),
            "after": {},
        }

        # 1. ã‚µã‚¤ã‚ºé–¾å€¤ã«ã‚ˆã‚‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        size_stats = self.cleanup_by_size_threshold(target_size_mb=300)
        cleanup_result["stats"] = size_stats
        cleanup_result["actions_taken"].append(
            f"ã‚µã‚¤ã‚ºæœ€é©åŒ–: {size_stats.files_processed}å€‹å‡¦ç†"
        )

        # 2. å¤ã„åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
        deleted_gz = self.delete_old_compressed_files(max_age_days=30)
        cleanup_result["actions_taken"].append(f"å¤ã„åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {deleted_gz}å€‹")

        # 3. ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self._cleanup_specific_patterns()
        cleanup_result["actions_taken"].append("ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

        # æœ€çµ‚çŠ¶æ…‹ã®æ¸¬å®š
        final_log_files = self.scan_log_files()
        cleanup_result["after"] = {
            "total_files": len(final_log_files),
            "total_size_mb": sum(f.size_mb for f in final_log_files),
            "reduction_mb": cleanup_result["before"]["total_size_mb"]
            - sum(f.size_mb for f in final_log_files),
        }

        execution_time = (datetime.now() - start_time).total_seconds()
        cleanup_result["execution_time_seconds"] = execution_time

        return cleanup_result

    def _cleanup_specific_patterns(self):
        """ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        patterns_to_cleanup = ["*.tmp", "*.temp", "*~", "*.bak", "core.*"]

        for pattern in patterns_to_cleanup:
            for file_path in self.logs_dir.rglob(pattern):
                try:
                    file_path.unlink()
                    logger.info(f"Deleted temporary file: {file_path}")
                except Exception as e:
                    logger.warning(f"Could not delete {file_path}: {e}")

    def generate_cleanup_report(self) -> Dict[str, any]:
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        log_files = self.scan_log_files()

        # ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
        type_stats = defaultdict(
            lambda: {"count": 0, "total_size_mb": 0, "avg_age_days": 0}
        )

        for log_file in log_files:
            type_stats[log_file.type]["count"] += 1
            type_stats[log_file.type]["total_size_mb"] += log_file.size_mb
            type_stats[log_file.type]["avg_age_days"] += log_file.age_days

        # å¹³å‡ã‚’è¨ˆç®—
        for stats in type_stats.values():
            if stats["count"] > 0:
                stats["avg_age_days"] = stats["avg_age_days"] / stats["count"]

        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        recommendations = []
        total_size = sum(f.size_mb for f in log_files)

        if total_size > 500:
            recommendations.append("ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒ500MBã‚’è¶…éã€‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’æ¨å¥¨")

        large_files = [f for f in log_files if f.size_mb > 50]
        if large_files:
            recommendations.append(f"{len(large_files)}å€‹ã®å¤§å‹ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«(50MB+)ã‚’åœ§ç¸®æ¨å¥¨")

        old_files = [f for f in log_files if f.age_days > 14]
        if old_files:
            recommendations.append(f"{len(old_files)}å€‹ã®å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«(14æ—¥+)ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¨å¥¨")

        return {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_files": len(log_files),
                "total_size_mb": total_size,
                "largest_file": max(log_files, key=lambda x: x.size_mb).path.name
                if log_files
                else None,
                "oldest_file": max(log_files, key=lambda x: x.age_days).path.name
                if log_files
                else None,
            },
            "type_statistics": dict(type_stats),
            "recommendations": recommendations,
            "cleanup_preview": {
                "estimated_compression_savings_mb": sum(
                    f.size_mb * 0.9 for f in log_files if f.size_mb > 10
                ),
                "deletable_old_files": len([f for f in log_files if f.age_days > 30]),
                "large_files_to_compress": len(
                    [f for f in log_files if f.size_mb > 20]
                ),
            },
        }

    def export_report(self, output_path: str):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        report = self.generate_cleanup_report()

        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        logger.info(f"Cleanup report exported to {output_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ§¹ Log Cleanup System - ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å®¹é‡æœ€é©åŒ–")
    print("=" * 60)

    cleanup_system = LogCleanupSystem()

    # ç¾åœ¨ã®çŠ¶æ³åˆ†æ
    print("ğŸ“Š ç¾åœ¨ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³åˆ†æ...")
    report = cleanup_system.generate_cleanup_report()

    print(f"\nğŸ“‚ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªçµ±è¨ˆ:")
    print(f"   ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {report['summary']['total_files']}")
    print(f"   ç·ã‚µã‚¤ã‚º: {report['summary']['total_size_mb']:.1f}MB")
    if report["summary"]["largest_file"]:
        print(f"   æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«: {report['summary']['largest_file']}")

    print(f"\nğŸ“‹ ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ:")
    for log_type, stats in report["type_statistics"].items():
        print(
            f"   {log_type}: {stats['count']}å€‹, {stats['total_size_mb']:.1f}MB, å¹³å‡{stats['avg_age_days']:.1f}æ—¥"
        )

    print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
    for rec in report["recommendations"]:
        print(f"   â€¢ {rec}")

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
    if report["summary"]["total_size_mb"] > 300:
        print(f"\nğŸ§¹ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...")
        cleanup_result = cleanup_system.execute_scheduled_cleanup()

        print(f"âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†:")
        print(f"   å‡¦ç†å‰: {cleanup_result['before']['total_size_mb']:.1f}MB")
        print(f"   å‡¦ç†å¾Œ: {cleanup_result['after']['total_size_mb']:.1f}MB")
        print(f"   å‰Šæ¸›é‡: {cleanup_result['after']['reduction_mb']:.1f}MB")
        print(f"   å®Ÿè¡Œæ™‚é–“: {cleanup_result['execution_time_seconds']:.1f}ç§’")

        print(f"\nğŸ“‹ å®Ÿè¡Œã•ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        for action in cleanup_result["actions_taken"]:
            print(f"   â€¢ {action}")

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    cleanup_system.export_report(
        "/home/aicompany/ai_co/ai_todo/log_cleanup_report.json"
    )
    print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ")

    print(f"\nğŸ‰ Log Cleanup System æœ€é©åŒ–å®Œäº†ï¼")


if __name__ == "__main__":
    main()
