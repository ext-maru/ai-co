#!/usr/bin/env python3
"""
æ©Ÿæ¢°å­¦ç¿’åˆ†æã‚¨ãƒ³ã‚¸ãƒ³
ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã®é«˜åº¦ãªäºˆæ¸¬ãƒ»åˆ†æãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

è¨­è¨ˆ: RAGã‚¨ãƒ«ãƒ€ãƒ¼ Ã— ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼
æ‰¿èª: ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºè©•è­°ä¼š
å®Ÿè£…æ—¥: 2025å¹´7æœˆ10æ—¥
"""

import asyncio
import json
import logging
import pickle
import warnings
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN, KMeans
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import accuracy_score, mean_squared_error, silhouette_score
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

warnings.filterwarnings("ignore")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelType(Enum):
    """ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—"""

    REGRESSION = "regression"
    CLASSIFICATION = "classification"
    CLUSTERING = "clustering"
    TIME_SERIES = "time_series"
    ANOMALY_DETECTION = "anomaly_detection"


@dataclass
class MLModel:
    """æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ç®¡ç†"""

    model_id: str
    model_type: ModelType
    algorithm: str
    features: List[str]
    target: Optional[str]
    parameters: Dict[str, Any]
    metrics: Dict[str, float]
    trained_at: datetime
    model_object: Any = None


class FeatureEngineer:
    """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""

    def __init__(self):
        self.scalers = {}
        self.encoders = {}

    async def engineer_features(
        self, df: pd.DataFrame, feature_config: Dict
    ) -> pd.DataFrame:
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        result = df.copy()

        # æ™‚ç³»åˆ—ç‰¹å¾´é‡
        if feature_config.get("time_features", False):
            result = await self._add_time_features(result)

        # çµ±è¨ˆç‰¹å¾´é‡
        if feature_config.get("statistical_features", False):
            result = await self._add_statistical_features(result)

        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡
        if feature_config.get("encode_categorical", False):
            result = await self._encode_categorical(result)

        # ç›¸äº’ä½œç”¨ç‰¹å¾´é‡
        if feature_config.get("interaction_features", False):
            result = await self._add_interaction_features(result)

        # æ¬ æå€¤å‡¦ç†
        if feature_config.get("handle_missing", True):
            result = await self._handle_missing_values(result)

        return result

    async def _add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ™‚ç³»åˆ—ç‰¹å¾´é‡è¿½åŠ """
        time_columns = df.select_dtypes(include=["datetime64"]).columns

        for col in time_columns:
            df[f"{col}_hour"] = df[col].dt.hour
            df[f"{col}_dayofweek"] = df[col].dt.dayofweek
            df[f"{col}_month"] = df[col].dt.month
            df[f"{col}_quarter"] = df[col].dt.quarter
            df[f"{col}_is_weekend"] = df[col].dt.dayofweek.isin([5, 6]).astype(int)

        return df

    async def _add_statistical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """çµ±è¨ˆçš„ç‰¹å¾´é‡è¿½åŠ """
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        # ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆ
        for col in numeric_cols:
            if len(df) > 10:
                df[f"{col}_rolling_mean_7"] = (
                    df[col].rolling(window=7, min_periods=1).mean()
                )
                df[f"{col}_rolling_std_7"] = (
                    df[col].rolling(window=7, min_periods=1).std()
                )

        return df

    async def _encode_categorical(self, df: pd.DataFrame) -> pd.DataFrame:
        """ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        categorical_cols = df.select_dtypes(include=["object"]).columns

        for col in categorical_cols:
            if col not in self.encoders:
                self.encoders[col] = LabelEncoder()
                df[f"{col}_encoded"] = self.encoders[col].fit_transform(
                    df[col].fillna("unknown")
                )
            else:
                df[f"{col}_encoded"] = self.encoders[col].transform(
                    df[col].fillna("unknown")
                )

        return df

    async def _add_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç›¸äº’ä½œç”¨ç‰¹å¾´é‡è¿½åŠ """
        numeric_cols = df.select_dtypes(include=[np.number]).columns[:5]  # æœ€å¤§5åˆ—

        for i, col1 in enumerate(numeric_cols):
            for col2 in numeric_cols[i + 1 :]:
                df[f"{col1}_x_{col2}"] = df[col1] * df[col2]
                df[f"{col1}_div_{col2}"] = df[col1] / (df[col2] + 1e-8)

        return df

    async def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¬ æå€¤å‡¦ç†"""
        # æ•°å€¤å‹: ä¸­å¤®å€¤ã§è£œå®Œ
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            df[col].fillna(df[col].median(), inplace=True)

        # ã‚«ãƒ†ã‚´ãƒªå‹: æœ€é »å€¤ã§è£œå®Œ
        categorical_cols = df.select_dtypes(include=["object"]).columns
        for col in categorical_cols:
            df[col].fillna(
                df[col].mode()[0] if not df[col].mode().empty else "unknown",
                inplace=True,
            )

        return df


class ModelTrainer:
    """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        self.models = {
            ModelType.REGRESSION: {
                "random_forest": RandomForestRegressor,
                "linear_regression": LinearRegression,
            },
            ModelType.CLASSIFICATION: {
                "random_forest": RandomForestClassifier,
                "logistic_regression": LogisticRegression,
            },
            ModelType.CLUSTERING: {"kmeans": KMeans, "dbscan": DBSCAN},
        }

    async def train_model(
        self,
        X: pd.DataFrame,
        y: Optional[pd.Series],
        model_type: ModelType,
        algorithm: str,
        params: Dict = None,
    ) -> MLModel:
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""

        logger.info(f"ğŸ§  ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹: {model_type.value} - {algorithm}")

        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model_class = self.models[model_type][algorithm]
        model = model_class(**(params or {}))

        # è¨“ç·´
        if model_type in [ModelType.REGRESSION, ModelType.CLASSIFICATION]:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            model.fit(X_train, y_train)

            # è©•ä¾¡
            y_pred = model.predict(X_test)

            if model_type == ModelType.REGRESSION:
                metrics = {
                    "mse": mean_squared_error(y_test, y_pred),
                    "rmse": np.sqrt(mean_squared_error(y_test, y_pred)),
                    "r2": model.score(X_test, y_test),
                }
            else:
                metrics = {
                    "accuracy": accuracy_score(y_test, y_pred),
                    "cross_val_score": np.mean(cross_val_score(model, X, y, cv=5)),
                }

        elif model_type == ModelType.CLUSTERING:
            model.fit(X)

            if algorithm == "kmeans":
                metrics = {
                    "inertia": model.inertia_,
                    "silhouette_score": silhouette_score(X, model.labels_)
                    if len(set(model.labels_)) > 1
                    else 0,
                }
            else:
                metrics = {
                    "n_clusters": len(set(model.labels_))
                    - (1 if -1 in model.labels_ else 0)
                }

        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ä½œæˆ
        ml_model = MLModel(
            model_id=f"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            model_type=model_type,
            algorithm=algorithm,
            features=list(X.columns),
            target=y.name if y is not None else None,
            parameters=params or {},
            metrics=metrics,
            trained_at=datetime.now(),
            model_object=model,
        )

        logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: {ml_model.model_id}")
        logger.info(f"ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {metrics}")

        return ml_model


class PredictionEngine:
    """äºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        self.active_models: Dict[str, MLModel] = {}

    async def predict(
        self, model: MLModel, X: pd.DataFrame
    ) -> Union[np.ndarray, pd.DataFrame]:
        """äºˆæ¸¬å®Ÿè¡Œ"""
        if model.model_object is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“")

        predictions = model.model_object.predict(X)

        # äºˆæ¸¬çµæœã‚’DataFrameã§è¿”ã™
        if model.model_type == ModelType.CLUSTERING:
            result = pd.DataFrame(
                {"cluster": predictions, "confidence": 1.0}  # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®å ´åˆã¯ä¿¡é ¼åº¦1.0
            )
        else:
            # ç¢ºç‡äºˆæ¸¬ãŒå¯èƒ½ãªå ´åˆ
            if hasattr(model.model_object, "predict_proba"):
                probabilities = model.model_object.predict_proba(X)
                confidence = np.max(probabilities, axis=1)
            else:
                confidence = np.ones(len(predictions))

            result = pd.DataFrame({"prediction": predictions, "confidence": confidence})

        return result

    async def batch_predict(self, model: MLModel, data_generator) -> List[pd.DataFrame]:
        """ãƒãƒƒãƒäºˆæ¸¬"""
        results = []

        async for batch in data_generator:
            predictions = await self.predict(model, batch)
            results.append(predictions)

        return results


class AnomalyDetector:
    """ç•°å¸¸æ¤œçŸ¥ã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        self.threshold_multiplier = 3  # æ¨™æº–åå·®ã®å€æ•°

    async def detect_anomalies(
        self, df: pd.DataFrame, method: str = "statistical"
    ) -> pd.DataFrame:
        """ç•°å¸¸æ¤œçŸ¥å®Ÿè¡Œ"""
        result = df.copy()

        if method == "statistical":
            anomalies = await self._statistical_anomaly_detection(df)
        elif method == "isolation_forest":
            anomalies = await self._isolation_forest_detection(df)
        elif method == "clustering":
            anomalies = await self._clustering_based_detection(df)
        else:
            raise ValueError(f"æœªå¯¾å¿œã®ç•°å¸¸æ¤œçŸ¥æ‰‹æ³•: {method}")

        result["is_anomaly"] = anomalies
        result["anomaly_score"] = await self._calculate_anomaly_scores(df, anomalies)

        return result

    async def _statistical_anomaly_detection(self, df: pd.DataFrame) -> np.ndarray:
        """çµ±è¨ˆçš„ç•°å¸¸æ¤œçŸ¥"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        anomaly_mask = np.zeros(len(df), dtype=bool)

        for col in numeric_cols:
            mean = df[col].mean()
            std = df[col].std()

            # 3Ïƒãƒ«ãƒ¼ãƒ«
            lower_bound = mean - self.threshold_multiplier * std
            upper_bound = mean + self.threshold_multiplier * std

            col_anomalies = (df[col] < lower_bound) | (df[col] > upper_bound)
            anomaly_mask = anomaly_mask | col_anomalies

        return anomaly_mask

    async def _isolation_forest_detection(self, df: pd.DataFrame) -> np.ndarray:
        """Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥"""
        from sklearn.ensemble import IsolationForest

        numeric_df = df.select_dtypes(include=[np.number])

        model = IsolationForest(contamination=0.1, random_state=42)
        predictions = model.fit_predict(numeric_df)

        return predictions == -1

    async def _clustering_based_detection(self, df: pd.DataFrame) -> np.ndarray:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ç•°å¸¸æ¤œçŸ¥"""
        numeric_df = df.select_dtypes(include=[np.number])

        # DBSCANä½¿ç”¨
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        clusters = dbscan.fit_predict(StandardScaler().fit_transform(numeric_df))

        # ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆï¼ˆ-1ï¼‰ã‚’ç•°å¸¸ã¨ã™ã‚‹
        return clusters == -1

    async def _calculate_anomaly_scores(
        self, df: pd.DataFrame, anomalies: np.ndarray
    ) -> np.ndarray:
        """ç•°å¸¸ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        scores = np.zeros(len(df))

        for col in numeric_cols:
            mean = df[col].mean()
            std = df[col].std()

            # æ¨™æº–åŒ–ã•ã‚ŒãŸè·é›¢ã‚’ã‚¹ã‚³ã‚¢ã¨ã™ã‚‹
            col_scores = np.abs(df[col] - mean) / (std + 1e-8)
            scores = np.maximum(scores, col_scores)

        return scores


class MLAnalyticsEngine:
    """æ©Ÿæ¢°å­¦ç¿’åˆ†æã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆ"""

    def __init__(self, project_root: Path):
        self.project_root = Path(project_root)
        self.models_dir = self.project_root / "ml_models"
        self.models_dir.mkdir(exist_ok=True)

        self.feature_engineer = FeatureEngineer()
        self.model_trainer = ModelTrainer()
        self.prediction_engine = PredictionEngine()
        self.anomaly_detector = AnomalyDetector()

        self.trained_models: Dict[str, MLModel] = {}

    async def train_and_save_model(
        self,
        data: pd.DataFrame,
        target_col: Optional[str],
        model_type: ModelType,
        algorithm: str,
        feature_config: Dict = None,
        model_params: Dict = None,
    ) -> MLModel:
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨ä¿å­˜"""

        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        engineered_data = await self.feature_engineer.engineer_features(
            data, feature_config or {}
        )

        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™
        if target_col:
            X = engineered_data.drop(columns=[target_col])
            y = engineered_data[target_col]
        else:
            X = engineered_data
            y = None

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = await self.model_trainer.train_model(
            X, y, model_type, algorithm, model_params
        )

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        await self._save_model(model)
        self.trained_models[model.model_id] = model

        return model

    async def predict_with_model(
        self, model_id: str, data: pd.DataFrame
    ) -> pd.DataFrame:
        """ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬"""
        if model_id not in self.trained_models:
            model = await self._load_model(model_id)
            self.trained_models[model_id] = model
        else:
            model = self.trained_models[model_id]

        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆè¨“ç·´æ™‚ã¨åŒã˜å‡¦ç†ï¼‰
        engineered_data = await self.feature_engineer.engineer_features(
            data, {"handle_missing": True}  # æœ€ä½é™ã®å‡¦ç†
        )

        # è¨“ç·´æ™‚ã®ç‰¹å¾´é‡ã«åˆã‚ã›ã‚‹
        missing_features = set(model.features) - set(engineered_data.columns)
        for feature in missing_features:
            engineered_data[feature] = 0

        X = engineered_data[model.features]

        # äºˆæ¸¬å®Ÿè¡Œ
        predictions = await self.prediction_engine.predict(model, X)

        return predictions

    async def detect_anomalies_in_data(
        self, data: pd.DataFrame, method: str = "statistical"
    ) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ¤œçŸ¥"""
        return await self.anomaly_detector.detect_anomalies(data, method)

    async def _save_model(self, model: MLModel):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        model_path = self.models_dir / f"{model.model_id}.pkl"

        # ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆ¥é€”ä¿å­˜
        model_obj_path = self.models_dir / f"{model.model_id}_object.pkl"

        with open(model_obj_path, "wb") as f:
            pickle.dump(model.model_object, f)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä»¥å¤–ï¼‰
        metadata = {
            "model_id": model.model_id,
            "model_type": model.model_type.value,
            "algorithm": model.algorithm,
            "features": model.features,
            "target": model.target,
            "parameters": model.parameters,
            "metrics": model.metrics,
            "trained_at": model.trained_at.isoformat(),
        }

        with open(model_path, "w") as f:
            json.dump(metadata, f, indent=2)

        logger.info(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")

    async def _load_model(self, model_id: str) -> MLModel:
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        model_path = self.models_dir / f"{model_id}.pkl"
        model_obj_path = self.models_dir / f"{model_id}_object.pkl"

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(model_path, "r") as f:
            metadata = json.load(f)

        # ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆèª­ã¿è¾¼ã¿
        with open(model_obj_path, "rb") as f:
            model_object = pickle.load(f)

        # MLModelã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå†æ§‹ç¯‰
        model = MLModel(
            model_id=metadata["model_id"],
            model_type=ModelType(metadata["model_type"]),
            algorithm=metadata["algorithm"],
            features=metadata["features"],
            target=metadata["target"],
            parameters=metadata["parameters"],
            metrics=metadata["metrics"],
            trained_at=datetime.fromisoformat(metadata["trained_at"]),
            model_object=model_object,
        )

        return model

    async def get_model_performance_report(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = {"total_models": len(self.trained_models), "models": []}

        for model_id, model in self.trained_models.items():
            model_info = {
                "model_id": model_id,
                "type": model.model_type.value,
                "algorithm": model.algorithm,
                "metrics": model.metrics,
                "feature_count": len(model.features),
                "trained_at": model.trained_at.isoformat(),
            }
            report["models"].append(model_info)

        return report


# ä½¿ç”¨ä¾‹
async def main():
    """MLã‚¨ãƒ³ã‚¸ãƒ³ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ"""
    engine = MLAnalyticsEngine(Path("/home/aicompany/ai_co"))

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    sample_data = pd.DataFrame(
        {
            "timestamp": pd.date_range("2025-07-01", periods=100, freq="H"),
            "value": np.random.normal(100, 15, 100),
            "category": np.random.choice(["A", "B", "C"], 100),
        }
    )
    sample_data["target"] = sample_data["value"] + np.random.normal(0, 5, 100)

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = await engine.train_and_save_model(
        data=sample_data,
        target_col="target",
        model_type=ModelType.REGRESSION,
        algorithm="random_forest",
        feature_config={
            "time_features": True,
            "statistical_features": True,
            "encode_categorical": True,
        },
        model_params={"n_estimators": 100, "random_state": 42},
    )

    print(f"è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: {model.model_id}")
    print(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {model.metrics}")

    # ç•°å¸¸æ¤œçŸ¥
    anomaly_result = await engine.detect_anomalies_in_data(sample_data)
    print(f"ç•°å¸¸æ¤œçŸ¥çµæœ: {anomaly_result['is_anomaly'].sum()}ä»¶ã®ç•°å¸¸ã‚’æ¤œå‡º")


if __name__ == "__main__":
    asyncio.run(main())
