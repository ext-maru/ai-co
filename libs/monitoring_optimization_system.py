#!/usr/bin/env python3
"""
Monitoring & Optimization System
ç›£è¦–ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ  - Phase 6

PostgreSQL MCPã®ç›£è¦–ãƒ»æœ€é©åŒ–ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã€è‡ªå‹•æœ€é©åŒ–ã€äºˆæ¸¬åˆ†æã‚’æä¾›

æ©Ÿèƒ½:
ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
ğŸ”§ è‡ªå‹•æœ€é©åŒ–
ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†
ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import sys
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import asyncio
import json
import time
import logging
import psutil
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import statistics
from collections import deque
import asyncpg

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
from libs.four_sages_postgres_mcp_integration import FourSagesPostgresMCPIntegration
from libs.advanced_search_analytics_platform import AdvancedSearchAnalyticsPlatform
from libs.automated_learning_system import AutomatedLearningSystem

logger = logging.getLogger(__name__)


class MonitoringLevel(Enum):
    """ç›£è¦–ãƒ¬ãƒ™ãƒ«"""

    BASIC = "basic"
    DETAILED = "detailed"
    COMPREHENSIVE = "comprehensive"


class OptimizationStrategy(Enum):
    """æœ€é©åŒ–æˆ¦ç•¥"""

    CONSERVATIVE = "conservative"
    BALANCED = "balanced"
    AGGRESSIVE = "aggressive"


class AlertSeverity(Enum):
    """ã‚¢ãƒ©ãƒ¼ãƒˆé‡è¦åº¦"""

    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class SystemMetrics:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    disk_usage: float
    network_io: Dict[str, int]
    database_connections: int
    query_performance: Dict[str, float]
    error_rate: float
    response_time: float


@dataclass
class DatabaseMetrics:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    timestamp: datetime
    connection_count: int
    active_queries: int
    slow_queries: int
    cache_hit_ratio: float
    index_usage: Dict[str, float]
    table_sizes: Dict[str, int]
    vacuum_stats: Dict[str, Any]
    replication_lag: float


@dataclass
class PerformanceAlert:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆ"""

    alert_id: str
    timestamp: datetime
    severity: AlertSeverity
    component: str
    metric: str
    current_value: float
    threshold: float
    message: str
    suggested_action: str


class SystemMonitor:
    """ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–"""

    def __init__(self, level: MonitoringLevel = MonitoringLevel.DETAILED):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""

        self.level = level
        self.metrics_history = deque(maxlen=1000)
        self.alerts = []
        self.thresholds = {
            "cpu_usage": 80.0,
            "memory_usage": 85.0,
            "disk_usage": 90.0,
            "error_rate": 5.0,
            "response_time": 1.0,
            "slow_queries": 10,
        }

        logger.info(f"ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–åˆæœŸåŒ–å®Œäº† (ãƒ¬ãƒ™ãƒ«: {level.value})")

    async def collect_system_metrics(self) -> SystemMetrics:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        try:
            # CPUä½¿ç”¨ç‡
            cpu_usage = psutil.cpu_percent(interval=0.1)

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            memory_usage = memory.percent

            # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡
            disk = psutil.disk_usage("/")
            disk_usage = disk.used / disk.total * 100

            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯IO
            network = psutil.net_io_counters()
            network_io = {
                "bytes_sent": network.bytes_sent,
                "bytes_recv": network.bytes_recv,
                "packets_sent": network.packets_sent,
                "packets_recv": network.packets_recv,
            }

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæ•°ï¼ˆæ¨å®šï¼‰
            db_connections = len(
                [p for p in psutil.process_iter() if "postgres" in p.name().lower()]
            )

            # ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
            query_performance = {
                "avg_query_time": 0.25,
                "max_query_time": 1.2,
                "min_query_time": 0.05,
            }

            # ã‚¨ãƒ©ãƒ¼ç‡ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
            error_rate = 2.1

            # å¿œç­”æ™‚é–“ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
            response_time = 0.35

            metrics = SystemMetrics(
                timestamp=datetime.now(),
                cpu_usage=cpu_usage,
                memory_usage=memory_usage,
                disk_usage=disk_usage,
                network_io=network_io,
                database_connections=db_connections,
                query_performance=query_performance,
                error_rate=error_rate,
                response_time=response_time,
            )

            self.metrics_history.append(metrics)
            return metrics

        except Exception as e:
            logger.error(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    async def collect_database_metrics(
        self, db_config: Dict[str, Any]
    ) -> DatabaseMetrics:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        try:
            # PostgreSQLæ¥ç¶š
            conn = await asyncpg.connect(
                host=db_config.get("host", "localhost"),
                port=db_config.get("port", 5432),
                database=db_config.get("database", "elders_guild"),
                user=db_config.get("user", "postgres"),
                password=db_config.get("password", "password"),
            )

            # æ¥ç¶šæ•°
            connection_count = await conn.fetchval(
                "SELECT count(*) FROM pg_stat_activity"
            )

            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¯ã‚¨ãƒªæ•°
            active_queries = await conn.fetchval(
                "SELECT count(*) FROM pg_stat_activity WHERE state = 'active'"
            )

            # é…ã„ã‚¯ã‚¨ãƒªæ•°ï¼ˆæ¨å®šï¼‰
            slow_queries = await conn.fetchval(
                "SELECT count(*) FROM pg_stat_activity WHERE state =  \
                    'active' AND query_start < NOW() - INTERVAL '30 seconds'"
            )

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡
            cache_stats = await conn.fetchrow(
                "SELECT sum(heap_blks_hit) as hits, sum(heap_blks_read) as reads FROM " \
                    "pg_statio_user_tables"
            )

            if cache_stats["reads"] > 0:
                cache_hit_ratio = (
                    cache_stats["hits"]
                    / (cache_stats["hits"] + cache_stats["reads"])
                    * 100
                )
            else:
                cache_hit_ratio = 100.0

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½¿ç”¨ç‡
            index_usage = {}
            index_stats = await conn.fetch(
                "SELECT schemaname, tablename, indexname, idx_scan FROM pg_stat_user_indexes"
            )

            for row in index_stats:
                table_name = f"{row['schemaname']}.{row['tablename']}"
                if table_name not in index_usage:
                    index_usage[table_name] = 0
                index_usage[table_name] += row["idx_scan"]

            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚º
            table_sizes = {}
            size_stats = await conn.fetch(
                "SELECT schemaname, tablename, pg_total_relation_size(schemaname||'." \
                    "'||tablename) as size FROM pg_tables WHERE schemaname = 'public'"
            )

            for row in size_stats:
                table_name = f"{row['schemaname']}.{row['tablename']}"
                table_sizes[table_name] = row["size"]

            # VACUUMçµ±è¨ˆ
            vacuum_stats = {
                "last_vacuum": "N/A",
                "last_autovacuum": "N/A",
                "last_analyze": "N/A",
            }

            # ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
            replication_lag = 0.0

            await conn.close()

            metrics = DatabaseMetrics(
                timestamp=datetime.now(),
                connection_count=connection_count,
                active_queries=active_queries,
                slow_queries=slow_queries,
                cache_hit_ratio=cache_hit_ratio,
                index_usage=index_usage,
                table_sizes=table_sizes,
                vacuum_stats=vacuum_stats,
                replication_lag=replication_lag,
            )

            return metrics

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤
            return DatabaseMetrics(
                timestamp=datetime.now(),
                connection_count=5,
                active_queries=2,
                slow_queries=0,
                cache_hit_ratio=95.0,
                index_usage={},
                table_sizes={},
                vacuum_stats={},
                replication_lag=0.0,
            )

    async def check_thresholds(self, metrics: SystemMetrics) -> List[PerformanceAlert]:
        """é–¾å€¤ãƒã‚§ãƒƒã‚¯"""
        alerts = []

        # CPUä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        if metrics.cpu_usage > self.thresholds["cpu_usage"]:
            alert = PerformanceAlert(
                alert_id=f"cpu_high_{int(time.time())}",
                timestamp=datetime.now(),
                severity=AlertSeverity.WARNING,
                component="system",
                metric="cpu_usage",
                current_value=metrics.cpu_usage,
                threshold=self.thresholds["cpu_usage"],
                message=f"CPUä½¿ç”¨ç‡ãŒé«˜ã„: {metrics.cpu_usage:.1f}%",
                suggested_action="ãƒ—ãƒ­ã‚»ã‚¹è² è·ã®ç¢ºèªã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¤œè¨",
            )
            alerts.append(alert)

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        if metrics.memory_usage > self.thresholds["memory_usage"]:
            alert = PerformanceAlert(
                alert_id=f"memory_high_{int(time.time())}",
                timestamp=datetime.now(),
                severity=AlertSeverity.WARNING,
                component="system",
                metric="memory_usage",
                current_value=metrics.memory_usage,
                threshold=self.thresholds["memory_usage"],
                message=f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„: {metrics.memory_usage:.1f}%",
                suggested_action="ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®ç¢ºèªã¨ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³",
            )
            alerts.append(alert)

        # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        if metrics.disk_usage > self.thresholds["disk_usage"]:
            alert = PerformanceAlert(
                alert_id=f"disk_high_{int(time.time())}",
                timestamp=datetime.now(),
                severity=AlertSeverity.ERROR,
                component="system",
                metric="disk_usage",
                current_value=metrics.disk_usage,
                threshold=self.thresholds["disk_usage"],
                message=f"ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡ãŒé«˜ã„: {metrics.disk_usage:.1f}%",
                suggested_action="ãƒ‡ã‚£ã‚¹ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ç¢ºä¿ã¨ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–",
            )
            alerts.append(alert)

        # ã‚¨ãƒ©ãƒ¼ç‡ãƒã‚§ãƒƒã‚¯
        if metrics.error_rate > self.thresholds["error_rate"]:
            alert = PerformanceAlert(
                alert_id=f"error_rate_high_{int(time.time())}",
                timestamp=datetime.now(),
                severity=AlertSeverity.ERROR,
                component="application",
                metric="error_rate",
                current_value=metrics.error_rate,
                threshold=self.thresholds["error_rate"],
                message=f"ã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ã„: {metrics.error_rate:.1f}%",
                suggested_action="ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ç¢ºèªã¨ãƒã‚°ä¿®æ­£",
            )
            alerts.append(alert)

        # å¿œç­”æ™‚é–“ãƒã‚§ãƒƒã‚¯
        if metrics.response_time > self.thresholds["response_time"]:
            alert = PerformanceAlert(
                alert_id=f"response_time_high_{int(time.time())}",
                timestamp=datetime.now(),
                severity=AlertSeverity.WARNING,
                component="application",
                metric="response_time",
                current_value=metrics.response_time,
                threshold=self.thresholds["response_time"],
                message=f"å¿œç­”æ™‚é–“ãŒé…ã„: {metrics.response_time:.3f}ç§’",
                suggested_action="ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¢ºèª",
            )
            alerts.append(alert)

        self.alerts.extend(alerts)
        return alerts

    def get_monitoring_status(self) -> Dict[str, Any]:
        """ç›£è¦–çŠ¶æ³å–å¾—"""
        return {
            "monitoring_level": self.level.value,
            "metrics_count": len(self.metrics_history),
            "alerts_count": len(self.alerts),
            "thresholds": self.thresholds,
            "last_collection": (
                self.metrics_history[-1].timestamp.isoformat()
                if self.metrics_history
                else None
            ),
        }


class PerformanceOptimizer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–"""

    def __init__(self, strategy: OptimizationStrategy = OptimizationStrategy.BALANCED):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.strategy = strategy
        self.optimizations_applied = []
        self.performance_history = deque(maxlen=100)

        logger.info(f"ğŸ”§ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–åˆæœŸåŒ–å®Œäº† (æˆ¦ç•¥: {strategy.value})")

    async def analyze_performance(
        self, metrics: SystemMetrics, db_metrics: DatabaseMetrics
    ) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        try:
            analysis = {
                "timestamp": datetime.now().isoformat(),
                "system_health": self._assess_system_health(metrics),
                "database_health": self._assess_database_health(db_metrics),
                "bottlenecks": self._identify_bottlenecks(metrics, db_metrics),
                "optimization_opportunities": self._find_optimization_opportunities(
                    metrics, db_metrics
                ),
                "recommendations": self._generate_recommendations(metrics, db_metrics),
            }

            self.performance_history.append(analysis)
            return analysis

        except Exception as e:
            logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _assess_system_health(self, metrics: SystemMetrics) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹è©•ä¾¡"""
        health_score = 0

        if metrics.cpu_usage < 70:
            health_score += 25
        elif metrics.cpu_usage < 85:
            health_score += 15

        if metrics.memory_usage < 80:
            health_score += 25
        elif metrics.memory_usage < 90:
            health_score += 15

        if metrics.disk_usage < 85:
            health_score += 25
        elif metrics.disk_usage < 95:
            health_score += 15

        if metrics.error_rate < 2:
            health_score += 25
        elif metrics.error_rate < 5:
            health_score += 15

        if health_score >= 80:
            return "excellent"
        elif health_score >= 60:
            return "good"
        elif health_score >= 40:
            return "fair"
        else:
            return "poor"

    def _assess_database_health(self, db_metrics: DatabaseMetrics) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ˜ãƒ«ã‚¹è©•ä¾¡"""
        health_score = 0

        if db_metrics.connection_count < 50:
            health_score += 25
        elif db_metrics.connection_count < 100:
            health_score += 15

        if db_metrics.slow_queries < 5:
            health_score += 25
        elif db_metrics.slow_queries < 10:
            health_score += 15

        if db_metrics.cache_hit_ratio > 95:
            health_score += 25
        elif db_metrics.cache_hit_ratio > 90:
            health_score += 15

        if db_metrics.replication_lag < 0.1:
            health_score += 25
        elif db_metrics.replication_lag < 0.5:
            health_score += 15

        if health_score >= 80:
            return "excellent"
        elif health_score >= 60:
            return "good"
        elif health_score >= 40:
            return "fair"
        else:
            return "poor"

    def _identify_bottlenecks(
        self, metrics: SystemMetrics, db_metrics: DatabaseMetrics
    ) -> List[str]:
        """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š"""
        bottlenecks = []

        if metrics.cpu_usage > 85:
            bottlenecks.append("cpu_high")

        if metrics.memory_usage > 90:
            bottlenecks.append("memory_high")

        if metrics.disk_usage > 95:
            bottlenecks.append("disk_full")

        if db_metrics.slow_queries > 10:
            bottlenecks.append("slow_queries")

        if db_metrics.cache_hit_ratio < 90:
            bottlenecks.append("cache_miss")

        if db_metrics.connection_count > 100:
            bottlenecks.append("connection_pool")

        return bottlenecks

    def _find_optimization_opportunities(
        self, metrics: SystemMetrics, db_metrics: DatabaseMetrics
    ) -> List[str]:
        """æœ€é©åŒ–æ©Ÿä¼šç™ºè¦‹"""
        opportunities = []

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–
        if db_metrics.cache_hit_ratio < 95:
            opportunities.append("index_optimization")

        # ã‚¯ã‚¨ãƒªæœ€é©åŒ–
        if db_metrics.slow_queries > 5:
            opportunities.append("query_optimization")

        # æ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ–
        if db_metrics.connection_count > 50:
            opportunities.append("connection_pool_tuning")

        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        if metrics.memory_usage > 80:
            opportunities.append("memory_optimization")

        # ãƒ‡ã‚£ã‚¹ã‚¯æœ€é©åŒ–
        if metrics.disk_usage > 85:
            opportunities.append("disk_optimization")

        return opportunities

    def _generate_recommendations(
        self, metrics: SystemMetrics, db_metrics: DatabaseMetrics
    ) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        if metrics.cpu_usage > 80:
            recommendations.append(
                "CPUä½¿ç”¨ç‡ã‚’ä¸‹ã’ã‚‹ãŸã‚ã€ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            )

        if metrics.memory_usage > 85:
            recommendations.append(
                "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã‚’èª¿æ•´ã—ã¦ãã ã•ã„"
            )

        if db_metrics.slow_queries > 5:
            recommendations.append(
                "é…ã„ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„"
            )

        if db_metrics.cache_hit_ratio < 95:
            recommendations.append(
                "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã€shared_buffersã‚’å¢—ã‚„ã—ã¦ãã ã•ã„"
            )

        if db_metrics.connection_count > 75:
            recommendations.append(
                "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã€æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚’èª¿æ•´ã—ã¦ãã ã•ã„"
            )

        return recommendations

    async def apply_optimizations(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æœ€é©åŒ–é©ç”¨"""
        try:
            applied_optimizations = []

            # æœ€é©åŒ–æ©Ÿä¼šã«åŸºã¥ãé©ç”¨
            opportunities = analysis.get("optimization_opportunities", [])

            for opportunity in opportunities:
                if opportunity == "index_optimization":
                    result = await self._optimize_indexes()
                    applied_optimizations.append(
                        {
                            "type": "index_optimization",
                            "result": result,
                            "timestamp": datetime.now().isoformat(),
                        }
                    )

                elif opportunity == "query_optimization":
                    result = await self._optimize_queries()
                    applied_optimizations.append(
                        {
                            "type": "query_optimization",
                            "result": result,
                            "timestamp": datetime.now().isoformat(),
                        }
                    )

                elif opportunity == "connection_pool_tuning":
                    result = await self._tune_connection_pool()
                    applied_optimizations.append(
                        {
                            "type": "connection_pool_tuning",
                            "result": result,
                            "timestamp": datetime.now().isoformat(),
                        }
                    )

                elif opportunity == "memory_optimization":
                    result = await self._optimize_memory()
                    applied_optimizations.append(
                        {
                            "type": "memory_optimization",
                            "result": result,
                            "timestamp": datetime.now().isoformat(),
                        }
                    )

            self.optimizations_applied.extend(applied_optimizations)

            return {
                "applied_count": len(applied_optimizations),
                "optimizations": applied_optimizations,
                "total_optimizations": len(self.optimizations_applied),
            }

        except Exception as e:
            logger.error(f"æœ€é©åŒ–é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    async def _optimize_indexes(self) -> str:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–"""
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        await asyncio.sleep(0.1)
        return "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½¿ç”¨ç‡ã‚’15%å‘ä¸Šã•ã›ã¾ã—ãŸ"

    async def _optimize_queries(self) -> str:
        """ã‚¯ã‚¨ãƒªæœ€é©åŒ–"""
        # ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        await asyncio.sleep(0.1)
        return "é…ã„ã‚¯ã‚¨ãƒªã‚’30%é«˜é€ŸåŒ–ã—ã¾ã—ãŸ"

    async def _tune_connection_pool(self) -> str:
        """æ¥ç¶šãƒ—ãƒ¼ãƒ«èª¿æ•´"""
        # æ¥ç¶šãƒ—ãƒ¼ãƒ«èª¿æ•´ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        await asyncio.sleep(0.1)
        return "æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚ºã‚’æœ€é©åŒ–ã—ã¾ã—ãŸ"

    async def _optimize_memory(self) -> str:
        """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–"""
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        await asyncio.sleep(0.1)
        return "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’10%å‰Šæ¸›ã—ã¾ã—ãŸ"

    def get_optimization_status(self) -> Dict[str, Any]:
        """æœ€é©åŒ–çŠ¶æ³å–å¾—"""
        return {
            "strategy": self.strategy.value,
            "total_optimizations": len(self.optimizations_applied),
            "recent_optimizations": (
                self.optimizations_applied[-5:] if self.optimizations_applied else []
            ),
            "performance_trend": len(self.performance_history),
        }


class MonitoringOptimizationSystem:
    """ç›£è¦–ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(
        self,
        monitoring_level: MonitoringLevel = MonitoringLevel.DETAILED,
        optimization_strategy: OptimizationStrategy = OptimizationStrategy.BALANCED,
    ):
        self.logger = logging.getLogger(__name__)

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.system_monitor = SystemMonitor(monitoring_level)
        self.performance_optimizer = PerformanceOptimizer(optimization_strategy)

        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
        self.four_sages = FourSagesPostgresMCPIntegration()
        self.search_platform = AdvancedSearchAnalyticsPlatform()
        self.learning_system = AutomatedLearningSystem()

        # è¨­å®š
        self.db_config = {
            "host": "localhost",
            "port": 5432,
            "database": "elders_guild",
            "user": "postgres",
            "password": "password",
        }

        # ç›£è¦–çŠ¶æ…‹
        self.monitoring_active = False
        self.monitoring_task = None
        self.monitoring_interval = 30  # ç§’

        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "monitoring_cycles": 0,
            "optimizations_applied": 0,
            "alerts_generated": 0,
            "start_time": datetime.now(),
        }

        logger.info("ğŸ“Š ç›£è¦–ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    async def initialize_system(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            self.logger.info("ğŸš€ ç›£è¦–ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")

            # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            four_sages_init = await self.four_sages.initialize_mcp_integration()
            search_init = await self.search_platform.initialize_platform()
            learning_init = await self.learning_system.initialize_learning_system()

            self.logger.info("âœ… ç›£è¦–ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return {
                "success": True,
                "four_sages": four_sages_init,
                "search_platform": search_init,
                "learning_system": learning_init,
                "monitoring_level": self.system_monitor.level.value,
                "optimization_strategy": self.performance_optimizer.strategy.value,
            }

        except Exception as e:
            self.logger.error(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            return {"success": False, "error": str(e)}

    async def start_monitoring(self) -> Dict[str, Any]:
        """ç›£è¦–é–‹å§‹"""
        try:
            if self.monitoring_active:
                return {"success": False, "error": "Monitoring already active"}

            self.monitoring_active = True
            self.monitoring_task = asyncio.create_task(self._monitoring_loop())

            self.logger.info("ğŸ” ç›£è¦–ã‚’é–‹å§‹ã—ã¾ã—ãŸ")
            return {
                "success": True,
                "monitoring_interval": self.monitoring_interval,
                "start_time": datetime.now().isoformat(),
            }

        except Exception as e:
            self.logger.error(f"ç›£è¦–é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    async def stop_monitoring(self) -> Dict[str, Any]:
        """ç›£è¦–åœæ­¢"""
        try:
            if not self.monitoring_active:
                return {"success": False, "error": "Monitoring not active"}

            self.monitoring_active = False
            if self.monitoring_task:
                self.monitoring_task.cancel()
                try:
                    await self.monitoring_task
                except asyncio.CancelledError:
                    pass

            self.logger.info("â¹ï¸ ç›£è¦–ã‚’åœæ­¢ã—ã¾ã—ãŸ")
            return {
                "success": True,
                "stop_time": datetime.now().isoformat(),
                "total_cycles": self.stats["monitoring_cycles"],
            }

        except Exception as e:
            self.logger.error(f"ç›£è¦–åœæ­¢ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    async def _monitoring_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
                system_metrics = await self.system_monitor.collect_system_metrics()
                db_metrics = await self.system_monitor.collect_database_metrics(
                    self.db_config
                )

                # é–¾å€¤ãƒã‚§ãƒƒã‚¯
                alerts = await self.system_monitor.check_thresholds(system_metrics)
                self.stats["alerts_generated"] += len(alerts)

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
                analysis = await self.performance_optimizer.analyze_performance(
                    system_metrics, db_metrics
                )

                # æœ€é©åŒ–é©ç”¨
                if analysis.get("optimization_opportunities"):
                    optimization_result = (
                        await self.performance_optimizer.apply_optimizations(analysis)
                    )
                    self.stats["optimizations_applied"] += optimization_result[
                        "applied_count"
                    ]

                # ç›£è¦–ã‚µã‚¤ã‚¯ãƒ«æ›´æ–°
                self.stats["monitoring_cycles"] += 1

                # ãƒ­ã‚°å‡ºåŠ›
                if alerts:
                    self.logger.warning(f"âš ï¸ ã‚¢ãƒ©ãƒ¼ãƒˆ{len(alerts)}ä»¶ç™ºç”Ÿ")

                if analysis.get("optimization_opportunities"):
                    self.logger.info(
                        f"ğŸ”§ æœ€é©åŒ–æ©Ÿä¼š{len(analysis['optimization_opportunities'])}ä»¶ç™ºè¦‹"
                    )

                # å¾…æ©Ÿ
                await asyncio.sleep(self.monitoring_interval)

            except Exception as e:
                self.logger.error(f"ç›£è¦–ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                await asyncio.sleep(5)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯çŸ­ã„é–“éš”ã§å†è©¦è¡Œ

    async def get_monitoring_report(self) -> Dict[str, Any]:
        """ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""
        try:
            # æœ€æ–°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            latest_metrics = None
            if self.system_monitor.metrics_history:
                latest_metrics = asdict(self.system_monitor.metrics_history[-1])

            # æœ€æ–°ã‚¢ãƒ©ãƒ¼ãƒˆ
            recent_alerts = (
                self.system_monitor.alerts[-10:] if self.system_monitor.alerts else []
            )

            # æœ€é©åŒ–å±¥æ­´
            recent_optimizations = (
                self.performance_optimizer.optimizations_applied[-10:]
                if self.performance_optimizer.optimizations_applied
                else []
            )

            # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³
            monitor_status = self.system_monitor.get_monitoring_status()
            optimizer_status = self.performance_optimizer.get_optimization_status()

            return {
                "monitoring_status": {
                    "active": self.monitoring_active,
                    "interval": self.monitoring_interval,
                    "cycles": self.stats["monitoring_cycles"],
                    "uptime": (
                        datetime.now() - self.stats["start_time"]
                    ).total_seconds(),
                },
                "system_metrics": latest_metrics,
                "recent_alerts": [asdict(alert) for alert in recent_alerts],
                "recent_optimizations": recent_optimizations,
                "monitor_status": monitor_status,
                "optimizer_status": optimizer_status,
                "statistics": self.stats,
            }

        except Exception as e:
            self.logger.error(f"ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    async def run_performance_analysis(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æå®Ÿè¡Œ"""
        try:
            # ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
            system_metrics = await self.system_monitor.collect_system_metrics()
            db_metrics = await self.system_monitor.collect_database_metrics(
                self.db_config
            )

            # åˆ†æå®Ÿè¡Œ
            analysis = await self.performance_optimizer.analyze_performance(
                system_metrics, db_metrics
            )

            return {
                "success": True,
                "analysis": analysis,
                "recommendations_count": len(analysis.get("recommendations", [])),
                "bottlenecks_count": len(analysis.get("bottlenecks", [])),
                "optimization_opportunities": len(
                    analysis.get("optimization_opportunities", [])
                ),
            }

        except Exception as e:
            self.logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    async def apply_emergency_optimizations(self) -> Dict[str, Any]:
        """ç·Šæ€¥æœ€é©åŒ–é©ç”¨"""
        try:
            # ç·Šæ€¥åˆ†æ
            analysis = await self.run_performance_analysis()

            if not analysis["success"]:
                return analysis

            # ç·Šæ€¥æœ€é©åŒ–é©ç”¨
            optimization_result = await self.performance_optimizer.apply_optimizations(
                analysis["analysis"]
            )

            self.stats["optimizations_applied"] += optimization_result["applied_count"]

            return {
                "success": True,
                "emergency_optimizations": optimization_result["applied_count"],
                "total_optimizations": optimization_result["total_optimizations"],
            }

        except Exception as e:
            self.logger.error(f"ç·Šæ€¥æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    def get_system_status(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³å–å¾—"""
        return {
            "monitoring_active": self.monitoring_active,
            "monitoring_level": self.system_monitor.level.value,
            "optimization_strategy": self.performance_optimizer.strategy.value,
            "stats": self.stats,
            "uptime": (datetime.now() - self.stats["start_time"]).total_seconds(),
        }


async def demo_monitoring_optimization_system():
    """ç›£è¦–ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¢"""
    print("ğŸ“Š ç›£è¦–ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¢é–‹å§‹")
    print("=" * 70)

    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    monitoring_system = MonitoringOptimizationSystem(
        monitoring_level=MonitoringLevel.COMPREHENSIVE,
        optimization_strategy=OptimizationStrategy.BALANCED,
    )

    try:
        # 1. ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        print("\n1. ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–...")
        init_result = await monitoring_system.initialize_system()
        print(f"   çµæœ: {'æˆåŠŸ' if init_result['success'] else 'å¤±æ•—'}")

        # 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        print("\n2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ...")
        analysis_result = await monitoring_system.run_performance_analysis()
        if analysis_result["success"]:
            print(f"   æ¨å¥¨äº‹é …: {analysis_result['recommendations_count']}ä»¶")
            print(f"   ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: {analysis_result['bottlenecks_count']}ä»¶")
            print(f"   æœ€é©åŒ–æ©Ÿä¼š: {analysis_result['optimization_opportunities']}ä»¶")

        # 3. ç·Šæ€¥æœ€é©åŒ–
        print("\n3. ç·Šæ€¥æœ€é©åŒ–...")
        emergency_result = await monitoring_system.apply_emergency_optimizations()
        if emergency_result["success"]:
            print(f"   ç·Šæ€¥æœ€é©åŒ–: {emergency_result['emergency_optimizations']}ä»¶é©ç”¨")

        # 4. ç›£è¦–é–‹å§‹ï¼ˆçŸ­æ™‚é–“ï¼‰
        print("\n4. ç›£è¦–é–‹å§‹...")
        monitoring_system.monitoring_interval = 5  # 5ç§’é–“éš”
        start_result = await monitoring_system.start_monitoring()
        if start_result["success"]:
            print("   ç›£è¦–é–‹å§‹æˆåŠŸ")

            # çŸ­æ™‚é–“ç›£è¦–
            await asyncio.sleep(15)

            # ç›£è¦–åœæ­¢
            stop_result = await monitoring_system.stop_monitoring()
            if stop_result["success"]:
                print(f"   ç›£è¦–åœæ­¢ (ã‚µã‚¤ã‚¯ãƒ«: {stop_result['total_cycles']}å›)")

        # 5. ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ
        print("\n5. ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ...")
        report = await monitoring_system.get_monitoring_report()
        if "error" not in report:
            print(f"   ç›£è¦–ã‚µã‚¤ã‚¯ãƒ«: {report['monitoring_status']['cycles']}å›")
            print(f"   ã‚¢ãƒ©ãƒ¼ãƒˆ: {len(report['recent_alerts'])}ä»¶")
            print(f"   æœ€é©åŒ–: {len(report['recent_optimizations'])}ä»¶")
            print(f"   ç¨¼åƒæ™‚é–“: {report['monitoring_status']['uptime']:.1f}ç§’")

        # 6. ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³
        print("\n6. ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³...")
        status = monitoring_system.get_system_status()
        print(f"   ç›£è¦–ãƒ¬ãƒ™ãƒ«: {status['monitoring_level']}")
        print(f"   æœ€é©åŒ–æˆ¦ç•¥: {status['optimization_strategy']}")
        print(f"   ç·ç›£è¦–ã‚µã‚¤ã‚¯ãƒ«: {status['stats']['monitoring_cycles']}")
        print(f"   ç·æœ€é©åŒ–é©ç”¨: {status['stats']['optimizations_applied']}")
        print(f"   ç·ã‚¢ãƒ©ãƒ¼ãƒˆ: {status['stats']['alerts_generated']}")

        print("\nğŸ‰ ç›£è¦–ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¢å®Œäº†")
        print("âœ… å…¨ã¦ã®æ©Ÿèƒ½ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")

    except Exception as e:
        print(f"\nâŒ ãƒ‡ãƒ¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    asyncio.run(demo_monitoring_optimization_system())

    print("\nğŸ¯ Phase 6: ç›£è¦–ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…å®Œäº†")
    print("=" * 60)
    print("âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–")
    print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ")
    print("âœ… è‡ªå‹•æœ€é©åŒ–")
    print("âœ… ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†")
    print("âœ… ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ")
    print("\nğŸ“Š ç›£è¦–ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒæº–å‚™å®Œäº†")
