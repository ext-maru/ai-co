#!/usr/bin/env python3
"""
ä¸¦åˆ—å®Ÿè¡Œç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  - ç‹¬ç«‹ã—ãŸã‚¿ã‚¹ã‚¯ã®ä¸¦åˆ—å®Ÿè¡Œã¨åŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹ç®¡ç†

IntelligentTaskSplitterã§åˆ†å‰²ã•ã‚ŒãŸç‹¬ç«‹ã‚¿ã‚¹ã‚¯ã‚’åŠ¹ç‡çš„ã«ä¸¦åˆ—å®Ÿè¡Œã—ã€
ãƒªã‚½ãƒ¼ã‚¹ã®æœ€é©åŒ–ã¨ã‚¿ã‚¹ã‚¯ã®å„ªå…ˆåº¦ç®¡ç†ã‚’è¡Œã†
"""

import json
import logging
import sqlite3

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
import sys
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import pika

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from core import BaseManager
from libs.intelligent_task_splitter import (
    IntelligentTaskSplitter,
    SubTask,
    TaskComplexity,
)

logger = logging.getLogger(__name__)


class ExecutionStatus(Enum):
    """å®Ÿè¡ŒçŠ¶æ…‹"""

    PENDING = "pending"  # å¾…æ©Ÿä¸­
    RUNNING = "running"  # å®Ÿè¡Œä¸­
    COMPLETED = "completed"  # å®Œäº†
    FAILED = "failed"  # å¤±æ•—
    CANCELLED = "cancelled"  # ã‚­ãƒ£ãƒ³ã‚»ãƒ«
    TIMEOUT = "timeout"  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    DEPENDENCY_BLOCKED = "dependency_blocked"  # ä¾å­˜é–¢ä¿‚ã§ãƒ–ãƒ­ãƒƒã‚¯


class ResourceType(Enum):
    """ãƒªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—"""

    CPU = "cpu"
    MEMORY = "memory"
    NETWORK = "network"
    DISK = "disk"
    WORKER = "worker"


@dataclass
class ExecutionGroup:
    """ä¸¦åˆ—å®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—"""

    group_id: str
    project_id: str
    phase: str
    tasks: List[SubTask]
    max_parallel: int
    priority: int
    estimated_duration: float
    resource_requirements: Dict[ResourceType, float]
    created_at: datetime


@dataclass
class ExecutionResult:
    """å®Ÿè¡Œçµæœ"""

    task_id: str
    status: ExecutionStatus
    start_time: datetime
    end_time: Optional[datetime]
    duration: float
    output: str
    error: Optional[str]
    resource_usage: Dict[ResourceType, float]
    worker_id: Optional[str]


class ParallelExecutionManager(BaseManager):
    """ä¸¦åˆ—å®Ÿè¡Œç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, max_workers: int = 4):
        super().__init__("ParallelExecutionManager")
        self.db_path = PROJECT_ROOT / "db" / "parallel_execution.db"
        self.max_workers = max_workers
        self.task_splitter = IntelligentTaskSplitter()

        # ThreadPoolExecutor for parallel execution
        self.executor = ThreadPoolExecutor(max_workers=max_workers)

        # RabbitMQæ¥ç¶š
        self.connection = None
        self.channel = None

        # å®Ÿè¡ŒçŠ¶æ…‹ç®¡ç†
        self.running_tasks: Dict[str, threading.Thread] = {}
        self.execution_groups: Dict[str, ExecutionGroup] = {}
        self.resource_usage: Dict[ResourceType, float] = {
            ResourceType.CPU: 0.0,
            ResourceType.MEMORY: 0.0,
            ResourceType.NETWORK: 0.0,
            ResourceType.DISK: 0.0,
            ResourceType.WORKER: 0.0,
        }

        # ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™è¨­å®š
        self.resource_limits = {
            ResourceType.CPU: 80.0,  # CPUä½¿ç”¨ç‡80%ã¾ã§
            ResourceType.MEMORY: 70.0,  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡70%ã¾ã§
            ResourceType.NETWORK: 60.0,  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½¿ç”¨ç‡60%ã¾ã§
            ResourceType.DISK: 50.0,  # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡50%ã¾ã§
            ResourceType.WORKER: 100.0,  # ãƒ¯ãƒ¼ã‚«ãƒ¼100%ã¾ã§
        }

        # å®Ÿè¡Œçµ±è¨ˆ
        self.execution_stats = {
            "total_tasks": 0,
            "completed_tasks": 0,
            "failed_tasks": 0,
            "parallel_groups": 0,
            "avg_execution_time": 0.0,
            "resource_efficiency": 0.0,
        }

        self.initialize()

    def initialize(self) -> bool:
        """åˆæœŸåŒ–å‡¦ç†"""
        try:
            self._init_database()
            self._connect_rabbitmq()
            return True
        except Exception as e:
            self.handle_error(e, "åˆæœŸåŒ–")
            return False

    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        with sqlite3.connect(self.db_path) as conn:
            # å®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS execution_groups (
                    group_id TEXT PRIMARY KEY,
                    project_id TEXT NOT NULL,
                    phase TEXT NOT NULL,
                    task_count INTEGER NOT NULL,
                    max_parallel INTEGER NOT NULL,
                    priority INTEGER DEFAULT 0,
                    estimated_duration REAL,
                    resource_requirements TEXT,
                    status TEXT DEFAULT 'pending',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    started_at TIMESTAMP,
                    completed_at TIMESTAMP
                )
            """
            )

            # ä¸¦åˆ—å®Ÿè¡Œå±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS execution_history (
                    execution_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    task_id TEXT NOT NULL,
                    group_id TEXT NOT NULL,
                    worker_id TEXT,
                    status TEXT NOT NULL,
                    start_time TIMESTAMP NOT NULL,
                    end_time TIMESTAMP,
                    duration REAL,
                    output TEXT,
                    error TEXT,
                    resource_usage TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (group_id) REFERENCES execution_groups(group_id)
                )
            """
            )

            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³ãƒ†ãƒ¼ãƒ–ãƒ«
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS resource_usage (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    resource_type TEXT NOT NULL,
                    usage_percentage REAL NOT NULL,
                    limit_percentage REAL NOT NULL,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # å®Ÿè¡Œçµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS execution_statistics (
                    stat_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    total_tasks INTEGER DEFAULT 0,
                    completed_tasks INTEGER DEFAULT 0,
                    failed_tasks INTEGER DEFAULT 0,
                    parallel_groups INTEGER DEFAULT 0,
                    avg_execution_time REAL DEFAULT 0.0,
                    resource_efficiency REAL DEFAULT 0.0,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_execution_groups_project ON execution_groups(project_id)"
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_execution_history_task ON execution_history(task_id)"
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_resource_usage_type ON resource_usage(resource_type)"
            )

    def _connect_rabbitmq(self):
        """RabbitMQæ¥ç¶š"""
        try:
            self.connection = pika.BlockingConnection(
                pika.ConnectionParameters("localhost")
            )
            self.channel = self.connection.channel()

            # ã‚­ãƒ¥ãƒ¼å®£è¨€
            self.channel.queue_declare(
                queue="ai_tasks", durable=True, arguments={"x-max-priority": 10}
            )
            self.channel.queue_declare(
                queue="parallel_tasks", durable=True, arguments={"x-max-priority": 10}
            )

            logger.info("âœ… RabbitMQæ¥ç¶šæˆåŠŸ")
        except Exception as e:
            logger.warning(f"RabbitMQæ¥ç¶šå¤±æ•—: {e}")
            self.connection = None
            self.channel = None

    def create_execution_group(
        self,
        project_id: str,
        phase: str,
        tasks: List[SubTask],
        max_parallel: int = None,
    ) -> str:
        """å®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆ"""
        try:
            group_id = (
                f"group_{project_id}_{phase}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )

            # ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½æ•°ã‚’æ±ºå®š
            if max_parallel is None:
                parallel_tasks = [t for t in tasks if t.can_parallel]
                max_parallel = min(len(parallel_tasks), self.max_workers)

            # ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶ã‚’è¨ˆç®—
            resource_requirements = self._calculate_resource_requirements(tasks)

            # å®Ÿè¡Œæ™‚é–“ã‚’æ¨å®š
            estimated_duration = self._estimate_execution_time(tasks, max_parallel)

            # å®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
            execution_group = ExecutionGroup(
                group_id=group_id,
                project_id=project_id,
                phase=phase,
                tasks=tasks,
                max_parallel=max_parallel,
                priority=max([t.priority for t in tasks]),
                estimated_duration=estimated_duration,
                resource_requirements=resource_requirements,
                created_at=datetime.now(),
            )

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    """
                    INSERT INTO execution_groups
                    (group_id, project_id, phase, task_count, max_parallel, priority,
                     estimated_duration, resource_requirements, status)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        group_id,
                        project_id,
                        phase,
                        len(tasks),
                        max_parallel,
                        execution_group.priority,
                        estimated_duration,
                        json.dumps(
                            {k.value: v for k, v in resource_requirements.items()}
                        ),
                        ExecutionStatus.PENDING.value,
                    ),
                )

            self.execution_groups[group_id] = execution_group

            logger.info(f"ğŸ“¦ å®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆ: {group_id} ({len(tasks)}ã‚¿ã‚¹ã‚¯, ä¸¦åˆ—åº¦{max_parallel})")
            return group_id

        except Exception as e:
            logger.error(f"å®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def execute_group_parallel(self, group_id: str) -> bool:
        """ã‚°ãƒ«ãƒ¼ãƒ—ã®ä¸¦åˆ—å®Ÿè¡Œ"""
        try:
            if group_id not in self.execution_groups:
                logger.error(f"å®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {group_id}")
                return False

            execution_group = self.execution_groups[group_id]

            # ãƒªã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯
            if not self._check_resource_availability(
                execution_group.resource_requirements
            ):
                logger.warning(f"ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ã«ã‚ˆã‚Šå®Ÿè¡Œå»¶æœŸ: {group_id}")
                return False

            logger.info(f"ğŸš€ ä¸¦åˆ—å®Ÿè¡Œé–‹å§‹: {group_id}")

            # å®Ÿè¡Œé–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
            start_time = datetime.now()
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    """
                    UPDATE execution_groups
                    SET status = ?, started_at = ?
                    WHERE group_id = ?
                """,
                    (ExecutionStatus.RUNNING.value, start_time, group_id),
                )

            # ã‚¿ã‚¹ã‚¯ã‚’ä¾å­˜é–¢ä¿‚ã«åŸºã¥ã„ã¦åˆ†é›¢
            independent_tasks, dependent_tasks = self._separate_tasks_by_dependencies(
                execution_group.tasks
            )

            # ç‹¬ç«‹ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            parallel_results = []
            if independent_tasks:
                parallel_results = self._execute_tasks_parallel(
                    independent_tasks, execution_group.max_parallel
                )

            # ä¾å­˜é–¢ä¿‚ã®ã‚ã‚‹ã‚¿ã‚¹ã‚¯ã‚’é †æ¬¡å®Ÿè¡Œ
            sequential_results = []
            if dependent_tasks:
                sequential_results = self._execute_tasks_sequential(dependent_tasks)

            # å…¨çµæœã‚’ã¾ã¨ã‚ã‚‹
            all_results = parallel_results + sequential_results

            # å®Ÿè¡Œå®Œäº†å‡¦ç†
            self._complete_group_execution(group_id, all_results, start_time)

            # çµ±è¨ˆã‚’æ›´æ–°
            self._update_execution_statistics(all_results)

            logger.info(f"âœ… ä¸¦åˆ—å®Ÿè¡Œå®Œäº†: {group_id} ({len(all_results)}ã‚¿ã‚¹ã‚¯)")
            return True

        except Exception as e:
            logger.error(f"ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _separate_tasks_by_dependencies(
        self, tasks: List[SubTask]
    ) -> Tuple[List[SubTask], List[SubTask]]:
        """ã‚¿ã‚¹ã‚¯ã‚’ä¾å­˜é–¢ä¿‚ã«åŸºã¥ã„ã¦åˆ†é›¢"""
        independent_tasks = []
        dependent_tasks = []

        for task in tasks:
            if not task.dependencies or len(task.dependencies) == 0:
                independent_tasks.append(task)
            else:
                dependent_tasks.append(task)

        logger.info(f"ğŸ“Š ã‚¿ã‚¹ã‚¯åˆ†é›¢: ç‹¬ç«‹{len(independent_tasks)}, ä¾å­˜{len(dependent_tasks)}")
        return independent_tasks, dependent_tasks

    def _execute_tasks_parallel(
        self, tasks: List[SubTask], max_parallel: int
    ) -> List[ExecutionResult]:
        """ä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ"""
        try:
            logger.info(f"ğŸ”„ ä¸¦åˆ—å®Ÿè¡Œé–‹å§‹: {len(tasks)}ã‚¿ã‚¹ã‚¯ (ä¸¦åˆ—åº¦{max_parallel})")

            # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
            tasks.sort(key=lambda t: t.priority, reverse=True)

            # ä¸¦åˆ—å®Ÿè¡Œ
            results = []
            future_to_task = {}

            with ThreadPoolExecutor(max_workers=max_parallel) as executor:
                # ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
                for task in tasks:
                    future = executor.submit(self._execute_single_task, task)
                    future_to_task[future] = task

                # çµæœã‚’åé›†
                for future in as_completed(future_to_task, timeout=300):  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    task = future_to_task[future]
                    try:
                        result = future.result()
                        results.append(result)
                        logger.info(f"âœ… ä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Œäº†: {task.id}")
                    except Exception as e:
                        logger.error(f"âŒ ä¸¦åˆ—ã‚¿ã‚¹ã‚¯å¤±æ•—: {task.id} - {e}")
                        results.append(
                            ExecutionResult(
                                task_id=task.id,
                                status=ExecutionStatus.FAILED,
                                start_time=datetime.now(),
                                end_time=datetime.now(),
                                duration=0.0,
                                output="",
                                error=str(e),
                                resource_usage={},
                                worker_id=None,
                            )
                        )

            logger.info(f"ğŸ¯ ä¸¦åˆ—å®Ÿè¡Œå®Œäº†: {len(results)}çµæœ")
            return results

        except Exception as e:
            logger.error(f"ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _execute_tasks_sequential(self, tasks: List[SubTask]) -> List[ExecutionResult]:
        """ä¾å­˜é–¢ä¿‚ã®ã‚ã‚‹ã‚¿ã‚¹ã‚¯ã®é †æ¬¡å®Ÿè¡Œ"""
        try:
            logger.info(f"ğŸ”„ é †æ¬¡å®Ÿè¡Œé–‹å§‹: {len(tasks)}ã‚¿ã‚¹ã‚¯")

            # ä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ã—ã¦å®Ÿè¡Œé †åºã‚’æ±ºå®š
            ordered_tasks = self._order_tasks_by_dependencies(tasks)

            results = []
            for task in ordered_tasks:
                # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
                if self._check_task_dependencies(task, results):
                    result = self._execute_single_task(task)
                    results.append(result)
                    logger.info(f"âœ… é †æ¬¡ã‚¿ã‚¹ã‚¯å®Œäº†: {task.id}")
                else:
                    logger.warning(f"âš ï¸ ä¾å­˜é–¢ä¿‚æœªè§£æ±º: {task.id}")
                    results.append(
                        ExecutionResult(
                            task_id=task.id,
                            status=ExecutionStatus.DEPENDENCY_BLOCKED,
                            start_time=datetime.now(),
                            end_time=datetime.now(),
                            duration=0.0,
                            output="",
                            error="ä¾å­˜é–¢ä¿‚æœªè§£æ±º",
                            resource_usage={},
                            worker_id=None,
                        )
                    )

            logger.info(f"ğŸ¯ é †æ¬¡å®Ÿè¡Œå®Œäº†: {len(results)}çµæœ")
            return results

        except Exception as e:
            logger.error(f"é †æ¬¡å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _execute_single_task(self, task: SubTask) -> ExecutionResult:
        """å˜ä¸€ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œ"""
        start_time = datetime.now()

        try:
            logger.info(f"ğŸ”¥ ã‚¿ã‚¹ã‚¯å®Ÿè¡Œé–‹å§‹: {task.id}")

            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã‚’äºˆæ¸¬ãƒ»è¨˜éŒ²
            predicted_resource_usage = self._predict_resource_usage(task)
            self._update_resource_usage(predicted_resource_usage, add=True)

            # ã‚¿ã‚¹ã‚¯ã‚’ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚­ãƒ¥ãƒ¼ã«é€ä¿¡
            if self.channel:
                task_data = {
                    "task_id": task.id,
                    "task_type": "parallel_task",
                    "prompt": f"{task.title}\n\n{task.description}",
                    "complexity": task.complexity.value,
                    "estimated_hours": task.estimated_hours,
                    "required_skills": task.required_skills,
                    "priority": task.priority,
                    "can_parallel": task.can_parallel,
                    "dependencies": task.dependencies,
                    "created_at": datetime.now().isoformat(),
                }

                self.channel.basic_publish(
                    exchange="",
                    routing_key="parallel_tasks",
                    body=json.dumps(task_data, ensure_ascii=False),
                    properties=pika.BasicProperties(
                        delivery_mode=2, priority=min(task.priority + 3, 10)  # æ°¸ç¶šåŒ–
                    ),
                )

                # å®Ÿè¡Œå®Œäº†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã¯çµæœå¾…ã¡ï¼‰
                execution_time = self._simulate_execution_time(task)
                time.sleep(min(execution_time, 1.0))  # æœ€å¤§1ç§’å¾…æ©Ÿ

                output = f"ã‚¿ã‚¹ã‚¯å®Ÿè¡Œå®Œäº†: {task.title}"
                status = ExecutionStatus.COMPLETED
                error = None

            else:
                # RabbitMQæ¥ç¶šãŒãªã„å ´åˆã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                execution_time = self._simulate_execution_time(task)
                time.sleep(min(execution_time, 0.5))  # æœ€å¤§0.5ç§’å¾…æ©Ÿ

                output = f"[ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³] ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ: {task.title}"
                status = ExecutionStatus.COMPLETED
                error = None

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã‚’è§£é™¤
            self._update_resource_usage(predicted_resource_usage, add=False)

            result = ExecutionResult(
                task_id=task.id,
                status=status,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                output=output,
                error=error,
                resource_usage=predicted_resource_usage,
                worker_id=f"worker_{threading.current_thread().ident}",
            )

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
            self._save_execution_result(result)

            logger.info(f"âœ… ã‚¿ã‚¹ã‚¯å®Ÿè¡Œå®Œäº†: {task.id} ({duration:.1f}ç§’)")
            return result

        except Exception as e:
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            logger.error(f"âŒ ã‚¿ã‚¹ã‚¯å®Ÿè¡Œå¤±æ•—: {task.id} - {e}")

            return ExecutionResult(
                task_id=task.id,
                status=ExecutionStatus.FAILED,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                output="",
                error=str(e),
                resource_usage={},
                worker_id=None,
            )

    def _order_tasks_by_dependencies(self, tasks: List[SubTask]) -> List[SubTask]:
        """ä¾å­˜é–¢ä¿‚ã«åŸºã¥ã„ã¦ã‚¿ã‚¹ã‚¯ã‚’ä¸¦ã³æ›¿ãˆ"""
        ordered = []
        remaining = tasks.copy()

        while remaining:
            # ä¾å­˜é–¢ä¿‚ã®ãªã„ã‚¿ã‚¹ã‚¯ã‚’æ¢ã™
            ready_tasks = []
            for task in remaining:
                if not task.dependencies or all(
                    dep in [t.id for t in ordered] for dep in task.dependencies
                ):
                    ready_tasks.append(task)

            if not ready_tasks:
                # å¾ªç’°ä¾å­˜é–¢ä¿‚ã®å¯èƒ½æ€§
                logger.warning("å¾ªç’°ä¾å­˜é–¢ä¿‚æ¤œå‡º - æ®‹ã‚Šã‚¿ã‚¹ã‚¯ã‚’å¼·åˆ¶è¿½åŠ ")
                ready_tasks = remaining

            # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
            ready_tasks.sort(key=lambda t: t.priority, reverse=True)

            # æœ€åˆã®ã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ 
            task = ready_tasks[0]
            ordered.append(task)
            remaining.remove(task)

        return ordered

    def _check_task_dependencies(
        self, task: SubTask, completed_results: List[ExecutionResult]
    ) -> bool:
        """ã‚¿ã‚¹ã‚¯ã®ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯"""
        if not task.dependencies:
            return True

        completed_task_ids = {
            result.task_id
            for result in completed_results
            if result.status == ExecutionStatus.COMPLETED
        }

        return all(dep_id in completed_task_ids for dep_id in task.dependencies)

    def _calculate_resource_requirements(
        self, tasks: List[SubTask]
    ) -> Dict[ResourceType, float]:
        """ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶è¨ˆç®—"""
        requirements = {resource: 0.0 for resource in ResourceType}

        for task in tasks:
            # è¤‡é›‘åº¦ã«åŸºã¥ã„ã¦ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶ã‚’è¨ˆç®—
            complexity_factor = {
                TaskComplexity.SIMPLE: 1.0,
                TaskComplexity.MODERATE: 2.0,
                TaskComplexity.COMPLEX: 4.0,
                TaskComplexity.VERY_COMPLEX: 8.0,
            }.get(task.complexity, 2.0)

            requirements[ResourceType.CPU] += complexity_factor * 10
            requirements[ResourceType.MEMORY] += complexity_factor * 5
            requirements[ResourceType.NETWORK] += complexity_factor * 2
            requirements[ResourceType.DISK] += complexity_factor * 1
            requirements[ResourceType.WORKER] += 1

        return requirements

    def _estimate_execution_time(
        self, tasks: List[SubTask], max_parallel: int
    ) -> float:
        """å®Ÿè¡Œæ™‚é–“æ¨å®š"""
        # ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½ã‚¿ã‚¹ã‚¯ã¨é †æ¬¡å®Ÿè¡Œã‚¿ã‚¹ã‚¯ã‚’åˆ†é›¢
        parallel_tasks = [t for t in tasks if t.can_parallel]
        sequential_tasks = [t for t in tasks if not t.can_parallel]

        # ä¸¦åˆ—å®Ÿè¡Œæ™‚é–“ = æœ€é•·ã‚¿ã‚¹ã‚¯æ™‚é–“ * (ã‚¿ã‚¹ã‚¯æ•° / ä¸¦åˆ—åº¦)
        parallel_time = 0.0
        if parallel_tasks:
            avg_parallel_time = sum(t.estimated_hours for t in parallel_tasks) / len(
                parallel_tasks
            )
            parallel_time = avg_parallel_time * (len(parallel_tasks) / max_parallel)

        # é †æ¬¡å®Ÿè¡Œæ™‚é–“ = å…¨ã‚¿ã‚¹ã‚¯æ™‚é–“ã®åˆè¨ˆ
        sequential_time = sum(t.estimated_hours for t in sequential_tasks)

        return parallel_time + sequential_time

    def _check_resource_availability(
        self, requirements: Dict[ResourceType, float]
    ) -> bool:
        """ãƒªã‚½ãƒ¼ã‚¹å¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯"""
        for resource_type, required in requirements.items():
            current_usage = self.resource_usage.get(resource_type, 0.0)
            limit = self.resource_limits.get(resource_type, 100.0)

            if current_usage + required > limit:
                logger.warning(
                    f"ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³: {resource_type.value} ({current_usage + required} > {limit})"
                )
                return False

        return True

    def _predict_resource_usage(self, task: SubTask) -> Dict[ResourceType, float]:
        """ã‚¿ã‚¹ã‚¯ã®ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡äºˆæ¸¬"""
        complexity_factor = {
            TaskComplexity.SIMPLE: 1.0,
            TaskComplexity.MODERATE: 2.0,
            TaskComplexity.COMPLEX: 4.0,
            TaskComplexity.VERY_COMPLEX: 8.0,
        }.get(task.complexity, 2.0)

        return {
            ResourceType.CPU: complexity_factor * 5,
            ResourceType.MEMORY: complexity_factor * 3,
            ResourceType.NETWORK: complexity_factor * 1,
            ResourceType.DISK: complexity_factor * 0.5,
            ResourceType.WORKER: 1,
        }

    def _update_resource_usage(
        self, usage: Dict[ResourceType, float], add: bool = True
    ):
        """ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡æ›´æ–°"""
        factor = 1 if add else -1

        for resource_type, amount in usage.items():
            current = self.resource_usage.get(resource_type, 0.0)
            self.resource_usage[resource_type] = max(0.0, current + (amount * factor))

    def _simulate_execution_time(self, task: SubTask) -> float:
        """å®Ÿè¡Œæ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        base_time = {
            TaskComplexity.SIMPLE: 0.1,
            TaskComplexity.MODERATE: 0.3,
            TaskComplexity.COMPLEX: 0.5,
            TaskComplexity.VERY_COMPLEX: 1.0,
        }.get(task.complexity, 0.3)

        return base_time * (1 + task.estimated_hours * 0.1)

    def _save_execution_result(self, result: ExecutionResult):
        """å®Ÿè¡Œçµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    """
                    INSERT INTO execution_history
                    (task_id, group_id, worker_id, status, start_time, end_time,
                     duration, output, error, resource_usage)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        result.task_id,
                        "current_group",  # å®Ÿéš›ã®group_idã‚’ä½¿ç”¨
                        result.worker_id,
                        result.status.value,
                        result.start_time,
                        result.end_time,
                        result.duration,
                        result.output,
                        result.error,
                        json.dumps(
                            {k.value: v for k, v in result.resource_usage.items()}
                        ),
                    ),
                )
        except Exception as e:
            logger.error(f"å®Ÿè¡Œçµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _complete_group_execution(
        self, group_id: str, results: List[ExecutionResult], start_time: datetime
    ):
        """ã‚°ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œå®Œäº†å‡¦ç†"""
        try:
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            # å®Ÿè¡ŒçŠ¶æ…‹ã‚’æ±ºå®š
            failed_count = sum(1 for r in results if r.status == ExecutionStatus.FAILED)
            status = (
                ExecutionStatus.COMPLETED
                if failed_count == 0
                else ExecutionStatus.FAILED
            )

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    """
                    UPDATE execution_groups
                    SET status = ?, completed_at = ?
                    WHERE group_id = ?
                """,
                    (status.value, end_time, group_id),
                )

            # çµ±è¨ˆæ›´æ–°
            self.execution_stats["parallel_groups"] += 1

            logger.info(
                f"ğŸ“Š ã‚°ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œå®Œäº†: {group_id} ({duration:.1f}ç§’, æˆåŠŸ{len(results)-failed_count}/{len(results)})"
            )

        except Exception as e:
            logger.error(f"ã‚°ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œå®Œäº†å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

    def _update_execution_statistics(self, results: List[ExecutionResult]):
        """å®Ÿè¡Œçµ±è¨ˆæ›´æ–°"""
        try:
            completed_count = sum(
                1 for r in results if r.status == ExecutionStatus.COMPLETED
            )
            failed_count = sum(1 for r in results if r.status == ExecutionStatus.FAILED)
            avg_duration = (
                sum(r.duration for r in results) / len(results) if results else 0.0
            )

            # çµ±è¨ˆæ›´æ–°
            self.execution_stats["total_tasks"] += len(results)
            self.execution_stats["completed_tasks"] += completed_count
            self.execution_stats["failed_tasks"] += failed_count
            self.execution_stats["avg_execution_time"] = avg_duration

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    """
                    INSERT INTO execution_statistics
                    (total_tasks, completed_tasks, failed_tasks, parallel_groups, avg_execution_time)
                    VALUES (?, ?, ?, ?, ?)
                """,
                    (
                        self.execution_stats["total_tasks"],
                        self.execution_stats["completed_tasks"],
                        self.execution_stats["failed_tasks"],
                        self.execution_stats["parallel_groups"],
                        self.execution_stats["avg_execution_time"],
                    ),
                )

        except Exception as e:
            logger.error(f"çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def get_execution_status(self, group_id: str) -> Dict[str, Any]:
        """å®Ÿè¡ŒçŠ¶æ…‹å–å¾—"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±å–å¾—
                cursor = conn.execute(
                    """
                    SELECT project_id, phase, task_count, max_parallel, status,
                           started_at, completed_at, estimated_duration
                    FROM execution_groups WHERE group_id = ?
                """,
                    (group_id,),
                )

                row = cursor.fetchone()
                if not row:
                    return {"error": "ã‚°ãƒ«ãƒ¼ãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}

                group_info = {
                    "group_id": group_id,
                    "project_id": row[0],
                    "phase": row[1],
                    "task_count": row[2],
                    "max_parallel": row[3],
                    "status": row[4],
                    "started_at": row[5],
                    "completed_at": row[6],
                    "estimated_duration": row[7],
                }

                # å®Ÿè¡Œå±¥æ­´å–å¾—
                cursor = conn.execute(
                    """
                    SELECT task_id, status, start_time, end_time, duration, error
                    FROM execution_history WHERE group_id = ?
                    ORDER BY start_time
                """,
                    (group_id,),
                )

                tasks = []
                for row in cursor:
                    tasks.append(
                        {
                            "task_id": row[0],
                            "status": row[1],
                            "start_time": row[2],
                            "end_time": row[3],
                            "duration": row[4],
                            "error": row[5],
                        }
                    )

                group_info["tasks"] = tasks
                return group_info

        except Exception as e:
            logger.error(f"å®Ÿè¡ŒçŠ¶æ…‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def get_resource_usage(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³å–å¾—"""
        return {
            "current_usage": {k.value: v for k, v in self.resource_usage.items()},
            "limits": {k.value: v for k, v in self.resource_limits.items()},
            "utilization": {
                k.value: (v / self.resource_limits[k] * 100)
                if self.resource_limits[k] > 0
                else 0
                for k, v in self.resource_usage.items()
            },
        }

    def get_execution_statistics(self) -> Dict[str, Any]:
        """å®Ÿè¡Œçµ±è¨ˆå–å¾—"""
        return self.execution_stats.copy()

    def shutdown(self):
        """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å‡¦ç†"""
        try:
            logger.info("ğŸ”„ ParallelExecutionManager ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³é–‹å§‹")

            # ThreadPoolExecutorã‚’åœæ­¢
            if hasattr(self, "executor"):
                self.executor.shutdown(wait=True)

            # RabbitMQæ¥ç¶šã‚’é–‰ã˜ã‚‹
            if self.connection and not self.connection.is_closed:
                self.connection.close()

            logger.info("âœ… ParallelExecutionManager ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å®Œäº†")

        except Exception as e:
            logger.error(f"ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    manager = ParallelExecutionManager(max_workers=3)

    print("=" * 80)
    print("ğŸš€ Parallel Execution Manager Test")
    print("=" * 80)

    # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
    test_project_id = "test_project_002"
    test_phase = "development"

    # ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ä½œæˆ
    from libs.intelligent_task_splitter import SubTask, TaskComplexity, TaskType

    test_tasks = [
        SubTask(
            id="task_001",
            parent_task_id="parent_001",
            title="APIå®Ÿè£…",
            description="REST API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å®Ÿè£…",
            task_type=TaskType.IMPLEMENTATION,
            complexity=TaskComplexity.MODERATE,
            estimated_hours=4.0,
            dependencies=[],
            required_skills=["Python", "FastAPI"],
            priority=5,
            can_parallel=True,
            order_index=1,
        ),
        SubTask(
            id="task_002",
            parent_task_id="parent_001",
            title="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å®Ÿè£…",
            description="SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å®Ÿè£…",
            task_type=TaskType.IMPLEMENTATION,
            complexity=TaskComplexity.MODERATE,
            estimated_hours=3.0,
            dependencies=[],
            required_skills=["SQL", "SQLite"],
            priority=4,
            can_parallel=True,
            order_index=2,
        ),
        SubTask(
            id="task_003",
            parent_task_id="parent_001",
            title="çµ±åˆãƒ†ã‚¹ãƒˆ",
            description="API ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±åˆãƒ†ã‚¹ãƒˆ",
            task_type=TaskType.TESTING,
            complexity=TaskComplexity.COMPLEX,
            estimated_hours=2.0,
            dependencies=["task_001", "task_002"],
            required_skills=["pytest", "testing"],
            priority=3,
            can_parallel=False,
            order_index=3,
        ),
    ]

    # å®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆ
    print(f"\nğŸ“¦ å®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆ: {test_project_id} - {test_phase}")
    group_id = manager.create_execution_group(
        test_project_id, test_phase, test_tasks, max_parallel=2
    )
    print(f"ã‚°ãƒ«ãƒ¼ãƒ—ID: {group_id}")

    # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³ç¢ºèª
    print(f"\nğŸ“Š ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³:")
    resource_usage = manager.get_resource_usage()
    for resource, usage in resource_usage["current_usage"].items():
        limit = resource_usage["limits"][resource]
        utilization = resource_usage["utilization"][resource]
        print(f"  {resource}: {usage:.1f}/{limit:.1f} ({utilization:.1f}%)")

    # ä¸¦åˆ—å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
    print(f"\nğŸš€ ä¸¦åˆ—å®Ÿè¡Œãƒ†ã‚¹ãƒˆé–‹å§‹")
    success = manager.execute_group_parallel(group_id)
    print(f"å®Ÿè¡Œçµæœ: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'}")

    # å®Ÿè¡ŒçŠ¶æ…‹ç¢ºèª
    print(f"\nğŸ“‹ å®Ÿè¡ŒçŠ¶æ…‹:")
    status = manager.get_execution_status(group_id)
    print(f"  ã‚°ãƒ«ãƒ¼ãƒ—çŠ¶æ…‹: {status.get('status', 'Unknown')}")
    print(f"  ã‚¿ã‚¹ã‚¯æ•°: {status.get('task_count', 0)}")
    print(f"  ä¸¦åˆ—åº¦: {status.get('max_parallel', 0)}")

    if "tasks" in status:
        print(f"  ã‚¿ã‚¹ã‚¯è©³ç´°:")
        for task in status["tasks"]:
            print(
                f"    {task['task_id']}: {task['status']} ({task.get('duration', 0):.1f}ç§’)"
            )

    # çµ±è¨ˆæƒ…å ±
    print(f"\nğŸ“ˆ å®Ÿè¡Œçµ±è¨ˆ:")
    stats = manager.get_execution_statistics()
    print(f"  ç·ã‚¿ã‚¹ã‚¯æ•°: {stats['total_tasks']}")
    print(f"  å®Œäº†ã‚¿ã‚¹ã‚¯æ•°: {stats['completed_tasks']}")
    print(f"  å¤±æ•—ã‚¿ã‚¹ã‚¯æ•°: {stats['failed_tasks']}")
    print(f"  ä¸¦åˆ—ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {stats['parallel_groups']}")
    print(f"  å¹³å‡å®Ÿè¡Œæ™‚é–“: {stats['avg_execution_time']:.1f}ç§’")

    # ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
    manager.shutdown()
