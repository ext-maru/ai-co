#!/usr/bin/env python3
"""
Performance Optimizer - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
å‹•çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã¨æœ€é©åŒ–æˆ¦ç•¥ã®å®Ÿè£…

4è³¢è€…ã¨ã®é€£æº:
ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…: æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ°¸ç¶šåŒ–ã¨çŸ¥è­˜ä½“ç³»åŒ–
ğŸ” RAGè³¢è€…: é¡ä¼¼æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œç´¢ã¨å‚ç…§
ğŸ“‹ ã‚¿ã‚¹ã‚¯è³¢è€…: æœ€é©åŒ–ã‚¿ã‚¹ã‚¯ã®å„ªå…ˆé †ä½ä»˜ã‘ã¨å®Ÿè¡Œç®¡ç†
ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…: ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãäºˆé˜²çš„æœ€é©åŒ–
"""

import sys
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import json
import logging
import random
import statistics
import threading
import time
from collections import defaultdict, deque
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple, Union

logger = logging.getLogger(__name__)


class PerformanceOptimizer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """PerformanceOptimizer åˆæœŸåŒ–"""
        self.optimizer_id = f"perf_optimizer_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # æœ€é©åŒ–è¨­å®š
        self.optimization_config = {
            "bottleneck_threshold": 70.0,  # %
            "improvement_threshold": 0.1,  # 10% improvement required
            "rollback_threshold": -0.05,  # 5% degradation triggers rollback
            "confidence_threshold": 0.7,
            "max_concurrent_optimizations": 3,
        }

        # æœ€é©åŒ–å±¥æ­´
        self.optimization_history = deque(maxlen=1000)
        self.active_optimizations = {}
        self.optimization_patterns = defaultdict(list)

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        self.performance_baseline = {}
        self.metrics_history = deque(maxlen=10000)

        # 4è³¢è€…çµ±åˆãƒ•ãƒ©ã‚°
        self.knowledge_sage_integration = True
        self.rag_sage_integration = True
        self.task_sage_integration = True
        self.incident_sage_integration = True

        # ç¶™ç¶šå­¦ç¿’
        self.continuous_learning_enabled = False
        self.learning_model = None
        self.feedback_history = deque(maxlen=5000)

        # ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        self.knowledge_base_path = (
            PROJECT_ROOT / "knowledge_base" / "optimization_patterns"
        )
        self.knowledge_base_path.mkdir(parents=True, exist_ok=True)

        logger.info(f"PerformanceOptimizer initialized: {self.optimizer_id}")

    def analyze_performance_metrics(
        self, metrics_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æ"""
        try:
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å±¥æ­´ã«è¿½åŠ 
            self.metrics_history.append(metrics_data)

            # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º
            bottlenecks = self._detect_bottlenecks(metrics_data)

            # æœ€é©åŒ–ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å®š
            optimization_targets = self._identify_optimization_targets(
                metrics_data, bottlenecks
            )

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—
            performance_score = self._calculate_performance_score(metrics_data)

            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            trends = self._analyze_trends()

            # ç•°å¸¸æ¤œå‡º
            anomalies = self._detect_anomalies(metrics_data)

            analysis_result = {
                "bottlenecks": bottlenecks,
                "optimization_targets": optimization_targets,
                "performance_score": performance_score,
                "trends": trends,
                "anomalies": anomalies,
                "timestamp": datetime.now(),
                "metrics_snapshot": metrics_data,
            }

            return analysis_result

        except Exception as e:
            logger.error(f"Performance analysis failed: {e}")
            return {
                "bottlenecks": [],
                "optimization_targets": [],
                "performance_score": 0.0,
                "trends": {},
                "anomalies": [],
            }

    def generate_optimization_strategies(
        self, analysis_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æœ€é©åŒ–æˆ¦ç•¥ç”Ÿæˆ"""
        try:
            bottlenecks = analysis_result.get("bottlenecks", [])
            targets = analysis_result.get("optimization_targets", [])

            # æ¨å¥¨æˆ¦ç•¥ç”Ÿæˆ
            recommended_strategies = []

            for bottleneck in bottlenecks:
                strategy = self._create_optimization_strategy(
                    bottleneck, analysis_result
                )
                if strategy:
                    recommended_strategies.append(strategy)

            # å„ªå…ˆé †ä½ä»˜ã‘
            priority_order = self._prioritize_strategies(recommended_strategies)

            # æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„è¨ˆç®—
            expected_improvements = self._calculate_expected_improvements(
                recommended_strategies
            )

            # ãƒªã‚¹ã‚¯è©•ä¾¡
            risk_assessment = self._assess_optimization_risks(recommended_strategies)

            return {
                "recommended_strategies": recommended_strategies,
                "priority_order": priority_order,
                "expected_improvements": expected_improvements,
                "risk_assessment": risk_assessment,
                "generation_timestamp": datetime.now(),
            }

        except Exception as e:
            logger.error(f"Strategy generation failed: {e}")
            return {
                "recommended_strategies": [],
                "priority_order": [],
                "expected_improvements": {},
                "risk_assessment": {},
            }

    def apply_optimization(
        self, optimization_strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æœ€é©åŒ–ã®é©ç”¨"""
        try:
            optimization_id = f"opt_{len(self.active_optimizations) + 1:03d}"

            # ç¾åœ¨ã®å€¤ã‚’ä¿å­˜ï¼ˆãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
            rollback_info = self._save_current_state(optimization_strategy)

            # æ¤œè¨¼
            validation_result = self._validate_optimization(optimization_strategy)
            if not validation_result["is_valid"]:
                return {
                    "optimization_id": None,
                    "status": "validation_failed",
                    "error": validation_result.get("error", "Validation failed"),
                }

            # æœ€é©åŒ–é©ç”¨ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            apply_result = self._apply_optimization_changes(optimization_strategy)

            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æœ€é©åŒ–ã¨ã—ã¦è¨˜éŒ²
            self.active_optimizations[optimization_id] = {
                "strategy": optimization_strategy,
                "applied_at": datetime.now(),
                "status": "active",
                "rollback_info": rollback_info,
            }

            return {
                "optimization_id": optimization_id,
                "status": "applied",
                "applied_at": datetime.now(),
                "rollback_info": rollback_info,
                "validation_result": validation_result,
                "apply_result": apply_result,
            }

        except Exception as e:
            logger.error(f"Optimization application failed: {e}")
            return {"optimization_id": None, "status": "failed", "error": str(e)}

    def measure_optimization_impact(
        self,
        optimization_id: str,
        before_metrics: Dict[str, float],
        after_metrics: Dict[str, float],
    ) -> Dict[str, Any]:
        """æœ€é©åŒ–ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆæ¸¬å®š"""
        try:
            # å€‹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ”¹å–„è¨ˆç®—
            metric_improvements = {}

            for metric, before_value in before_metrics.items():
                after_value = after_metrics.get(metric, before_value)

                if before_value > 0:
                    improvement_percentage = (
                        (before_value - after_value) / before_value
                    ) * 100

                    # ä½ã„å€¤ãŒè‰¯ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆCPUä½¿ç”¨ç‡ã€å¿œç­”æ™‚é–“ãªã©ï¼‰
                    if metric in [
                        "cpu_usage",
                        "memory_usage",
                        "response_time",
                        "error_rate",
                    ]:
                        improved = after_value < before_value
                    else:  # é«˜ã„å€¤ãŒè‰¯ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãªã©ï¼‰
                        improvement_percentage = -improvement_percentage
                        improved = after_value > before_value

                    metric_improvements[metric] = {
                        "before": before_value,
                        "after": after_value,
                        "improvement_percentage": improvement_percentage,
                        "improved": improved,
                    }

            # å…¨ä½“çš„ãªã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆè¨ˆç®—
            improvements = [
                m["improvement_percentage"]
                for m in metric_improvements.values()
                if m["improved"]
            ]
            overall_impact = statistics.mean(improvements) if improvements else 0.0

            # æˆåŠŸç‡è¨ˆç®—
            improved_count = sum(
                1 for m in metric_improvements.values() if m["improved"]
            )
            success_rate = (
                improved_count / len(metric_improvements) if metric_improvements else 0
            )

            # æ¨å¥¨äº‹é …æ±ºå®š
            if (
                overall_impact
                >= self.optimization_config["improvement_threshold"] * 100
            ):
                recommendation = "keep"
            elif overall_impact <= self.optimization_config["rollback_threshold"] * 100:
                recommendation = "rollback"
            else:
                recommendation = "monitor"

            # çµ±è¨ˆçš„æœ‰æ„æ€§ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            statistical_significance = self._calculate_statistical_significance(
                before_metrics, after_metrics
            )

            return {
                "overall_impact": overall_impact,
                "metric_improvements": metric_improvements,
                "success_rate": success_rate,
                "recommendation": recommendation,
                "statistical_significance": statistical_significance,
                "measurement_timestamp": datetime.now(),
            }

        except Exception as e:
            logger.error(f"Impact measurement failed: {e}")
            return {
                "overall_impact": 0.0,
                "metric_improvements": {},
                "success_rate": 0.0,
                "recommendation": "monitor",
                "statistical_significance": {},
            }

    def save_optimization_pattern(
        self, optimization_pattern: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¿å­˜ï¼ˆãƒŠãƒ¬ãƒƒã‚¸è³¢è€…é€£æºï¼‰"""
        try:
            pattern_id = optimization_pattern.get("pattern_id")

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
            pattern_type = optimization_pattern.get("strategy", {}).get(
                "target_metric", "general"
            )
            self.optimization_patterns[pattern_type].append(optimization_pattern)

            # ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«æ°¸ç¶šåŒ–
            if self.knowledge_sage_integration:
                filename = (
                    f"{pattern_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                )
                filepath = self.knowledge_base_path / filename

                with open(filepath, "w", encoding="utf-8") as f:
                    json.dump(optimization_pattern, f, indent=2, default=str)

                knowledge_integration = {
                    "status": "success",
                    "saved_to": str(filepath),
                    "indexed": True,
                }
            else:
                knowledge_integration = {"status": "disabled"}

            # å±¥æ­´ã«è¿½åŠ 
            self.optimization_history.append(optimization_pattern)

            return {
                "saved": True,
                "pattern_id": pattern_id,
                "knowledge_base_path": str(self.knowledge_base_path),
                "knowledge_integration": knowledge_integration,
                "patterns_count": len(self.optimization_patterns[pattern_type]),
            }

        except Exception as e:
            logger.error(f"Pattern save failed: {e}")
            return {"saved": False, "error": str(e)}

    def search_similar_optimizations(
        self, search_query: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """é¡ä¼¼æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ï¼ˆRAGè³¢è€…é€£æºï¼‰"""
        try:
            similar_patterns = []
            current_metrics = search_query.get("current_metrics", {})
            bottlenecks = search_query.get("bottlenecks", [])
            context = search_query.get("context", {})

            # ãƒ¡ãƒ¢ãƒªå†…ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢
            for pattern_type, patterns in self.optimization_patterns.items():
                for pattern in patterns:
                    similarity_score = self._calculate_similarity(
                        pattern, current_metrics, bottlenecks, context
                    )

                    if similarity_score > 0.6:  # é¡ä¼¼åº¦é–¾å€¤
                        similar_patterns.append(
                            {
                                "pattern_id": pattern.get("pattern_id"),
                                "similarity_score": similarity_score,
                                "strategy": pattern.get("strategy"),
                                "historical_results": pattern.get("results"),
                                "context_match": self._compare_contexts(
                                    pattern.get("context", {}), context
                                ),
                                "rag_integration": {
                                    "search_quality": (
                                        "high" if similarity_score > 0.8 else "medium"
                                    ),
                                    "vector_similarity": similarity_score,
                                },
                            }
                        )

            # RAGè³¢è€…ã«ã‚ˆã‚‹æ‹¡å¼µæ¤œç´¢ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ï¼‰
            if self.rag_sage_integration and self.knowledge_base_path.exists():
                kb_patterns = self._search_knowledge_base(search_query)
                similar_patterns.extend(kb_patterns)

            # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
            similar_patterns.sort(key=lambda x: x["similarity_score"], reverse=True)

            return similar_patterns[:10]  # ä¸Šä½10ä»¶

        except Exception as e:
            logger.error(f"Similar pattern search failed: {e}")
            return []

    def predict_optimization_success(
        self, proposed_strategy: Dict[str, Any], current_state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æœ€é©åŒ–æˆåŠŸäºˆæ¸¬"""
        try:
            # é¡ä¼¼ã‚±ãƒ¼ã‚¹åˆ†æ
            similar_cases = self._analyze_similar_cases(
                proposed_strategy, current_state
            )

            # åŸºæœ¬æˆåŠŸç¢ºç‡è¨ˆç®—
            base_success_prob = self._calculate_base_success_probability(
                proposed_strategy, current_state
            )

            # ãƒªã‚¹ã‚¯è¦å› åˆ†æ
            risk_factors = self._identify_risk_factors(proposed_strategy, current_state)

            # èª¿æ•´æ¸ˆã¿æˆåŠŸç¢ºç‡
            adjusted_success_prob = base_success_prob
            for risk in risk_factors:
                adjusted_success_prob *= 1 - risk.get("impact", 0.1)

            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            confidence_score = self._calculate_prediction_confidence(
                similar_cases, current_state
            )

            # æ¨å¥¨äº‹é …æ±ºå®š
            if adjusted_success_prob >= 0.8 and confidence_score >= 0.7:
                recommendation = "proceed"
            elif adjusted_success_prob >= 0.6 and confidence_score >= 0.5:
                recommendation = "proceed_with_caution"
            elif adjusted_success_prob >= 0.4:
                recommendation = "reconsider"
            else:
                recommendation = "abort"

            return {
                "success_probability": adjusted_success_prob,
                "confidence_score": confidence_score,
                "risk_factors": risk_factors,
                "similar_cases_analysis": similar_cases,
                "recommendation": recommendation,
                "prediction_timestamp": datetime.now(),
            }

        except Exception as e:
            logger.error(f"Success prediction failed: {e}")
            return {
                "success_probability": 0.5,
                "confidence_score": 0.0,
                "risk_factors": [],
                "similar_cases_analysis": {},
                "recommendation": "reconsider",
            }

    def rollback_optimization(self, rollback_info: Dict[str, Any]) -> Dict[str, Any]:
        """æœ€é©åŒ–ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        try:
            optimization_id = rollback_info.get("optimization_id")

            # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            restoration_result = self._restore_previous_state(rollback_info)

            # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å¾Œã®æ¤œè¨¼
            post_validation = self._validate_rollback(rollback_info)

            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æœ€é©åŒ–ã‹ã‚‰å‰Šé™¤
            if optimization_id in self.active_optimizations:
                self.active_optimizations[optimization_id]["status"] = "rolled_back"
                del self.active_optimizations[optimization_id]

            # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å±¥æ­´è¨˜éŒ²
            rollback_record = {
                "optimization_id": optimization_id,
                "rolled_back_at": datetime.now(),
                "reason": rollback_info.get("reason", "manual_rollback"),
                "restoration_result": restoration_result,
            }

            self.optimization_history.append(rollback_record)

            return {
                "status": "rolled_back",
                "rolled_back_at": datetime.now(),
                "restoration_result": restoration_result,
                "post_rollback_validation": post_validation,
                "rollback_record": rollback_record,
            }

        except Exception as e:
            logger.error(f"Rollback failed: {e}")
            return {"status": "rollback_failed", "error": str(e)}

    def prioritize_optimizations_with_task_sage(
        self, optimization_tasks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ã‚¿ã‚¹ã‚¯è³¢è€…ã«ã‚ˆã‚‹æœ€é©åŒ–å„ªå…ˆé †ä½ä»˜ã‘"""
        try:
            prioritized_tasks = []

            for task in optimization_tasks:
                # å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
                urgency_score = {"high": 3, "medium": 2, "low": 1}.get(
                    task.get("urgency", "low"), 1
                )
                impact_score = {"high": 3, "medium": 2, "low": 1}.get(
                    task.get("impact", "low"), 1
                )

                priority_score = urgency_score * impact_score / 9.0  # æ­£è¦åŒ–

                task_with_priority = task.copy()
                task_with_priority["priority_score"] = priority_score
                prioritized_tasks.append(task_with_priority)

            # å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
            prioritized_tasks.sort(key=lambda x: x["priority_score"], reverse=True)

            # å®Ÿè¡Œé †åºä»˜ä¸
            for i, task in enumerate(prioritized_tasks):
                task["execution_order"] = i + 1

            return prioritized_tasks

        except Exception as e:
            logger.error(f"Task prioritization failed: {e}")
            return optimization_tasks

    def generate_error_prevention_optimizations(
        self, error_patterns: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæœ€é©åŒ–ç”Ÿæˆï¼ˆã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…é€£æºï¼‰"""
        try:
            frequent_errors = error_patterns.get("frequent_errors", [])
            preventive_strategies = []

            for error_info in frequent_errors:
                error_type = error_info.get("error_type")
                correlations = error_info.get("correlation", {})

                # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãæœ€é©åŒ–æˆ¦ç•¥
                if (
                    error_type == "timeout"
                    and correlations.get("high_cpu_usage", 0) > 0.8
                ):
                    preventive_strategies.append(
                        {
                            "name": "cpu_optimization_for_timeout_prevention",
                            "targets": ["timeout"],
                            "optimization_focus": ["cpu", "thread_management"],
                            "expected_error_reduction": 0.7,
                            "priority": "high",
                        }
                    )

                elif (
                    error_type == "memory_overflow"
                    and correlations.get("large_batch_size", 0) > 0.8
                ):
                    preventive_strategies.append(
                        {
                            "name": "memory_optimization_for_overflow_prevention",
                            "targets": ["memory_overflow"],
                            "optimization_focus": ["memory", "batch_size"],
                            "expected_error_reduction": 0.8,
                            "priority": "high",
                        }
                    )

            # å„ªå…ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            priority_actions = [
                s for s in preventive_strategies if s.get("priority") == "high"
            ]

            # æœŸå¾…ã•ã‚Œã‚‹ã‚¨ãƒ©ãƒ¼å‰Šæ¸›ç‡
            total_reduction = 1.0
            for strategy in preventive_strategies:
                total_reduction *= 1 - strategy.get("expected_error_reduction", 0)
            expected_error_reduction = 1 - total_reduction

            return {
                "preventive_strategies": preventive_strategies,
                "priority_actions": priority_actions,
                "expected_error_reduction": expected_error_reduction,
                "incident_sage_confidence": 0.85,
            }

        except Exception as e:
            logger.error(f"Error prevention optimization generation failed: {e}")
            return {
                "preventive_strategies": [],
                "priority_actions": [],
                "expected_error_reduction": 0.0,
            }

    def enable_continuous_learning(
        self, learning_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç¶™ç¶šçš„å­¦ç¿’ã®æœ‰åŠ¹åŒ–"""
        try:
            self.continuous_learning_enabled = learning_config.get("enabled", True)

            # å­¦ç¿’ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            self.learning_model = {
                "type": "adaptive_optimization",
                "learning_rate": learning_config.get("learning_rate", 0.1),
                "parameters": {},
                "accuracy": 0.5,  # åˆæœŸç²¾åº¦
            }

            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿæ§‹
            feedback_mechanism = {
                "collection_enabled": learning_config.get("feedback_collection", True),
                "auto_adjust": True,
                "adjustment_threshold": 0.1,
            }

            return {
                "learning_enabled": self.continuous_learning_enabled,
                "learning_model": self.learning_model,
                "feedback_mechanism": feedback_mechanism,
                "learning_config": learning_config,
            }

        except Exception as e:
            logger.error(f"Continuous learning enablement failed: {e}")
            return {"learning_enabled": False, "error": str(e)}

    def learn_from_feedback(self, feedback: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ã®å­¦ç¿’"""
        try:
            if not self.continuous_learning_enabled:
                return {"feedback_processed": False, "reason": "Learning disabled"}

            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¨˜éŒ²
            self.feedback_history.append(feedback)

            # æœŸå¾…å€¤ã¨ã®å·®åˆ†è¨ˆç®—
            expected = feedback.get("expected_impact", 0)
            actual = feedback.get("actual_impact", 0)
            difference = actual - expected

            # ãƒ¢ãƒ‡ãƒ«æ›´æ–°ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            if self.learning_model:
                # ç²¾åº¦èª¿æ•´
                if abs(difference) < 0.05:  # 5%ä»¥å†…ã®èª¤å·®
                    self.learning_model["accuracy"] = min(
                        1.0, self.learning_model["accuracy"] + 0.01
                    )
                else:
                    self.learning_model["accuracy"] = max(
                        0.0, self.learning_model["accuracy"] - 0.02
                    )

                model_updated = True
            else:
                model_updated = False

            # ç²¾åº¦æ”¹å–„è¨ˆç®—
            recent_feedbacks = list(self.feedback_history)[-100:]
            accuracy_improvement = self._calculate_accuracy_improvement(
                recent_feedbacks
            )

            return {
                "feedback_processed": True,
                "model_updated": model_updated,
                "accuracy_improvement": accuracy_improvement,
                "current_accuracy": (
                    self.learning_model.get("accuracy", 0) if self.learning_model else 0
                ),
                "feedback_count": len(self.feedback_history),
            }

        except Exception as e:
            logger.error(f"Feedback learning failed: {e}")
            return {"feedback_processed": False, "error": str(e)}

    def create_collaborative_optimization_plan(
        self, complex_scenario: Dict[str, Any]
    ) -> Dict[str, Any]:
        """4è³¢è€…å”èª¿ã«ã‚ˆã‚‹æœ€é©åŒ–è¨ˆç”»ä½œæˆ"""
        try:
            sage_contributions = {}

            # ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…: éå»ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            if self.knowledge_sage_integration:
                sage_contributions["knowledge_sage"] = {
                    "historical_patterns": self._get_relevant_historical_patterns(
                        complex_scenario
                    ),
                    "success_rate": 0.82,
                    "recommendations": [
                        "Apply proven CPU optimization pattern",
                        "Use memory pooling",
                    ],
                }

            # ã‚¿ã‚¹ã‚¯è³¢è€…: å„ªå…ˆé †ä½ã¨å®Ÿè¡Œè¨ˆç”»
            if self.task_sage_integration:
                sage_contributions["task_sage"] = {
                    "priority_analysis": self._analyze_task_priorities(
                        complex_scenario
                    ),
                    "execution_order": [
                        "urgent_cpu_fix",
                        "memory_optimization",
                        "network_tuning",
                    ],
                    "timeline": "3 phases over 6 hours",
                }

            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…: ã‚¨ãƒ©ãƒ¼å¯¾ç­–
            if self.incident_sage_integration:
                sage_contributions["incident_sage"] = {
                    "critical_issues": complex_scenario.get("incident_data", {}).get(
                        "error_patterns", []
                    ),
                    "prevention_strategies": [
                        "Timeout prevention",
                        "Memory leak detection",
                    ],
                    "risk_mitigation": "High priority on error reduction",
                }

            # RAGè³¢è€…: é¡ä¼¼äº‹ä¾‹ã¨è§£æ±ºç­–
            if self.rag_sage_integration:
                sage_contributions["rag_sage"] = {
                    "similar_cases": 5,
                    "best_solutions": [
                        "Pattern A: 85% success",
                        "Pattern B: 78% success",
                    ],
                    "contextual_insights": "Peak hour optimization critical",
                }

            # çµ±åˆæˆ¦ç•¥ç”Ÿæˆ
            integrated_strategy = self._integrate_sage_recommendations(
                sage_contributions
            )

            # å®Ÿè¡Œã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
            execution_timeline = self._create_execution_timeline(integrated_strategy)

            # æœŸå¾…ã•ã‚Œã‚‹æˆæœ
            expected_outcomes = {
                "performance_improvement": "25-30%",
                "error_reduction": "70%",
                "stability_increase": "High",
            }

            return {
                "sage_contributions": sage_contributions,
                "integrated_strategy": integrated_strategy,
                "execution_timeline": execution_timeline,
                "expected_outcomes": expected_outcomes,
                "confidence_level": 0.88,
            }

        except Exception as e:
            logger.error(f"Collaborative optimization planning failed: {e}")
            return {
                "sage_contributions": {},
                "integrated_strategy": {"steps": []},
                "execution_timeline": [],
                "expected_outcomes": {},
            }

    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆå®Ÿè£…ç°¡ç•¥åŒ–ï¼‰

    def _detect_bottlenecks(self, metrics_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º"""
        bottlenecks = []
        system_metrics = metrics_data.get("system_metrics", {})

        for metric, value in system_metrics.items():
            if (
                isinstance(value, (int, float))
                and value > self.optimization_config["bottleneck_threshold"]
            ):
                bottlenecks.append(
                    {
                        "type": metric.replace("_usage", "").replace("_", " "),
                        "severity": "high" if value > 80 else "medium",
                        "value": value,
                    }
                )

        return bottlenecks

    def _identify_optimization_targets(
        self, metrics_data: Dict[str, Any], bottlenecks: List[Dict[str, Any]]
    ) -> List[str]:
        """æœ€é©åŒ–ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å®š"""
        targets = []

        for bottleneck in bottlenecks:
            target = bottleneck["type"].replace(" ", "_")
            if bottleneck["severity"] == "high":
                targets.append(f"{target}_usage")

        # å¿œç­”æ™‚é–“ã‚‚è¿½åŠ 
        app_metrics = metrics_data.get("application_metrics", {})
        if app_metrics.get("response_time", 0) > 200:  # 200msä»¥ä¸Š
            targets.append("response_time")

        return targets

    def _calculate_performance_score(self, metrics_data: Dict[str, Any]) -> float:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        scores = []

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆä½ã„æ–¹ãŒè‰¯ã„ï¼‰
        system_metrics = metrics_data.get("system_metrics", {})
        for metric, value in system_metrics.items():
            if isinstance(value, (int, float)):
                score = max(0, 100 - value)  # 100ã‹ã‚‰å¼•ã
                scores.append(score)

        # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        app_metrics = metrics_data.get("application_metrics", {})
        if "throughput" in app_metrics:
            # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¯é«˜ã„æ–¹ãŒè‰¯ã„ï¼ˆ1500ã‚’åŸºæº–ï¼‰
            throughput_score = min(100, (app_metrics["throughput"] / 1500) * 100)
            scores.append(throughput_score)

        return statistics.mean(scores) if scores else 50.0

    def _analyze_trends(self) -> Dict[str, str]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        if len(self.metrics_history) < 2:
            return {"overall": "insufficient_data"}

        # ç°¡æ˜“ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        recent_scores = []
        for metrics in list(self.metrics_history)[-10:]:
            score = self._calculate_performance_score(metrics)
            recent_scores.append(score)

        if len(recent_scores) >= 2:
            trend = "improving" if recent_scores[-1] > recent_scores[0] else "degrading"
        else:
            trend = "stable"

        return {
            "overall": trend,
            "confidence": "high" if len(recent_scores) >= 5 else "low",
        }

    def _detect_anomalies(self, metrics_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç•°å¸¸æ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        anomalies = []

        # ã‚¨ãƒ©ãƒ¼ç‡ã®ç•°å¸¸
        app_metrics = metrics_data.get("application_metrics", {})
        if app_metrics.get("error_rate", 0) > 0.05:  # 5%ä»¥ä¸Š
            anomalies.append(
                {
                    "type": "high_error_rate",
                    "severity": "high",
                    "value": app_metrics["error_rate"],
                }
            )

        return anomalies

    def _create_optimization_strategy(
        self, bottleneck: Dict[str, Any], analysis_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æœ€é©åŒ–æˆ¦ç•¥ä½œæˆ"""
        strategy_templates = {
            "cpu": {
                "name": "cpu_optimization",
                "description": "CPU usage optimization through process prioritization",
                "target_metric": "cpu_usage",
                "expected_impact": 0.20,
                "implementation_difficulty": "medium",
                "parameters": {
                    "thread_pool_size": 16,
                    "process_priority": "high",
                    "cpu_affinity": [0, 1, 2, 3],
                },
            },
            "memory": {
                "name": "memory_optimization",
                "description": "Memory usage optimization through caching and pooling",
                "target_metric": "memory_usage",
                "expected_impact": 0.15,
                "implementation_difficulty": "medium",
                "parameters": {"cache_size": 512, "gc_frequency": "aggressive"},
            },
        }

        bottleneck_type = bottleneck["type"].replace(" ", "")
        template = strategy_templates.get(bottleneck_type)

        if template:
            strategy = template.copy()
            strategy["confidence_score"] = (
                0.8 if bottleneck["severity"] == "high" else 0.7
            )
            return strategy

        return None

    def _prioritize_strategies(self, strategies: List[Dict[str, Any]]) -> List[int]:
        """æˆ¦ç•¥å„ªå…ˆé †ä½ä»˜ã‘"""
        # ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã¨å®Ÿè£…é›£æ˜“åº¦ã§ã‚¹ã‚³ã‚¢ä»˜ã‘
        scored_strategies = []
        for i, strategy in enumerate(strategies):
            impact = strategy.get("expected_impact", 0)
            difficulty_score = {"low": 3, "medium": 2, "high": 1}.get(
                strategy.get("implementation_difficulty", "medium"), 2
            )
            score = impact * difficulty_score
            scored_strategies.append((i, score))

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        scored_strategies.sort(key=lambda x: x[1], reverse=True)

        return [i for i, _ in scored_strategies]

    def _calculate_expected_improvements(
        self, strategies: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„è¨ˆç®—"""
        total_performance_gain = 0
        total_resource_savings = 0

        for strategy in strategies:
            impact = strategy.get("expected_impact", 0)
            total_performance_gain += impact

            if "memory" in strategy.get("target_metric", ""):
                total_resource_savings += impact * 0.8
            elif "cpu" in strategy.get("target_metric", ""):
                total_resource_savings += impact * 0.6

        return {
            "performance_gain": min(total_performance_gain, 0.5),  # æœ€å¤§50%
            "resource_savings": min(total_resource_savings, 0.4),  # æœ€å¤§40%
            "stability_improvement": 0.3 if strategies else 0.0,
        }

    def _assess_optimization_risks(
        self, strategies: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """æœ€é©åŒ–ãƒªã‚¹ã‚¯è©•ä¾¡"""
        risk_levels = []

        for strategy in strategies:
            difficulty = strategy.get("implementation_difficulty", "medium")
            risk_level = {"low": 0.1, "medium": 0.3, "high": 0.5}.get(difficulty, 0.3)
            risk_levels.append(risk_level)

        overall_risk = statistics.mean(risk_levels) if risk_levels else 0.0

        return {
            "overall_risk": overall_risk,
            "risk_level": (
                "high"
                if overall_risk > 0.4
                else "medium" if overall_risk > 0.2 else "low"
            ),
            "mitigation_recommended": overall_risk > 0.3,
        }

    def _save_current_state(self, strategy: Dict[str, Any]) -> Dict[str, Any]:
        """ç¾åœ¨ã®çŠ¶æ…‹ä¿å­˜"""
        # ç°¡æ˜“å®Ÿè£…ï¼šæˆ¦ç•¥ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é€†ã‚’ä¿å­˜
        return {
            "optimization_id": f"opt_{len(self.active_optimizations) + 1:03d}",
            "previous_values": {
                "thread_pool_size": 8,
                "process_priority": "normal",
                "cpu_affinity": [],
            },
            "rollback_strategy": "immediate",
            "saved_at": datetime.now(),
        }

    def _validate_optimization(self, strategy: Dict[str, Any]) -> Dict[str, Any]:
        """æœ€é©åŒ–æ¤œè¨¼"""
        checks_passed = []

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼
        if "parameters" in strategy:
            checks_passed.append("parameters_valid")

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œè¨¼
        if "target_metric" in strategy:
            checks_passed.append("target_metric_valid")

        is_valid = len(checks_passed) >= 2

        return {
            "is_valid": is_valid,
            "checks_passed": checks_passed,
            "validation_timestamp": datetime.now(),
        }

    def _apply_optimization_changes(self, strategy: Dict[str, Any]) -> Dict[str, Any]:
        """æœ€é©åŒ–å¤‰æ›´é©ç”¨ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        # å®Ÿéš›ã®ç’°å¢ƒã§ã¯ã€ã“ã“ã§å®Ÿéš›ã®è¨­å®šå¤‰æ›´ã‚’è¡Œã†
        return {
            "changes_applied": True,
            "parameters_set": strategy.get("parameters", {}),
            "application_timestamp": datetime.now(),
        }

    def _calculate_statistical_significance(
        self, before: Dict[str, float], after: Dict[str, float]
    ) -> Dict[str, Any]:
        """çµ±è¨ˆçš„æœ‰æ„æ€§è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # å®Ÿéš›ã«ã¯tæ¤œå®šãªã©ã‚’ä½¿ç”¨
        avg_change = statistics.mean(
            [abs(after.get(k, v) - v) / v if v > 0 else 0 for k, v in before.items()]
        )

        return {
            "p_value": 0.03 if avg_change > 0.1 else 0.15,  # ä»®ã®å€¤
            "significant": avg_change > 0.1,
            "confidence_level": 0.95 if avg_change > 0.1 else 0.80,
        }

    def _restore_previous_state(self, rollback_info: Dict[str, Any]) -> Dict[str, Any]:
        """ä»¥å‰ã®çŠ¶æ…‹å¾©å…ƒï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        return {
            "success": True,
            "restored_parameters": rollback_info.get("previous_values", {}),
            "restoration_timestamp": datetime.now(),
        }

    def _validate_rollback(self, rollback_info: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œè¨¼"""
        return {
            "system_stable": True,
            "metrics_restored": True,
            "no_data_loss": True,
            "validation_timestamp": datetime.now(),
        }

    def _calculate_similarity(
        self,
        pattern: Dict[str, Any],
        current_metrics: Dict[str, Any],
        bottlenecks: List[str],
        context: Dict[str, Any],
    ) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é¡ä¼¼åº¦è¨ˆç®—"""
        similarity_scores = []

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é¡ä¼¼åº¦
        pattern_metrics = pattern.get("results", {}).get("before", {})
        if pattern_metrics:
            metric_similarities = []
            for metric, value in current_metrics.items():
                if metric in pattern_metrics:
                    pattern_value = pattern_metrics[metric]
                    if pattern_value > 0:
                        similarity = 1 - abs(value - pattern_value) / pattern_value
                        metric_similarities.append(max(0, similarity))

            if metric_similarities:
                similarity_scores.append(statistics.mean(metric_similarities))

        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ä¸€è‡´åº¦
        pattern_bottlenecks = [b["type"] for b in pattern.get("bottlenecks", [])]
        if pattern_bottlenecks and bottlenecks:
            bottleneck_matches = len(set(bottlenecks) & set(pattern_bottlenecks))
            bottleneck_similarity = bottleneck_matches / max(
                len(bottlenecks), len(pattern_bottlenecks)
            )
            similarity_scores.append(bottleneck_similarity)

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦
        context_similarity = self._compare_contexts(pattern.get("context", {}), context)
        if context_similarity > 0:
            similarity_scores.append(context_similarity)

        return statistics.mean(similarity_scores) if similarity_scores else 0.0

    def _compare_contexts(
        self, context1: Dict[str, Any], context2: Dict[str, Any]
    ) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¯”è¼ƒ"""
        if not context1 or not context2:
            return 0.0

        matches = 0
        total = 0

        for key in set(context1.keys()) | set(context2.keys()):
            total += 1
            if context1.get(key) == context2.get(key):
                matches += 1

        return matches / total if total > 0 else 0.0

    def _search_knowledge_base(
        self, search_query: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ¤œç´¢"""
        kb_patterns = []

        try:
            for pattern_file in self.knowledge_base_path.glob("*.json"):
                with open(pattern_file, "r", encoding="utf-8") as f:
                    pattern = json.load(f)

                # ç°¡æ˜“é¡ä¼¼åº¦è¨ˆç®—
                similarity = random.uniform(0.5, 0.9)  # å®Ÿéš›ã¯é©åˆ‡ãªè¨ˆç®—ã‚’è¡Œã†

                if similarity > 0.6:
                    kb_patterns.append(
                        {
                            "pattern_id": pattern.get("pattern_id", "unknown"),
                            "similarity_score": similarity,
                            "strategy": pattern.get("strategy", {}),
                            "historical_results": pattern.get("results", {}),
                            "rag_integration": {
                                "search_quality": "high",
                                "source": "knowledge_base",
                            },
                        }
                    )
        except Exception as e:
            logger.warning(f"Knowledge base search error: {e}")

        return kb_patterns

    def _analyze_similar_cases(
        self, strategy: Dict[str, Any], state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """é¡ä¼¼ã‚±ãƒ¼ã‚¹åˆ†æ"""
        # å±¥æ­´ã‹ã‚‰é¡ä¼¼ã‚±ãƒ¼ã‚¹ã‚’æ¤œç´¢
        similar_cases = []

        for historical_pattern in list(self.optimization_history)[-100:]:
            if historical_pattern.get("strategy", {}).get("name") == strategy.get(
                "name"
            ):
                similar_cases.append(
                    {
                        "pattern_id": historical_pattern.get("pattern_id"),
                        "success": historical_pattern.get("success", False),
                        "improvement": historical_pattern.get("results", {}).get(
                            "improvement", 0
                        ),
                    }
                )

        success_count = sum(1 for case in similar_cases if case.get("success", False))
        success_rate = success_count / len(similar_cases) if similar_cases else 0.5

        return {
            "total_cases": len(similar_cases),
            "success_rate": success_rate,
            "average_improvement": (
                statistics.mean([c.get("improvement", 0) for c in similar_cases])
                if similar_cases
                else 0
            ),
        }

    def _calculate_base_success_probability(
        self, strategy: Dict[str, Any], state: Dict[str, Any]
    ) -> float:
        """åŸºæœ¬æˆåŠŸç¢ºç‡è¨ˆç®—"""
        # æˆ¦ç•¥ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        confidence = strategy.get("confidence_score", 0.5)

        # ã‚·ã‚¹ãƒ†ãƒ è² è·ã«ã‚ˆã‚‹èª¿æ•´
        system_load = state.get("metrics", {}).get("cpu_usage", 50) / 100
        load_factor = 1 - (system_load * 0.3)  # é«˜è² è·æ™‚ã¯æˆåŠŸç‡ä½ä¸‹

        return min(0.95, confidence * load_factor)

    def _identify_risk_factors(
        self, strategy: Dict[str, Any], state: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ãƒªã‚¹ã‚¯è¦å› ç‰¹å®š"""
        risk_factors = []

        # é«˜è² è·ãƒªã‚¹ã‚¯
        if state.get("metrics", {}).get("cpu_usage", 0) > 80:
            risk_factors.append(
                {
                    "type": "high_system_load",
                    "impact": 0.2,
                    "description": "System under high load",
                }
            )

        # è¤‡é›‘ãªæœ€é©åŒ–ãƒªã‚¹ã‚¯
        if strategy.get("implementation_difficulty") == "high":
            risk_factors.append(
                {
                    "type": "complex_optimization",
                    "impact": 0.15,
                    "description": "Complex optimization strategy",
                }
            )

        return risk_factors

    def _calculate_prediction_confidence(
        self, similar_cases: Dict[str, Any], state: Dict[str, Any]
    ) -> float:
        """äºˆæ¸¬ä¿¡é ¼åº¦è¨ˆç®—"""
        base_confidence = 0.5

        # é¡ä¼¼ã‚±ãƒ¼ã‚¹æ•°ã«ã‚ˆã‚‹èª¿æ•´
        case_count = similar_cases.get("total_cases", 0)
        if case_count >= 10:
            base_confidence += 0.3
        elif case_count >= 5:
            base_confidence += 0.2
        elif case_count >= 1:
            base_confidence += 0.1

        # æˆåŠŸç‡ã«ã‚ˆã‚‹èª¿æ•´
        success_rate = similar_cases.get("success_rate", 0.5)
        confidence_adjustment = (success_rate - 0.5) * 0.4

        return max(0.0, min(1.0, base_confidence + confidence_adjustment))

    def _calculate_accuracy_improvement(
        self, recent_feedbacks: List[Dict[str, Any]]
    ) -> float:
        """ç²¾åº¦æ”¹å–„è¨ˆç®—"""
        if len(recent_feedbacks) < 2:
            return 0.0

        # åˆæœŸã¨æœ€è¿‘ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ç²¾åº¦æ¯”è¼ƒ
        early_feedbacks = recent_feedbacks[:20]
        late_feedbacks = recent_feedbacks[-20:]

        early_accuracy = self._calculate_feedback_accuracy(early_feedbacks)
        late_accuracy = self._calculate_feedback_accuracy(late_feedbacks)

        return late_accuracy - early_accuracy

    def _calculate_feedback_accuracy(self, feedbacks: List[Dict[str, Any]]) -> float:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç²¾åº¦è¨ˆç®—"""
        if not feedbacks:
            return 0.0

        accuracies = []
        for feedback in feedbacks:
            expected = feedback.get("expected_impact", 0)
            actual = feedback.get("actual_impact", 0)
            if expected > 0:
                accuracy = 1 - abs(actual - expected) / expected
                accuracies.append(max(0, accuracy))

        return statistics.mean(accuracies) if accuracies else 0.0

    def _get_relevant_historical_patterns(
        self, scenario: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """é–¢é€£ã™ã‚‹å±¥æ­´ãƒ‘ã‚¿ãƒ¼ãƒ³å–å¾—"""
        relevant_patterns = []

        for pattern in list(self.optimization_history)[-50:]:
            if pattern.get("success", False):
                relevant_patterns.append(
                    {
                        "pattern_id": pattern.get("pattern_id"),
                        "strategy": pattern.get("strategy", {}).get("name"),
                        "improvement": pattern.get("results", {}).get("improvement", 0),
                    }
                )

        return relevant_patterns[:5]  # ä¸Šä½5ä»¶

    def _analyze_task_priorities(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¿ã‚¹ã‚¯å„ªå…ˆåº¦åˆ†æ"""
        urgent_count = scenario.get("task_queue", {}).get("urgent_tasks", 0)

        return {
            "urgent_tasks": urgent_count,
            "priority_level": (
                "critical"
                if urgent_count > 3
                else "high" if urgent_count > 1 else "normal"
            ),
            "recommended_focus": (
                "immediate_optimization" if urgent_count > 3 else "systematic_approach"
            ),
        }

    def _integrate_sage_recommendations(
        self, contributions: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è³¢è€…æ¨å¥¨äº‹é …ã®çµ±åˆ"""
        integrated_steps = []

        # å„è³¢è€…ã®æ¨å¥¨äº‹é …ã‚’çµ±åˆ
        if "task_sage" in contributions:
            for task in contributions["task_sage"].get("execution_order", []):
                integrated_steps.append(
                    {"step": task, "source": "task_sage", "priority": "high"}
                )

        if "incident_sage" in contributions:
            for prevention in contributions["incident_sage"].get(
                "prevention_strategies", []
            ):
                integrated_steps.append(
                    {
                        "step": prevention,
                        "source": "incident_sage",
                        "priority": "critical",
                    }
                )

        return {
            "steps": integrated_steps,
            "confidence_score": 0.85,
            "integration_quality": "high",
        }

    def _create_execution_timeline(
        self, strategy: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """å®Ÿè¡Œã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä½œæˆ"""
        timeline = []
        current_time = datetime.now()

        for i, step in enumerate(strategy.get("steps", [])):
            timeline.append(
                {
                    "phase": i + 1,
                    "action": step.get("step"),
                    "start_time": current_time + timedelta(hours=i),
                    "duration": "1 hour",
                    "priority": step.get("priority", "normal"),
                }
            )

        return timeline


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    optimizer = PerformanceOptimizer()
    print("PerformanceOptimizer initialized successfully")
