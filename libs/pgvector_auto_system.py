#!/usr/bin/env python3
"""
pgvectorè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ 
ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ãƒ»è‡ªå‹•æ›´æ–°ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ
"""

import os
import asyncio
import logging
import json
import hashlib
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import sqlite3
import subprocess

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class KnowledgeBaseWatcher(FileSystemEventHandler):
    """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–"""
    
    def __init__(self, callback):
        self.callback = callback
        self.last_modified = {}
        
    def on_modified(self, event):
        if event.is_directory or not event.src_path.endswith('.md'):
            return
            
        # é‡è¤‡ã‚¤ãƒ™ãƒ³ãƒˆé˜²æ­¢
        now = time.time()
        if event.src_path in self.last_modified:
            if now - self.last_modified[event.src_path] < 2.0:
                return
                
        self.last_modified[event.src_path] = now
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
        asyncio.create_task(self.callback('modified', event.src_path))
        
    def on_created(self, event):
        if event.is_directory or not event.src_path.endswith('.md'):
            return
        asyncio.create_task(self.callback('created', event.src_path))
        
    def on_deleted(self, event):
        if event.is_directory or not event.src_path.endswith('.md'):
            return
        asyncio.create_task(self.callback('deleted', event.src_path))

class PgVectorAutoSystem:
    """pgvectorè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.config = {
            'knowledge_base_path': '/home/aicompany/ai_co/knowledge_base',
            'sqlite_db': '/home/aicompany/ai_co/knowledge_base/integrated_knowledge.db',
            'watch_enabled': True,
            'auto_reindex_interval': 3600,  # 1æ™‚é–“
            'health_check_interval': 300,   # 5åˆ†
            'alert_log': '/home/aicompany/ai_co/logs/pgvector_alerts.log'
        }
        self.observer = None
        self.running = False
        self.stats = {
            'files_processed': 0,
            'last_update': None,
            'errors_count': 0,
            'start_time': datetime.now()
        }
        
    async def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        logger.info("ğŸ”„ pgvectorè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        self.running = True
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é–‹å§‹
        if self.config['watch_enabled']:
            await self._start_file_watcher()
            
        # å®šæœŸã‚¿ã‚¹ã‚¯é–‹å§‹
        asyncio.create_task(self._periodic_reindex())
        asyncio.create_task(self._periodic_health_check())
        
        logger.info("âœ… è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒä¸­")
        
    async def _start_file_watcher(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–é–‹å§‹"""
        logger.info("ğŸ‘ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–é–‹å§‹")
        
        self.observer = Observer()
        handler = KnowledgeBaseWatcher(self._handle_file_event)
        
        self.observer.schedule(
            handler,
            self.config['knowledge_base_path'],
            recursive=True
        )
        
        self.observer.start()
        logger.info(f"ğŸ“ ç›£è¦–å¯¾è±¡: {self.config['knowledge_base_path']}")
        
    async def _handle_file_event(self, event_type: str, file_path: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†"""
        logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆ: {event_type} - {file_path}")
        
        try:
            if event_type in ['created', 'modified']:
                await self._process_file_update(file_path)
            elif event_type == 'deleted':
                await self._process_file_deletion(file_path)
                
            self.stats['files_processed'] += 1
            self.stats['last_update'] = datetime.now()
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            self.stats['errors_count'] += 1
            await self._send_alert(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_path} - {e}")
            
    async def _process_file_update(self, file_path: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°å‡¦ç†"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹èª­ã¿è¾¼ã¿
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            if len(content.strip()) < 50:
                logger.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒçŸ­ã™ãã¾ã™: {file_path}")
                return
                
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            relative_path = os.path.relpath(file_path, self.config['knowledge_base_path'])
            file_hash = hashlib.md5(content.encode()).hexdigest()
            
            # SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
            await self._update_sqlite_record(relative_path, content, file_hash)
            
            logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°å®Œäº†: {relative_path}")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            raise
            
    async def _process_file_deletion(self, file_path: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å‡¦ç†"""
        try:
            relative_path = os.path.relpath(file_path, self.config['knowledge_base_path'])
            
            # SQLiteã‹ã‚‰å‰Šé™¤
            conn = sqlite3.connect(self.config['sqlite_db'])
            cursor = conn.execute(
                "DELETE FROM knowledge_documents WHERE source_file = ?",
                [relative_path]
            )
            deleted_count = cursor.rowcount
            conn.commit()
            conn.close()
            
            logger.info(f"ğŸ—‘ï¸ ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å®Œäº†: {relative_path} ({deleted_count}ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤)")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            raise
            
    async def _update_sqlite_record(self, source_file: str, content: str, file_hash: str):
        """SQLiteãƒ¬ã‚³ãƒ¼ãƒ‰æ›´æ–°"""
        try:
            conn = sqlite3.connect(self.config['sqlite_db'])
            
            # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºèª
            cursor = conn.execute(
                "SELECT file_hash FROM knowledge_documents WHERE source_file = ? LIMIT 1",
                [source_file]
            )
            existing = cursor.fetchone()
            
            if existing and existing[0] == file_hash:
                # ãƒãƒƒã‚·ãƒ¥ãŒåŒã˜å ´åˆã¯æ›´æ–°ä¸è¦
                conn.close()
                return
                
            # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤
            conn.execute("DELETE FROM knowledge_documents WHERE source_file = ?", [source_file])
            
            # æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰æŒ¿å…¥
            import uuid
            from libs.knowledge_integration_mapper import KnowledgeIntegrationMapper
            
            mapper = KnowledgeIntegrationMapper()
            title = Path(source_file).stem
            priority = mapper.get_file_priority(source_file)
            category = mapper.categorize_file(source_file)
            chunks = mapper.chunk_content(content)
            
            for i, chunk in enumerate(chunks):
                doc_uuid = str(uuid.uuid4())
                metadata = {
                    'processed_at': datetime.now().isoformat(),
                    'auto_updated': True,
                    'chunk_size': len(chunk),
                    'update_source': 'file_watcher'
                }
                
                conn.execute("""
                    INSERT INTO knowledge_documents (
                        uuid, title, content, source_file, file_hash,
                        chunk_index, total_chunks, category, priority,
                        file_size, metadata
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, [
                    doc_uuid, title, chunk, source_file, file_hash,
                    i, len(chunks), category, priority,
                    len(content), json.dumps(metadata)
                ])
                
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"âŒ SQLiteæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            raise
            
    async def _periodic_reindex(self):
        """å®šæœŸå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
        while self.running:
            try:
                await asyncio.sleep(self.config['auto_reindex_interval'])
                
                if not self.running:
                    break
                    
                logger.info("ğŸ”„ å®šæœŸå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹")
                
                # PostgreSQLçµ±åˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨
                cmd = ['python3', '/home/aicompany/ai_co/libs/pgvector_unified_manager.py', 'migrate']
                result = subprocess.run(cmd, capture_output=True, text=True)
                
                if result.returncode == 0:
                    logger.info("âœ… å®šæœŸå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†")
                else:
                    logger.error(f"âŒ å®šæœŸå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¤±æ•—: {result.stderr}")
                    await self._send_alert(f"å®šæœŸå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¤±æ•—: {result.stderr}")
                    
            except Exception as e:
                logger.error(f"âŒ å®šæœŸå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                await self._send_alert(f"å®šæœŸå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                
    async def _periodic_health_check(self):
        """å®šæœŸãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        while self.running:
            try:
                await asyncio.sleep(self.config['health_check_interval'])
                
                if not self.running:
                    break
                    
                # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
                cmd = ['python3', '/home/aicompany/ai_co/libs/pgvector_unified_manager.py', 'health']
                result = subprocess.run(cmd, capture_output=True, text=True)
                
                if result.returncode == 0:
                    health_data = json.loads(result.stdout)
                    
                    if health_data['overall_status'] == 'error':
                        await self._send_alert(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼æ¤œå‡º: {health_data}")
                    elif health_data['overall_status'] == 'degraded':
                        logger.warning(f"âš ï¸ ã‚·ã‚¹ãƒ†ãƒ æ©Ÿèƒ½ä½ä¸‹: {health_data}")
                        
                else:
                    await self._send_alert(f"ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å¤±æ•—: {result.stderr}")
                    
            except Exception as e:
                logger.error(f"âŒ å®šæœŸãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
                
    async def _send_alert(self, message: str):
        """ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        alert_data = {
            'timestamp': datetime.now().isoformat(),
            'message': message,
            'system': 'pgvector_auto_system',
            'stats': self.stats
        }
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²
        try:
            os.makedirs(os.path.dirname(self.config['alert_log']), exist_ok=True)
            with open(self.config['alert_log'], 'a', encoding='utf-8') as f:
                f.write(json.dumps(alert_data, ensure_ascii=False) + '\n')
        except Exception as e:
            logger.error(f"âŒ ã‚¢ãƒ©ãƒ¼ãƒˆãƒ­ã‚°æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            
        logger.error(f"ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆ: {message}")
        
    async def get_status(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³å–å¾—"""
        uptime = datetime.now() - self.stats['start_time']
        
        # JSONç›´åˆ—åŒ–å¯èƒ½ãªå½¢å¼ã«å¤‰æ›
        stats_json = self.stats.copy()
        stats_json['start_time'] = self.stats['start_time'].isoformat()
        if stats_json.get('last_update'):
            stats_json['last_update'] = stats_json['last_update'].isoformat()
        
        return {
            'running': self.running,
            'uptime_seconds': uptime.total_seconds(),
            'uptime_human': str(uptime),
            'stats': stats_json,
            'config': self.config,
            'file_watcher_active': self.observer is not None and self.observer.is_alive() if self.observer else False
        }
        
    async def stop(self):
        """ã‚·ã‚¹ãƒ†ãƒ åœæ­¢"""
        logger.info("ğŸ›‘ pgvectorè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ åœæ­¢")
        self.running = False
        
        if self.observer:
            self.observer.stop()
            self.observer.join()
            
        logger.info("âœ… ã‚·ã‚¹ãƒ†ãƒ åœæ­¢å®Œäº†")

# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³é‹ç”¨
async def main():
    """ãƒ¡ã‚¤ãƒ³é‹ç”¨"""
    import sys
    
    auto_system = PgVectorAutoSystem()
    
    if len(sys.argv) > 1 and sys.argv[1] == 'daemon':
        # ãƒ‡ãƒ¼ãƒ¢ãƒ³ãƒ¢ãƒ¼ãƒ‰
        try:
            await auto_system.start_monitoring()
            
            # ç„¡é™ãƒ«ãƒ¼ãƒ—ã§ç¨¼åƒ
            while auto_system.running:
                await asyncio.sleep(10)
                
        except KeyboardInterrupt:
            logger.info("ğŸ“ åœæ­¢ã‚·ã‚°ãƒŠãƒ«å—ä¿¡")
        finally:
            await auto_system.stop()
            
    elif len(sys.argv) > 1 and sys.argv[1] == 'status':
        # çŠ¶æ³ç¢ºèªãƒ¢ãƒ¼ãƒ‰
        # å®Ÿè¡Œä¸­ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒã‚ã‚‹ã‹ç¢ºèª
        status = await auto_system.get_status()
        print(json.dumps(status, indent=2, ensure_ascii=False))
        
    else:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python3 pgvector_auto_system.py daemon   # ãƒ‡ãƒ¼ãƒ¢ãƒ³èµ·å‹•")
        print("  python3 pgvector_auto_system.py status   # çŠ¶æ³ç¢ºèª")
        return 1
        
    return 0

if __name__ == "__main__":
    exit(asyncio.run(main()))