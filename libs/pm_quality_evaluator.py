#!/usr/bin/env python3
"""
PMå“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  - PMãŒç´å¾—ã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã™ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿèƒ½
"""

import json
import logging
import re
import sqlite3
import subprocess

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from core import BaseManager
from libs.error_intelligence_manager import ErrorIntelligenceManager
from libs.test_manager import TestManager

logger = logging.getLogger(__name__)


class PMQualityEvaluator(BaseManager):
    """PMå“è³ªè©•ä¾¡ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        super().__init__("PMQualityEvaluator")
        self.db_path = PROJECT_ROOT / "db" / "pm_quality_records.db"
        self.test_manager = TestManager(str(PROJECT_ROOT))
        self.error_manager = ErrorIntelligenceManager()

        # å“è³ªè©•ä¾¡åŸºæº–
        self.quality_criteria = {
            "test_success_rate": 95.0,  # ãƒ†ã‚¹ãƒˆæˆåŠŸç‡ï¼ˆ%ï¼‰
            "code_quality_score": 80.0,  # ã‚³ãƒ¼ãƒ‰å“è³ªã‚¹ã‚³ã‚¢
            "requirement_compliance": 90.0,  # è¦ä»¶é©åˆåº¦ï¼ˆ%ï¼‰
            "error_rate": 5.0,  # ã‚¨ãƒ©ãƒ¼ç‡ï¼ˆ%ï¼‰ä»¥ä¸‹
            "performance_score": 75.0,  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢
            "security_score": 85.0,  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢
        }

        # é‡è¦åº¦é‡ã¿ä»˜ã‘
        self.criterion_weights = {
            "test_success_rate": 0.25,
            "code_quality_score": 0.20,
            "requirement_compliance": 0.25,
            "error_rate": 0.15,
            "performance_score": 0.10,
            "security_score": 0.05,
        }

        # åˆæ ¼åŸºæº–
        self.pass_threshold = 80.0  # ç·åˆã‚¹ã‚³ã‚¢80%ä»¥ä¸Šã§åˆæ ¼
        self.max_retry_attempts = 3  # æœ€å¤§å†è©¦è¡Œå›æ•°

        self.initialize()

    def initialize(self) -> bool:
        """åˆæœŸåŒ–å‡¦ç†"""
        try:
            self._init_database()
            return True
        except Exception as e:
            self.handle_error(e, "åˆæœŸåŒ–")
            return False

    def _init_database(self):
        """å“è³ªè©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        with sqlite3.connect(self.db_path) as conn:
            # å“è³ªè©•ä¾¡è¨˜éŒ²ãƒ†ãƒ¼ãƒ–ãƒ«
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS quality_evaluations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    task_id TEXT NOT NULL,
                    attempt_number INTEGER DEFAULT 1,
                    evaluation_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    test_success_rate REAL,
                    code_quality_score REAL,
                    requirement_compliance REAL,
                    error_rate REAL,
                    performance_score REAL,
                    security_score REAL,
                    overall_score REAL,
                    pm_approved BOOLEAN DEFAULT 0,
                    feedback_message TEXT,
                    retry_required BOOLEAN DEFAULT 0,
                    files_evaluated TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS feedback_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    task_id TEXT NOT NULL,
                    attempt_number INTEGER,
                    feedback_type TEXT,
                    feedback_content TEXT,
                    improvement_suggestions TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS learning_patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    pattern_type TEXT NOT NULL,
                    pattern_description TEXT,
                    success_rate REAL,
                    usage_count INTEGER DEFAULT 1,
                    last_used TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_task_id ON quality_evaluations(task_id)"
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_pm_approved ON quality_evaluations(pm_approved)"
            )

    def evaluate_task_quality(
        self, task_id: str, task_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚¿ã‚¹ã‚¯ã®å“è³ªã‚’ç·åˆè©•ä¾¡"""
        try:
            logger.info(f"ğŸ¯ å“è³ªè©•ä¾¡é–‹å§‹: {task_id}")

            # å„åŸºæº–ã§è©•ä¾¡
            evaluation_results = {}

            # 1. ãƒ†ã‚¹ãƒˆæˆåŠŸç‡è©•ä¾¡
            evaluation_results["test_success_rate"] = self._evaluate_test_success(
                task_data
            )

            # 2. ã‚³ãƒ¼ãƒ‰å“è³ªè©•ä¾¡
            evaluation_results["code_quality_score"] = self._evaluate_code_quality(
                task_data
            )

            # 3. è¦ä»¶é©åˆåº¦è©•ä¾¡
            evaluation_results[
                "requirement_compliance"
            ] = self._evaluate_requirement_compliance(task_data)

            # 4. ã‚¨ãƒ©ãƒ¼ç‡è©•ä¾¡
            evaluation_results["error_rate"] = self._evaluate_error_rate(task_data)

            # 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
            evaluation_results["performance_score"] = self._evaluate_performance(
                task_data
            )

            # 6. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è©•ä¾¡
            evaluation_results["security_score"] = self._evaluate_security(task_data)

            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            overall_score = self._calculate_overall_score(evaluation_results)
            evaluation_results["overall_score"] = overall_score

            # PMæ‰¿èªåˆ¤å®š
            pm_approved = overall_score >= self.pass_threshold
            evaluation_results["pm_approved"] = pm_approved

            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
            feedback_message = self._generate_feedback_message(evaluation_results)
            evaluation_results["feedback_message"] = feedback_message

            # å†è©¦è¡Œè¦å¦åˆ¤å®š
            retry_required = not pm_approved and self._should_retry(task_id)
            evaluation_results["retry_required"] = retry_required

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
            self._record_evaluation(task_id, evaluation_results, task_data)

            logger.info(
                f"âœ… å“è³ªè©•ä¾¡å®Œäº†: {task_id} - ã‚¹ã‚³ã‚¢: {overall_score:.1f}% - {'æ‰¿èª' if pm_approved else 'è¦æ”¹å–„'}"
            )

            return evaluation_results

        except Exception as e:
            logger.error(f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "overall_score": 0.0,
                "pm_approved": False,
                "feedback_message": f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}",
                "retry_required": False,
            }

    def _evaluate_test_success(self, task_data: Dict[str, Any]) -> float:
        """ãƒ†ã‚¹ãƒˆæˆåŠŸç‡è©•ä¾¡"""
        try:
            files_created = task_data.get("files_created", [])
            python_files = [f for f in files_created if f.endswith(".py")]

            if not python_files:
                return 100.0  # Pythonãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯æº€ç‚¹

            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_results = []
            for py_file in python_files:
                file_path = Path(py_file)

                # å¯¾å¿œã™ã‚‹ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
                if file_path.parts[0] in ["workers", "libs"]:
                    test_file = (
                        PROJECT_ROOT / "tests" / "unit" / f"test_{file_path.name}"
                    )

                    if test_file.exists():
                        result = self.test_manager.run_specific_test(str(test_file))
                        test_results.append(result.get("success", False))
                    else:
                        # åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                        result = self._run_syntax_check(str(PROJECT_ROOT / py_file))
                        test_results.append(result.get("success", False))

            if not test_results:
                return 100.0

            success_rate = (sum(test_results) / len(test_results)) * 100
            return success_rate

        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def _evaluate_code_quality(self, task_data: Dict[str, Any]) -> float:
        """ã‚³ãƒ¼ãƒ‰å“è³ªè©•ä¾¡"""
        try:
            files_created = task_data.get("files_created", [])
            python_files = [f for f in files_created if f.endswith(".py")]

            if not python_files:
                return 100.0

            quality_scores = []

            for py_file in python_files:
                file_path = PROJECT_ROOT / py_file
                if file_path.exists():
                    score = self._analyze_code_quality(file_path)
                    quality_scores.append(score)

            if not quality_scores:
                return 100.0

            return sum(quality_scores) / len(quality_scores)

        except Exception as e:
            logger.error(f"ã‚³ãƒ¼ãƒ‰å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def _evaluate_requirement_compliance(self, task_data: Dict[str, Any]) -> float:
        """è¦ä»¶é©åˆåº¦è©•ä¾¡"""
        try:
            # åŸºæœ¬çš„ãªè¦ä»¶ãƒã‚§ãƒƒã‚¯
            prompt = task_data.get("prompt", "")
            response = task_data.get("response", "")
            files_created = task_data.get("files_created", [])

            compliance_score = 0.0

            # ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆè¦ä»¶
            if "ãƒ•ã‚¡ã‚¤ãƒ«" in prompt or "file" in prompt.lower():
                if files_created:
                    compliance_score += 30.0

            # å®Ÿè£…è¦ä»¶
            if "class" in prompt.lower() or "ã‚¯ãƒ©ã‚¹" in prompt:
                if self._check_class_implementation(files_created):
                    compliance_score += 25.0

            # æ©Ÿèƒ½è¦ä»¶
            if "function" in prompt.lower() or "é–¢æ•°" in prompt:
                if self._check_function_implementation(files_created):
                    compliance_score += 25.0

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å“è³ª
            if response and len(response) > 100:
                compliance_score += 20.0

            return min(compliance_score, 100.0)

        except Exception as e:
            logger.error(f"è¦ä»¶é©åˆåº¦è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def _evaluate_error_rate(self, task_data: Dict[str, Any]) -> float:
        """ã‚¨ãƒ©ãƒ¼ç‡è©•ä¾¡ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰"""
        try:
            error_trace = task_data.get("error_trace", "")
            status = task_data.get("status", "completed")

            if status == "completed" and not error_trace:
                return 100.0  # ã‚¨ãƒ©ãƒ¼ãªã—

            if status == "failed":
                return 0.0  # å®Œå…¨å¤±æ•—

            # ã‚¨ãƒ©ãƒ¼ã®é‡è¦åº¦åˆ†æ
            if error_trace:
                analysis = self.error_manager.analyze_error(error_trace)
                severity = analysis.get("severity", "low")

                if severity == "high":
                    return 20.0
                elif severity == "medium":
                    return 60.0
                else:
                    return 80.0

            return 100.0

        except Exception as e:
            logger.error(f"ã‚¨ãƒ©ãƒ¼ç‡è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def _evaluate_performance(self, task_data: Dict[str, Any]) -> float:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡"""
        try:
            duration = task_data.get("duration", 0.0)

            # å®Ÿè¡Œæ™‚é–“ã«åŸºã¥ãè©•ä¾¡
            if duration <= 10.0:
                return 100.0
            elif duration <= 30.0:
                return 80.0
            elif duration <= 60.0:
                return 60.0
            elif duration <= 120.0:
                return 40.0
            else:
                return 20.0

        except Exception as e:
            logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 75.0

    def _evaluate_security(self, task_data: Dict[str, Any]) -> float:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è©•ä¾¡"""
        try:
            files_created = task_data.get("files_created", [])

            security_score = 100.0

            for file_path in files_created:
                if file_path.endswith(".py"):
                    full_path = PROJECT_ROOT / file_path
                    if full_path.exists():
                        issues = self._check_security_issues(full_path)
                        security_score -= len(issues) * 10

            return max(security_score, 0.0)

        except Exception as e:
            logger.error(f"ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 85.0

    def _calculate_overall_score(self, evaluation_results: Dict[str, float]) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        weighted_score = 0.0

        for criterion, weight in self.criterion_weights.items():
            score = evaluation_results.get(criterion, 0.0)
            weighted_score += score * weight

        return weighted_score

    def _generate_feedback_message(self, evaluation_results: Dict[str, Any]) -> str:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ"""
        overall_score = evaluation_results.get("overall_score", 0.0)
        pm_approved = evaluation_results.get("pm_approved", False)

        if pm_approved:
            return f"âœ… PMæ‰¿èª: ç·åˆã‚¹ã‚³ã‚¢ {overall_score:.1f}% - å“è³ªåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã™"

        # æ”¹å–„ç‚¹ã‚’ç‰¹å®š
        improvements = []

        for criterion, threshold in self.quality_criteria.items():
            current_score = evaluation_results.get(criterion, 0.0)

            if criterion == "error_rate":
                if current_score < 100 - threshold:  # ã‚¨ãƒ©ãƒ¼ç‡ã¯é€†è»¢
                    improvements.append(f"ã‚¨ãƒ©ãƒ¼ç‡æ”¹å–„ (ç¾åœ¨: {100-current_score:.1f}%)")
            else:
                if current_score < threshold:
                    improvements.append(f"{criterion}æ”¹å–„ (ç¾åœ¨: {current_score:.1f}%)")

        message = f"âŒ PMå†è©•ä¾¡è¦: ç·åˆã‚¹ã‚³ã‚¢ {overall_score:.1f}%\n"
        message += "æ”¹å–„ç‚¹:\n"
        for improvement in improvements[:3]:  # æœ€å¤§3ã¤ã®æ”¹å–„ç‚¹
            message += f"  - {improvement}\n"

        return message

    def _should_retry(self, task_id: str) -> bool:
        """å†è©¦è¡Œã™ã¹ãã‹ã®åˆ¤å®š"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                "SELECT COUNT(*) FROM quality_evaluations WHERE task_id = ?", (task_id,)
            )
            attempt_count = cursor.fetchone()[0]

            return attempt_count < self.max_retry_attempts

    def _record_evaluation(
        self,
        task_id: str,
        evaluation_results: Dict[str, Any],
        task_data: Dict[str, Any],
    ):
        """è©•ä¾¡çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²"""
        with sqlite3.connect(self.db_path) as conn:
            # è©¦è¡Œå›æ•°å–å¾—
            cursor = conn.execute(
                "SELECT COUNT(*) FROM quality_evaluations WHERE task_id = ?", (task_id,)
            )
            attempt_number = cursor.fetchone()[0] + 1

            conn.execute(
                """
                INSERT INTO quality_evaluations
                (task_id, attempt_number, test_success_rate, code_quality_score,
                 requirement_compliance, error_rate, performance_score, security_score,
                 overall_score, pm_approved, feedback_message, retry_required, files_evaluated)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    task_id,
                    attempt_number,
                    evaluation_results.get("test_success_rate", 0.0),
                    evaluation_results.get("code_quality_score", 0.0),
                    evaluation_results.get("requirement_compliance", 0.0),
                    evaluation_results.get("error_rate", 0.0),
                    evaluation_results.get("performance_score", 0.0),
                    evaluation_results.get("security_score", 0.0),
                    evaluation_results.get("overall_score", 0.0),
                    evaluation_results.get("pm_approved", False),
                    evaluation_results.get("feedback_message", ""),
                    evaluation_results.get("retry_required", False),
                    json.dumps(task_data.get("files_created", [])),
                ),
            )

    def _run_syntax_check(self, file_path: str) -> Dict[str, Any]:
        """æ§‹æ–‡ãƒã‚§ãƒƒã‚¯"""
        try:
            cmd = ["python3", "-m", "py_compile", file_path]
            result = subprocess.run(cmd, capture_output=True, text=True)

            return {
                "success": result.returncode == 0,
                "output": result.stdout,
                "errors": result.stderr,
            }
        except Exception as e:
            return {"success": False, "output": "", "errors": str(e)}

    def _analyze_code_quality(self, file_path: Path) -> float:
        """ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æ"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            quality_score = 100.0

            # åŸºæœ¬çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
            lines = content.split("\n")

            # 1. è¡Œã®é•·ã•ãƒã‚§ãƒƒã‚¯
            long_lines = [line for line in lines if len(line) > 120]
            quality_score -= len(long_lines) * 2

            # 2. ã‚³ãƒ¡ãƒ³ãƒˆç‡ãƒã‚§ãƒƒã‚¯
            comment_lines = [line for line in lines if line.strip().startswith("#")]
            if len(lines) > 0:
                comment_ratio = len(comment_lines) / len(lines)
                if comment_ratio < 0.1:  # 10%æœªæº€
                    quality_score -= 10

            # 3. é–¢æ•°ã®è¤‡é›‘ã•ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            function_lines = [line for line in lines if "def " in line]
            if len(function_lines) > 10:  # é–¢æ•°ãŒå¤šã™ãã‚‹
                quality_score -= 5

            return max(quality_score, 0.0)

        except Exception as e:
            logger.error(f"ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return 75.0

    def _check_class_implementation(self, files_created: List[str]) -> bool:
        """ã‚¯ãƒ©ã‚¹å®Ÿè£…ãƒã‚§ãƒƒã‚¯"""
        for file_path in files_created:
            if file_path.endswith(".py"):
                full_path = PROJECT_ROOT / file_path
                if full_path.exists():
                    try:
                        with open(full_path, "r", encoding="utf-8") as f:
                            content = f.read()
                            if "class " in content:
                                return True
                    except:
                        pass
        return False

    def _check_function_implementation(self, files_created: List[str]) -> bool:
        """é–¢æ•°å®Ÿè£…ãƒã‚§ãƒƒã‚¯"""
        for file_path in files_created:
            if file_path.endswith(".py"):
                full_path = PROJECT_ROOT / file_path
                if full_path.exists():
                    try:
                        with open(full_path, "r", encoding="utf-8") as f:
                            content = f.read()
                            if "def " in content:
                                return True
                    except:
                        pass
        return False

    def _check_security_issues(self, file_path: Path) -> List[str]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œãƒã‚§ãƒƒã‚¯"""
        issues = []

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            dangerous_patterns = [
                r"eval\s*\(",
                r"exec\s*\(",
                r"subprocess\.call\s*\(",
                r"os\.system\s*\(",
                r"input\s*\(",
                r"raw_input\s*\(",
            ]

            for pattern in dangerous_patterns:
                if re.search(pattern, content):
                    issues.append(f"Potentially dangerous pattern: {pattern}")

        except Exception as e:
            logger.error(f"ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

        return issues

    def get_evaluation_history(self, task_id: str) -> List[Dict[str, Any]]:
        """è©•ä¾¡å±¥æ­´å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                """
                SELECT * FROM quality_evaluations
                WHERE task_id = ?
                ORDER BY attempt_number
            """,
                (task_id,),
            )

            columns = [desc[0] for desc in cursor.description]
            history = []

            for row in cursor:
                record = dict(zip(columns, row))
                history.append(record)

            return history

    def get_quality_statistics(self) -> Dict[str, Any]:
        """å“è³ªçµ±è¨ˆæƒ…å ±å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            stats = {}

            # å…¨ä½“çµ±è¨ˆ
            cursor = conn.execute(
                """
                SELECT
                    COUNT(*) as total_evaluations,
                    AVG(overall_score) as avg_score,
                    COUNT(CASE WHEN pm_approved = 1 THEN 1 END) as approved_count,
                    COUNT(CASE WHEN retry_required = 1 THEN 1 END) as retry_count
                FROM quality_evaluations
            """
            )

            row = cursor.fetchone()
            stats["total_evaluations"] = row[0]
            stats["average_score"] = row[1] or 0.0
            stats["approval_rate"] = (row[2] / row[0] * 100) if row[0] > 0 else 0.0
            stats["retry_rate"] = (row[3] / row[0] * 100) if row[0] > 0 else 0.0

            # åŸºæº–åˆ¥çµ±è¨ˆ
            cursor = conn.execute(
                """
                SELECT
                    AVG(test_success_rate) as avg_test_success,
                    AVG(code_quality_score) as avg_code_quality,
                    AVG(requirement_compliance) as avg_requirement,
                    AVG(error_rate) as avg_error_rate,
                    AVG(performance_score) as avg_performance,
                    AVG(security_score) as avg_security
                FROM quality_evaluations
            """
            )

            row = cursor.fetchone()
            stats["criteria_averages"] = {
                "test_success_rate": row[0] or 0.0,
                "code_quality_score": row[1] or 0.0,
                "requirement_compliance": row[2] or 0.0,
                "error_rate": row[3] or 0.0,
                "performance_score": row[4] or 0.0,
                "security_score": row[5] or 0.0,
            }

            return stats


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    evaluator = PMQualityEvaluator()

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_task_data = {
        "task_id": "test_task_001",
        "status": "completed",
        "files_created": ["workers/test_worker.py"],
        "duration": 25.0,
        "prompt": "Create a test worker class",
        "response": "Created TestWorker class with proper initialization and methods",
        "error_trace": "",
    }

    print("=== PM Quality Evaluator Test ===")
    result = evaluator.evaluate_task_quality("test_task_001", test_task_data)

    print(f"Overall Score: {result['overall_score']:.1f}%")
    print(f"PM Approved: {result['pm_approved']}")
    print(f"Feedback: {result['feedback_message']}")
    print(f"Retry Required: {result['retry_required']}")

    print("\n=== Quality Statistics ===")
    stats = evaluator.get_quality_statistics()
    print(f"Total Evaluations: {stats['total_evaluations']}")
    print(f"Average Score: {stats['average_score']:.1f}%")
    print(f"Approval Rate: {stats['approval_rate']:.1f}%")
