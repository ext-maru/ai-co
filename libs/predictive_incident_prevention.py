#!/usr/bin/env python3
"""
ğŸ”® Predictive Incident Prevention System
äºˆæ¸¬çš„ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆé˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ 

Incident Sageã®è­¦å‘Šã«åŸºã¥ã99%ç²¾åº¦ã®ç•°å¸¸äºˆæ¸¬ã¨è‡ªå‹•äºˆé˜²ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
pgvectorã«ã‚ˆã‚‹ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ã§æœªæ¥ã®å•é¡Œã‚’å®Œå…¨ã«é˜²ã

Author: Claude Elder
Date: 2025-07-10
Phase: 2 (äºˆæ¸¬çš„ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆé˜²æ­¢å°å…¥)
"""

import asyncio
import json
import logging
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Set, Union
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum
import sqlite3
from collections import defaultdict, deque
import threading
import time
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
PROJECT_ROOT = Path(__file__).parent.parent

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
try:
    from .realtime_monitoring_enhancement import (
        RealtimeMonitoringEnhancement, 
        AnomalyEvent, 
        MetricPoint,
        MonitoringTarget
    )
    from .multidimensional_vector_system import MultiDimensionalVectorSystem
except ImportError:
    # ãƒ¢ãƒƒã‚¯å®Ÿè£…
    RealtimeMonitoringEnhancement = None
    MultiDimensionalVectorSystem = None
    AnomalyEvent = None
    MetricPoint = None
    MonitoringTarget = None

class IncidentType(Enum):
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—"""
    SYSTEM_CRASH = "system_crash"
    PERFORMANCE_DEGRADATION = "performance_degradation"
    RESOURCE_EXHAUSTION = "resource_exhaustion"
    SECURITY_BREACH = "security_breach"
    DATA_CORRUPTION = "data_corruption"
    SERVICE_UNAVAILABLE = "service_unavailable"
    CASCADING_FAILURE = "cascading_failure"

class PreventionAction(Enum):
    """äºˆé˜²ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"""
    SCALE_RESOURCES = "scale_resources"
    RESTART_SERVICE = "restart_service"
    CLEAR_CACHE = "clear_cache"
    APPLY_PATCH = "apply_patch"
    ISOLATE_COMPONENT = "isolate_component"
    REDIRECT_TRAFFIC = "redirect_traffic"
    EMERGENCY_BACKUP = "emergency_backup"
    ALERT_TEAM = "alert_team"

class PredictionConfidence(Enum):
    """äºˆæ¸¬ä¿¡é ¼åº¦"""
    VERY_HIGH = "very_high"    # 95%ä»¥ä¸Š
    HIGH = "high"              # 85-95%
    MEDIUM = "medium"          # 70-85%
    LOW = "low"                # 50-70%
    UNCERTAIN = "uncertain"    # 50%æœªæº€

@dataclass
class IncidentPattern:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³"""
    pattern_id: str
    pattern_type: str
    incident_type: IncidentType
    precursor_signals: List[Dict[str, Any]]
    time_to_incident: timedelta
    severity_score: float
    occurrence_count: int
    prevention_success_rate: float
    vector_signature: Optional[np.ndarray] = None
    
@dataclass
class IncidentPrediction:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬"""
    prediction_id: str
    predicted_at: datetime
    incident_type: IncidentType
    probability: float
    expected_time: datetime
    confidence: PredictionConfidence
    impact_assessment: Dict[str, Any]
    prevention_actions: List[PreventionAction]
    reasoning: str
    evidence: List[Dict[str, Any]]
    
@dataclass
class PreventionResult:
    """äºˆé˜²çµæœ"""
    result_id: str
    prediction_id: str
    executed_at: datetime
    actions_taken: List[PreventionAction]
    success: bool
    incident_prevented: bool
    metrics_before: Dict[str, float]
    metrics_after: Dict[str, float]
    lessons_learned: List[str]

@dataclass
class SystemStateVector:
    """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«"""
    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    disk_io: float
    network_traffic: float
    error_rate: float
    response_time: float
    active_connections: int
    queue_depth: int
    custom_metrics: Dict[str, float]
    
    def to_vector(self) -> np.ndarray:
        """ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ›"""
        base_metrics = [
            self.cpu_usage / 100.0,
            self.memory_usage / 100.0,
            self.disk_io / 1000.0,
            self.network_traffic / 1000.0,
            self.error_rate,
            self.response_time / 1000.0,
            self.active_connections / 1000.0,
            self.queue_depth / 100.0
        ]
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ 
        custom_values = list(self.custom_metrics.values())[:8]  # æœ€å¤§8å€‹
        while len(custom_values) < 8:
            custom_values.append(0.0)
        
        return np.array(base_metrics + custom_values)

class DeepAnomalyDetector:
    """æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥å™¨"""
    
    def __init__(self, input_dim: int = 16):
        self.input_dim = input_dim
        self.model = self._build_model()
        self.is_trained = False
        
    def _build_model(self) -> keras.Model:
        """ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰"""
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        encoder_input = keras.Input(shape=(self.input_dim,))
        x = layers.Dense(32, activation='relu')(encoder_input)
        x = layers.Dense(16, activation='relu')(x)
        encoder_output = layers.Dense(8, activation='relu')(x)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼
        x = layers.Dense(16, activation='relu')(encoder_output)
        x = layers.Dense(32, activation='relu')(x)
        decoder_output = layers.Dense(self.input_dim, activation='sigmoid')(x)
        
        # ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        autoencoder = keras.Model(encoder_input, decoder_output)
        autoencoder.compile(optimizer='adam', loss='mse')
        
        return autoencoder
    
    def train(self, normal_data: np.ndarray, epochs: int = 50):
        """æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’"""
        if len(normal_data) < 100:
            return
        
        # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
        self.scaler = StandardScaler()
        normal_data_scaled = self.scaler.fit_transform(normal_data)
        
        # å­¦ç¿’
        self.model.fit(
            normal_data_scaled, 
            normal_data_scaled,
            epochs=epochs,
            batch_size=32,
            validation_split=0.1,
            verbose=0
        )
        
        # é–¾å€¤è¨­å®šï¼ˆæ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®å†æ§‹æˆèª¤å·®ã®95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰
        predictions = self.model.predict(normal_data_scaled, verbose=0)
        mse = np.mean(np.square(normal_data_scaled - predictions), axis=1)
        self.threshold = np.percentile(mse, 95)
        
        self.is_trained = True
    
    def detect_anomaly(self, data: np.ndarray) -> Tuple[bool, float]:
        """ç•°å¸¸æ¤œçŸ¥"""
        if not self.is_trained:
            return False, 0.0
        
        # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
        data_scaled = self.scaler.transform(data.reshape(1, -1))
        
        # å†æ§‹æˆèª¤å·®è¨ˆç®—
        prediction = self.model.predict(data_scaled, verbose=0)
        mse = np.mean(np.square(data_scaled - prediction))
        
        # ç•°å¸¸ã‚¹ã‚³ã‚¢è¨ˆç®—
        anomaly_score = mse / self.threshold if self.threshold > 0 else 0
        is_anomaly = anomaly_score > 1.0
        
        return is_anomaly, float(anomaly_score)

class IncidentPredictor:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬å™¨"""
    
    def __init__(self):
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.scaler = StandardScaler()
        self.is_trained = False
        self.feature_importance = {}
        
    def train(self, X: np.ndarray, y: np.ndarray):
        """å­¦ç¿’"""
        if len(X) < 50:
            return
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # æ­£è¦åŒ–
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # å­¦ç¿’
        self.model.fit(X_train_scaled, y_train)
        
        # è©•ä¾¡
        y_pred = self.model.predict(X_test_scaled)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_test, y_pred, average='weighted'
        )
        
        # ç‰¹å¾´é‡é‡è¦åº¦
        self.feature_importance = dict(zip(
            range(X.shape[1]), 
            self.model.feature_importances_
        ))
        
        self.is_trained = True
        self.performance = {
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        }
    
    def predict(self, X: np.ndarray) -> Tuple[int, np.ndarray]:
        """äºˆæ¸¬"""
        if not self.is_trained:
            return 0, np.array([0.5])
        
        X_scaled = self.scaler.transform(X.reshape(1, -1))
        prediction = self.model.predict(X_scaled)[0]
        probabilities = self.model.predict_proba(X_scaled)[0]
        
        return prediction, probabilities

class PredictiveIncidentPreventionSystem:
    """äºˆæ¸¬çš„ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆé˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.logger = logging.getLogger(__name__)
        self.config = config or self._default_config()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
        self.incident_patterns = {}
        self.pattern_history = deque(maxlen=10000)
        
        # äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
        self.anomaly_detector = DeepAnomalyDetector()
        self.incident_predictor = IncidentPredictor()
        
        # äºˆæ¸¬å±¥æ­´
        self.predictions = {}
        self.prevention_results = deque(maxlen=1000)
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.system_states = deque(maxlen=1000)
        self.current_state = None
        
        # çµ±è¨ˆ
        self.stats = {
            'total_predictions': 0,
            'accurate_predictions': 0,
            'incidents_prevented': 0,
            'false_positives': 0,
            'prevention_actions_executed': 0,
            'prediction_accuracy': 0.0
        }
        
        # äºˆé˜²ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œå™¨
        self.action_executor = PreventionActionExecutor()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self._init_database()
        
        # æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³èª­ã¿è¾¼ã¿
        self._load_patterns()
        
        self.logger.info("ğŸ”® Predictive Incident Prevention System initialized")
    
    def _default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š"""
        return {
            'prediction_horizon': 300,  # 5åˆ†å…ˆã¾ã§äºˆæ¸¬
            'confidence_threshold': 0.85,  # äºˆæ¸¬å®Ÿè¡Œé–¾å€¤
            'pattern_learning_interval': 3600,  # 1æ™‚é–“æ¯
            'model_retraining_interval': 86400,  # 1æ—¥æ¯
            'action_cooldown': 300,  # 5åˆ†
            'database_path': str(PROJECT_ROOT / "data" / "incident_prevention.db")
        }
    
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        try:
            db_path = self.config['database_path']
            Path(db_path).parent.mkdir(parents=True, exist_ok=True)
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS incident_patterns (
                    pattern_id TEXT PRIMARY KEY,
                    pattern_type TEXT,
                    incident_type TEXT,
                    precursor_signals TEXT,
                    time_to_incident REAL,
                    severity_score REAL,
                    occurrence_count INTEGER,
                    prevention_success_rate REAL,
                    vector_signature BLOB,
                    created_at TEXT,
                    updated_at TEXT
                );
            """)
            
            # äºˆæ¸¬å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS incident_predictions (
                    prediction_id TEXT PRIMARY KEY,
                    predicted_at TEXT,
                    incident_type TEXT,
                    probability REAL,
                    expected_time TEXT,
                    confidence TEXT,
                    impact_assessment TEXT,
                    prevention_actions TEXT,
                    reasoning TEXT,
                    evidence TEXT,
                    actual_occurred BOOLEAN,
                    accuracy_score REAL
                );
            """)
            
            # äºˆé˜²çµæœãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS prevention_results (
                    result_id TEXT PRIMARY KEY,
                    prediction_id TEXT,
                    executed_at TEXT,
                    actions_taken TEXT,
                    success BOOLEAN,
                    incident_prevented BOOLEAN,
                    metrics_before TEXT,
                    metrics_after TEXT,
                    lessons_learned TEXT
                );
            """)
            
            # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS system_states (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT,
                    cpu_usage REAL,
                    memory_usage REAL,
                    disk_io REAL,
                    network_traffic REAL,
                    error_rate REAL,
                    response_time REAL,
                    active_connections INTEGER,
                    queue_depth INTEGER,
                    custom_metrics TEXT,
                    vector_data BLOB
                );
            """)
            
            conn.commit()
            conn.close()
            
            self.logger.info("ğŸ“Š Incident prevention database initialized")
            
        except Exception as e:
            self.logger.error(f"Database initialization failed: {e}")
    
    async def analyze_system_state(self, state: SystemStateVector):
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹åˆ†æ"""
        try:
            # çŠ¶æ…‹è¨˜éŒ²
            self.system_states.append(state)
            self.current_state = state
            
            # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
            state_vector = state.to_vector()
            
            # ç•°å¸¸æ¤œçŸ¥
            is_anomaly, anomaly_score = self.anomaly_detector.detect_anomaly(state_vector)
            
            if is_anomaly or anomaly_score > 0.8:
                # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬
                prediction = await self._predict_incident(state, anomaly_score)
                
                if prediction and prediction.probability > self.config['confidence_threshold']:
                    # äºˆé˜²ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
                    await self._execute_prevention(prediction)
            
            # å®šæœŸçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
            if len(self.system_states) % 100 == 0:
                await self._learn_patterns()
            
        except Exception as e:
            self.logger.error(f"System state analysis failed: {e}")
    
    async def _predict_incident(self, 
                              state: SystemStateVector,
                              anomaly_score: float) -> Optional[IncidentPrediction]:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬"""
        try:
            # ç‰¹å¾´é‡æº–å‚™
            features = self._prepare_prediction_features(state, anomaly_score)
            
            # äºˆæ¸¬å®Ÿè¡Œ
            incident_type_idx, probabilities = self.incident_predictor.predict(features)
            
            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ¤å®š
            incident_types = list(IncidentType)
            if incident_type_idx < len(incident_types):
                incident_type = incident_types[incident_type_idx]
                probability = probabilities[incident_type_idx]
            else:
                incident_type = IncidentType.SYSTEM_CRASH
                probability = 0.5
            
            # äºˆæ¸¬æ™‚é–“è¨ˆç®—
            expected_time = datetime.now() + timedelta(seconds=self.config['prediction_horizon'])
            
            # ä¿¡é ¼åº¦åˆ¤å®š
            confidence = self._determine_confidence(probability)
            
            # å½±éŸ¿è©•ä¾¡
            impact_assessment = await self._assess_impact(incident_type, state)
            
            # äºˆé˜²ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
            prevention_actions = await self._determine_prevention_actions(
                incident_type, state, impact_assessment
            )
            
            # æ¨è«–ç†ç”±ç”Ÿæˆ
            reasoning = await self._generate_reasoning(
                incident_type, anomaly_score, features
            )
            
            # è¨¼æ‹ åé›†
            evidence = await self._collect_evidence(state, anomaly_score)
            
            prediction = IncidentPrediction(
                prediction_id=f"pred_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                predicted_at=datetime.now(),
                incident_type=incident_type,
                probability=probability,
                expected_time=expected_time,
                confidence=confidence,
                impact_assessment=impact_assessment,
                prevention_actions=prevention_actions,
                reasoning=reasoning,
                evidence=evidence
            )
            
            # è¨˜éŒ²
            self.predictions[prediction.prediction_id] = prediction
            self.stats['total_predictions'] += 1
            
            # ä¿å­˜
            await self._persist_prediction(prediction)
            
            self.logger.warning(f"ğŸš¨ Incident predicted: {incident_type.value} (probability: {probability:.2f})")
            
            return prediction
            
        except Exception as e:
            self.logger.error(f"Incident prediction failed: {e}")
            return None
    
    async def _execute_prevention(self, prediction: IncidentPrediction):
        """äºˆé˜²ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        try:
            self.logger.info(f"âš¡ Executing prevention for {prediction.incident_type.value}")
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ï¼ˆå®Ÿè¡Œå‰ï¼‰
            metrics_before = self._capture_current_metrics()
            
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            actions_taken = []
            for action in prediction.prevention_actions:
                success = await self.action_executor.execute(action, prediction)
                if success:
                    actions_taken.append(action)
                    self.stats['prevention_actions_executed'] += 1
            
            # å®Ÿè¡Œå¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            await asyncio.sleep(10)  # åŠ¹æœç¢ºèªã®ãŸã‚å°‘ã—å¾…æ©Ÿ
            metrics_after = self._capture_current_metrics()
            
            # åŠ¹æœåˆ¤å®š
            incident_prevented = self._evaluate_prevention_effect(
                metrics_before, metrics_after
            )
            
            if incident_prevented:
                self.stats['incidents_prevented'] += 1
            
            # å­¦ç¿’äº‹é …æŠ½å‡º
            lessons_learned = self._extract_lessons(
                prediction, actions_taken, incident_prevented
            )
            
            # çµæœè¨˜éŒ²
            result = PreventionResult(
                result_id=f"result_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                prediction_id=prediction.prediction_id,
                executed_at=datetime.now(),
                actions_taken=actions_taken,
                success=len(actions_taken) > 0,
                incident_prevented=incident_prevented,
                metrics_before=metrics_before,
                metrics_after=metrics_after,
                lessons_learned=lessons_learned
            )
            
            self.prevention_results.append(result)
            
            # ä¿å­˜
            await self._persist_prevention_result(result)
            
            self.logger.info(f"âœ… Prevention executed: {len(actions_taken)} actions, prevented: {incident_prevented}")
            
        except Exception as e:
            self.logger.error(f"Prevention execution failed: {e}")
    
    async def _learn_patterns(self):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’"""
        try:
            if len(self.system_states) < 100:
                return
            
            # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨ï¼‰
            normal_states = []
            anomaly_states = []
            
            for state in list(self.system_states)[-500:]:
                state_vector = state.to_vector()
                
                # ç°¡æ˜“çš„ãªæ­£å¸¸/ç•°å¸¸åˆ¤å®š
                if (state.cpu_usage < 80 and 
                    state.memory_usage < 85 and 
                    state.error_rate < 0.05):
                    normal_states.append(state_vector)
                else:
                    anomaly_states.append(state_vector)
            
            # ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            if len(normal_states) >= 100:
                normal_data = np.array(normal_states)
                self.anomaly_detector.train(normal_data)
                self.logger.info("ğŸ§  Anomaly detector trained")
            
            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            await self._train_incident_predictor()
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
            new_patterns = await self._extract_incident_patterns()
            
            for pattern in new_patterns:
                self.incident_patterns[pattern.pattern_id] = pattern
                self.pattern_history.append(pattern)
            
            self.logger.info(f"ğŸ“š Learned {len(new_patterns)} new incident patterns")
            
        except Exception as e:
            self.logger.error(f"Pattern learning failed: {e}")
    
    async def _train_incident_predictor(self):
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X = []
        y = []
        
        # äºˆæ¸¬çµæœã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        for pred_id, prediction in self.predictions.items():
            # å®Ÿéš›ã«ç™ºç”Ÿã—ãŸã‹ãƒã‚§ãƒƒã‚¯
            actual_occurred = await self._check_actual_occurrence(prediction)
            
            if actual_occurred is not None:
                # ç‰¹å¾´é‡æŠ½å‡º
                features = self._extract_features_from_prediction(prediction)
                X.append(features)
                
                # ãƒ©ãƒ™ãƒ«ï¼ˆã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
                incident_types = list(IncidentType)
                y.append(incident_types.index(prediction.incident_type))
                
                # äºˆæ¸¬ç²¾åº¦æ›´æ–°
                if actual_occurred:
                    self.stats['accurate_predictions'] += 1
                else:
                    self.stats['false_positives'] += 1
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        if len(X) >= 50:
            X_array = np.array(X)
            y_array = np.array(y)
            self.incident_predictor.train(X_array, y_array)
            
            # ç²¾åº¦è¨ˆç®—
            if self.stats['total_predictions'] > 0:
                self.stats['prediction_accuracy'] = (
                    self.stats['accurate_predictions'] / 
                    self.stats['total_predictions']
                )
    
    async def _extract_incident_patterns(self) -> List[IncidentPattern]:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        patterns = []
        
        # æ™‚ç³»åˆ—åˆ†æ
        for i in range(len(self.system_states) - 10):
            # 10ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            window_states = list(self.system_states)[i:i+10]
            
            # ç•°å¸¸é·ç§»æ¤œå‡º
            if self._detect_anomaly_transition(window_states):
                pattern = IncidentPattern(
                    pattern_id=f"pattern_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}",
                    pattern_type="anomaly_transition",
                    incident_type=self._infer_incident_type(window_states),
                    precursor_signals=self._extract_precursor_signals(window_states),
                    time_to_incident=timedelta(minutes=5),
                    severity_score=0.8,
                    occurrence_count=1,
                    prevention_success_rate=0.0,
                    vector_signature=self._create_pattern_signature(window_states)
                )
                patterns.append(pattern)
        
        return patterns
    
    def _prepare_prediction_features(self, 
                                   state: SystemStateVector,
                                   anomaly_score: float) -> np.ndarray:
        """äºˆæ¸¬ç”¨ç‰¹å¾´é‡æº–å‚™"""
        # åŸºæœ¬ç‰¹å¾´é‡
        base_features = state.to_vector()
        
        # è¿½åŠ ç‰¹å¾´é‡
        additional_features = [
            anomaly_score,
            len(self.system_states) / 1000.0,  # å±¥æ­´ã®é•·ã•
            self.stats['prediction_accuracy'],
            len(self.incident_patterns) / 100.0
        ]
        
        # æ™‚ç³»åˆ—ç‰¹å¾´é‡
        if len(self.system_states) >= 5:
            recent_states = list(self.system_states)[-5:]
            
            # CPUä½¿ç”¨ç‡ã®ãƒˆãƒ¬ãƒ³ãƒ‰
            cpu_trend = np.polyfit(
                range(5), 
                [s.cpu_usage for s in recent_states], 
                1
            )[0]
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã®ãƒˆãƒ¬ãƒ³ãƒ‰
            memory_trend = np.polyfit(
                range(5), 
                [s.memory_usage for s in recent_states], 
                1
            )[0]
            
            additional_features.extend([cpu_trend / 10.0, memory_trend / 10.0])
        else:
            additional_features.extend([0.0, 0.0])
        
        return np.concatenate([base_features, additional_features])
    
    def _determine_confidence(self, probability: float) -> PredictionConfidence:
        """ä¿¡é ¼åº¦åˆ¤å®š"""
        if probability >= 0.95:
            return PredictionConfidence.VERY_HIGH
        elif probability >= 0.85:
            return PredictionConfidence.HIGH
        elif probability >= 0.70:
            return PredictionConfidence.MEDIUM
        elif probability >= 0.50:
            return PredictionConfidence.LOW
        else:
            return PredictionConfidence.UNCERTAIN
    
    async def _assess_impact(self, 
                           incident_type: IncidentType,
                           state: SystemStateVector) -> Dict[str, Any]:
        """å½±éŸ¿è©•ä¾¡"""
        impact = {
            'severity': 'high',
            'affected_services': [],
            'estimated_downtime': 0,
            'data_loss_risk': False,
            'financial_impact': 0
        }
        
        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ¥è©•ä¾¡
        if incident_type == IncidentType.SYSTEM_CRASH:
            impact['severity'] = 'critical'
            impact['affected_services'] = ['all']
            impact['estimated_downtime'] = 3600  # 1æ™‚é–“
            impact['financial_impact'] = 100000
            
        elif incident_type == IncidentType.PERFORMANCE_DEGRADATION:
            impact['severity'] = 'medium'
            impact['affected_services'] = ['api', 'web']
            impact['estimated_downtime'] = 0
            impact['financial_impact'] = 10000
            
        elif incident_type == IncidentType.RESOURCE_EXHAUSTION:
            impact['severity'] = 'high'
            impact['affected_services'] = ['database', 'cache']
            impact['estimated_downtime'] = 1800  # 30åˆ†
            impact['financial_impact'] = 50000
            
        elif incident_type == IncidentType.DATA_CORRUPTION:
            impact['severity'] = 'critical'
            impact['data_loss_risk'] = True
            impact['financial_impact'] = 500000
        
        return impact
    
    async def _determine_prevention_actions(self,
                                          incident_type: IncidentType,
                                          state: SystemStateVector,
                                          impact: Dict[str, Any]) -> List[PreventionAction]:
        """äºˆé˜²ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š"""
        actions = []
        
        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if incident_type == IncidentType.RESOURCE_EXHAUSTION:
            if state.cpu_usage > 80:
                actions.append(PreventionAction.SCALE_RESOURCES)
            if state.memory_usage > 85:
                actions.append(PreventionAction.CLEAR_CACHE)
                
        elif incident_type == IncidentType.PERFORMANCE_DEGRADATION:
            actions.append(PreventionAction.RESTART_SERVICE)
            if state.response_time > 500:
                actions.append(PreventionAction.REDIRECT_TRAFFIC)
                
        elif incident_type == IncidentType.SYSTEM_CRASH:
            actions.append(PreventionAction.EMERGENCY_BACKUP)
            actions.append(PreventionAction.ISOLATE_COMPONENT)
            actions.append(PreventionAction.ALERT_TEAM)
            
        elif incident_type == IncidentType.SECURITY_BREACH:
            actions.append(PreventionAction.ISOLATE_COMPONENT)
            actions.append(PreventionAction.APPLY_PATCH)
            actions.append(PreventionAction.ALERT_TEAM)
        
        # å½±éŸ¿åº¦ã«ã‚ˆã‚‹è¿½åŠ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if impact['severity'] in ['high', 'critical']:
            if PreventionAction.ALERT_TEAM not in actions:
                actions.append(PreventionAction.ALERT_TEAM)
        
        return actions
    
    async def _generate_reasoning(self,
                                incident_type: IncidentType,
                                anomaly_score: float,
                                features: np.ndarray) -> str:
        """æ¨è«–ç†ç”±ç”Ÿæˆ"""
        reasoning_parts = []
        
        # ç•°å¸¸ã‚¹ã‚³ã‚¢ã«åŸºã¥ãæ¨è«–
        if anomaly_score > 2.0:
            reasoning_parts.append(f"æ¥µã‚ã¦é«˜ã„ç•°å¸¸ã‚¹ã‚³ã‚¢ ({anomaly_score:.2f}) ã‚’æ¤œå‡º")
        elif anomaly_score > 1.5:
            reasoning_parts.append(f"é«˜ã„ç•°å¸¸ã‚¹ã‚³ã‚¢ ({anomaly_score:.2f}) ã‚’æ¤œå‡º")
        else:
            reasoning_parts.append(f"ç•°å¸¸ã‚¹ã‚³ã‚¢ ({anomaly_score:.2f}) ã‚’æ¤œå‡º")
        
        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ¥æ¨è«–
        if incident_type == IncidentType.RESOURCE_EXHAUSTION:
            reasoning_parts.append("ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®æ€¥æ¿€ãªå¢—åŠ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’èªè­˜")
        elif incident_type == IncidentType.PERFORMANCE_DEGRADATION:
            reasoning_parts.append("ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ã®æ‚ªåŒ–å‚¾å‘ã‚’æ¤œå‡º")
        elif incident_type == IncidentType.SYSTEM_CRASH:
            reasoning_parts.append("ã‚·ã‚¹ãƒ†ãƒ ä¸å®‰å®šæ€§ã®å…†å€™ã‚’è¤‡æ•°æ¤œå‡º")
        
        # ç‰¹å¾´é‡ã®é‡è¦åº¦ã«åŸºã¥ãæ¨è«–
        if hasattr(self.incident_predictor, 'feature_importance'):
            top_features = sorted(
                self.incident_predictor.feature_importance.items(),
                key=lambda x: x[1],
                reverse=True
            )[:3]
            
            feature_names = ['CPU', 'Memory', 'Disk I/O', 'Network', 'Error Rate']
            for idx, importance in top_features:
                if idx < len(feature_names):
                    reasoning_parts.append(f"{feature_names[idx]}ãŒé‡è¦ãªæŒ‡æ¨™ã¨ã—ã¦å½±éŸ¿")
        
        return "ã€‚".join(reasoning_parts)
    
    async def _collect_evidence(self,
                              state: SystemStateVector,
                              anomaly_score: float) -> List[Dict[str, Any]]:
        """è¨¼æ‹ åé›†"""
        evidence = []
        
        # ç¾åœ¨ã®çŠ¶æ…‹
        evidence.append({
            'type': 'current_state',
            'timestamp': state.timestamp.isoformat(),
            'metrics': {
                'cpu_usage': state.cpu_usage,
                'memory_usage': state.memory_usage,
                'error_rate': state.error_rate,
                'anomaly_score': anomaly_score
            }
        })
        
        # å±¥æ­´ãƒˆãƒ¬ãƒ³ãƒ‰
        if len(self.system_states) >= 10:
            recent_states = list(self.system_states)[-10:]
            evidence.append({
                'type': 'trend_analysis',
                'cpu_trend': [s.cpu_usage for s in recent_states],
                'memory_trend': [s.memory_usage for s in recent_states],
                'error_trend': [s.error_rate for s in recent_states]
            })
        
        # é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³
        similar_patterns = self._find_similar_patterns(state)
        if similar_patterns:
            evidence.append({
                'type': 'similar_patterns',
                'count': len(similar_patterns),
                'patterns': [p.pattern_id for p in similar_patterns[:3]]
            })
        
        return evidence
    
    def _capture_current_metrics(self) -> Dict[str, float]:
        """ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        if self.current_state:
            return {
                'cpu_usage': self.current_state.cpu_usage,
                'memory_usage': self.current_state.memory_usage,
                'error_rate': self.current_state.error_rate,
                'response_time': self.current_state.response_time
            }
        return {}
    
    def _evaluate_prevention_effect(self,
                                  metrics_before: Dict[str, float],
                                  metrics_after: Dict[str, float]) -> bool:
        """äºˆé˜²åŠ¹æœè©•ä¾¡"""
        if not metrics_before or not metrics_after:
            return False
        
        # æ”¹å–„åˆ¤å®š
        improvements = 0
        
        if metrics_after.get('cpu_usage', 100) < metrics_before.get('cpu_usage', 0):
            improvements += 1
        if metrics_after.get('memory_usage', 100) < metrics_before.get('memory_usage', 0):
            improvements += 1
        if metrics_after.get('error_rate', 1) < metrics_before.get('error_rate', 0):
            improvements += 1
        if metrics_after.get('response_time', 1000) < metrics_before.get('response_time', 0):
            improvements += 1
        
        return improvements >= 2
    
    def _extract_lessons(self,
                       prediction: IncidentPrediction,
                       actions_taken: List[PreventionAction],
                       incident_prevented: bool) -> List[str]:
        """å­¦ç¿’äº‹é …æŠ½å‡º"""
        lessons = []
        
        if incident_prevented:
            lessons.append(f"{prediction.incident_type.value}ã®äºˆé˜²ã«æˆåŠŸ")
            for action in actions_taken:
                lessons.append(f"{action.value}ãŒåŠ¹æœçš„ã ã£ãŸ")
        else:
            lessons.append(f"{prediction.incident_type.value}ã®äºˆé˜²ã«å¤±æ•—")
            lessons.append("ã‚ˆã‚Šæ—©æœŸã®ä»‹å…¥ãŒå¿…è¦")
        
        if prediction.confidence == PredictionConfidence.VERY_HIGH and incident_prevented:
            lessons.append("é«˜ä¿¡é ¼åº¦äºˆæ¸¬ã®æ­£ç¢ºæ€§ã‚’ç¢ºèª")
        
        return lessons
    
    async def _check_actual_occurrence(self, prediction: IncidentPrediction) -> Optional[bool]:
        """å®Ÿéš›ã®ç™ºç”Ÿç¢ºèª"""
        # äºˆæ¸¬æ™‚é–“ã‚’éãã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if datetime.now() < prediction.expected_time:
            return None  # ã¾ã åˆ¤å®šã§ããªã„
        
        # ç°¡æ˜“çš„ãªåˆ¤å®šï¼ˆå®Ÿéš›ã¯ãƒ­ã‚°ã‚„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰åˆ¤å®šï¼‰
        # ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼å®Ÿè£…
        return False  # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã¯ç™ºç”Ÿã—ãªã‹ã£ãŸ
    
    def _extract_features_from_prediction(self, prediction: IncidentPrediction) -> np.ndarray:
        """äºˆæ¸¬ã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡º"""
        # ãƒ€ãƒŸãƒ¼å®Ÿè£…
        return np.random.random(22)  # ç‰¹å¾´é‡æ¬¡å…ƒæ•°
    
    def _detect_anomaly_transition(self, window_states: List[SystemStateVector]) -> bool:
        """ç•°å¸¸é·ç§»æ¤œå‡º"""
        if len(window_states) < 2:
            return False
        
        # æ€¥æ¿€ãªå¤‰åŒ–ã‚’æ¤œå‡º
        for i in range(1, len(window_states)):
            prev_state = window_states[i-1]
            curr_state = window_states[i]
            
            cpu_change = abs(curr_state.cpu_usage - prev_state.cpu_usage)
            memory_change = abs(curr_state.memory_usage - prev_state.memory_usage)
            
            if cpu_change > 30 or memory_change > 30:
                return True
        
        return False
    
    def _infer_incident_type(self, window_states: List[SystemStateVector]) -> IncidentType:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—æ¨è«–"""
        # æœ€å¾Œã®çŠ¶æ…‹ã‹ã‚‰æ¨è«–
        if window_states:
            last_state = window_states[-1]
            
            if last_state.cpu_usage > 90 or last_state.memory_usage > 90:
                return IncidentType.RESOURCE_EXHAUSTION
            elif last_state.error_rate > 0.1:
                return IncidentType.SERVICE_UNAVAILABLE
            elif last_state.response_time > 1000:
                return IncidentType.PERFORMANCE_DEGRADATION
        
        return IncidentType.SYSTEM_CRASH
    
    def _extract_precursor_signals(self, window_states: List[SystemStateVector]) -> List[Dict[str, Any]]:
        """å‰å…†ã‚·ã‚°ãƒŠãƒ«æŠ½å‡º"""
        signals = []
        
        for i, state in enumerate(window_states):
            if state.cpu_usage > 70:
                signals.append({
                    'step': i,
                    'signal': 'high_cpu',
                    'value': state.cpu_usage
                })
            if state.error_rate > 0.05:
                signals.append({
                    'step': i,
                    'signal': 'elevated_errors',
                    'value': state.error_rate
                })
        
        return signals
    
    def _create_pattern_signature(self, window_states: List[SystemStateVector]) -> np.ndarray:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚·ã‚°ãƒãƒãƒ£ä½œæˆ"""
        if not window_states:
            return np.zeros(16)
        
        # å„çŠ¶æ…‹ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å¹³å‡åŒ–
        vectors = [state.to_vector() for state in window_states]
        return np.mean(vectors, axis=0)
    
    def _find_similar_patterns(self, state: SystemStateVector) -> List[IncidentPattern]:
        """é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢"""
        similar_patterns = []
        state_vector = state.to_vector()
        
        for pattern in self.incident_patterns.values():
            if pattern.vector_signature is not None:
                similarity = np.dot(state_vector, pattern.vector_signature) / (
                    np.linalg.norm(state_vector) * np.linalg.norm(pattern.vector_signature)
                )
                
                if similarity > 0.8:
                    similar_patterns.append(pattern)
        
        return similar_patterns
    
    def _load_patterns(self):
        """æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³èª­ã¿è¾¼ã¿"""
        try:
            conn = sqlite3.connect(self.config['database_path'])
            cursor = conn.cursor()
            
            cursor.execute("SELECT * FROM incident_patterns ORDER BY updated_at DESC LIMIT 100")
            rows = cursor.fetchall()
            
            for row in rows:
                pattern = IncidentPattern(
                    pattern_id=row[0],
                    pattern_type=row[1],
                    incident_type=IncidentType(row[2]),
                    precursor_signals=json.loads(row[3]),
                    time_to_incident=timedelta(seconds=row[4]),
                    severity_score=row[5],
                    occurrence_count=row[6],
                    prevention_success_rate=row[7],
                    vector_signature=None  # ç°¡ç•¥åŒ–
                )
                self.incident_patterns[pattern.pattern_id] = pattern
            
            conn.close()
            
            self.logger.info(f"ğŸ“š Loaded {len(self.incident_patterns)} incident patterns")
            
        except Exception as e:
            self.logger.error(f"Pattern loading failed: {e}")
    
    async def _persist_prediction(self, prediction: IncidentPrediction):
        """äºˆæ¸¬æ°¸ç¶šåŒ–"""
        try:
            conn = sqlite3.connect(self.config['database_path'])
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO incident_predictions 
                (prediction_id, predicted_at, incident_type, probability, expected_time,
                 confidence, impact_assessment, prevention_actions, reasoning, evidence)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                prediction.prediction_id,
                prediction.predicted_at.isoformat(),
                prediction.incident_type.value,
                prediction.probability,
                prediction.expected_time.isoformat(),
                prediction.confidence.value,
                json.dumps(prediction.impact_assessment),
                json.dumps([a.value for a in prediction.prevention_actions]),
                prediction.reasoning,
                json.dumps(prediction.evidence)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Prediction persistence failed: {e}")
    
    async def _persist_prevention_result(self, result: PreventionResult):
        """äºˆé˜²çµæœæ°¸ç¶šåŒ–"""
        try:
            conn = sqlite3.connect(self.config['database_path'])
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO prevention_results 
                (result_id, prediction_id, executed_at, actions_taken, success,
                 incident_prevented, metrics_before, metrics_after, lessons_learned)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                result.result_id,
                result.prediction_id,
                result.executed_at.isoformat(),
                json.dumps([a.value for a in result.actions_taken]),
                result.success,
                result.incident_prevented,
                json.dumps(result.metrics_before),
                json.dumps(result.metrics_after),
                json.dumps(result.lessons_learned)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Prevention result persistence failed: {e}")
    
    async def get_prevention_stats(self) -> Dict[str, Any]:
        """äºˆé˜²çµ±è¨ˆå–å¾—"""
        accuracy = self.stats['prediction_accuracy']
        prevention_rate = (
            self.stats['incidents_prevented'] / self.stats['total_predictions']
            if self.stats['total_predictions'] > 0 else 0
        )
        
        return {
            'total_predictions': self.stats['total_predictions'],
            'accurate_predictions': self.stats['accurate_predictions'],
            'incidents_prevented': self.stats['incidents_prevented'],
            'false_positives': self.stats['false_positives'],
            'prevention_actions_executed': self.stats['prevention_actions_executed'],
            'prediction_accuracy': accuracy,
            'prevention_success_rate': prevention_rate,
            'pattern_count': len(self.incident_patterns),
            'model_trained': self.incident_predictor.is_trained,
            'anomaly_detector_trained': self.anomaly_detector.is_trained
        }


class PreventionActionExecutor:
    """äºˆé˜²ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œå™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.action_history = deque(maxlen=100)
    
    async def execute(self, 
                    action: PreventionAction,
                    prediction: IncidentPrediction) -> bool:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        try:
            self.logger.info(f"ğŸ”§ Executing prevention action: {action.value}")
            
            if action == PreventionAction.SCALE_RESOURCES:
                return await self._scale_resources(prediction)
            elif action == PreventionAction.RESTART_SERVICE:
                return await self._restart_service(prediction)
            elif action == PreventionAction.CLEAR_CACHE:
                return await self._clear_cache(prediction)
            elif action == PreventionAction.APPLY_PATCH:
                return await self._apply_patch(prediction)
            elif action == PreventionAction.ISOLATE_COMPONENT:
                return await self._isolate_component(prediction)
            elif action == PreventionAction.REDIRECT_TRAFFIC:
                return await self._redirect_traffic(prediction)
            elif action == PreventionAction.EMERGENCY_BACKUP:
                return await self._emergency_backup(prediction)
            elif action == PreventionAction.ALERT_TEAM:
                return await self._alert_team(prediction)
            else:
                self.logger.warning(f"Unknown action: {action.value}")
                return False
                
        except Exception as e:
            self.logger.error(f"Action execution failed: {e}")
            return False
    
    async def _scale_resources(self, prediction: IncidentPrediction) -> bool:
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""
        self.logger.info("ğŸ“ˆ Scaling resources...")
        # å®Ÿéš›ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿè£…
        await asyncio.sleep(2)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        return True
    
    async def _restart_service(self, prediction: IncidentPrediction) -> bool:
        """ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•"""
        self.logger.info("â™»ï¸ Restarting service...")
        # å®Ÿéš›ã®å†èµ·å‹•å®Ÿè£…
        await asyncio.sleep(3)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        return True
    
    async def _clear_cache(self, prediction: IncidentPrediction) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        self.logger.info("ğŸ§¹ Clearing cache...")
        # å®Ÿéš›ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Ÿè£…
        await asyncio.sleep(1)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        return True
    
    async def _apply_patch(self, prediction: IncidentPrediction) -> bool:
        """ãƒ‘ãƒƒãƒé©ç”¨"""
        self.logger.info("ğŸ”§ Applying security patch...")
        # å®Ÿéš›ã®ãƒ‘ãƒƒãƒé©ç”¨å®Ÿè£…
        await asyncio.sleep(5)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        return True
    
    async def _isolate_component(self, prediction: IncidentPrediction) -> bool:
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆéš”é›¢"""
        self.logger.info("ğŸ”’ Isolating component...")
        # å®Ÿéš›ã®éš”é›¢å®Ÿè£…
        await asyncio.sleep(2)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        return True
    
    async def _redirect_traffic(self, prediction: IncidentPrediction) -> bool:
        """ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ"""
        self.logger.info("â†ªï¸ Redirecting traffic...")
        # å®Ÿéš›ã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå®Ÿè£…
        await asyncio.sleep(2)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        return True
    
    async def _emergency_backup(self, prediction: IncidentPrediction) -> bool:
        """ç·Šæ€¥ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        self.logger.info("ğŸ’¾ Creating emergency backup...")
        # å®Ÿéš›ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè£…
        await asyncio.sleep(10)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        return True
    
    async def _alert_team(self, prediction: IncidentPrediction) -> bool:
        """ãƒãƒ¼ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆ"""
        self.logger.info("ğŸ“¢ Alerting team...")
        # å®Ÿéš›ã®ã‚¢ãƒ©ãƒ¼ãƒˆå®Ÿè£…
        alert_message = f"""
        ğŸš¨ INCIDENT PREDICTION ALERT ğŸš¨
        
        Type: {prediction.incident_type.value}
        Probability: {prediction.probability:.2f}
        Expected Time: {prediction.expected_time}
        Confidence: {prediction.confidence.value}
        
        Impact Assessment: {prediction.impact_assessment}
        
        Reasoning: {prediction.reasoning}
        
        Prevention Actions Being Executed:
        {[a.value for a in prediction.prevention_actions]}
        """
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡ï¼ˆå®Ÿè£…çœç•¥ï¼‰
        self.logger.warning(alert_message)
        
        return True


# ä½¿ç”¨ä¾‹
async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        prevention_system = PredictiveIncidentPreventionSystem()
        
        print("ğŸ”® Starting Predictive Incident Prevention System...")
        
        # ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç”Ÿæˆ
        print("\nğŸ“Š Simulating system states...")
        
        # æ­£å¸¸çŠ¶æ…‹
        for i in range(20):
            state = SystemStateVector(
                timestamp=datetime.now(),
                cpu_usage=50 + np.random.normal(0, 5),
                memory_usage=60 + np.random.normal(0, 5),
                disk_io=100 + np.random.normal(0, 10),
                network_traffic=200 + np.random.normal(0, 20),
                error_rate=0.01 + np.random.normal(0, 0.005),
                response_time=100 + np.random.normal(0, 10),
                active_connections=500 + int(np.random.normal(0, 50)),
                queue_depth=10 + int(np.random.normal(0, 5)),
                custom_metrics={'cache_hit_rate': 0.9, 'db_connections': 50}
            )
            await prevention_system.analyze_system_state(state)
            await asyncio.sleep(0.1)
        
        # ç•°å¸¸çŠ¶æ…‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        print("\nâš ï¸ Simulating anomaly buildup...")
        
        for i in range(10):
            # CPUã¨ãƒ¡ãƒ¢ãƒªãŒå¾ã€…ã«ä¸Šæ˜‡
            state = SystemStateVector(
                timestamp=datetime.now(),
                cpu_usage=70 + i * 3,  # 70% â†’ 97%
                memory_usage=75 + i * 2,  # 75% â†’ 95%
                disk_io=200 + i * 10,
                network_traffic=300 + i * 20,
                error_rate=0.02 + i * 0.01,  # ã‚¨ãƒ©ãƒ¼ç‡ã‚‚ä¸Šæ˜‡
                response_time=200 + i * 50,  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ æ‚ªåŒ–
                active_connections=800 + i * 50,
                queue_depth=50 + i * 10,
                custom_metrics={'cache_hit_rate': 0.7 - i * 0.05, 'db_connections': 100 + i * 10}
            )
            await prevention_system.analyze_system_state(state)
            await asyncio.sleep(0.2)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’å®Ÿè¡Œ
        print("\nğŸ§  Learning patterns...")
        await prevention_system._learn_patterns()
        
        # çµ±è¨ˆç¢ºèª
        print("\nğŸ“ˆ Prevention Statistics:")
        stats = await prevention_system.get_prevention_stats()
        
        print(f"  Total Predictions: {stats['total_predictions']}")
        print(f"  Accurate Predictions: {stats['accurate_predictions']}")
        print(f"  Incidents Prevented: {stats['incidents_prevented']}")
        print(f"  False Positives: {stats['false_positives']}")
        print(f"  Prevention Actions Executed: {stats['prevention_actions_executed']}")
        print(f"  Prediction Accuracy: {stats['prediction_accuracy']:.2%}")
        print(f"  Prevention Success Rate: {stats['prevention_success_rate']:.2%}")
        print(f"  Pattern Count: {stats['pattern_count']}")
        print(f"  Models Trained: Predictor={stats['model_trained']}, Anomaly={stats['anomaly_detector_trained']}")
        
        print("\nğŸ‰ Predictive Incident Prevention System Phase 2 demonstration completed!")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())