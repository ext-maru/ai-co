#!/usr/bin/env python3
"""
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–“ä¾å­˜é–¢ä¿‚ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆDAGãƒ™ãƒ¼ã‚¹ï¼‰
ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«åŸºã¥ãå®Ÿè£…
"""

import asyncio
import json
from collections import defaultdict, deque
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
from dataclasses import dataclass, field
from enum import Enum
import logging

# import networkx as nx  # Optional for advanced graph analysis
# import matplotlib.pyplot as plt  # Optional for visualization
from concurrent.futures import ThreadPoolExecutor, as_completed

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
PROJECT_ROOT = Path(__file__).parent.parent
import sys

sys.path.insert(0, str(PROJECT_ROOT))


class DependencyType(Enum):
    """ä¾å­˜é–¢ä¿‚ã‚¿ã‚¤ãƒ—"""

    BUILD = "build"  # ãƒ“ãƒ«ãƒ‰æ™‚ä¾å­˜
    RUNTIME = "runtime"  # å®Ÿè¡Œæ™‚ä¾å­˜
    TEST = "test"  # ãƒ†ã‚¹ãƒˆæ™‚ä¾å­˜
    OPTIONAL = "optional"  # ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ä¾å­˜


@dataclass
class ProjectNode:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒ¼ãƒ‰"""

    id: str
    name: str
    path: Path
    type: str = "standard"  # standard, library, service, tool
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Dependency:
    """ä¾å­˜é–¢ä¿‚"""

    from_project: str
    to_project: str
    type: DependencyType
    version: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class CircularDependencyError(Exception):
    """å¾ªç’°ä¾å­˜ã‚¨ãƒ©ãƒ¼"""

    def __init__(self, cycle: List[str]):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.cycle = cycle
        super().__init__(f"å¾ªç’°ä¾å­˜ã‚’æ¤œå‡º: {' -> '.join(cycle)} -> {cycle[0]}")


class ProjectDependencyGraph:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¾å­˜é–¢ä¿‚ã‚°ãƒ©ãƒ•ï¼ˆã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ä»•æ§˜ï¼‰"""

    def __init__(self):
        """åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰"""
        # ã‚°ãƒ©ãƒ•æ§‹é€ 
        self.nodes: Dict[str, ProjectNode] = {}
        self.graph: Dict[str, Set[str]] = defaultdict(set)
        self.reverse_graph: Dict[str, Set[str]] = defaultdict(set)
        self.dependencies: Dict[Tuple[str, str], Dependency] = {}

        # ãƒ­ã‚°è¨­å®šï¼ˆå…ˆã«è¨­å®šï¼‰
        self.logger = logging.getLogger("ProjectDependencyGraph")
        self.logger.setLevel(logging.INFO)

        # NetworkXã‚°ãƒ©ãƒ•ï¼ˆé«˜åº¦ãªåˆ†æç”¨ï¼‰
        self.nx_graph = None
        self.networkx_available = False
        self.logger.warning("NetworkX not available. Using built-in graph analysis.")

        # ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹è¨­å®š
        self.config = {
            "max_parallel_projects": 10,  # æœ€å¤§ä¸¦åˆ—å®Ÿè¡Œæ•°
            "cycle_detection": True,  # å¾ªç’°ä¾å­˜æ¤œå‡º
            "auto_resolve_versions": True,  # ãƒãƒ¼ã‚¸ãƒ§ãƒ³è‡ªå‹•è§£æ±º
            "cache_analysis": True,  # åˆ†æçµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
            "visualization": False,  # å¯è¦–åŒ–æ©Ÿèƒ½ï¼ˆmatplotlibãªã—ã§ç„¡åŠ¹åŒ–ï¼‰
            "ai_optimization": True,  # AIæœ€é©åŒ–é€£æº
        }

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._execution_order_cache = None
        self._critical_path_cache = None

        # 4è³¢è€…ã¨ã®é€£æºæº–å‚™
        self.sage_insights = {
            "task_sage": [],  # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œé †åºã®çŸ¥è¦‹
            "knowledge_sage": [],  # ä¾å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çŸ¥è­˜
            "incident_sage": [],  # ä¾å­˜é–¢ä¿‚èµ·å› ã®ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ
            "rag_sage": [],  # ä»–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³
        }

    def add_project(self, project: ProjectNode):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒ¼ãƒ‰è¿½åŠ """
        self.nodes[project.id] = project
        if self.networkx_available:
            self.nx_graph.add_node(project.id, **project.__dict__)
        self._invalidate_cache()

        self.logger.info(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¿½åŠ : {project.name} ({project.id})")

    def add_dependency(self, dependency: Dependency) -> bool:
        """ä¾å­˜é–¢ä¿‚è¿½åŠ ï¼ˆã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã®å“è³ªãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        from_id = dependency.from_project
        to_id = dependency.to_project

        # è‡ªå·±ä¾å­˜ãƒã‚§ãƒƒã‚¯
        if from_id == to_id:
            self.logger.warning(f"è‡ªå·±ä¾å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—: {from_id}")
            return False

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå­˜åœ¨ç¢ºèª
        if from_id not in self.nodes or to_id not in self.nodes:
            self.logger.error(f"å­˜åœ¨ã—ãªã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {from_id} -> {to_id}")
            return False

        # å¾ªç’°ä¾å­˜ãƒã‚§ãƒƒã‚¯ï¼ˆäº‹å‰ï¼‰
        if self.config["cycle_detection"]:
            if self._would_create_cycle(from_id, to_id):
                cycle = self._find_cycle_path(from_id, to_id)
                raise CircularDependencyError(cycle)

        # ä¾å­˜é–¢ä¿‚ç™»éŒ²
        self.graph[from_id].add(to_id)
        self.reverse_graph[to_id].add(from_id)
        self.dependencies[(from_id, to_id)] = dependency
        if self.networkx_available:
            self.nx_graph.add_edge(from_id, to_id, **dependency.__dict__)

        self._invalidate_cache()

        self.logger.info(
            f"ä¾å­˜é–¢ä¿‚è¿½åŠ : {from_id} -> {to_id} (type: {dependency.type.value})"
        )
        return True

    def _would_create_cycle(self, from_id: str, to_id: str) -> bool:
        """å¾ªç’°ä¾å­˜ã‚’ä½œæˆã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        # DFSã§åˆ°é”å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        visited = set()
        stack = [to_id]

        while stack:
            current = stack.pop()
            if current == from_id:
                return True
            if current in visited:
                continue
            visited.add(current)
            stack.extend(self.graph.get(current, []))

        return False

    def _find_cycle_path(self, from_id: str, to_id: str) -> List[str]:
        """å¾ªç’°ãƒ‘ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹"""
        # ç°¡æ˜“å®Ÿè£…ï¼ˆè©³ç´°ãªçµŒè·¯æ¢ç´¢ï¼‰
        path = [from_id]
        current = to_id

        while current != from_id:
            path.append(current)
            # æ¬¡ã®ãƒãƒ¼ãƒ‰ã‚’æ¢ã™
            for next_node in self.graph.get(current, []):
                if next_node in path or next_node == from_id:
                    path.append(next_node)
                    return path
            current = (
                list(self.graph.get(current, []))[0]
                if self.graph.get(current)
                else current
            )

        return path

    def get_execution_order(self) -> List[List[str]]:
        """å®Ÿè¡Œé †åºã‚’å–å¾—ï¼ˆä¸¦åˆ—å®Ÿè¡Œå¯èƒ½ãªãƒ¬ãƒ™ãƒ«ã”ã¨ï¼‰"""
        if self._execution_order_cache is not None:
            return self._execution_order_cache

        # ã‚«ãƒ¼ãƒ³ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆãƒ¬ãƒ™ãƒ«ä»˜ããƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆï¼‰
        in_degree = defaultdict(int)

        # å…¥æ¬¡æ•°è¨ˆç®—
        for node in self.nodes:
            for dependent in self.graph.get(node, []):
                in_degree[dependent] += 1

        # å…¥æ¬¡æ•°0ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰é–‹å§‹
        current_level = [node for node in self.nodes if in_degree[node] == 0]
        levels = []
        processed = set()

        while current_level:
            levels.append(current_level[:])
            next_level = []

            for node in current_level:
                processed.add(node)
                for dependent in self.graph.get(node, []):
                    in_degree[dependent] -= 1
                    if in_degree[dependent] == 0:
                        next_level.append(dependent)

            current_level = next_level

        # å…¨ãƒãƒ¼ãƒ‰ãŒå‡¦ç†ã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
        if len(processed) != len(self.nodes):
            unprocessed = set(self.nodes.keys()) - processed
            self.logger.error(f"å¾ªç’°ä¾å­˜ã®å¯èƒ½æ€§: {unprocessed}")

        self._execution_order_cache = levels
        return levels

    def get_parallel_groups(self) -> List[Dict[str, Any]]:
        """ä¸¦åˆ—å®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—ã‚’è©³ç´°æƒ…å ±ä»˜ãã§å–å¾—"""
        levels = self.get_execution_order()
        groups = []

        for i, level in enumerate(levels):
            group = {
                "level": i + 1,
                "projects": level,
                "can_parallel": True,
                "estimated_time": self._estimate_level_time(level),
                "dependencies_from_previous": self._get_dependencies_from_previous(
                    level, i
                ),
            }
            groups.append(group)

        return groups

    def _estimate_level_time(self, level: List[str]) -> float:
        """ãƒ¬ãƒ™ãƒ«ã®æ¨å®šå®Ÿè¡Œæ™‚é–“ï¼ˆä¸¦åˆ—å®Ÿè¡Œã‚’è€ƒæ…®ï¼‰"""
        if not level:
            return 0.0

        # å„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ¨å®šæ™‚é–“ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ï¼‰
        times = []
        for project_id in level:
            project = self.nodes.get(project_id)
            if project and "estimated_minutes" in project.metadata:
                times.append(project.metadata["estimated_minutes"])
            else:
                times.append(10.0)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10åˆ†

        # ä¸¦åˆ—å®Ÿè¡Œæ™‚ã¯æœ€å¤§æ™‚é–“
        return max(times)

    def _get_dependencies_from_previous(
        self, level: List[str], level_index: int
    ) -> Dict[str, List[str]]:
        """å‰ãƒ¬ãƒ™ãƒ«ã‹ã‚‰ã®ä¾å­˜é–¢ä¿‚"""
        deps = {}
        if level_index == 0:
            return deps

        for project_id in level:
            deps[project_id] = list(self.reverse_graph.get(project_id, []))

        return deps

    def find_critical_path(self) -> List[str]:
        """ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹"""
        if self._critical_path_cache is not None:
            return self._critical_path_cache

        if (
            not self.networkx_available
            or not self.nx_graph
            or not self.nx_graph.nodes()
        ):
            # NetworkXãªã—ã§ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            return self._calculate_critical_path_simple()

        # é‡ã¿ä»˜ã‘ã‚’è¿½åŠ ï¼ˆæ¨å®šæ™‚é–“ï¼‰
        for node in self.nx_graph.nodes():
            project = self.nodes.get(node)
            weight = project.metadata.get("estimated_minutes", 10) if project else 10
            self.nx_graph.nodes[node]["weight"] = weight

        # DAGã®æœ€é•·ãƒ‘ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹
        try:
            import networkx as nx

            critical_path = nx.dag_longest_path(self.nx_graph, weight="weight")
            self._critical_path_cache = critical_path
            return critical_path
        except Exception as e:
            self.logger.error(f"NetworkX critical path calculation failed: {e}")
            return self._calculate_critical_path_simple()

    def analyze_impact(self, project_id: str) -> Dict[str, Any]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå¤‰æ›´ã®å½±éŸ¿åˆ†æ"""
        if project_id not in self.nodes:
            return {"error": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“"}

        # ç›´æ¥å½±éŸ¿ã‚’å—ã‘ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
        direct_impact = list(self.reverse_graph.get(project_id, []))

        # é–“æ¥çš„ãªå½±éŸ¿ï¼ˆæ¨ç§»çš„é–‰åŒ…ï¼‰
        all_impacted = set()
        queue = deque(direct_impact)

        while queue:
            current = queue.popleft()
            if current not in all_impacted:
                all_impacted.add(current)
                queue.extend(self.reverse_graph.get(current, []))

        return {
            "project": project_id,
            "direct_impact": direct_impact,
            "total_impacted": list(all_impacted),
            "impact_count": len(all_impacted),
            "critical_path_member": project_id in self.find_critical_path(),
            "rebuild_required": list(all_impacted),
        }

    def optimize_with_ai(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """AIæœ€é©åŒ–ã¨ã®é€£æºï¼ˆã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼‰"""
        optimization_result = {
            "original_levels": self.get_execution_order(),
            "optimized_levels": [],
            "improvements": [],
            "sage_recommendations": {},
        }

        # ã‚¿ã‚¹ã‚¯è³¢è€…ã®çŸ¥è¦‹ã‚’æ´»ç”¨
        task_sage_insight = {
            "parallel_efficiency": self._calculate_parallel_efficiency(),
            "bottlenecks": self._identify_bottlenecks(),
            "optimization_potential": self._calculate_optimization_potential(),
        }

        optimization_result["sage_recommendations"]["task_sage"] = task_sage_insight

        # ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ã®ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
        knowledge_sage_insight = {
            "common_patterns": self._identify_dependency_patterns(),
            "anti_patterns": self._detect_anti_patterns(),
            "best_practices": self._suggest_best_practices(),
        }

        optimization_result["sage_recommendations"][
            "knowledge_sage"
        ] = knowledge_sage_insight

        # æœ€é©åŒ–ææ¡ˆ
        if task_sage_insight["optimization_potential"] > 0.2:
            optimization_result["improvements"].append(
                {
                    "type": "parallel_optimization",
                    "description": "ä¸¦åˆ—å®Ÿè¡Œã®æœ€é©åŒ–ã«ã‚ˆã‚Š20%ä»¥ä¸Šã®æ”¹å–„ãŒè¦‹è¾¼ã‚ã¾ã™",
                    "action": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ†å‰²ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
                }
            )

        return optimization_result

    def _calculate_parallel_efficiency(self) -> float:
        """ä¸¦åˆ—å®Ÿè¡ŒåŠ¹ç‡ã®è¨ˆç®—"""
        levels = self.get_execution_order()
        if not levels:
            return 0.0

        total_projects = len(self.nodes)
        level_count = len(levels)

        # ç†æƒ³çš„ãªä¸¦åˆ—å®Ÿè¡Œã¨ã®æ¯”è¼ƒ
        ideal_levels = max(1, total_projects // self.config["max_parallel_projects"])
        efficiency = ideal_levels / level_count if level_count > 0 else 0

        return min(1.0, efficiency)

    def _identify_bottlenecks(self) -> List[str]:
        """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç‰¹å®š"""
        bottlenecks = []

        # å¤šãã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒä¾å­˜ã—ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰
        for node in self.nodes:
            dependent_count = len(self.reverse_graph.get(node, []))
            if dependent_count >= 3:  # 3ã¤ä»¥ä¸Šã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒä¾å­˜
                bottlenecks.append(node)

        return bottlenecks

    def _calculate_optimization_potential(self) -> float:
        """æœ€é©åŒ–å¯èƒ½æ€§ã®è¨ˆç®—"""
        current_efficiency = self._calculate_parallel_efficiency()

        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®å½±éŸ¿ã‚’è¨ˆç®—
        bottlenecks = self._identify_bottlenecks()
        bottleneck_impact = len(bottlenecks) / len(self.nodes) if self.nodes else 0

        # æœ€é©åŒ–å¯èƒ½æ€§ = 1 - ç¾åœ¨ã®åŠ¹ç‡ - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®å½±éŸ¿
        potential = max(0, 1 - current_efficiency - bottleneck_impact)

        return potential

    def _identify_dependency_patterns(self) -> List[Dict[str, Any]]:
        """ä¾å­˜é–¢ä¿‚ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è­˜åˆ¥"""
        patterns = []

        # ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ‘ã‚¿ãƒ¼ãƒ³
        if self._has_layered_pattern():
            patterns.append(
                {
                    "type": "layered_architecture",
                    "description": "ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º",
                    "quality": "good",
                }
            )

        # ã‚¹ã‚¿ãƒ¼å‹ä¾å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        star_centers = self._find_star_patterns()
        if star_centers:
            patterns.append(
                {
                    "type": "star_dependency",
                    "description": f"ã‚¹ã‚¿ãƒ¼å‹ä¾å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º: {star_centers}",
                    "quality": "warning",
                    "recommendation": "å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®åˆ†å‰²ã‚’æ¤œè¨",
                }
            )

        return patterns

    def _detect_anti_patterns(self) -> List[Dict[str, Any]]:
        """ã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        anti_patterns = []

        # ç›¸äº’ä¾å­˜ã®æ¤œå‡º
        mutual_deps = self._find_mutual_dependencies()
        if mutual_deps:
            anti_patterns.append(
                {
                    "type": "mutual_dependency",
                    "description": f"ç›¸äº’ä¾å­˜ã‚’æ¤œå‡º: {mutual_deps}",
                    "severity": "high",
                    "recommendation": "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹åˆ†é›¢ã‚’æ¤œè¨",
                }
            )

        # æ·±ã„ä¾å­˜éšå±¤
        max_depth = self._calculate_max_dependency_depth()
        if max_depth > 5:
            anti_patterns.append(
                {
                    "type": "deep_hierarchy",
                    "description": f"ä¾å­˜éšå±¤ãŒæ·±ã™ãã¾ã™ï¼ˆ{max_depth}å±¤ï¼‰",
                    "severity": "medium",
                    "recommendation": "ä¸­é–“å±¤ã®çµ±åˆã‚’æ¤œè¨",
                }
            )

        return anti_patterns

    def _suggest_best_practices(self) -> List[str]:
        """ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã®ææ¡ˆ"""
        suggestions = []

        # ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
        suggestions.append("ğŸ›ï¸ å„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ç‹¬ç«‹ã—ã¦ãƒ†ã‚¹ãƒˆå¯èƒ½ã«ã™ã‚‹")
        suggestions.append("ğŸ”„ å¾ªç’°ä¾å­˜ã¯çµ¶å¯¾ã«é¿ã‘ã‚‹")
        suggestions.append("ğŸ“¦ å…±é€šæ©Ÿèƒ½ã¯ç‹¬ç«‹ã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ã—ã¦åˆ†é›¢")
        suggestions.append("ğŸ¯ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹ã®æœ€å°åŒ–ã‚’æ„è­˜")
        suggestions.append("âš¡ ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½ãªæ§‹é€ ã‚’ç¶­æŒ")

        return suggestions

    def _has_layered_pattern(self) -> bool:
        """ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        levels = self.get_execution_order()
        # å„ãƒ¬ãƒ™ãƒ«ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ã‚’ãƒã‚§ãƒƒã‚¯
        # ç°¡æ˜“çš„ãªå®Ÿè£…
        return len(levels) >= 3

    def _find_star_patterns(self) -> List[str]:
        """ã‚¹ã‚¿ãƒ¼å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¸­å¿ƒã‚’è¦‹ã¤ã‘ã‚‹"""
        star_centers = []
        threshold = 5  # 5ã¤ä»¥ä¸Šã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ä¾å­˜ã•ã‚Œã¦ã„ã‚‹

        for node in self.nodes:
            if len(self.reverse_graph.get(node, [])) >= threshold:
                star_centers.append(node)

        return star_centers

    def _find_mutual_dependencies(self) -> List[Tuple[str, str]]:
        """ç›¸äº’ä¾å­˜ã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆæœ¬æ¥ã¯é¿ã‘ã‚‹ã¹ãï¼‰"""
        mutual = []
        checked = set()

        for node in self.nodes:
            for dependent in self.graph.get(node, []):
                if (dependent, node) not in checked and node in self.graph.get(
                    dependent, []
                ):
                    mutual.append((node, dependent))
                    checked.add((node, dependent))
                    checked.add((dependent, node))

        return mutual

    def _calculate_max_dependency_depth(self) -> int:
        """æœ€å¤§ä¾å­˜æ·±åº¦ã®è¨ˆç®—"""
        if not self.nodes:
            return 0

        levels = self.get_execution_order()
        return len(levels)

    def _calculate_critical_path_simple(self) -> List[str]:
        """NetworkXãªã—ã®ç°¡æ˜“ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹è¨ˆç®—"""
        if not self.nodes:
            return []

        # å®Ÿè¡Œé †åºãƒ¬ãƒ™ãƒ«ã‹ã‚‰æœ€é•·ãƒ‘ã‚¹ã‚’æ¨å®š
        levels = self.get_execution_order()
        if not levels:
            return []

        # å„ãƒ¬ãƒ™ãƒ«ã‹ã‚‰æœ€ã‚‚é‡ã„ãƒãƒ¼ãƒ‰ã‚’é¸æŠ
        critical_path = []
        for level in levels:
            if level:
                # æ¨å®šæ™‚é–“ã§æœ€ã‚‚é‡ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é¸æŠ
                heaviest_project = max(
                    level,
                    key=lambda p: self.nodes.get(
                        p,
                        type("obj", (object,), {"metadata": {"estimated_minutes": 10}}),
                    ).metadata.get("estimated_minutes", 10),
                )
                critical_path.append(heaviest_project)

        return critical_path

    def visualize(
        self, output_path: Optional[Path] = None, layout: str = "hierarchical"
    ):
        """ä¾å­˜é–¢ä¿‚ã‚°ãƒ©ãƒ•ã®å¯è¦–åŒ–ï¼ˆã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ä»•æ§˜ï¼‰"""
        if not self.config["visualization"]:
            return

        try:
            import matplotlib.pyplot as plt
        except ImportError:
            self.logger.warning("matplotlib not installed. Skipping visualization.")
            return

        plt.figure(figsize=(12, 8))

        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆé¸æŠ
        if layout == "hierarchical":
            # éšå±¤çš„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆãƒ¬ãƒ™ãƒ«ã”ã¨ï¼‰
            levels = self.get_execution_order()
            pos = {}
            y_offset = 0

            for level_idx, level in enumerate(levels):
                x_offset = -(len(level) - 1) / 2
                for proj_idx, project in enumerate(level):
                    pos[project] = (x_offset + proj_idx, -y_offset)
                y_offset += 1
        else:
            # è‡ªå‹•ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
            import networkx as nx

            pos = nx.spring_layout(self.nx_graph)

        # ãƒãƒ¼ãƒ‰ã®è‰²åˆ†ã‘
        node_colors = []
        for node in self.nx_graph.nodes():
            project = self.nodes.get(node)
            if project:
                if project.type == "library":
                    node_colors.append("lightblue")
                elif project.type == "service":
                    node_colors.append("lightgreen")
                elif project.type == "tool":
                    node_colors.append("lightyellow")
                else:
                    node_colors.append("lightgray")
            else:
                node_colors.append("white")

        # ã‚°ãƒ©ãƒ•æç”»
        import networkx as nx

        nx.draw(
            self.nx_graph,
            pos,
            node_color=node_colors,
            node_size=3000,
            font_size=10,
            font_weight="bold",
            arrows=True,
            edge_color="gray",
            arrowsize=20,
            with_labels=True,
        )

        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹ã‚’å¼·èª¿
        critical_path = self.find_critical_path()
        if len(critical_path) > 1:
            critical_edges = [
                (critical_path[i], critical_path[i + 1])
                for i in range(len(critical_path) - 1)
            ]
            nx.draw_networkx_edges(
                self.nx_graph,
                pos,
                edgelist=critical_edges,
                edge_color="red",
                width=3,
                arrows=True,
                arrowsize=25,
            )

        plt.title(
            "ğŸ›ï¸ ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¾å­˜é–¢ä¿‚ã‚°ãƒ©ãƒ•",
            fontsize=16,
            fontweight="bold",
        )

        # å‡¡ä¾‹
        try:
            from matplotlib.patches import Patch
        except ImportError:
            self.logger.warning("matplotlib.patches not available. Skipping legend.")
            if output_path:
                plt.savefig(output_path, dpi=300, bbox_inches="tight")
                self.logger.info(f"ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {output_path}")
            else:
                plt.show()
            plt.close()
            return
        legend_elements = [
            Patch(facecolor="lightblue", label="ãƒ©ã‚¤ãƒ–ãƒ©ãƒª"),
            Patch(facecolor="lightgreen", label="ã‚µãƒ¼ãƒ“ã‚¹"),
            Patch(facecolor="lightyellow", label="ãƒ„ãƒ¼ãƒ«"),
            Patch(facecolor="lightgray", label="æ¨™æº–"),
            Patch(facecolor="red", label="ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹"),
        ]
        plt.legend(handles=legend_elements, loc="upper right")

        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches="tight")
            self.logger.info(f"ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {output_path}")
        else:
            plt.show()

        plt.close()

    def export_mermaid(self) -> str:
        """Mermaidå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        mermaid = ["graph TD"]

        # ã‚¹ã‚¿ã‚¤ãƒ«å®šç¾©
        mermaid.append(
            "    classDef library fill:#add8e6,stroke:#333,stroke-width:2px;"
        )
        mermaid.append(
            "    classDef service fill:#90ee90,stroke:#333,stroke-width:2px;"
        )
        mermaid.append("    classDef tool fill:#ffffe0,stroke:#333,stroke-width:2px;")
        mermaid.append(
            "    classDef critical fill:#ff6b6b,stroke:#333,stroke-width:3px;"
        )

        # ãƒãƒ¼ãƒ‰å®šç¾©
        for node_id, node in self.nodes.items():
            label = f"{node.name}"
            mermaid.append(f"    {node_id}[{label}]")

            # ã‚¯ãƒ©ã‚¹é©ç”¨
            if node.type == "library":
                mermaid.append(f"    class {node_id} library")
            elif node.type == "service":
                mermaid.append(f"    class {node_id} service")
            elif node.type == "tool":
                mermaid.append(f"    class {node_id} tool")

        # ã‚¨ãƒƒã‚¸å®šç¾©
        for (from_id, to_id), dep in self.dependencies.items():
            arrow = "-->"
            if dep.type == DependencyType.OPTIONAL:
                arrow = "-.->"
            label = f"|{dep.type.value}|" if dep.type != DependencyType.BUILD else ""
            mermaid.append(f"    {from_id} {arrow}{label} {to_id}")

        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹å¼·èª¿
        critical_path = self.find_critical_path()
        for node in critical_path:
            mermaid.append(f"    class {node} critical")

        return "\n".join(mermaid)

    def _invalidate_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–"""
        self._execution_order_cache = None
        self._critical_path_cache = None

    def save_state(self, file_path: Path):
        """çŠ¶æ…‹ã‚’ä¿å­˜"""
        state = {
            "nodes": {k: v.__dict__ for k, v in self.nodes.items()},
            "dependencies": [
                {
                    "from": dep.from_project,
                    "to": dep.to_project,
                    "type": dep.type.value,
                    "version": dep.version,
                    "metadata": dep.metadata,
                }
                for dep in self.dependencies.values()
            ],
            "sage_insights": self.sage_insights,
            "timestamp": datetime.now().isoformat(),
        }

        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(state, f, indent=2, ensure_ascii=False, default=str)

    def load_state(self, file_path: Path):
        """çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿"""
        with open(file_path, "r", encoding="utf-8") as f:
            state = json.load(f)

        # ãƒãƒ¼ãƒ‰å¾©å…ƒ
        for node_id, node_data in state["nodes"].items():
            node = ProjectNode(
                id=node_data["id"],
                name=node_data["name"],
                path=Path(node_data["path"]),
                type=node_data.get("type", "standard"),
                metadata=node_data.get("metadata", {}),
            )
            self.add_project(node)

        # ä¾å­˜é–¢ä¿‚å¾©å…ƒ
        for dep_data in state["dependencies"]:
            dep = Dependency(
                from_project=dep_data["from"],
                to_project=dep_data["to"],
                type=DependencyType(dep_data["type"]),
                version=dep_data.get("version"),
                metadata=dep_data.get("metadata", {}),
            )
            self.add_dependency(dep)

        # è³¢è€…ã®çŸ¥è¦‹å¾©å…ƒ
        self.sage_insights = state.get("sage_insights", self.sage_insights)


# ãƒ‡ãƒ¢å®Ÿè¡Œ
def demo():
    """ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    graph = ProjectDependencyGraph()

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®šç¾©
    projects = [
        ProjectNode(
            "frontend",
            "ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰",
            Path("frontend"),
            "service",
            {"estimated_minutes": 30},
        ),
        ProjectNode("api", "API", Path("api"), "service", {"estimated_minutes": 20}),
        ProjectNode(
            "database",
            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
            Path("database"),
            "service",
            {"estimated_minutes": 10},
        ),
        ProjectNode(
            "auth-lib",
            "èªè¨¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒª",
            Path("libs/auth"),
            "library",
            {"estimated_minutes": 15},
        ),
        ProjectNode(
            "monitoring",
            "ç›£è¦–ãƒ„ãƒ¼ãƒ«",
            Path("monitoring"),
            "tool",
            {"estimated_minutes": 25},
        ),
        ProjectNode(
            "common-lib",
            "å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒª",
            Path("libs/common"),
            "library",
            {"estimated_minutes": 5},
        ),
    ]

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¿½åŠ 
    for project in projects:
        graph.add_project(project)

    # ä¾å­˜é–¢ä¿‚å®šç¾©
    dependencies = [
        Dependency("frontend", "api", DependencyType.RUNTIME),
        Dependency("frontend", "auth-lib", DependencyType.BUILD),
        Dependency("api", "database", DependencyType.RUNTIME),
        Dependency("api", "auth-lib", DependencyType.BUILD),
        Dependency("api", "common-lib", DependencyType.BUILD),
        Dependency("auth-lib", "common-lib", DependencyType.BUILD),
        Dependency("monitoring", "api", DependencyType.RUNTIME, version="optional"),
    ]

    # ä¾å­˜é–¢ä¿‚è¿½åŠ 
    for dep in dependencies:
        try:
            graph.add_dependency(dep)
        except CircularDependencyError as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

    # å®Ÿè¡Œé †åºã®å–å¾—
    print("ğŸ›ï¸ ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¾å­˜é–¢ä¿‚åˆ†æ")
    print("=" * 60)

    print("\nğŸ“Š ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½ã‚°ãƒ«ãƒ¼ãƒ—:")
    groups = graph.get_parallel_groups()
    for group in groups:
        print(f"\nLevel {group['level']}: {group['projects']}")
        print(f"  æ¨å®šæ™‚é–“: {group['estimated_time']}åˆ†ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰")
        if group["dependencies_from_previous"]:
            print(f"  ä¾å­˜å…ƒ: {group['dependencies_from_previous']}")

    # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹
    critical_path = graph.find_critical_path()
    print(f"\nğŸ¯ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹: {' -> '.join(critical_path)}")

    # å½±éŸ¿åˆ†æ
    print("\nğŸ“ˆ å½±éŸ¿åˆ†æï¼ˆcommon-libã‚’å¤‰æ›´ã—ãŸå ´åˆï¼‰:")
    impact = graph.analyze_impact("common-lib")
    print(f"  ç›´æ¥å½±éŸ¿: {impact['direct_impact']}")
    print(f"  å…¨å½±éŸ¿ç¯„å›²: {impact['total_impacted']}")
    print(f"  å†ãƒ“ãƒ«ãƒ‰å¿…è¦: {impact['rebuild_required']}")

    # AIæœ€é©åŒ–ææ¡ˆ
    print("\nğŸ¤– AIæœ€é©åŒ–ææ¡ˆ:")
    optimization = graph.optimize_with_ai({})
    print(
        f"  ä¸¦åˆ—å®Ÿè¡ŒåŠ¹ç‡: {optimization['sage_recommendations']['task_sage']['parallel_efficiency']:.1%}"
    )
    print(
        f"  ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: {optimization['sage_recommendations']['task_sage']['bottlenecks']}"
    )

    # Mermaidå‡ºåŠ›
    print("\nğŸ“ Mermaidå›³:")
    print(graph.export_mermaid())

    # å¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # graph.visualize(Path("dependency_graph.png"))


if __name__ == "__main__":
    demo()
