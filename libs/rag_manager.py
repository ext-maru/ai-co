#!/usr/bin/env python3
"""
ğŸ” RAG Manager - ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰çŸ¥è­˜æ¢ç´¢ã‚·ã‚¹ãƒ†ãƒ 
RAGè³¢è€… (Search Mystic) ã®å®Œå…¨å®Ÿè£…

æ©Ÿèƒ½:
- æƒ…å ±æ¢ç´¢ã¨ç†è§£
- è†¨å¤§ãªçŸ¥è­˜ã‹ã‚‰æœ€é©è§£ç™ºè¦‹
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã€çŸ¥è­˜çµ±åˆã€å›ç­”ç”Ÿæˆ
- 4è³¢è€…é€£æºã«ã‚ˆã‚‹è‡ªå¾‹å­¦ç¿’

ä½œæˆè€…: ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼
ä½œæˆæ—¥: 2025-07-19
"""

import hashlib
import json
import logging
import os
import re
import sqlite3
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """æ¤œç´¢çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    content: str
    source: str
    relevance_score: float
    timestamp: datetime
    metadata: Dict[str, Any]


@dataclass
class KnowledgeItem:
    """çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    id: str
    content: str
    source: str
    category: str
    tags: List[str]
    created_at: datetime
    updated_at: datetime
    access_count: int


class RagManager:
    """
    ğŸ” RAGè³¢è€… (Search Mystic) - å®Œå…¨å®Ÿè£…

    ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã®çŸ¥è­˜æ¢ç´¢ã‚·ã‚¹ãƒ†ãƒ 
    è†¨å¤§ãªçŸ¥è­˜ã‹ã‚‰æœ€é©è§£ã‚’ç™ºè¦‹ã™ã‚‹
    """

    def __init__(
        self, knowledge_base_path: str = "/home/aicompany/ai_co/knowledge_base"
    ):
        """RAG Managerã‚’åˆæœŸåŒ–"""
        self.knowledge_base_path = Path(knowledge_base_path)
        self.db_path = self.knowledge_base_path / "rag_knowledge.db"
        self.cache_path = self.knowledge_base_path / "search_cache.json"

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.knowledge_base_path.mkdir(exist_ok=True)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self._init_database()

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–
        self.search_cache = self._load_cache()

        logger.info("ğŸ” RAG Manager (Search Mystic) åˆæœŸåŒ–å®Œäº†")

    def _init_database(self):
        """çŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ ãƒ†ãƒ¼ãƒ–ãƒ«
                cursor.execute(
                    """
                    CREATE TABLE IF NOT EXISTS knowledge_items (
                        id TEXT PRIMARY KEY,
                        content TEXT NOT NULL,
                        source TEXT NOT NULL,
                        category TEXT NOT NULL,
                        tags TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        access_count INTEGER DEFAULT 0
                    )
                """
                )

                # æ¤œç´¢å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
                cursor.execute(
                    """
                    CREATE TABLE IF NOT EXISTS search_history (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        query TEXT NOT NULL,
                        results_count INTEGER DEFAULT 0,
                        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        execution_time REAL DEFAULT 0.0
                    )
                """
                )

                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_content ON knowledge_items(content)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_category ON knowledge_items(category)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_source ON knowledge_items(source)"
                )

                conn.commit()
                logger.info("ğŸ“Š RAGçŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†")

        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _load_cache(self) -> Dict[str, Any]:
        """æ¤œç´¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            if self.cache_path.exists():
                with open(self.cache_path, "r", encoding="utf-8") as f:
                    cache = json.load(f)
                logger.info(f"ğŸ’¾ æ¤œç´¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ­ãƒ¼ãƒ‰å®Œäº† ({len(cache)} ã‚¨ãƒ³ãƒˆãƒª)")
                return cache
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")

        return {}

    def _save_cache(self):
        """æ¤œç´¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
        try:
            with open(self.cache_path, "w", encoding="utf-8") as f:
                json.dump(
                    self.search_cache, f, ensure_ascii=False, indent=2, default=str
                )
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def add_knowledge(
        self, content: str, source: str, category: str, tags: List[str] = None
    ) -> str:
        """çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿½åŠ """
        try:
            # IDã‚’ç”Ÿæˆ
            knowledge_id = hashlib.md5(f"{content}{source}".encode()).hexdigest()

            # ã‚¿ã‚°ã‚’JSONå½¢å¼ã§ä¿å­˜
            tags_json = json.dumps(tags or [])

            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    INSERT OR REPLACE INTO knowledge_items
                    (id, content, source, category, tags, updated_at, access_count)
                    VALUES (?, ?, ?, ?, ?, CURRENT_TIMESTAMP, 0)
                """,
                    (knowledge_id, content, source, category, tags_json),
                )
                conn.commit()

            logger.info(f"ğŸ“š çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ è¿½åŠ : {knowledge_id} ({category})")
            return knowledge_id

        except Exception as e:
            logger.error(f"âŒ çŸ¥è­˜è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def search_knowledge(
        self, query: str, category: str = None, limit: int = 10
    ) -> List[SearchResult]:
        """çŸ¥è­˜ã‚’æ¤œç´¢"""
        start_time = time.time()

        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
            cache_key = f"{query}_{category}_{limit}"
            if cache_key in self.search_cache:
                cached_result = self.search_cache[cache_key]
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æœŸé™ãƒã‚§ãƒƒã‚¯ï¼ˆ1æ™‚é–“ï¼‰
                cache_time = datetime.fromisoformat(cached_result["timestamp"])
                if datetime.now() - cache_time < timedelta(hours=1):
                    logger.info(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ¤œç´¢çµæœå–å¾—: {query}")
                    return [SearchResult(**item) for item in cached_result["results"]]

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
            results = self._search_database(query, category, limit)

            # æ¤œç´¢å±¥æ­´ã‚’è¨˜éŒ²
            execution_time = time.time() - start_time
            self._record_search_history(query, len(results), execution_time)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.search_cache[cache_key] = {
                "timestamp": datetime.now().isoformat(),
                "results": [
                    {
                        "content": r.content,
                        "source": r.source,
                        "relevance_score": r.relevance_score,
                        "timestamp": r.timestamp.isoformat(),
                        "metadata": r.metadata,
                    }
                    for r in results
                ],
            }
            self._save_cache()

            logger.info(f"ğŸ” æ¤œç´¢å®Œäº†: '{query}' -> {len(results)}ä»¶ ({execution_time:.3f}s)")
            return results

        except Exception as e:
            logger.error(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _search_database(
        self, query: str, category: str, limit: int
    ) -> List[SearchResult]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # åŸºæœ¬æ¤œç´¢ã‚¯ã‚¨ãƒª
                base_query = """
                    SELECT content, source, category, tags, created_at, updated_at, access_count
                    FROM knowledge_items
                    WHERE content LIKE ?
                """
                params = [f"%{query}%"]

                # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿
                if category:
                    base_query += " AND category = ?"
                    params.append(category)

                base_query += " ORDER BY access_count DESC, updated_at DESC LIMIT ?"
                params.append(limit)

                cursor.execute(base_query, params)
                rows = cursor.fetchall()

                # SearchResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                results = []
                for row in rows:
                    (
                        content,
                        source,
                        cat,
                        tags_json,
                        created_at,
                        updated_at,
                        access_count,
                    ) = row

                    # é–¢é€£æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    relevance_score = self._calculate_relevance(
                        query, content, access_count
                    )

                    # ã‚¿ã‚°ã‚’ãƒ‘ãƒ¼ã‚¹
                    try:
                        tags = json.loads(tags_json)
                    except:
                        tags = []

                    result = SearchResult(
                        content=content,
                        source=source,
                        relevance_score=relevance_score,
                        timestamp=datetime.fromisoformat(updated_at),
                        metadata={
                            "category": cat,
                            "tags": tags,
                            "access_count": access_count,
                            "created_at": created_at,
                        },
                    )
                    results.append(result)

                    # ã‚¢ã‚¯ã‚»ã‚¹å›æ•°ã‚’å¢—åŠ 
                    cursor.execute(
                        "UPDATE knowledge_items SET access_count = access_count + 1 WHERE content = ? AND source = ?",
                        (content, source),
                    )

                conn.commit()
                return results

        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _calculate_relevance(
        self, query: str, content: str, access_count: int
    ) -> float:
        """é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        try:
            # å˜èªä¸€è‡´åº¦
            query_words = set(query.lower().split())
            content_words = set(content.lower().split())
            word_match = len(query_words & content_words) / max(len(query_words), 1)

            # ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ãƒœãƒ¼ãƒŠã‚¹
            access_bonus = min(access_count / 100, 0.3)

            # æœ€çµ‚ã‚¹ã‚³ã‚¢
            score = word_match + access_bonus
            return min(score, 1.0)

        except Exception:
            return 0.1

    def _record_search_history(
        self, query: str, results_count: int, execution_time: float
    ):
        """æ¤œç´¢å±¥æ­´ã‚’è¨˜éŒ²"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    INSERT INTO search_history (query, results_count, execution_time)
                    VALUES (?, ?, ?)
                """,
                    (query, results_count, execution_time),
                )
                conn.commit()
        except Exception as e:
            logger.warning(f"âš ï¸ æ¤œç´¢å±¥æ­´è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    def index_knowledge_base(self) -> int:
        """knowledge_baseãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰çŸ¥è­˜ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
        try:
            indexed_count = 0

            # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            for md_file in self.knowledge_base_path.glob("**/*.md"):
                try:
                    with open(md_file, "r", encoding="utf-8") as f:
                        content = f.read()

                    # ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®š
                    category = self._infer_category(md_file.name, content)

                    # ã‚¿ã‚°ã‚’æŠ½å‡º
                    tags = self._extract_tags(content)

                    # çŸ¥è­˜ã‚’è¿½åŠ 
                    self.add_knowledge(content, str(md_file), category, tags)
                    indexed_count += 1

                except Exception as e:
                    logger.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {md_file}: {e}")

            logger.info(f"ğŸ“š çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†: {indexed_count}ãƒ•ã‚¡ã‚¤ãƒ«")
            return indexed_count

        except Exception as e:
            logger.error(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    def _infer_category(self, filename: str, content: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã¨å†…å®¹ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®š"""
        filename_lower = filename.lower()
        content_lower = content.lower()

        # ã‚«ãƒ†ã‚´ãƒªæ¨å®šãƒ«ãƒ¼ãƒ«
        if "tdd" in filename_lower or "test" in filename_lower:
            return "testing"
        elif "elder" in filename_lower or "guild" in filename_lower:
            return "elders_guild"
        elif "guide" in filename_lower or "doc" in filename_lower:
            return "documentation"
        elif "api" in filename_lower or "service" in filename_lower:
            return "development"
        elif "protocol" in filename_lower or "process" in filename_lower:
            return "process"
        elif any(word in content_lower for word in ["error", "incident", "failure"]):
            return "incident_management"
        else:
            return "general"

    def _extract_tags(self, content: str) -> List[str]:
        """å†…å®¹ã‹ã‚‰ã‚¿ã‚°ã‚’æŠ½å‡º"""
        tags = set()

        # æŠ€è¡“ç”¨èªã‚’æ¤œç´¢
        tech_terms = [
            "python",
            "javascript",
            "docker",
            "git",
            "github",
            "tdd",
            "testing",
            "api",
            "database",
            "sql",
            "elder",
            "guild",
            "sage",
            "workflow",
        ]

        content_lower = content.lower()
        for term in tech_terms:
            if term in content_lower:
                tags.add(term)

        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ã‚¿ã‚°æŠ½å‡º
        headers = re.findall(r"#+\s+(.+)", content)
        for header in headers[:3]:  # æœ€åˆã®3å€‹ã®ãƒ˜ãƒƒãƒ€ãƒ¼
            words = re.findall(r"\w+", header.lower())
            tags.update([w for w in words if len(w) > 3])

        return list(tags)[:10]  # æœ€å¤§10å€‹ã®ã‚¿ã‚°

    def get_knowledge_stats(self) -> Dict[str, Any]:
        """çŸ¥è­˜ãƒ™ãƒ¼ã‚¹çµ±è¨ˆã‚’å–å¾—"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # ç·çŸ¥è­˜æ•°
                cursor.execute("SELECT COUNT(*) FROM knowledge_items")
                total_items = cursor.fetchone()[0]

                # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
                cursor.execute(
                    """
                    SELECT category, COUNT(*)
                    FROM knowledge_items
                    GROUP BY category
                    ORDER BY COUNT(*) DESC
                """
                )
                category_stats = dict(cursor.fetchall())

                # æ¤œç´¢çµ±è¨ˆ
                cursor.execute(
                    """
                    SELECT COUNT(*), AVG(execution_time), MAX(timestamp)
                    FROM search_history
                    WHERE timestamp > datetime('now', '-24 hours')
                """
                )
                search_stats = cursor.fetchone()

                return {
                    "total_knowledge_items": total_items,
                    "categories": category_stats,
                    "recent_searches": {
                        "count": search_stats[0] or 0,
                        "avg_time": search_stats[1] or 0,
                        "last_search": search_stats[2] or "N/A",
                    },
                    "cache_size": len(self.search_cache),
                }

        except Exception as e:
            logger.error(f"âŒ çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def consult_on_issue(self, issue_title: str, issue_body: str) -> Dict[str, Any]:
        """
        ã‚¤ã‚·ãƒ¥ãƒ¼ã«å¯¾ã—ã¦RAGè³¢è€…ã¨ã—ã¦ç›¸è«‡ã«å¿œç­”
        4è³¢è€…é€£æºã§å‘¼ã³å‡ºã•ã‚Œã‚‹ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰
        """
        try:
            logger.info(f"ğŸ§™â€â™‚ï¸ RAGè³¢è€…ç›¸è«‡é–‹å§‹: {issue_title}")

            # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
            search_query = f"{issue_title} {issue_body}"

            # é–¢é€£çŸ¥è­˜ã‚’æ¤œç´¢
            results = self.search_knowledge(search_query, limit=5)

            # æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’åˆ†æ
            recommendations = self._analyze_recommendations(
                issue_title, issue_body, results
            )

            # æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯åˆ†æ
            tech_stack = self._analyze_tech_stack(issue_body)

            # è¤‡é›‘åº¦è©•ä¾¡
            complexity = self._evaluate_complexity(issue_title, issue_body)

            consultation_result = {
                "status": "success",
                "issue_analysis": {
                    "title": issue_title,
                    "complexity": complexity,
                    "tech_stack": tech_stack,
                },
                "recommendations": recommendations,
                "related_knowledge": [
                    {
                        "content": r.content[:200] + "..."
                        if len(r.content) > 200
                        else r.content,
                        "source": r.source,
                        "relevance": r.relevance_score,
                    }
                    for r in results
                ],
                "consultation_metadata": {
                    "search_results_count": len(results),
                    "consultation_time": datetime.now().isoformat(),
                    "sage": "RAGè³¢è€… (Search Mystic)",
                },
            }

            logger.info(f"âœ… RAGè³¢è€…ç›¸è«‡å®Œäº†: {len(results)}ä»¶ã®é–¢é€£çŸ¥è­˜")
            return consultation_result

        except Exception as e:
            logger.error(f"âŒ RAGè³¢è€…ç›¸è«‡ã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "error", "error": str(e), "sage": "RAGè³¢è€… (Search Mystic)"}

    def _analyze_recommendations(
        self, title: str, body: str, search_results: List[SearchResult]
    ) -> List[str]:
        """æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’åˆ†æ"""
        recommendations = []

        # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡åˆ†æ
        text = f"{title} {body}".lower()

        # æŠ€è¡“åˆ¥æ¨å¥¨
        if "test" in text or "tdd" in text:
            recommendations.append("TDDï¼ˆãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™ºï¼‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¨å¥¨")

        if "api" in text or "endpoint" in text:
            recommendations.append("APIè¨­è¨ˆã¨OpenAPIä»•æ§˜æ›¸ä½œæˆã‚’æ¨å¥¨")

        if "database" in text or "sql" in text:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã¨ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç”»ã‚’æ¨å¥¨")

        if "ui" in text or "frontend" in text:
            recommendations.append("ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè¨­è¨ˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚¬ã‚¤ãƒ‰æº–æ‹ ã‚’æ¨å¥¨")

        # é–¢é€£çŸ¥è­˜ã‹ã‚‰ã®æ¨å¥¨
        for result in search_results:
            if result.relevance_score > 0.7:
                if "pattern" in result.content.lower():
                    recommendations.append("æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ´»ç”¨ã‚’æ¨å¥¨")
                if "error" in result.content.lower():
                    recommendations.append("ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ã‚’æ¨å¥¨")

        return recommendations[:5]  # æœ€å¤§5å€‹

    def _analyze_tech_stack(self, body: str) -> List[str]:
        """æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã‚’åˆ†æ"""
        tech_stack = []
        body_lower = body.lower()

        # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª
        languages = ["python", "javascript", "typescript", "java", "go", "rust"]
        for lang in languages:
            if lang in body_lower:
                tech_stack.append(lang.title())

        # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
        frameworks = [
            "react",
            "vue",
            "angular",
            "django",
            "flask",
            "fastapi",
            "express",
        ]
        for fw in frameworks:
            if fw in body_lower:
                tech_stack.append(fw.title())

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        databases = ["postgresql", "mysql", "mongodb", "redis", "sqlite"]
        for db in databases:
            if db in body_lower:
                tech_stack.append(db.upper())

        # ã‚¤ãƒ³ãƒ•ãƒ©
        infra = ["docker", "kubernetes", "aws", "gcp", "azure"]
        for inf in infra:
            if inf in body_lower:
                tech_stack.append(inf.upper())

        return tech_stack

    def _evaluate_complexity(self, title: str, body: str) -> str:
        """è¤‡é›‘åº¦ã‚’è©•ä¾¡"""
        text = f"{title} {body}".lower()
        complexity_score = 0

        # è¤‡é›‘åº¦æŒ‡æ¨™
        complexity_indicators = [
            ("integration", 2),
            ("api", 1),
            ("database", 1),
            ("authentication", 2),
            ("security", 2),
            ("performance", 1),
            ("scalability", 2),
            ("migration", 2),
            ("refactor", 1),
            ("architecture", 3),
            ("system", 1),
            ("multi", 2),
        ]

        for indicator, score in complexity_indicators:
            if indicator in text:
                complexity_score += score

        # é•·ã•ã«ã‚ˆã‚‹èª¿æ•´
        if len(body) > 500:
            complexity_score += 1
        if len(body) > 1000:
            complexity_score += 1

        # è¤‡é›‘åº¦åˆ†é¡
        if complexity_score <= 2:
            return "low"
        elif complexity_score <= 5:
            return "medium"
        else:
            return "high"


# äº’æ›æ€§é–¢æ•°
def setup(*args, **kwargs):
    """RAG Manager ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    logger.info("ğŸ” RAG Manager setupå®Ÿè¡Œ")
    manager = RagManager()
    manager.index_knowledge_base()
    return manager


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸ” RAG Manager ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–‹å§‹")

    manager = RagManager()

    # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    indexed = manager.index_knowledge_base()

    # çµ±è¨ˆè¡¨ç¤º
    stats = manager.get_knowledge_stats()
    logger.info(f"ğŸ“Š RAG Managerçµ±è¨ˆ: {stats}")

    logger.info("ğŸ RAG Manager ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œå®Œäº†")


# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
__all__ = ["RagManager", "SearchResult", "KnowledgeItem", "setup", "main"]


if __name__ == "__main__":
    main()
