#!/usr/bin/env python3
"""
ğŸš¨ Real-time Monitoring Enhancement System
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ 

Incident Sageã®å¡æ™ºã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ãƒ»äºˆæ¸¬çš„ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆé˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ 
pgvectorã‚’æ´»ç”¨ã—ãŸå¤šæ¬¡å…ƒç•°å¸¸æ¤œçŸ¥ã¨ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå•é¡Œè§£æ±º

Author: Claude Elder
Date: 2025-07-10
Phase: 1 (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–å¼·åŒ–)
"""

import asyncio
import json
import logging
import sqlite3
import threading
import time
import warnings
from collections import defaultdict, deque
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import aiohttp
import numpy as np
import websockets
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings("ignore")

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
PROJECT_ROOT = Path(__file__).parent.parent


class AnomalyType(Enum):
    """ç•°å¸¸ã‚¿ã‚¤ãƒ—å®šç¾©"""

    PERFORMANCE = "performance"  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç•°å¸¸
    RESOURCE = "resource"  # ãƒªã‚½ãƒ¼ã‚¹ç•°å¸¸
    ERROR_RATE = "error_rate"  # ã‚¨ãƒ©ãƒ¼ç‡ç•°å¸¸
    BEHAVIOR = "behavior"  # æŒ™å‹•ç•°å¸¸
    SECURITY = "security"  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç•°å¸¸
    AVAILABILITY = "availability"  # å¯ç”¨æ€§ç•°å¸¸


class SeverityLevel(Enum):
    """é‡è¦åº¦ãƒ¬ãƒ™ãƒ«"""

    INFO = "info"  # æƒ…å ±
    WARNING = "warning"  # è­¦å‘Š
    ERROR = "error"  # ã‚¨ãƒ©ãƒ¼
    CRITICAL = "critical"  # ç·Šæ€¥


class MonitoringTarget(Enum):
    """ç›£è¦–å¯¾è±¡"""

    SYSTEM = "system"  # ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“
    SERVICE = "service"  # å€‹åˆ¥ã‚µãƒ¼ãƒ“ã‚¹
    ENDPOINT = "endpoint"  # APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    DATABASE = "database"  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
    NETWORK = "network"  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    CUSTOM = "custom"  # ã‚«ã‚¹ã‚¿ãƒ 


@dataclass
class MetricPoint:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒã‚¤ãƒ³ãƒˆ"""

    timestamp: datetime
    target: MonitoringTarget
    metric_name: str
    value: float
    tags: Dict[str, str]
    metadata: Dict[str, Any]


@dataclass
class AnomalyEvent:
    """ç•°å¸¸ã‚¤ãƒ™ãƒ³ãƒˆ"""

    event_id: str
    detected_at: datetime
    anomaly_type: AnomalyType
    severity: SeverityLevel
    target: MonitoringTarget
    metric_name: str
    current_value: float
    expected_range: Tuple[float, float]
    anomaly_score: float
    context: Dict[str, Any]
    suggested_actions: List[str]
    auto_resolved: bool = False
    resolved_at: Optional[datetime] = None


@dataclass
class IncidentPrediction:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬"""

    prediction_id: str
    predicted_at: datetime
    incident_type: str
    probability: float
    expected_time: datetime
    impact_assessment: Dict[str, Any]
    prevention_actions: List[Dict[str, Any]]
    confidence: float


@dataclass
class SystemHealthReport:
    """ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ãƒ¬ãƒãƒ¼ãƒˆ"""

    timestamp: datetime
    overall_health: float
    component_health: Dict[str, float]
    active_anomalies: List[AnomalyEvent]
    predicted_incidents: List[IncidentPrediction]
    recommendations: List[str]


class RealtimeMonitoringEnhancement:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, config: Dict[str, Any] = None):
        self.logger = logging.getLogger(__name__)
        self.config = config or self._default_config()

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ãƒˆã‚¢
        self.metrics_buffer = defaultdict(lambda: deque(maxlen=1000))
        self.baseline_stats = {}

        # ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«
        self.isolation_forest = IsolationForest(
            n_estimators=100, contamination=0.1, random_state=42
        )
        self.scaler = StandardScaler()

        # ç•°å¸¸ã‚¤ãƒ™ãƒ³ãƒˆç®¡ç†
        self.active_anomalies = {}
        self.anomaly_history = deque(maxlen=10000)

        # äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
        self.incident_predictions = {}
        self.prediction_accuracy = defaultdict(float)

        # WebSocketã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç®¡ç†
        self.ws_clients = set()

        # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰
        self.monitoring_active = False
        self.monitoring_thread = None

        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "total_metrics_processed": 0,
            "total_anomalies_detected": 0,
            "total_incidents_prevented": 0,
            "false_positive_rate": 0.0,
            "mean_detection_time": 0.0,
        }

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self._init_database()

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å­¦ç¿’
        self._load_baseline_data()

        self.logger.info("ğŸš¨ Real-time Monitoring Enhancement System initialized")

    def _default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š"""
        return {
            "monitoring_interval": 5,  # ç§’
            "anomaly_threshold": 0.7,
            "prediction_horizon": 300,  # 5åˆ†å…ˆã¾ã§äºˆæ¸¬
            "baseline_window": 86400,  # 24æ™‚é–“
            "alert_cooldown": 300,  # 5åˆ†
            "auto_resolve_timeout": 1800,  # 30åˆ†
            "websocket_port": 8765,
            "database_path": str(PROJECT_ROOT / "data" / "monitoring.db"),
        }

    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        try:
            db_path = self.config["database_path"]
            Path(db_path).parent.mkdir(parents=True, exist_ok=True)

            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT,
                    target TEXT,
                    metric_name TEXT,
                    value REAL,
                    tags TEXT,
                    metadata TEXT
                );
            """
            )

            # ç•°å¸¸ã‚¤ãƒ™ãƒ³ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS anomaly_events (
                    event_id TEXT PRIMARY KEY,
                    detected_at TEXT,
                    anomaly_type TEXT,
                    severity TEXT,
                    target TEXT,
                    metric_name TEXT,
                    current_value REAL,
                    expected_min REAL,
                    expected_max REAL,
                    anomaly_score REAL,
                    context TEXT,
                    suggested_actions TEXT,
                    auto_resolved BOOLEAN,
                    resolved_at TEXT
                );
            """
            )

            # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS incident_predictions (
                    prediction_id TEXT PRIMARY KEY,
                    predicted_at TEXT,
                    incident_type TEXT,
                    probability REAL,
                    expected_time TEXT,
                    impact_assessment TEXT,
                    prevention_actions TEXT,
                    confidence REAL,
                    actual_occurred BOOLEAN,
                    accuracy_score REAL
                );
            """
            )

            # ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS health_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT,
                    overall_health REAL,
                    component_health TEXT,
                    active_anomalies INTEGER,
                    predicted_incidents INTEGER,
                    recommendations TEXT
                );
            """
            )

            conn.commit()
            conn.close()

            self.logger.info("ğŸ“Š Monitoring database initialized")

        except Exception as e:
            self.logger.error(f"Database initialization failed: {e}")

    async def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        if self.monitoring_active:
            self.logger.warning("Monitoring already active")
            return

        self.monitoring_active = True

        # ç›£è¦–ã‚¿ã‚¹ã‚¯é–‹å§‹
        tasks = [
            asyncio.create_task(self._metric_collection_loop()),
            asyncio.create_task(self._anomaly_detection_loop()),
            asyncio.create_task(self._incident_prediction_loop()),
            asyncio.create_task(self._auto_resolution_loop()),
            asyncio.create_task(self._websocket_server()),
        ]

        self.logger.info("ğŸš€ Real-time monitoring started")

        try:
            await asyncio.gather(*tasks)
        except Exception as e:
            self.logger.error(f"Monitoring error: {e}")
            self.monitoring_active = False

    async def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.monitoring_active = False
        self.logger.info("ğŸ›‘ Real-time monitoring stopped")

    async def ingest_metric(self, metric: MetricPoint):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–ã‚Šè¾¼ã¿"""
        try:
            # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
            key = f"{metric.target.value}:{metric.metric_name}"
            self.metrics_buffer[key].append(metric)

            # çµ±è¨ˆæ›´æ–°
            self.stats["total_metrics_processed"] += 1

            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç•°å¸¸æ¤œçŸ¥
            anomaly = await self._detect_anomaly(metric)
            if anomaly:
                await self._handle_anomaly(anomaly)

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ï¼ˆãƒãƒƒãƒå‡¦ç†ç”¨ï¼‰
            if self.stats["total_metrics_processed"] % 100 == 0:
                await self._persist_metrics()

        except Exception as e:
            self.logger.error(f"Metric ingestion failed: {e}")

    async def _metric_collection_loop(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
                system_metrics = await self._collect_system_metrics()
                for metric in system_metrics:
                    await self.ingest_metric(metric)

                # ã‚µãƒ¼ãƒ“ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
                service_metrics = await self._collect_service_metrics()
                for metric in service_metrics:
                    await self.ingest_metric(metric)

                await asyncio.sleep(self.config["monitoring_interval"])

            except Exception as e:
                self.logger.error(f"Metric collection error: {e}")
                await asyncio.sleep(10)

    async def _anomaly_detection_loop(self):
        """ç•°å¸¸æ¤œçŸ¥ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç•°å¸¸æ¤œçŸ¥
                for key, buffer in self.metrics_buffer.items():
                    if len(buffer) >= 10:  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
                        await self._analyze_metric_stream(key, buffer)

                # è¤‡åˆç•°å¸¸æ¤œçŸ¥
                await self._detect_complex_anomalies()

                await asyncio.sleep(self.config["monitoring_interval"] * 2)

            except Exception as e:
                self.logger.error(f"Anomaly detection error: {e}")
                await asyncio.sleep(10)

    async def _incident_prediction_loop(self):
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                # ç¾åœ¨ã®çŠ¶æ…‹åˆ†æ
                current_state = await self._analyze_current_state()

                # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬
                predictions = await self._predict_incidents(current_state)

                # äºˆæ¸¬çµæœå‡¦ç†
                for prediction in predictions:
                    await self._handle_prediction(prediction)

                await asyncio.sleep(60)  # 1åˆ†æ¯

            except Exception as e:
                self.logger.error(f"Incident prediction error: {e}")
                await asyncio.sleep(60)

    async def _auto_resolution_loop(self):
        """è‡ªå‹•è§£æ±ºãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªç•°å¸¸ãƒã‚§ãƒƒã‚¯
                for event_id, anomaly in list(self.active_anomalies.items()):
                    if await self._can_auto_resolve(anomaly):
                        await self._auto_resolve_anomaly(anomaly)

                await asyncio.sleep(30)  # 30ç§’æ¯

            except Exception as e:
                self.logger.error(f"Auto resolution error: {e}")
                await asyncio.sleep(30)

    async def _websocket_server(self):
        """WebSocketã‚µãƒ¼ãƒãƒ¼"""
        try:

            async def handle_client(websocket, path):
                self.ws_clients.add(websocket)
                try:
                    await websocket.wait_closed()
                finally:
                    self.ws_clients.remove(websocket)

            server = await websockets.serve(
                handle_client, "localhost", self.config["websocket_port"]
            )

            self.logger.info(
                f"ğŸ“¡ WebSocket server started on port {self.config['websocket_port']}"
            )
            await server.wait_closed()

        except Exception as e:
            self.logger.error(f"WebSocket server error: {e}")

    async def _collect_system_metrics(self) -> List[MetricPoint]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        metrics = []
        now = datetime.now()

        # CPUä½¿ç”¨ç‡ï¼ˆãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼‰
        metrics.append(
            MetricPoint(
                timestamp=now,
                target=MonitoringTarget.SYSTEM,
                metric_name="cpu_usage",
                value=np.random.normal(50, 10),
                tags={"host": "elders-guild-01"},
                metadata={"unit": "percent"},
            )
        )

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
        metrics.append(
            MetricPoint(
                timestamp=now,
                target=MonitoringTarget.SYSTEM,
                metric_name="memory_usage",
                value=np.random.normal(60, 15),
                tags={"host": "elders-guild-01"},
                metadata={"unit": "percent"},
            )
        )

        # ãƒ‡ã‚£ã‚¹ã‚¯I/O
        metrics.append(
            MetricPoint(
                timestamp=now,
                target=MonitoringTarget.SYSTEM,
                metric_name="disk_io",
                value=np.random.normal(100, 30),
                tags={"host": "elders-guild-01", "device": "sda"},
                metadata={"unit": "MB/s"},
            )
        )

        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯
        metrics.append(
            MetricPoint(
                timestamp=now,
                target=MonitoringTarget.NETWORK,
                metric_name="network_traffic",
                value=np.random.normal(200, 50),
                tags={"interface": "eth0"},
                metadata={"unit": "MB/s"},
            )
        )

        return metrics

    async def _collect_service_metrics(self) -> List[MetricPoint]:
        """ã‚µãƒ¼ãƒ“ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        metrics = []
        now = datetime.now()

        # APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ 
        metrics.append(
            MetricPoint(
                timestamp=now,
                target=MonitoringTarget.ENDPOINT,
                metric_name="response_time",
                value=np.random.normal(100, 20),
                tags={"endpoint": "/api/v1/search", "method": "GET"},
                metadata={"unit": "ms"},
            )
        )

        # ã‚¨ãƒ©ãƒ¼ç‡
        metrics.append(
            MetricPoint(
                timestamp=now,
                target=MonitoringTarget.SERVICE,
                metric_name="error_rate",
                value=np.random.normal(0.01, 0.005),
                tags={"service": "elder-api"},
                metadata={"unit": "ratio"},
            )
        )

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæ•°
        metrics.append(
            MetricPoint(
                timestamp=now,
                target=MonitoringTarget.DATABASE,
                metric_name="connection_count",
                value=np.random.normal(50, 10),
                tags={"database": "pgvector_db"},
                metadata={"unit": "connections"},
            )
        )

        return metrics

    async def _detect_anomaly(self, metric: MetricPoint) -> Optional[AnomalyEvent]:
        """å˜ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç•°å¸¸æ¤œçŸ¥"""
        key = f"{metric.target.value}:{metric.metric_name}"

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å–å¾—
        baseline = self.baseline_stats.get(key)
        if not baseline:
            return None

        # Zã‚¹ã‚³ã‚¢è¨ˆç®—
        z_score = (
            abs((metric.value - baseline["mean"]) / baseline["std"])
            if baseline["std"] > 0
            else 0
        )

        # ç•°å¸¸åˆ¤å®š
        if z_score > 3:  # 3Ïƒä»¥ä¸Š
            anomaly_score = min(1.0, z_score / 5)

            return AnomalyEvent(
                event_id=f"anomaly_{datetime.now(
                    ).strftime('%Y%m%d_%H%M%S')}_{key.replace(':',
                    '_'
                )}",
                detected_at=datetime.now(),
                anomaly_type=self._determine_anomaly_type(metric),
                severity=self._determine_severity(anomaly_score),
                target=metric.target,
                metric_name=metric.metric_name,
                current_value=metric.value,
                expected_range=(
                    baseline["mean"] - 3 * baseline["std"],
                    baseline["mean"] + 3 * baseline["std"],
                ),
                anomaly_score=anomaly_score,
                context={"tags": metric.tags, "metadata": metric.metadata},
                suggested_actions=await self._generate_suggested_actions(
                    metric, anomaly_score
                ),
            )

        return None

    async def _analyze_metric_stream(self, key: str, buffer: deque):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†æ"""
        if len(buffer) < 10:
            return

        # æœ€è¿‘ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
        recent_values = [m.value for m in list(buffer)[-30:]]

        # çµ±è¨ˆè¨ˆç®—
        mean = np.mean(recent_values)
        std = np.std(recent_values)

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ›´æ–°
        self.baseline_stats[key] = {
            "mean": mean,
            "std": std,
            "min": min(recent_values),
            "max": max(recent_values),
            "updated_at": datetime.now(),
        }

        # ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
        if len(recent_values) >= 20:
            trend = np.polyfit(range(len(recent_values)), recent_values, 1)[0]
            if abs(trend) > std * 0.1:  # æœ‰æ„ãªãƒˆãƒ¬ãƒ³ãƒ‰
                self.logger.info(f"ğŸ“ˆ Trend detected in {key}: {trend:.3f}")

    async def _detect_complex_anomalies(self):
        """è¤‡åˆç•°å¸¸æ¤œçŸ¥"""
        # è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç›¸é–¢åˆ†æ
        correlations = await self._analyze_metric_correlations()

        # ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        for pattern in correlations:
            if pattern["anomaly_score"] > self.config["anomaly_threshold"]:
                await self._handle_complex_anomaly(pattern)

    async def _analyze_metric_correlations(self) -> List[Dict[str, Any]]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›¸é–¢åˆ†æ"""
        patterns = []

        # CPU ã¨ ãƒ¡ãƒ¢ãƒªã®ç›¸é–¢ãƒã‚§ãƒƒã‚¯
        cpu_key = "system:cpu_usage"
        memory_key = "system:memory_usage"

        if cpu_key in self.metrics_buffer and memory_key in self.metrics_buffer:
            cpu_values = [m.value for m in list(self.metrics_buffer[cpu_key])[-20:]]
            memory_values = [
                m.value for m in list(self.metrics_buffer[memory_key])[-20:]
            ]

            if len(cpu_values) == len(memory_values) and len(cpu_values) >= 10:
                correlation = np.corrcoef(cpu_values, memory_values)[0, 1]

                # ç•°å¸¸ãªç›¸é–¢
                if abs(correlation) > 0.9:
                    patterns.append(
                        {
                            "type": "high_correlation",
                            "metrics": [cpu_key, memory_key],
                            "correlation": correlation,
                            "anomaly_score": abs(correlation),
                        }
                    )

        return patterns

    async def _predict_incidents(
        self, current_state: Dict[str, Any]
    ) -> List[IncidentPrediction]:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬"""
        predictions = []

        # ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡äºˆæ¸¬
        for key, buffer in self.metrics_buffer.items():
            if "usage" in key and len(buffer) >= 20:
                values = [m.value for m in list(buffer)[-20:]]

                # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
                x = np.arange(len(values))
                slope, intercept = np.polyfit(x, values, 1)

                # å°†æ¥å€¤äºˆæ¸¬
                future_steps = (
                    self.config["prediction_horizon"]
                    // self.config["monitoring_interval"]
                )
                predicted_value = slope * (len(values) + future_steps) + intercept

                # ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡ãƒªã‚¹ã‚¯
                if predicted_value > 90:  # 90%ä»¥ä¸Š
                    prediction = IncidentPrediction(
                        prediction_id=f"pred_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{key}",
                        predicted_at=datetime.now(),
                        incident_type="resource_exhaustion",
                        probability=min(1.0, predicted_value / 100),
                        expected_time=datetime.now()
                        + timedelta(seconds=self.config["prediction_horizon"]),
                        impact_assessment={
                            "affected_services": ["all"],
                            "severity": "high",
                            "estimated_downtime": 300,
                        },
                        prevention_actions=[
                            {
                                "action": "scale_resources",
                                "target": key,
                                "urgency": "high",
                            },
                            {
                                "action": "clear_cache",
                                "target": "system",
                                "urgency": "medium",
                            },
                        ],
                        confidence=0.7 + (0.3 * min(1.0, abs(slope) / 10)),
                    )
                    predictions.append(prediction)

        return predictions

    async def _analyze_current_state(self) -> Dict[str, Any]:
        """ç¾åœ¨çŠ¶æ…‹åˆ†æ"""
        state = {
            "timestamp": datetime.now(),
            "metrics_summary": {},
            "active_anomalies": len(self.active_anomalies),
            "system_load": 0.0,
        }

        # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚µãƒãƒªãƒ¼
        for key, buffer in self.metrics_buffer.items():
            if buffer:
                recent_values = [m.value for m in list(buffer)[-10:]]
                state["metrics_summary"][key] = {
                    "mean": np.mean(recent_values),
                    "std": np.std(recent_values),
                    "last": recent_values[-1],
                }

        # ã‚·ã‚¹ãƒ†ãƒ è² è·è¨ˆç®—
        cpu_key = "system:cpu_usage"
        if cpu_key in state["metrics_summary"]:
            state["system_load"] = state["metrics_summary"][cpu_key]["mean"] / 100

        return state

    async def _handle_anomaly(self, anomaly: AnomalyEvent):
        """ç•°å¸¸å‡¦ç†"""
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ç•°å¸¸ã«è¿½åŠ 
        self.active_anomalies[anomaly.event_id] = anomaly
        self.anomaly_history.append(anomaly)

        # çµ±è¨ˆæ›´æ–°
        self.stats["total_anomalies_detected"] += 1

        # ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
        await self._send_alert(anomaly)

        # è‡ªå‹•å¯¾å¿œå®Ÿè¡Œ
        if anomaly.severity in [SeverityLevel.ERROR, SeverityLevel.CRITICAL]:
            await self._execute_auto_response(anomaly)

        # WebSocketé€šçŸ¥
        await self._broadcast_anomaly(anomaly)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
        await self._persist_anomaly(anomaly)

        self.logger.warning(
            f"ğŸš¨ Anomaly detected: {anomaly.event_id} - {anomaly.anomaly_type.value}"
        )

    async def _handle_prediction(self, prediction: IncidentPrediction):
        """äºˆæ¸¬å‡¦ç†"""
        # äºˆæ¸¬è¨˜éŒ²
        self.incident_predictions[prediction.prediction_id] = prediction

        # é«˜ç¢ºç‡äºˆæ¸¬ã®å ´åˆã¯äºˆé˜²ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        if prediction.probability > 0.7 and prediction.confidence > 0.8:
            await self._execute_prevention_actions(prediction)
            self.stats["total_incidents_prevented"] += 1

        # é€šçŸ¥
        await self._send_prediction_alert(prediction)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
        await self._persist_prediction(prediction)

        self.logger.info(
            f"ğŸ”® Incident predicted: {prediction.incident_type} - Probability: {prediction.probability:.2f}"
        )

    async def _execute_auto_response(self, anomaly: AnomalyEvent):
        """è‡ªå‹•å¯¾å¿œå®Ÿè¡Œ"""
        for action in anomaly.suggested_actions:
            if "restart" in action.lower():
                self.logger.info(
                    f"â™»ï¸ Auto-response: Restarting service for {anomaly.metric_name}"
                )
                # å®Ÿéš›ã®ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•ã‚³ãƒ¼ãƒ‰
            elif "scale" in action.lower():
                self.logger.info(
                    f"ğŸ“ˆ Auto-response: Scaling resources for {anomaly.metric_name}"
                )
                # å®Ÿéš›ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚³ãƒ¼ãƒ‰
            elif "cache" in action.lower():
                self.logger.info(
                    f"ğŸ§¹ Auto-response: Clearing cache for {anomaly.metric_name}"
                )
                # å®Ÿéš›ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚³ãƒ¼ãƒ‰

    async def _execute_prevention_actions(self, prediction: IncidentPrediction):
        """äºˆé˜²ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        for action in prediction.prevention_actions:
            if action["urgency"] == "high":
                self.logger.info(
                    f"âš¡ Prevention: Executing {action['action']} on {action['target']}"
                )
                # å®Ÿéš›ã®äºˆé˜²ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ

    async def _can_auto_resolve(self, anomaly: AnomalyEvent) -> bool:
        """è‡ªå‹•è§£æ±ºå¯èƒ½åˆ¤å®š"""
        if anomaly.auto_resolved:
            return False

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
        elapsed = (datetime.now() - anomaly.detected_at).total_seconds()
        if elapsed > self.config["auto_resolve_timeout"]:
            return True

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ­£å¸¸åŒ–ãƒã‚§ãƒƒã‚¯
        key = f"{anomaly.target.value}:{anomaly.metric_name}"
        if key in self.metrics_buffer:
            recent_values = [m.value for m in list(self.metrics_buffer[key])[-5:]]
            if recent_values:
                current_value = recent_values[-1]
                if (
                    anomaly.expected_range[0]
                    <= current_value
                    <= anomaly.expected_range[1]
                ):
                    return True

        return False

    async def _auto_resolve_anomaly(self, anomaly: AnomalyEvent):
        """ç•°å¸¸è‡ªå‹•è§£æ±º"""
        anomaly.auto_resolved = True
        anomaly.resolved_at = datetime.now()

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ç•°å¸¸ã‹ã‚‰å‰Šé™¤
        if anomaly.event_id in self.active_anomalies:
            del self.active_anomalies[anomaly.event_id]

        # é€šçŸ¥
        await self._send_resolution_notification(anomaly)

        self.logger.info(f"âœ… Auto-resolved: {anomaly.event_id}")

    def _determine_anomaly_type(self, metric: MetricPoint) -> AnomalyType:
        """ç•°å¸¸ã‚¿ã‚¤ãƒ—åˆ¤å®š"""
        if "cpu" in metric.metric_name or "memory" in metric.metric_name:
            return AnomalyType.RESOURCE
        elif "response_time" in metric.metric_name:
            return AnomalyType.PERFORMANCE
        elif "error" in metric.metric_name:
            return AnomalyType.ERROR_RATE
        elif "security" in metric.metric_name:
            return AnomalyType.SECURITY
        else:
            return AnomalyType.BEHAVIOR

    def _determine_severity(self, anomaly_score: float) -> SeverityLevel:
        """é‡è¦åº¦åˆ¤å®š"""
        if anomaly_score >= 0.9:
            return SeverityLevel.CRITICAL
        elif anomaly_score >= 0.7:
            return SeverityLevel.ERROR
        elif anomaly_score >= 0.5:
            return SeverityLevel.WARNING
        else:
            return SeverityLevel.INFO

    async def _generate_suggested_actions(
        self, metric: MetricPoint, anomaly_score: float
    ) -> List[str]:
        """æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        actions = []

        if "cpu" in metric.metric_name and anomaly_score > 0.8:
            actions.extend(
                [
                    "Scale up compute resources",
                    "Identify and optimize CPU-intensive processes",
                    "Enable CPU throttling for non-critical services",
                ]
            )
        elif "memory" in metric.metric_name and anomaly_score > 0.8:
            actions.extend(
                [
                    "Increase memory allocation",
                    "Clear application caches",
                    "Restart memory-leaking services",
                ]
            )
        elif "error_rate" in metric.metric_name and anomaly_score > 0.7:
            actions.extend(
                [
                    "Review recent deployments",
                    "Check dependency services",
                    "Enable detailed error logging",
                ]
            )
        elif "response_time" in metric.metric_name and anomaly_score > 0.7:
            actions.extend(
                [
                    "Scale application instances",
                    "Optimize database queries",
                    "Enable caching layers",
                ]
            )

        return actions

    async def _send_alert(self, anomaly: AnomalyEvent):
        """ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        alert_message = {
            "type": "anomaly_alert",
            "event_id": anomaly.event_id,
            "severity": anomaly.severity.value,
            "message": f"{anomaly.anomaly_type.value} anomaly detected in {anomaly.metric_name}",
            "current_value": anomaly.current_value,
            "expected_range": anomaly.expected_range,
            "suggested_actions": anomaly.suggested_actions,
        }

        # WebSocketé€ä¿¡
        await self._broadcast_message(alert_message)

        # ãã®ä»–ã®é€šçŸ¥ãƒãƒ£ãƒãƒ«ï¼ˆãƒ¡ãƒ¼ãƒ«ã€Slackç­‰ï¼‰ã¸ã®é€ä¿¡ã‚‚ã“ã“ã§å®Ÿè£…

    async def _send_prediction_alert(self, prediction: IncidentPrediction):
        """äºˆæ¸¬ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        alert_message = {
            "type": "prediction_alert",
            "prediction_id": prediction.prediction_id,
            "incident_type": prediction.incident_type,
            "probability": prediction.probability,
            "expected_time": prediction.expected_time.isoformat(),
            "prevention_actions": prediction.prevention_actions,
        }

        await self._broadcast_message(alert_message)

    async def _send_resolution_notification(self, anomaly: AnomalyEvent):
        """è§£æ±ºé€šçŸ¥é€ä¿¡"""
        notification = {
            "type": "anomaly_resolved",
            "event_id": anomaly.event_id,
            "resolved_at": (
                anomaly.resolved_at.isoformat() if anomaly.resolved_at else None
            ),
            "auto_resolved": anomaly.auto_resolved,
        }

        await self._broadcast_message(notification)

    async def _broadcast_anomaly(self, anomaly: AnomalyEvent):
        """ç•°å¸¸æƒ…å ±ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ"""
        message = {
            "type": "anomaly_update",
            "anomaly": {
                "event_id": anomaly.event_id,
                "detected_at": anomaly.detected_at.isoformat(),
                "anomaly_type": anomaly.anomaly_type.value,
                "severity": anomaly.severity.value,
                "metric_name": anomaly.metric_name,
                "current_value": anomaly.current_value,
                "anomaly_score": anomaly.anomaly_score,
            },
        }

        await self._broadcast_message(message)

    async def _broadcast_message(self, message: Dict[str, Any]):
        """WebSocketãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ"""
        if self.ws_clients:
            message_json = json.dumps(message)
            await asyncio.gather(
                *[client.send(message_json) for client in self.ws_clients],
                return_exceptions=True,
            )

    async def _persist_metrics(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ°¸ç¶šåŒ–"""
        try:
            conn = sqlite3.connect(self.config["database_path"])
            cursor = conn.cursor()

            metrics_to_save = []
            for key, buffer in self.metrics_buffer.items():
                for metric in list(buffer)[-100:]:  # æœ€æ–°100ä»¶
                    metrics_to_save.append(
                        (
                            metric.timestamp.isoformat(),
                            metric.target.value,
                            metric.metric_name,
                            metric.value,
                            json.dumps(metric.tags),
                            json.dumps(metric.metadata),
                        )
                    )

            cursor.executemany(
                """
                INSERT INTO metrics (timestamp, target, metric_name, value, tags, metadata)
                VALUES (?, ?, ?, ?, ?, ?)
            """,
                metrics_to_save,
            )

            conn.commit()
            conn.close()

        except Exception as e:
            self.logger.error(f"Metrics persistence failed: {e}")

    async def _persist_anomaly(self, anomaly: AnomalyEvent):
        """ç•°å¸¸ã‚¤ãƒ™ãƒ³ãƒˆæ°¸ç¶šåŒ–"""
        try:
            conn = sqlite3.connect(self.config["database_path"])
            cursor = conn.cursor()

            cursor.execute(
                """
                INSERT OR REPLACE INTO anomaly_events
                (event_id, detected_at, anomaly_type, severity, target, metric_name,
                 current_value, expected_min, expected_max, anomaly_score, context,
                 suggested_actions, auto_resolved, resolved_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    anomaly.event_id,
                    anomaly.detected_at.isoformat(),
                    anomaly.anomaly_type.value,
                    anomaly.severity.value,
                    anomaly.target.value,
                    anomaly.metric_name,
                    anomaly.current_value,
                    anomaly.expected_range[0],
                    anomaly.expected_range[1],
                    anomaly.anomaly_score,
                    json.dumps(anomaly.context),
                    json.dumps(anomaly.suggested_actions),
                    anomaly.auto_resolved,
                    anomaly.resolved_at.isoformat() if anomaly.resolved_at else None,
                ),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            self.logger.error(f"Anomaly persistence failed: {e}")

    async def _persist_prediction(self, prediction: IncidentPrediction):
        """äºˆæ¸¬çµæœæ°¸ç¶šåŒ–"""
        try:
            conn = sqlite3.connect(self.config["database_path"])
            cursor = conn.cursor()

            cursor.execute(
                """
                INSERT INTO incident_predictions
                (prediction_id, predicted_at, incident_type, probability, expected_time,
                 impact_assessment, prevention_actions, confidence)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    prediction.prediction_id,
                    prediction.predicted_at.isoformat(),
                    prediction.incident_type,
                    prediction.probability,
                    prediction.expected_time.isoformat(),
                    json.dumps(prediction.impact_assessment),
                    json.dumps(prediction.prevention_actions),
                    prediction.confidence,
                ),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            self.logger.error(f"Prediction persistence failed: {e}")

    def _load_baseline_data(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        # åˆæœŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨­å®š
        self.baseline_stats = {
            "system:cpu_usage": {"mean": 50, "std": 10, "min": 0, "max": 100},
            "system:memory_usage": {"mean": 60, "std": 15, "min": 0, "max": 100},
            "system:disk_io": {"mean": 100, "std": 30, "min": 0, "max": 500},
            "network:network_traffic": {"mean": 200, "std": 50, "min": 0, "max": 1000},
            "endpoint:response_time": {"mean": 100, "std": 20, "min": 10, "max": 500},
            "service:error_rate": {"mean": 0.01, "std": 0.005, "min": 0, "max": 0.1},
            "database:connection_count": {"mean": 50, "std": 10, "min": 0, "max": 200},
        }

    async def get_system_health(self) -> SystemHealthReport:
        """ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå¥å…¨æ€§è¨ˆç®—
        component_health = {}

        for key, buffer in self.metrics_buffer.items():
            if buffer and key in self.baseline_stats:
                recent_values = [m.value for m in list(buffer)[-10:]]
                baseline = self.baseline_stats[key]

                # æ­£å¸¸ç¯„å›²å†…ã®å‰²åˆ
                normal_count = sum(
                    1
                    for v in recent_values
                    if baseline["mean"] - 3 * baseline["std"]
                    <= v
                    <= baseline["mean"] + 3 * baseline["std"]
                )
                health_score = normal_count / len(recent_values) if recent_values else 0

                component_health[key] = health_score

        # å…¨ä½“å¥å…¨æ€§
        overall_health = (
            np.mean(list(component_health.values())) if component_health else 1.0
        )

        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendations = []
        if overall_health < 0.8:
            recommendations.append(
                "System health is degraded. Review active anomalies."
            )
        if len(self.active_anomalies) > 5:
            recommendations.append(
                "Multiple active anomalies detected. Consider scaling resources."
            )
        if any(p.probability > 0.8 for p in self.incident_predictions.values()):
            recommendations.append(
                "High probability incidents predicted. Execute prevention actions."
            )

        return SystemHealthReport(
            timestamp=datetime.now(),
            overall_health=overall_health,
            component_health=component_health,
            active_anomalies=list(self.active_anomalies.values()),
            predicted_incidents=list(self.incident_predictions.values()),
            recommendations=recommendations,
        )

    async def handle_complex_anomaly(self, pattern: Dict[str, Any]):
        """è¤‡åˆç•°å¸¸å‡¦ç†"""
        # è¤‡åˆç•°å¸¸ã‚¤ãƒ™ãƒ³ãƒˆä½œæˆ
        anomaly = AnomalyEvent(
            event_id=f"complex_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            detected_at=datetime.now(),
            anomaly_type=AnomalyType.BEHAVIOR,
            severity=SeverityLevel.ERROR,
            target=MonitoringTarget.SYSTEM,
            metric_name="complex_pattern",
            current_value=pattern["anomaly_score"],
            expected_range=(0, 0.5),
            anomaly_score=pattern["anomaly_score"],
            context=pattern,
            suggested_actions=[
                "Investigate correlated metrics",
                "Check for cascading failures",
                "Review system dependencies",
            ],
        )

        await self._handle_anomaly(anomaly)


# ä½¿ç”¨ä¾‹
async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        monitoring = RealtimeMonitoringEnhancement()

        print("ğŸš¨ Starting Real-time Monitoring Enhancement System...")

        # ç›£è¦–é–‹å§‹ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
        monitoring_task = asyncio.create_task(monitoring.start_monitoring())

        # ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡
        print("\nğŸ“Š Sending test metrics...")

        for i in range(5):
            # æ­£å¸¸ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            await monitoring.ingest_metric(
                MetricPoint(
                    timestamp=datetime.now(),
                    target=MonitoringTarget.SYSTEM,
                    metric_name="cpu_usage",
                    value=50 + np.random.normal(0, 5),
                    tags={"host": "test-host"},
                    metadata={"unit": "percent"},
                )
            )

            # ç•°å¸¸ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆé«˜CPUï¼‰
            if i == 3:
                await monitoring.ingest_metric(
                    MetricPoint(
                        timestamp=datetime.now(),
                        target=MonitoringTarget.SYSTEM,
                        metric_name="cpu_usage",
                        value=95,  # ç•°å¸¸å€¤
                        tags={"host": "test-host"},
                        metadata={"unit": "percent"},
                    )
                )

            await asyncio.sleep(1)

        # ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ç¢ºèª
        print("\nğŸ¥ Checking system health...")
        health_report = await monitoring.get_system_health()
        print(f"Overall Health: {health_report.overall_health:.2f}")
        print(f"Active Anomalies: {len(health_report.active_anomalies)}")
        print(f"Predicted Incidents: {len(health_report.predicted_incidents)}")

        # çµ±è¨ˆæƒ…å ±
        print("\nğŸ“ˆ Monitoring Statistics:")
        print(f"Total Metrics Processed: {monitoring.stats['total_metrics_processed']}")
        print(
            f"Total Anomalies Detected: {monitoring.stats['total_anomalies_detected']}"
        )
        print(
            f"Total Incidents Prevented: {monitoring.stats['total_incidents_prevented']}"
        )

        # åœæ­¢
        await monitoring.stop_monitoring()
        monitoring_task.cancel()

        print("\nğŸ‰ Real-time Monitoring Enhancement System Phase 1 testing completed!")

    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
