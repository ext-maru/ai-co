#!/usr/bin/env python3
"""
Worker System Stabilization Knight - ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®‰å®šåŒ–å°‚é–€é¨å£«
å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã®èµ·å‹•ãƒ»è¨­å®šãƒ»ã‚¨ãƒ©ãƒ¼ä¿®å¾©ã‚’æ‹…å½“
"""

import json
import logging
import os
import subprocess
import sys
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import psutil

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from libs.incident_knights_framework import (
    Diagnosis,
    IncidentKnight,
    Issue,
    IssueCategory,
    IssueSeverity,
    KnightType,
    Resolution,
)

logger = logging.getLogger(__name__)


@dataclass
class WorkerIssue:
    """ãƒ¯ãƒ¼ã‚«ãƒ¼å•é¡Œãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    worker_name: str
    issue_type: (
        str  # "not_running", "config_error", "dependency_missing", "error_rate_high"
    )
    error_details: str
    affected_files: List[str]
    recovery_priority: str  # "critical", "high", "medium", "low"


class WorkerStabilizationKnight(IncidentKnight):
    """
    Worker System Stabilization Knight - ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®‰å®šåŒ–å°‚é–€é¨å£«

    æ©Ÿèƒ½:
    - å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ç¨¼åƒçŠ¶æ…‹ç›£è¦–
    - åœæ­¢ãƒ¯ãƒ¼ã‚«ãƒ¼ã®è‡ªå‹•å†èµ·å‹•
    - ãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®šã‚¨ãƒ©ãƒ¼ã®ä¿®å¾©
    - ä¾å­˜é–¢ä¿‚å•é¡Œã®è§£æ±º
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
    """

    def __init__(
        self,
        knight_id: str = "worker_stabilization_001",
        specialty: str = "Worker system stability",
    ):
        super().__init__(knight_id, KnightType.REPAIR, specialty)
        self.name = "Worker Stabilization Knight"
        self.project_root = Path(__file__).parent.parent

        # ç›£è¦–å¯¾è±¡ãƒ¯ãƒ¼ã‚«ãƒ¼å®šç¾©
        self.target_workers = [
            {
                "name": "enhanced_task_worker",
                "file": "workers/enhanced_task_worker.py",
                "config": "config/worker_config.json",
                "priority": "critical",
            },
            {
                "name": "task_worker",
                "file": "workers/task_worker.py",
                "config": "config/worker_config.json",
                "priority": "high",
            },
            {
                "name": "pm_worker",
                "file": "workers/pm_worker.py",
                "config": "config/worker_config.json",
                "priority": "medium",
            },
            {
                "name": "result_worker",
                "file": "workers/result_worker.py",
                "config": "config/worker_config.json",
                "priority": "medium",
            },
            {
                "name": "dialog_task_worker",
                "file": "workers/dialog_task_worker.py",
                "config": "config/worker_config.json",
                "priority": "low",
            },
        ]

        # ãƒ­ã‚°ç›£è¦–ãƒ‘ã‚¹
        self.log_paths = [
            self.project_root / "logs" / "workers",
            self.project_root / "logs" / "task_worker.log",
            self.project_root / "logs" / "pm_worker.log",
        ]

        self.worker_issues: List[WorkerIssue] = []

        logger.info(f"âš™ï¸ {self.name} åˆæœŸåŒ–å®Œäº†")

    async def patrol(self) -> List[Issue]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®å·¡å›ç›£è¦–"""
        logger.info("ğŸ” ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å·¡å›é–‹å§‹")

        issues = []

        # 1. ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹ç¢ºèª
        process_issues = await self._check_worker_processes()
        issues.extend(process_issues)

        # 2. ãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        config_issues = await self._check_worker_configurations()
        issues.extend(config_issues)

        # 3. ãƒ¯ãƒ¼ã‚«ãƒ¼ä¾å­˜é–¢ä¿‚ç¢ºèª
        dependency_issues = await self._check_worker_dependencies()
        issues.extend(dependency_issues)

        # 4. ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°åˆ†æ
        log_issues = await self._analyze_worker_logs()
        issues.extend(log_issues)

        # 5. ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ç¢ºèª
        resource_issues = await self._check_resource_usage()
        issues.extend(resource_issues)

        logger.info(f"ğŸ“Š ãƒ¯ãƒ¼ã‚«ãƒ¼å•é¡Œæ¤œå‡º: {len(issues)}ä»¶")
        return issues

    async def _check_worker_processes(self) -> List[Issue]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®çŠ¶æ…‹ç¢ºèª"""
        issues = []

        try:
            # psutilã§å®Ÿè¡Œä¸­ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¢ºèª
            running_processes = [
                p.info for p in psutil.process_iter(["pid", "name", "cmdline"])
            ]

            for worker in self.target_workers:
            # ç¹°ã‚Šè¿”ã—å‡¦ç†
                worker_running = False

                # ãƒ—ãƒ­ã‚»ã‚¹åã§ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’æ¢ç´¢
                for proc in running_processes:
                    cmdline = proc.get("cmdline", [])
                    if any(worker["name"] in str(cmd) for cmd in cmdline):
                        worker_running = True
                        break

                if not worker_running:
                    issues.append(
                        Issue(
                            id=f"worker_process_{worker['name']}",
                            category=IssueCategory.RESOURCE_EXHAUSTION,
                            severity=(
                                IssueSeverity.CRITICAL
                                if worker["priority"] == "critical"
                                else IssueSeverity.HIGH
                            ),
                            title=f"ãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢: {worker['name']}",
                            description=f"{worker['name']} ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“",
                            affected_component=worker["file"],
                            detected_at=datetime.now(),
                            metadata={
                                "worker_name": worker["name"],
                                "priority": worker["priority"],
                                "auto_fixable": True,
                                "restart_required": True,
                            },
                        )
                    )

                    self.worker_issues.append(
                        WorkerIssue(
                            worker_name=worker["name"],
                            issue_type="not_running",
                            error_details="ãƒ—ãƒ­ã‚»ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„",
                            affected_files=[worker["file"]],
                            recovery_priority=worker["priority"],
                        )
                    )

        except Exception as e:
            logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            issues.append(
                Issue(
                    id="worker_process_check_error",
                    category=IssueCategory.CONFIG_ERROR,
                    severity=IssueSeverity.HIGH,
                    title="ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèªã‚¨ãƒ©ãƒ¼",
                    description=f"ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹ç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}",
                    affected_component="process_monitor",
                    detected_at=datetime.now(),
                    metadata={"error": str(e)},
                )
            )

        return issues

    async def _check_worker_configurations(self) -> List[Issue]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª"""
        issues = []

        # åŸºæœ¬è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
        config_files = ["config/worker_config.json", "config/config.json", ".env"]

        for config_file in config_files:
            config_path = self.project_root / config_file

            if not config_path.exists():
                issues.append(
                    Issue(
                        id=f"config_missing_{config_file.replace('/', '_')}",
                        category=IssueCategory.CONFIG_ERROR,
                        severity=IssueSeverity.HIGH,
                        title=f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¸åœ¨: {config_file}",
                        description=f"ãƒ¯ãƒ¼ã‚«ãƒ¼å®Ÿè¡Œã«å¿…è¦ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                        affected_component=str(config_path),
                        detected_at=datetime.now(),
                        metadata={"auto_fixable": True, "config_type": "worker"},
                    )
                )
            else:
                # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹æ¤œè¨¼
                try:
                    if config_path.suffix == ".json":
                        # Deep nesting detected (depth: 5) - consider refactoring
                        with open(config_path) as f:
                            config_data = json.load(f)

                        # å¿…è¦ãªè¨­å®šé …ç›®ã®ç¢ºèª
                        required_keys = ["workers", "database", "logging"]
                        missing_keys = [
                            key for key in required_keys if key not in config_data
                        ]

                        if not (missing_keys):
                            continue  # Early return to reduce nesting
                        # Reduced nesting - original condition satisfied
                        if missing_keys:
                            issues.append(
                                Issue(
                                    id=f"config_incomplete_{config_file.replace('/', '_')}",
                                    category=IssueCategory.CONFIG_ERROR,
                                    severity=IssueSeverity.MEDIUM,
                                    title=f"è¨­å®šä¸å®Œå…¨: {config_file}",
                                    description=f"å¿…è¦ãªè¨­å®šé …ç›®ãŒä¸è¶³: {', '.join(missing_keys)}",
                                    affected_component=str(config_path),
                                    detected_at=datetime.now(),
                                    metadata={
                                        "missing_keys": missing_keys,
                                        "auto_fixable": True,
                                    },
                                )
                            )

                except Exception as e:
                    issues.append(
                        Issue(
                            id=f"config_parse_error_{config_file.replace('/', '_')}",
                            category=IssueCategory.CONFIG_ERROR,
                            severity=IssueSeverity.MEDIUM,
                            title=f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {config_file}",
                            description=f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã«å¤±æ•—: {str(e)}",
                            affected_component=str(config_path),
                            detected_at=datetime.now(),
                            metadata={"error": str(e), "auto_fixable": True},
                        )
                    )

        return issues

    async def _check_worker_dependencies(self) -> List[Issue]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ä¾å­˜é–¢ä¿‚ã®ç¢ºèª"""
        issues = []

        # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾å­˜é–¢ä¿‚ç¢ºèª
        for worker in self.target_workers:
            worker_path = self.project_root / worker["file"]

            if worker_path.exists():
                try:
                    with open(worker_path) as f:
                        content = f.read()

                    # ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã®æŠ½å‡ºã¨ç¢ºèª
                    import_lines = [
                        line.strip()
                        for line in content.split("\n")
                        if line.strip().startswith(("import ", "from "))
                    ]

                    missing_modules = []
                    for line in import_lines:
                        # Deep nesting detected (depth: 5) - consider refactoring
                        try:
                            # åŸºæœ¬çš„ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                            if not ("pika" in line and not self._check_module_available()):
                                continue  # Early return to reduce nesting
                            # Reduced nesting - original condition satisfied
                            if "pika" in line and not self._check_module_available(
                                "pika"
                            ):
                                missing_modules.append("pika")
                            elif (
                                "anthropic" in line
                                and not self._check_module_available("anthropic")
                            ):
                                missing_modules.append("anthropic")
                            elif "psutil" in line and not self._check_module_available(
                                "psutil"
                            ):
                                missing_modules.append("psutil")
                        except:
                            pass

                    if missing_modules:
                        issues.append(
                            Issue(
                                id=f"worker_dependencies_{worker['name']}",
                                category=IssueCategory.DEPENDENCY_MISSING,
                                severity=IssueSeverity.HIGH,
                                title=f"ä¾å­˜é–¢ä¿‚ä¸è¶³: {worker['name']}",
                                description=f"å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒä¸è¶³: {', '.join(missing_modules)}",
                                affected_component=worker["file"],
                                detected_at=datetime.now(),
                                metadata={
                                    "missing_modules": missing_modules,
                                    "worker_name": worker["name"],
                                    "auto_fixable": True,
                                },
                            )
                        )

                except Exception as e:
                    logger.error(f"ä¾å­˜é–¢ä¿‚ç¢ºèªã‚¨ãƒ©ãƒ¼ {worker['file']}: {e}")

        return issues

    def _check_module_available(self, module_name: str) -> bool:
        """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆ©ç”¨å¯èƒ½æ€§ç¢ºèª"""
        try:
            __import__(module_name)
            return True
        except ImportError:
            return False

    async def _analyze_worker_logs(self) -> List[Issue]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ­ã‚°ã®åˆ†æ"""
        issues = []

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        error_patterns = [
            ("ModuleNotFoundError", "dependency_missing"),
            ("ConnectionRefusedError", "connection_error"),
            ("API rate limit", "rate_limit"),
            ("Invalid API key", "api_key_error"),
            ("Permission denied", "permission_error"),
            ("Memory Error", "memory_error"),
            ("Traceback", "runtime_error"),
        ]

        # ç¹°ã‚Šè¿”ã—å‡¦ç†
        for log_path in self.log_paths:
            if log_path.exists() and log_path.is_file():
                try:
                    with open(log_path) as f:
                        log_content = f.read()

                    for pattern, error_type in error_patterns:
                        if not (pattern.lower() in log_content.lower()):
                            continue  # Early return to reduce nesting
                        # Reduced nesting - original condition satisfied
                        if pattern.lower() in log_content.lower():
                            # æœ€è¿‘ã®ã‚¨ãƒ©ãƒ¼ã‹ã©ã†ã‹ç¢ºèªï¼ˆç°¡æ˜“ç‰ˆï¼‰
                            recent_pattern = datetime.now().strftime("%Y-%m-%d")
                            if not (recent_pattern in str(datetime.now())):
                                continue  # Early return to reduce nesting
                            # Reduced nesting - original condition satisfied
                            if (
                                recent_pattern in log_content
                                or len(log_content.split(pattern)) > 2
                            ):
                                issues.append(
                                    Issue(
                                        id=f"log_error_{error_type}_{log_path.name}",
                                        category=IssueCategory.CONFIG_ERROR,
                                        severity=IssueSeverity.MEDIUM,
                                        title=f"ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼æ¤œå‡º: {pattern}",
                                        description=f"{log_path.name} ã§ {pattern} ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ",
                                        affected_component=str(log_path),
                                        detected_at=datetime.now(),
                                        metadata={
                                            "error_pattern": pattern,
                                            "error_type": error_type,
                                            "auto_fixable": True,
                                        },
                                    )
                                )

                except Exception as e:
                    logger.error(f"ãƒ­ã‚°åˆ†æã‚¨ãƒ©ãƒ¼ {log_path}: {e}")

        return issues

    async def _check_resource_usage(self) -> List[Issue]:
        """ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®ç¢ºèª"""
        issues = []

        try:
            # CPUãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()

            # CPUä½¿ç”¨ç‡ãŒé«˜ã™ãã‚‹å ´åˆ
            if cpu_percent > 90:
                issues.append(
                    Issue(
                        id="high_cpu_usage",
                        category=IssueCategory.RESOURCE_EXHAUSTION,
                        severity=IssueSeverity.HIGH,
                        title="CPUä½¿ç”¨ç‡ãŒéåº¦ã«é«˜ã„",
                        description=f"CPUä½¿ç”¨ç‡: {cpu_percent}% (é–¾å€¤: 90%)",
                        affected_component="system_resources",
                        detected_at=datetime.now(),
                        metadata={"cpu_percent": cpu_percent, "auto_fixable": True},
                    )
                )

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã™ãã‚‹å ´åˆ
            if memory.percent > 85:
                issues.append(
                    Issue(
                        id="high_memory_usage",
                        category=IssueCategory.RESOURCE_EXHAUSTION,
                        severity=IssueSeverity.HIGH,
                        title="ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒéåº¦ã«é«˜ã„",
                        description=f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory.percent}% (é–¾å€¤: 85%)",
                        affected_component="system_resources",
                        detected_at=datetime.now(),
                        metadata={
                            "memory_percent": memory.percent,
                            "auto_fixable": True,
                        },
                    )
                )

            # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç¢ºèª
            disk = psutil.disk_usage("/")
            if disk.percent > 90:
                issues.append(
                    Issue(
                        id="high_disk_usage",
                        category=IssueCategory.RESOURCE_EXHAUSTION,
                        severity=IssueSeverity.MEDIUM,
                        title="ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡ãŒé«˜ã„",
                        description=f"ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡: {disk.percent}% (é–¾å€¤: 90%)",
                        affected_component="disk_storage",
                        detected_at=datetime.now(),
                        metadata={"disk_percent": disk.percent, "auto_fixable": True},
                    )
                )

        except Exception as e:
            logger.error(f"ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")

        return issues

    async def investigate(self, issue: Issue) -> Diagnosis:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼å•é¡Œã®è©³ç´°èª¿æŸ»"""
        logger.info(f"ğŸ”¬ ãƒ¯ãƒ¼ã‚«ãƒ¼å•é¡Œè©³ç´°èª¿æŸ»: {issue.title}")

        diagnosis_data = {
            "issue_type": issue.category.value,
            "severity": issue.severity.value,
            "investigation_time": datetime.now().isoformat(),
        }

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®è©³ç´°èª¿æŸ»
        if "worker_process" in issue.id:
            diagnosis_data.update(await self._investigate_process_issue(issue))
        elif "config" in issue.id:
            diagnosis_data.update(await self._investigate_config_issue(issue))
        elif "dependencies" in issue.id:
            diagnosis_data.update(await self._investigate_dependency_issue(issue))
        elif "log_error" in issue.id:
            diagnosis_data.update(await self._investigate_log_issue(issue))
        elif "resource" in issue.id:
            diagnosis_data.update(await self._investigate_resource_issue(issue))

        return Diagnosis(
            issue_id=issue.id,
            root_cause=diagnosis_data.get("root_cause", "èª¿æŸ»ä¸­"),
            impact_assessment=diagnosis_data.get("impact", "ä¸­ç¨‹åº¦"),
            recommended_actions=diagnosis_data.get("actions", ["æ‰‹å‹•ç¢ºèªãŒå¿…è¦"]),
            estimated_fix_time=diagnosis_data.get("fix_time", 300),
            requires_approval=diagnosis_data.get("requires_approval", False),
            confidence_score=diagnosis_data.get("confidence", 0.8),
        )

    async def _investigate_process_issue(self, issue: Issue) -> Dict:
        """ãƒ—ãƒ­ã‚»ã‚¹å•é¡Œã®èª¿æŸ»"""
        return {
            "root_cause": "ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®ç•°å¸¸çµ‚äº†ã¾ãŸã¯æœªèµ·å‹•",
            "impact": "è©²å½“ãƒ¯ãƒ¼ã‚«ãƒ¼ã®æ©Ÿèƒ½ãŒå®Œå…¨åœæ­¢",
            "actions": ["ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•", "è¨­å®šç¢ºèª", "ãƒ­ã‚°åˆ†æ", "ä¾å­˜é–¢ä¿‚ç¢ºèª"],
            "fix_time": 180,
            "confidence": 0.9,
            "requires_approval": False,
        }

    async def _investigate_config_issue(self, issue: Issue) -> Dict:
        """è¨­å®šå•é¡Œã®èª¿æŸ»"""
        return {
            "root_cause": "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸å‚™ã¾ãŸã¯æ¬ å¦‚",
            "impact": "ãƒ¯ãƒ¼ã‚«ãƒ¼ã®èµ·å‹•ãƒ»å‹•ä½œã«æ”¯éšœ",
            "actions": ["è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ/ä¿®æ­£", "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š", "æ¤œè¨¼ãƒ†ã‚¹ãƒˆ"],
            "fix_time": 240,
            "confidence": 0.85,
            "requires_approval": False,
        }

    async def _investigate_dependency_issue(self, issue: Issue) -> Dict:
        """ä¾å­˜é–¢ä¿‚å•é¡Œã®èª¿æŸ»"""
        return {
            "root_cause": "å¿…è¦ãªPythonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¾ãŸã¯ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚ã®ä¸è¶³",
            "impact": "ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ç‰¹å®šæ©Ÿèƒ½ãŒä½¿ç”¨ä¸å¯",
            "actions": ["ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«", "è¦ä»¶ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°", "ä»®æƒ³ç’°å¢ƒç¢ºèª"],
            "fix_time": 300,
            "confidence": 0.8,
            "requires_approval": False,
        }

    async def _investigate_log_issue(self, issue: Issue) -> Dict:
        """ãƒ­ã‚°å•é¡Œã®èª¿æŸ»"""
        return {
            "root_cause": "ãƒ¯ãƒ¼ã‚«ãƒ¼å®Ÿè¡Œæ™‚ã®å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯è¨­å®šå•é¡Œ",
            "impact": "ã‚¨ãƒ©ãƒ¼é »åº¦ã«å¿œã˜ãŸæ€§èƒ½ä½ä¸‹",
            "actions": ["ã‚¨ãƒ©ãƒ¼åŸå› ç‰¹å®š", "ã‚³ãƒ¼ãƒ‰ä¿®æ­£", "è¨­å®šèª¿æ•´", "ç›£è¦–å¼·åŒ–"],
            "fix_time": 600,
            "confidence": 0.7,
            "requires_approval": False,
        }

    async def _investigate_resource_issue(self, issue: Issue) -> Dict:
        """ãƒªã‚½ãƒ¼ã‚¹å•é¡Œã®èª¿æŸ»"""
        return {
            "root_cause": "ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®éåº¦ãªä½¿ç”¨",
            "impact": "ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æ€§èƒ½ä½ä¸‹",
            "actions": ["ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨æœ€é©åŒ–", "ä¸è¦ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†", "è¨­å®šèª¿æ•´"],
            "fix_time": 480,
            "confidence": 0.75,
            "requires_approval": True,
        }

    async def resolve(self, diagnosis: Diagnosis) -> Resolution:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼å•é¡Œã®ä¿®å¾©å®Ÿè¡Œ"""
        logger.info(f"ğŸ”§ ãƒ¯ãƒ¼ã‚«ãƒ¼å•é¡Œä¿®å¾©å®Ÿè¡Œ: {diagnosis.issue_id}")

        try:
            success = False
            actions_taken = []

            # å•é¡Œç¨®åˆ¥ã«å¿œã˜ãŸä¿®å¾©å®Ÿè¡Œ
            if "worker_process" in diagnosis.issue_id:
                success, action = await self._fix_worker_process(diagnosis)
                actions_taken.append(action)
            elif "config" in diagnosis.issue_id:
                success, action = await self._fix_worker_config(diagnosis)
                actions_taken.append(action)
            elif "dependencies" in diagnosis.issue_id:
                success, action = await self._fix_worker_dependencies(diagnosis)
                actions_taken.append(action)
            elif "log_error" in diagnosis.issue_id:
                success, action = await self._fix_log_errors(diagnosis)
                actions_taken.append(action)
            elif "resource" in diagnosis.issue_id:
                success, action = await self._fix_resource_usage(diagnosis)
                actions_taken.append(action)

            return Resolution(
                issue_id=diagnosis.issue_id,
                success=success,
                actions_taken=actions_taken,
                time_taken=int(diagnosis.estimated_fix_time),
                side_effects=[],
                verification_results={"status": "verified" if success else "failed"},
            )

        except Exception as e:
            logger.error(f"ä¿®å¾©å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ {diagnosis.issue_id}: {e}")
            return Resolution(
                issue_id=diagnosis.issue_id,
                success=False,
                actions_taken=[f"ä¿®å¾©å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}"],
                time_taken=30,
                side_effects=["error_state"],
                verification_results={"error": str(e)},
            )

    async def _fix_worker_process(self, diagnosis: Diagnosis) -> tuple[bool, str]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®ä¿®å¾©"""
        try:
            # ãƒ—ãƒ­ã‚»ã‚¹åã‚’æŠ½å‡º
            issue_id = diagnosis.issue_id
            worker_name = issue_id.replace("worker_process_", "")

            # å¯¾è±¡ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
            worker_file = None
            for worker in self.target_workers:
                if worker["name"] == worker_name:
                    worker_file = self.project_root / worker["file"]
                    break

            if worker_file and worker_file.exists():
                # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’èµ·å‹•
                cmd = [sys.executable, str(worker_file)]
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    cwd=str(self.project_root),
                )

                # èµ·å‹•ç¢ºèªï¼ˆ2ç§’å¾…æ©Ÿï¼‰
                time.sleep(2)
                if process.poll() is None:  # ãƒ—ãƒ­ã‚»ã‚¹ãŒã¾ã å®Ÿè¡Œä¸­
                    logger.info(f"âœ… ãƒ¯ãƒ¼ã‚«ãƒ¼èµ·å‹•æˆåŠŸ: {worker_name}")
                    return True, f"worker_restarted_{worker_name}"
                else:
                    stderr = process.stderr.read().decode()
                    logger.error(f"ãƒ¯ãƒ¼ã‚«ãƒ¼èµ·å‹•å¤±æ•—: {stderr}")
                    return False, f"worker_restart_failed_{worker_name}: {stderr}"

            return False, f"worker_file_not_found_{worker_name}"

        except Exception as e:
            logger.error(f"ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ä¿®å¾©ã‚¨ãƒ©ãƒ¼: {e}")
            return False, f"worker_process_fix_failed: {str(e)}"

    async def _fix_worker_config(self, diagnosis: Diagnosis) -> tuple[bool, str]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®šã®ä¿®å¾©"""
        try:
            # åŸºæœ¬è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
            config_file = self.project_root / "config" / "worker_config.json"
            config_file.parent.mkdir(exist_ok=True)

            default_config = {
                "workers": {
                    "enhanced_task_worker": {
                        "enabled": True,
                        "max_retries": 3,
                        "timeout": 300,
                    },
                    "task_worker": {"enabled": True, "max_retries": 3, "timeout": 180},
                    "pm_worker": {"enabled": True, "max_retries": 2, "timeout": 240},
                },
                "database": {"type": "sqlite", "path": "data/workers.db"},
                "logging": {"level": "INFO", "file": "logs/workers.log"},
                "monitoring": {
                    "health_check_interval": 60,
                    "max_memory_usage": 0.8,
                    "max_cpu_usage": 0.7,
                },
            }

            with open(config_file, "w") as f:
                json.dump(default_config, f, indent=2)

            logger.info("âœ… ãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†")
            return True, "worker_config_created"

        except Exception as e:
            logger.error(f"ãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®šä¿®å¾©ã‚¨ãƒ©ãƒ¼: {e}")
            return False, f"worker_config_fix_failed: {str(e)}"

    async def _fix_worker_dependencies(self, diagnosis: Diagnosis) -> tuple[bool, str]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ä¾å­˜é–¢ä¿‚ã®ä¿®å¾©"""
        try:
            # åŸºæœ¬çš„ãªä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
            install_script = self.project_root / "scripts" / "install_dependencies.py"
            install_script.parent.mkdir(exist_ok=True)

            script_content = '''#!/usr/bin/env python3
"""
Worker Dependencies Installation Script
ãƒ¯ãƒ¼ã‚«ãƒ¼ä¾å­˜é–¢ä¿‚è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
"""

import subprocess
import sys

def install_package(package):
    """ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
        print(f"âœ… Installed: {package}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ Failed to install {package}: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    packages = [
        "pika",
        "anthropic",
        "psutil",
        "requests",
        "python-dotenv"
    ]

    print("ğŸ”§ Installing worker dependencies...")

    success_count = 0
    for package in packages:
        if install_package(package):
            success_count += 1

    print(f"ğŸ“Š Installation complete: {success_count}/{len(packages)} packages")

if __name__ == "__main__":
    main()
'''

            with open(install_script, "w") as f:
                f.write(script_content)

            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œå¯èƒ½ã«ã™ã‚‹
            install_script.chmod(0o755)

            logger.info("âœ… ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆå®Œäº†")
            return True, "dependency_script_created"

        except Exception as e:
            logger.error(f"ä¾å­˜é–¢ä¿‚ä¿®å¾©ã‚¨ãƒ©ãƒ¼: {e}")
            return False, f"dependency_fix_failed: {str(e)}"

    async def _fix_log_errors(self, diagnosis: Diagnosis) -> tuple[bool, str]:
        """ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼ã®ä¿®å¾©"""
        try:
            # ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            log_dir = self.project_root / "logs"

            rotated_count = 0
            for log_file in log_dir.glob("*.log"):
                if log_file.stat().st_size > 50 * 1024 * 1024:  # 50MBä»¥ä¸Š
                    backup_file = log_file.with_suffix(
                        f".{datetime.now().strftime('%Y%m%d_%H%M%S')}.bak"
                    )
                    log_file.rename(backup_file)
                    log_file.touch()
                    rotated_count += 1

            # ã‚¨ãƒ©ãƒ¼ç›£è¦–è¨­å®šä½œæˆ
            error_config = {
                "error_monitoring": {
                    "enabled": True,
                    "patterns": [
                        "ModuleNotFoundError",
                        "ConnectionRefusedError",
                        "API rate limit",
                        "Invalid API key",
                    ],
                    "action": "restart_worker",
                    "max_errors_per_hour": 10,
                }
            }

            config_file = self.project_root / "config" / "error_monitoring.json"
            with open(config_file, "w") as f:
                json.dump(error_config, f, indent=2)

            logger.info(f"âœ… ãƒ­ã‚°ä¿®å¾©å®Œäº†ï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: {rotated_count}ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")
            return True, f"log_errors_fixed_rotated_{rotated_count}"

        except Exception as e:
            logger.error(f"ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼ä¿®å¾©å¤±æ•—: {e}")
            return False, f"log_error_fix_failed: {str(e)}"

    async def _fix_resource_usage(self, diagnosis: Diagnosis) -> tuple[bool, str]:
        """ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®ä¿®å¾©"""
        try:
            # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–è¨­å®šä½œæˆ
            resource_config = {
                "resource_limits": {
                    "max_cpu_percent": 80.0,
                    "max_memory_percent": 75.0,
                    "monitoring_interval": 30,
                },
                "actions": {
                    "high_cpu": "reduce_worker_count",
                    "high_memory": "restart_workers",
                    "high_disk": "cleanup_logs",
                },
                "worker_optimization": {
                    "batch_size": 10,
                    "processing_delay": 0.1,
                    "max_concurrent": 5,
                },
            }

            config_file = self.project_root / "config" / "resource_monitoring.json"
            with open(config_file, "w") as f:
                json.dump(resource_config, f, indent=2)

            logger.info("âœ… ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–è¨­å®šä½œæˆå®Œäº†")
            return True, "resource_monitoring_configured"

        except Exception as e:
            logger.error(f"ãƒªã‚½ãƒ¼ã‚¹ä¿®å¾©ã‚¨ãƒ©ãƒ¼: {e}")
            return False, f"resource_fix_failed: {str(e)}"

    def get_knight_status(self) -> Dict[str, Any]:
        """é¨å£«ã®ç¾åœ¨çŠ¶æ…‹ã‚’å–å¾—"""
        return {
            "knight_id": self.knight_id,
            "name": self.name,
            "status": "active",
            "specialty": self.specialty,
            "target_workers": len(self.target_workers),
            "issues_detected": len(self.worker_issues),
            "last_patrol": getattr(self, "last_patrol", None),
            "success_rate": getattr(self, "success_rate", 0.0),
        }


if __name__ == "__main__":
    import asyncio

    async def test_worker_knight():
        """test_worker_knightãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰"""
        knight = WorkerStabilizationKnight()

        # å·¡å›ãƒ†ã‚¹ãƒˆ
        issues = await knight.patrol()
        print(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {len(issues)}ä»¶")

        # å•é¡ŒãŒã‚ã‚‹å ´åˆã¯èª¿æŸ»ã¨ä¿®å¾©
        for issue in issues[:3]:  # æœ€åˆã®3ä»¶ã‚’ãƒ†ã‚¹ãƒˆ
            diagnosis = await knight.investigate(issue)
            print(f"ğŸ”¬ èª¿æŸ»å®Œäº†: {diagnosis.root_cause}")

            resolution = await knight.resolve(diagnosis)
            print(f"ğŸ”§ ä¿®å¾©çµæœ: {resolution.success}")

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
        status = knight.get_knight_status()
        print(f"ğŸ›¡ï¸ é¨å£«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(test_worker_knight())
