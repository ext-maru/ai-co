#!/usr/bin/env python3
"""
ğŸ¯ Optimized Issue Orchestrator - æœ€é©åŒ–ã‚¤ã‚·ãƒ¥ãƒ¼å‡¦ç†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆå‡¦ç†ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–

æ©Ÿèƒ½:
- ã‚¤ã‚·ãƒ¥ãƒ¼å‡¦ç†å„ªå…ˆé †ä½ä»˜ã‘
- ä¸¦åˆ—å‡¦ç†ç®¡ç†
- ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ç›£è¦–
- å‡¦ç†å®Œäº†è¿½è·¡ã¨ãƒ¬ãƒãƒ¼ãƒˆ

ä½œæˆè€…: ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼
ä½œæˆæ—¥: 2025-07-19
"""

import asyncio
import logging
import os
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

import psutil

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from libs.integrations.github.enhanced_auto_issue_processor import (
    EnhancedAutoIssueProcessor,
)
from libs.integrations.github.issue_completion_manager import (
    CompletionResult,
    IssueCompletionManager,
)

# GitHub APIã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from github import Github
    from github.Issue import Issue

    GITHUB_AVAILABLE = True
except ImportError:
    Github = None
    Issue = None
    GITHUB_AVAILABLE = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ProcessingPriority(Enum):
    """å‡¦ç†å„ªå…ˆåº¦"""

    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4


class ResourceStatus(Enum):
    """ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³"""

    AVAILABLE = "available"
    BUSY = "busy"
    OVERLOAD = "overload"
    CRITICAL = "critical"


@dataclass
class ProcessingTask:
    """å‡¦ç†ã‚¿ã‚¹ã‚¯"""

    issue: Issue
    priority: ProcessingPriority
    estimated_time: float
    retry_count: int
    scheduled_at: datetime
    metadata: Dict[str, Any]


@dataclass
class ResourceMetrics:
    """ãƒªã‚½ãƒ¼ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    cpu_percent: float
    memory_percent: float
    disk_usage_percent: float
    active_processes: int
    timestamp: datetime


class OptimizedIssueOrchestrator:
    """
    ğŸ¯ æœ€é©åŒ–ã‚¤ã‚·ãƒ¥ãƒ¼å‡¦ç†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼

    ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã®å…¨ã‚¤ã‚·ãƒ¥ãƒ¼å‡¦ç†ã‚’çµ±åˆç®¡ç†ã—ã€
    æœ€é©ãªé †åºã¨ä¸¦åˆ—åº¦ã§å‡¦ç†ã‚’å®Ÿè¡Œ
    """

    def __init__(
        self, max_concurrent_tasks: int = 3, resource_check_interval: int = 30
    ):
        """ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–"""
        self.max_concurrent_tasks = max_concurrent_tasks
        self.resource_check_interval = resource_check_interval

        # å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.processor = EnhancedAutoIssueProcessor()
        self.completion_manager = IssueCompletionManager()

        # GitHub APIåˆæœŸåŒ–
        if GITHUB_AVAILABLE:
            github_token = os.getenv("GITHUB_TOKEN")
            if github_token:
                self.github = Github(github_token)
                self.repo = self.github.get_repo(
                    f"{os.getenv(
                        'GITHUB_REPO_OWNER',
                        'ext-maru')}/{os.getenv('GITHUB_REPO_NAME',
                        'ai-co'
                    )}"
                )
            else:
                logger.error("GITHUB_TOKENç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                self.github = None
                self.repo = None
        else:
            self.github = None
            self.repo = None

        # å‡¦ç†çŠ¶æ…‹ç®¡ç†
        self.active_tasks = {}
        self.task_queue = []
        self.processing_statistics = {
            "total_processed": 0,
            "successful_completions": 0,
            "failed_attempts": 0,
            "average_processing_time": 0.0,
            "start_time": datetime.now(),
        }

        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–
        self.resource_thresholds = {
            "cpu_max": 80.0,
            "memory_max": 85.0,
            "disk_max": 90.0,
        }

        logger.info("ğŸ¯ Optimized Issue Orchestrator åˆæœŸåŒ–å®Œäº†")

    def get_resource_status(self) -> Tuple[ResourceStatus, ResourceMetrics]:
        """ç¾åœ¨ã®ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³ã‚’å–å¾—"""
        try:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹å–å¾—
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage("/")

            metrics = ResourceMetrics(
                cpu_percent=cpu_percent,
                memory_percent=memory.percent,
                disk_usage_percent=(disk.used / disk.total) * 100,
                active_processes=len(self.active_tasks),
                timestamp=datetime.now(),
            )

            # çŠ¶æ³åˆ¤å®š
            if (
                cpu_percent > 95
                or memory.percent > 95
                or metrics.disk_usage_percent > 95
            ):
                status = ResourceStatus.CRITICAL
            elif (
                cpu_percent > self.resource_thresholds["cpu_max"]
                or memory.percent > self.resource_thresholds["memory_max"]
                or metrics.disk_usage_percent > self.resource_thresholds["disk_max"]
            ):
                status = ResourceStatus.OVERLOAD
            elif len(self.active_tasks) >= self.max_concurrent_tasks:
                status = ResourceStatus.BUSY
            else:
                status = ResourceStatus.AVAILABLE

            return status, metrics

        except Exception as e:
            logger.error(f"âŒ ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return ResourceStatus.CRITICAL, ResourceMetrics(0, 0, 0, 0, datetime.now())

    def prioritize_issue(self, issue: Issue) -> ProcessingPriority:
        """ã‚¤ã‚·ãƒ¥ãƒ¼ã®å„ªå…ˆåº¦ã‚’æ±ºå®š"""
        try:
            priority_score = 0

            # ãƒ©ãƒ™ãƒ«ãƒ™ãƒ¼ã‚¹ã®å„ªå…ˆåº¦
            label_priorities = {
                "critical": 10,
                "high": 8,
                "urgent": 8,
                "bug": 6,
                "feature": 4,
                "enhancement": 3,
                "documentation": 2,
                "question": 1,
            }

            for label in issue.labels:
                priority_score += label_priorities.get(label.name.lower(), 0)

            # ä½œæˆæ—¥ã‹ã‚‰ã®çµŒéæ™‚é–“ï¼ˆå¤ã„ã»ã©å„ªå…ˆï¼‰
            age_days = (datetime.now() - issue.created_at.replace(tzinfo=None)).days
            if age_days > 7:
                priority_score += 3
            elif age_days > 3:
                priority_score += 2
            elif age_days > 1:
                priority_score += 1

            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¤ã‚·ãƒ¥ãƒ¼ã»ã©å„ªå…ˆï¼‰
            if issue.comments > 5:
                priority_score += 2
            elif issue.comments > 2:
                priority_score += 1

            # ã‚¢ã‚µã‚¤ãƒ³çŠ¶æ³
            if issue.assignee:
                priority_score += 1

            # å„ªå…ˆåº¦åˆ†é¡
            if priority_score >= 12:
                return ProcessingPriority.CRITICAL
            elif priority_score >= 8:
                return ProcessingPriority.HIGH
            elif priority_score >= 4:
                return ProcessingPriority.MEDIUM
            else:
                return ProcessingPriority.LOW

        except Exception as e:
            logger.warning(f"å„ªå…ˆåº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return ProcessingPriority.MEDIUM

    def estimate_processing_time(
        self, issue: Issue, priority: ProcessingPriority
    ) -> float:
        """å‡¦ç†æ™‚é–“ã‚’æ¨å®š"""
        try:
            base_time = 300.0  # 5åˆ†ãƒ™ãƒ¼ã‚¹

            # å„ªå…ˆåº¦ã«ã‚ˆã‚‹èª¿æ•´
            priority_multipliers = {
                ProcessingPriority.CRITICAL: 0.8,  # é«˜å„ªå…ˆåº¦ã¯è¿…é€Ÿå‡¦ç†
                ProcessingPriority.HIGH: 1.0,
                ProcessingPriority.MEDIUM: 1.2,
                ProcessingPriority.LOW: 1.5,
            }

            # ã‚¤ã‚·ãƒ¥ãƒ¼ã®è¤‡é›‘åº¦ã«ã‚ˆã‚‹èª¿æ•´
            complexity_multiplier = 1.0

            # ã‚¿ã‚¤ãƒˆãƒ«ã®é•·ã•
            if len(issue.title) > 100:
                complexity_multiplier += 0.3
            elif len(issue.title) > 50:
                complexity_multiplier += 0.1

            # æœ¬æ–‡ã®é•·ã•
            if issue.body and len(issue.body) > 1000:
                complexity_multiplier += 0.5
            elif issue.body and len(issue.body) > 500:
                complexity_multiplier += 0.2

            # ãƒ©ãƒ™ãƒ«æ•°
            if len(issue.labels) > 5:
                complexity_multiplier += 0.2

            estimated_time = (
                base_time * priority_multipliers[priority] * complexity_multiplier
            )

            # æœ€å°ãƒ»æœ€å¤§åˆ¶é™
            return max(60.0, min(1800.0, estimated_time))  # 1åˆ†ã€œ30åˆ†

        except Exception as e:
            logger.warning(f"å‡¦ç†æ™‚é–“æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
            return 300.0

    async def scan_and_queue_issues(self) -> int:
        """ã‚¤ã‚·ãƒ¥ãƒ¼ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ """
        if not self.repo:
            logger.error("GitHubãƒªãƒã‚¸ãƒˆãƒªãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return 0

        try:
            logger.info("ğŸ” ã‚¤ã‚·ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ãƒ³é–‹å§‹")

            # ã‚ªãƒ¼ãƒ—ãƒ³ã‚¤ã‚·ãƒ¥ãƒ¼ã‚’å–å¾—
            open_issues = list(self.repo.get_issues(state="open"))
            queued_count = 0

            for issue in open_issues:
                # æ—¢ã«å‡¦ç†ä¸­ã¾ãŸã¯ã‚­ãƒ¥ãƒ¼ã«å…¥ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if issue.number in self.active_tasks or any(
                    task.issue.number == issue.number for task in self.task_queue
                ):
                    continue

                # å®Œäº†ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã§å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
                existing_record = self.completion_manager.get_issue_record(issue.number)
                if existing_record and existing_record.status.value in [
                    "completed",
                    "pr_created",
                ]:
                    continue

                # å‡¦ç†ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
                priority = self.prioritize_issue(issue)
                estimated_time = self.estimate_processing_time(issue, priority)

                task = ProcessingTask(
                    issue=issue,
                    priority=priority,
                    estimated_time=estimated_time,
                    retry_count=existing_record.retry_count if existing_record else 0,
                    scheduled_at=datetime.now(),
                    metadata={
                        "labels": [label.name for label in issue.labels],
                        "created_at": issue.created_at.isoformat(),
                        "comments": issue.comments,
                        "assignee": issue.assignee.login if issue.assignee else None,
                    },
                )

                self.task_queue.append(task)
                queued_count += 1

            # å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
            self.task_queue.sort(key=lambda t: (t.priority.value, t.scheduled_at))

            logger.info(
                f"ğŸ“‹ ã‚¤ã‚·ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ãƒ³å®Œäº†: {queued_count}ä»¶ã‚­ãƒ¥ãƒ¼è¿½åŠ , ç·ã‚­ãƒ¥ãƒ¼æ•°: {len(self.task_queue)}"
            )
            return queued_count

        except Exception as e:
            logger.error(f"âŒ ã‚¤ã‚·ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    async def process_single_issue(self, task: ProcessingTask) -> bool:
        """å˜ä¸€ã‚¤ã‚·ãƒ¥ãƒ¼ã‚’å‡¦ç†"""
        issue_number = task.issue.number

        try:
            logger.info(f"ğŸš€ ã‚¤ã‚·ãƒ¥ãƒ¼å‡¦ç†é–‹å§‹: #{issue_number} (å„ªå…ˆåº¦: {task.priority.name})")
            start_time = time.time()

            # å®Œäº†ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã«å‡¦ç†é–‹å§‹ã‚’è¨˜éŒ²
            self.completion_manager.start_issue_processing(
                issue_number, task.issue.title, task.metadata
            )

            # æ‹¡å¼µå‡¦ç†å™¨ã§å‡¦ç†å®Ÿè¡Œ
            result = await self.processor.process_issue_with_pr(task.issue)

            processing_time = time.time() - start_time

            # çµæœã«åŸºã¥ã„ã¦å®Œäº†è¨˜éŒ²
            if result.get("success", False):
                completion_result = CompletionResult.SUCCESS
                error_message = None

                # PRä½œæˆè¨˜éŒ²
                if result.get("pr_created", False):
                    self.completion_manager.record_pr_creation(
                        issue_number, result.get("pr_number"), result.get("pr_url")
                    )

                logger.info(f"âœ… ã‚¤ã‚·ãƒ¥ãƒ¼å‡¦ç†æˆåŠŸ: #{issue_number} ({processing_time:0.2f}s)")

            else:
                completion_result = CompletionResult.FAILED
                error_message = result.get("error", "Unknown error")
                logger.warning(f"âŒ ã‚¤ã‚·ãƒ¥ãƒ¼å‡¦ç†å¤±æ•—: #{issue_number} - {error_message}")

            # å®Œäº†è¨˜éŒ²
            self.completion_manager.complete_issue(
                issue_number, completion_result, error_message
            )

            # çµ±è¨ˆæ›´æ–°
            self.processing_statistics["total_processed"] += 1
            if completion_result == CompletionResult.SUCCESS:
                self.processing_statistics["successful_completions"] += 1
            else:
                self.processing_statistics["failed_attempts"] += 1

            # å¹³å‡å‡¦ç†æ™‚é–“æ›´æ–°
            total_time = self.processing_statistics["average_processing_time"] * (
                self.processing_statistics["total_processed"] - 1
            )
            self.processing_statistics["average_processing_time"] = (
                total_time + processing_time
            ) / self.processing_statistics["total_processed"]

            return completion_result == CompletionResult.SUCCESS

        except Exception as e:
            logger.error(f"âŒ ã‚¤ã‚·ãƒ¥ãƒ¼å‡¦ç†ä¾‹å¤–: #{issue_number} - {e}")

            # ã‚¨ãƒ©ãƒ¼è¨˜éŒ²
            self.completion_manager.complete_issue(
                issue_number, CompletionResult.FAILED, str(e)
            )
            self.processing_statistics["failed_attempts"] += 1

            return False

        finally:
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¿ã‚¹ã‚¯ã‹ã‚‰å‰Šé™¤
            if issue_number in self.active_tasks:
                del self.active_tasks[issue_number]

    async def execute_processing_cycle(self) -> Dict[str, Any]cycle_start = time.time()logger.info("ğŸ”„ å‡¦ç†ã‚µã‚¤ã‚¯ãƒ«é–‹å§‹")
    """ç†ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œ"""
:
        try:
            # ã‚¤ã‚·ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ãƒ³
            new_issues = await self.scan_and_queue_issues()

            # ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³ç¢ºèª
            resource_status, resource_metrics = self.get_resource_status()
            logger.info(
                (
                    f"f"ğŸ’» ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³: {resource_status.value} (CPU: {resource_metrics.cpu_percent:0.1f}%, Memory: "
                    f"{resource_metrics.memory_percent:0.1f}%)""
                )
            )

            # å‡¦ç†å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            if resource_status == ResourceStatus.CRITICAL:
                logger.warning("âš ï¸ ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ã«ã‚ˆã‚Šå‡¦ç†ã‚’å»¶æœŸ")
                return {
                    "status": "delayed",
                    "reason": "resource_critical",
                    "resource_metrics": resource_metrics.__dict__,
                }

            # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
            tasks_to_process = []
            available_slots = self.max_concurrent_tasks - len(self.active_tasks)

            # ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³ã«å¿œã˜ã¦ä¸¦åˆ—åº¦èª¿æ•´
            if resource_status == ResourceStatus.OVERLOAD:
                available_slots = min(available_slots, 1)
            elif resource_status == ResourceStatus.BUSY:
                available_slots = min(available_slots, 2)

            # ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œã®ãŸã‚ã«æº–å‚™
            for _ in range(min(available_slots, len(self.task_queue))):
                if not self.task_queue:
                    break

                task = self.task_queue.pop(0)
                self.active_tasks[task.issue.number] = task
                tasks_to_process.append(self.process_single_issue(task))

            # ä¸¦åˆ—å®Ÿè¡Œ
            if tasks_to_process:
                logger.info(f"âš¡ ä¸¦åˆ—å‡¦ç†é–‹å§‹: {len(tasks_to_process)}ã‚¿ã‚¹ã‚¯")
                results = await asyncio.gather(
                    *tasks_to_process, return_exceptions=True
                )
                successful_count = sum(1 for r in results if r is True)
                logger.info(f"ğŸ“Š ä¸¦åˆ—å‡¦ç†å®Œäº†: {successful_count}/{len(results)}æˆåŠŸ")

            cycle_time = time.time() - cycle_start

            return {
                "status": "completed",
                "new_issues_queued": new_issues,
                "tasks_processed": len(tasks_to_process),
                "successful_tasks": successful_count if tasks_to_process else 0,
                "cycle_time": cycle_time,
                "queue_size": len(self.task_queue),
                "active_tasks": len(self.active_tasks),
                "resource_status": resource_status.value,
                "resource_metrics": resource_metrics.__dict__,
            }

        except Exception as e:
            logger.error(f"âŒ å‡¦ç†ã‚µã‚¤ã‚¯ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "status": "error",
                "error": str(e),
                "cycle_time": time.time() - cycle_start,
            }

    async def run_continuous_processing(
        self, cycles: int = None, cycle_interval: int = 600
    ):
        """ç¶™ç¶šçš„å‡¦ç†ã‚’å®Ÿè¡Œ"""
        logger.info(f"ğŸš€ ç¶™ç¶šçš„å‡¦ç†é–‹å§‹ (ã‚µã‚¤ã‚¯ãƒ«é–“éš”: {cycle_interval}ç§’)")

        cycle_count = 0

        try:
            while cycles is None or cycle_count < cycles:
                cycle_result = await self.execute_processing_cycle()

                logger.info(f"ğŸ“‹ ã‚µã‚¤ã‚¯ãƒ«{cycle_count + 1}å®Œäº†: {cycle_result['status']}")

                # çµ±è¨ˆãƒ­ã‚°
                if cycle_count % 10 == 0:  # 10ã‚µã‚¤ã‚¯ãƒ«ã”ã¨
                    self.log_processing_statistics()

                cycle_count += 1

                # ã‚µã‚¤ã‚¯ãƒ«é–“éš”å¾…æ©Ÿ
                if cycles is None or cycle_count < cycles:
                    await asyncio.sleep(cycle_interval)

        except KeyboardInterrupt:
            logger.info("âŒ¨ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸­æ–­: ç¶™ç¶šçš„å‡¦ç†ã‚’åœæ­¢")
        except Exception as e:
            logger.error(f"âŒ ç¶™ç¶šçš„å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

        logger.info(f"ğŸ ç¶™ç¶šçš„å‡¦ç†çµ‚äº† (ç·ã‚µã‚¤ã‚¯ãƒ«æ•°: {cycle_count})")

    def log_processing_statistics(self):
        """å‡¦ç†çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›"""
        stats = self.processing_statistics
        uptime = datetime.now() - stats["start_time"]

        logger.info(
            f"""
"ğŸ“Š" å‡¦ç†çµ±è¨ˆã‚µãƒãƒªãƒ¼:
   - ç¨¼åƒæ™‚é–“: {uptime}
   - ç·å‡¦ç†æ•°: {stats['total_processed']}
   - æˆåŠŸæ•°: {stats['successful_completions']}
   - å¤±æ•—æ•°: {stats['failed_attempts']}
   - æˆåŠŸç‡: {(stats['successful_completions'] / max(stats['total_processed'], 1)) * 100:0.1f}%
   - å¹³å‡å‡¦ç†æ™‚é–“: {stats['average_processing_time']:0.2f}ç§’
   - ã‚­ãƒ¥ãƒ¼æ•°: {len(self.task_queue)}
   - ã‚¢ã‚¯ãƒ†ã‚£ãƒ–: {len(self.active_tasks)}
        """
        )

    def get_status_report(self) -> Dict[str, Any]resource_status, resource_metrics = self.get_resource_status():
    """ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—"""

        return {:
            "orchestrator_status": {
                "active_tasks": len(self.active_tasks),
                "queued_tasks": len(self.task_queue),
                "max_concurrent": self.max_concurrent_tasks,
            },
            "processing_statistics": self.processing_statistics.copy(),
            "resource_status": {
                "status": resource_status.value,
                "metrics": resource_metrics.__dict__,
                "thresholds": self.resource_thresholds,
            },
            "completion_statistics": self.completion_manager.get_completion_statistics(
                1
            ),  # ç›´è¿‘24æ™‚é–“
            "timestamp": datetime.now().isoformat(),
        }


# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
async def main()logger.info("ğŸ¯ Optimized Issue Orchestrator ãƒ†ã‚¹ãƒˆé–‹å§‹")
"""ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""

    orchestrator = OptimizedIssueOrchestrator(max_concurrent_tasks=2)

    # å˜ä¸€ã‚µã‚¤ã‚¯ãƒ«ãƒ†ã‚¹ãƒˆ
    result = await orchestrator.execute_processing_cycle()
    logger.info(f"ğŸ“‹ å‡¦ç†ã‚µã‚¤ã‚¯ãƒ«çµæœ: {result}")

    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ
    status = orchestrator.get_status_report()
    logger.info(f"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")

    logger.info("ğŸ Optimized Issue Orchestrator ãƒ†ã‚¹ãƒˆå®Œäº†")


if __name__ == "__main__":
    asyncio.run(main())
