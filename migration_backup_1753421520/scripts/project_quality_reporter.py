#!/usr/bin/env python3
"""
"ğŸ“Š" ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå“è³ªãƒ¬ãƒãƒ¼ã‚¿ãƒ¼
ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰å“è³ªåŸºæº–ã«åŸºã¥ãç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

HTML/JSON/Markdownå½¢å¼ã§ã®å‡ºåŠ›å¯¾å¿œ
"""

import asyncio
import json
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict
from typing import List
from typing import Optional

from rich.console import Console
from rich.panel import Panel
from rich.table import Table

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from libs.elder_council_summoner import ElderCouncilSummoner

console = Console()


class ProjectQualityReporter:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        self.console = console
        self.project_root = PROJECT_ROOT
        self.projects_dir = self.project_root / "projects"
        self.reports_dir = self.project_root / "quality_reports"
        self.reports_dir.mkdir(exist_ok=True)
        self.summoner = ElderCouncilSummoner()

    async def generate_report(self, project_name: Optional[str] = None):
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if project_name:
            # ç‰¹å®šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¬ãƒãƒ¼ãƒˆ
            await self.generate_project_report(project_name)
        else:
            # å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
            await self.generate_summary_report()

    async def generate_project_report(self, project_name: str):
        """å€‹åˆ¥ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        project_path = self.projects_dir / project_name

        if not project_path.exists():
            self.console.print(
                f"[red]ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ '{project_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“[/red]"
            )
            return

        self.console.print(
            Panel(
                f"ğŸ“Š å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {project_name}",
                title="Quality Report Generation",
                border_style="bright_blue",
            )
        )

        # ãƒ‡ãƒ¼ã‚¿åé›†
        report_data = await self.collect_project_data(project_path, project_name)

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆ3å½¢å¼ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_base = f"{project_name}_quality_report_{timestamp}"

        # JSONå½¢å¼
        json_path = self.reports_dir / f"{report_base}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        # Markdownå½¢å¼
        md_path = self.reports_dir / f"{report_base}.md"
        await self.generate_markdown_report(report_data, md_path)

        # HTMLå½¢å¼
        html_path = self.reports_dir / f"{report_base}.html"
        await self.generate_html_report(report_data, html_path)

        self.console.print(
            Panel(
                f"âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†\n\n"
                f"ğŸ“„ JSON: {json_path.name}\n"
                f"ğŸ“ Markdown: {md_path.name}\n"
                f"ğŸŒ HTML: {html_path.name}",
                title="ğŸ‰ å®Œäº†",
                border_style="green",
            )
        )

        # ã‚¨ãƒ«ãƒ€ãƒ¼è©•è­°ä¼šã¸ã®æå‡º
        await self.submit_to_elders(report_data)

    async def collect_project_data(self, project_path: Path, project_name: str) -> Dict:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿åé›†"""
        data = {
            "project_name": project_name,
            "generated_at": datetime.now().isoformat(),
            "overview": {},
            "quality_metrics": {},
            "test_results": {},
            "code_analysis": {},
            "pdca_status": {},
            "elders_compliance": {},
            "recommendations": [],
        }

        # åŸºæœ¬æƒ…å ±
        data["overview"] = await self.collect_overview(project_path)

        # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        data["quality_metrics"] = await self.collect_quality_metrics(project_path)

        # ãƒ†ã‚¹ãƒˆçµæœ
        data["test_results"] = await self.collect_test_results(project_path)

        # ã‚³ãƒ¼ãƒ‰åˆ†æ
        data["code_analysis"] = await self.collect_code_analysis(project_path)

        # PDCAçŠ¶æ³
        data["pdca_status"] = await self.collect_pdca_status(project_path)

        # ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰æº–æ‹ çŠ¶æ³
        data["elders_compliance"] = await self.check_elders_compliance(project_path)

        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        data["recommendations"] = self.generate_recommendations(data)

        return data

    async def collect_overview(self, project_path: Path) -> Dict:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦åé›†"""
        overview = {
            "created_at": None,
            "last_modified": None,
            "total_files": 0,
            "total_lines": 0,
            "languages": {},
            "framework": None,
            "size_mb": 0,
        }

        # ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ
        for ext in [".py", ".ts", ".tsx", ".js", ".jsx"]:
            files = list(project_path.rglob(f"*{ext}"))
            if files:
                lang = {
                    ".py": "Python",
                    ".ts": "TypeScript",
                    ".tsx": "TypeScript React",
                    ".js": "JavaScript",
                    ".jsx": "JavaScript React",
                }[ext]

                total_lines = 0
                for file in files:
                    try:
                        # Deep nesting detected (depth: 5) - consider refactoring
                        with open(file, "r", encoding="utf-8") as f:
                            total_lines += len(f.readlines())
                    except:
                        pass

                overview["languages"][lang] = {
                    "files": len(files),
                    "lines": total_lines,
                }
                overview["total_files"] += len(files)
                overview["total_lines"] += total_lines

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µã‚¤ã‚º
        total_size = sum(
            f.stat().st_size for f in project_path.rglob("*") if f.is_file()
        )
        overview["size_mb"] = round(total_size / (1024 * 1024), 2)

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        if (project_path / ".pdca" / "pdca_history.json").exists():
            with open(project_path / ".pdca" / "pdca_history.json", "r") as f:
                pdca = json.load(f)
                overview["created_at"] = pdca.get("created_at")

        overview["last_modified"] = datetime.fromtimestamp(
            max(f.stat().st_mtime for f in project_path.rglob("*") if f.is_file())
        ).isoformat()

        return overview

    async def collect_quality_metrics(self, project_path: Path) -> Dict:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        metrics = {
            "test_coverage": 0,
            "code_quality_score": 0,
            "documentation_coverage": 0,
            "type_coverage": 0,
            "security_score": 0,
            "performance_score": 0,
        }

        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸
        backend_path = project_path / "backend"
        if backend_path.exists():
            try:
                result = subprocess.run(
                    ["pytest", "--cov=app", "--cov-report=json", "--quiet"],
                    cwd=backend_path,
                    capture_output=True,
                    text=True,
                )

                coverage_file = backend_path / "coverage.json"
                if coverage_file.exists():
                    with open(coverage_file, "r") as f:
                        coverage_data = json.load(f)
                        metrics["test_coverage"] = coverage_data.get("totals", {}).get(
                            "percent_covered", 0
                        )
            except:
                pass

        # ã‚³ãƒ¼ãƒ‰å“è³ªã‚¹ã‚³ã‚¢ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        score = 70
        if (project_path / ".gitignore").exists():
            score += 5
        if (project_path / "README.md").exists():
            score += 10
        if (project_path / "docker-compose.yml").exists():
            score += 10
        if (project_path / ".github" / "workflows").exists():
            score += 5
        metrics["code_quality_score"] = min(100, score)

        # ãã®ä»–ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆTODO: å®Ÿè£…ï¼‰
        metrics["documentation_coverage"] = 65
        metrics["type_coverage"] = 75
        metrics["security_score"] = 85
        metrics["performance_score"] = 80

        return metrics

    async def collect_test_results(self, project_path: Path) -> Dict:
        """ãƒ†ã‚¹ãƒˆçµæœåé›†"""
        results = {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "duration_seconds": 0,
            "test_types": {},
        }

        # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
        backend_path = project_path / "backend"
        if (backend_path / "tests").exists():
            try:
                result = subprocess.run(
                    ["pytest", "--json-report", "--json-report-file=test_report.json"],
                    cwd=backend_path,
                    capture_output=True,
                    text=True,
                )

                report_file = backend_path / "test_report.json"
                if report_file.exists():
                    with open(report_file, "r") as f:
                        test_data = json.load(f)
                        summary = test_data.get("summary", {})
                        results["total_tests"] = summary.get("total", 0)
                        results["passed"] = summary.get("passed", 0)
                        results["failed"] = summary.get("failed", 0)
                        results["skipped"] = summary.get("skipped", 0)
                        results["duration_seconds"] = test_data.get("duration", 0)
            except:
                pass

        # ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ—åˆ†é¡
        results["test_types"] = {
            "unit": results["total_tests"] * 0.7,  # ä»®ã®å€¤
            "integration": results["total_tests"] * 0.2,
            "e2e": results["total_tests"] * 0.1,
        }

        return results

    async def collect_code_analysis(self, project_path: Path) -> Dict:
        """ã‚³ãƒ¼ãƒ‰åˆ†æçµæœåé›†"""
        analysis = {"complexity": {}, "duplication": {}, "issues": [], "hotspots": []}

        # è¤‡é›‘åº¦åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
        python_files = list(project_path.rglob("*.py"))
        if python_files:
            total_complexity = 0
            complex_functions = []

            for file in python_files[:10]:  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                try:
                    with open(file, "r", encoding="utf-8") as f:
                        content = f.read()
                        # ç°¡æ˜“çš„ãªè¤‡é›‘åº¦è¨ˆç®—ï¼ˆif, for, while, tryã®æ•°ï¼‰
                        complexity = (
                            content.count("if ")
                            + content.count("for ")
                            + content.count("while ")
                            + content.count("try:")
                        )
                        total_complexity += complexity
                        if not (complexity > 10):
                            continue  # Early return to reduce nesting
                        # Reduced nesting - original condition satisfied
                        if complexity > 10:
                            complex_functions.append(
                                {
                                    "file": str(file.relative_to(project_path)),
                                    "complexity": complexity,
                                }
                            )
                except:
                    pass

            analysis["complexity"] = {
                "average": total_complexity / len(python_files) if python_files else 0,
                "high_complexity_files": complex_functions[:5],
            }

        # èª²é¡Œæ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        issues = []
        if not (project_path / ".gitignore").exists():
            issues.append(
                {
                    "severity": "medium",
                    "type": "missing_file",
                    "message": ".gitignoreãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                }
            )

        if (
            not (project_path / "requirements.txt").exists()
            and (project_path / "backend").exists()
        ):
            issues.append(
                {
                    "severity": "high",
                    "type": "missing_file",
                    "message": "requirements.txtãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                }
            )

        analysis["issues"] = issues

        return analysis

    async def collect_pdca_status(self, project_path: Path) -> Dict:
        """PDCAçŠ¶æ³åé›†"""
        status = {
            "cycles_completed": 0,
            "last_cycle_date": None,
            "improvements_proposed": 0,
            "improvements_implemented": 0,
            "current_phase": "Plan",
        }

        pdca_file = project_path / ".pdca" / "pdca_history.json"
        if pdca_file.exists():
            with open(pdca_file, "r") as f:
                pdca_data = json.load(f)
                status["cycles_completed"] = len(pdca_data.get("cycles", []))

                if pdca_data.get("cycles"):
                    last_cycle = pdca_data["cycles"][-1]
                    status["last_cycle_date"] = last_cycle.get("timestamp")

                improvements = pdca_data.get("improvements", [])
                status["improvements_proposed"] = len(improvements)
                status["improvements_implemented"] = len(
                    [i for i in improvements if i.get("status") == "implemented"]
                )

        return status

    async def check_elders_compliance(self, project_path: Path) -> Dict:
        """ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰æº–æ‹ ãƒã‚§ãƒƒã‚¯"""
        compliance = {
            "tdd_enabled": False,
            "four_sages_integrated": False,
            "quality_dashboard": False,
            "cicd_pipeline": False,
            "compliance_score": 0,
        }

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
        if (project_path / "elders_config.json").exists():
            with open(project_path / "elders_config.json", "r") as f:
                config = json.load(f)
                sages = config.get("four_sages", {})
                compliance["four_sages_integrated"] = all(
                    sage.get("enabled", False) for sage in sages.values()
                )

        # CI/CDãƒã‚§ãƒƒã‚¯
        if (project_path / ".github" / "workflows").exists():
            compliance["cicd_pipeline"] = True

        # TDDãƒã‚§ãƒƒã‚¯
        if (project_path / "pytest.ini").exists() or (
            project_path / "backend" / "pytest.ini"
        ).exists():
            compliance["tdd_enabled"] = True

        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        score = 0
        if compliance["tdd_enabled"]:
            score += 25
        if compliance["four_sages_integrated"]:
            score += 25
        if compliance["cicd_pipeline"]:
            score += 25
        if compliance["quality_dashboard"]:
            score += 25

        compliance["compliance_score"] = score

        return compliance

    def generate_recommendations(self, data: Dict) -> List[Dict]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸
        coverage = data["quality_metrics"]["test_coverage"]
        if coverage < 80:
            recommendations.append(
                {
                    "category": "testing",
                    "priority": "high",
                    "title": "ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸å‘ä¸Š",
                    "description": f"ç¾åœ¨ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ {coverage:0.1f}% ã‚’80%ä»¥ä¸Šã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™",
                    "actions": [
                        "æœªãƒ†ã‚¹ãƒˆã®ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹ã‚’ç‰¹å®š",
                        "ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã®è¿½åŠ ",
                        "çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè£…",
                    ],
                }
            )

        # ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰æº–æ‹ 
        compliance_score = data["elders_compliance"]["compliance_score"]
        if compliance_score < 100:
            missing = []
            if not data["elders_compliance"]["tdd_enabled"]:
                missing.append("TDD")
            if not data["elders_compliance"]["four_sages_integrated"]:
                missing.append("4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ ")
            if not data["elders_compliance"]["cicd_pipeline"]:
                missing.append("CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")

            recommendations.append(
                {
                    "category": "compliance",
                    "priority": "medium",
                    "title": "ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰æº–æ‹ åº¦å‘ä¸Š",
                    "description": f"æº–æ‹ ã‚¹ã‚³ã‚¢ {compliance_score}% - ä»¥ä¸‹ã®è¦ç´ ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
                    "actions": missing,
                }
            )

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        if data["quality_metrics"]["documentation_coverage"] < 70:
            recommendations.append(
                {
                    "category": "documentation",
                    "priority": "medium",
                    "title": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå……å®ŸåŒ–",
                    "description": "ã‚³ãƒ¼ãƒ‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰ã®æ”¹å–„",
                    "actions": [
                        "ä¸»è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¸ã®docstringè¿½åŠ ",
                        "APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è‡ªå‹•ç”Ÿæˆ",
                        "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰ã®ä½œæˆ",
                    ],
                }
            )

        return recommendations

    async def generate_markdown_report(self, data: Dict, output_path: Path):
        """Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        md_content = f"""# ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå“è³ªãƒ¬ãƒãƒ¼ãƒˆ: {data['project_name']}

ç”Ÿæˆæ—¥æ™‚: {data['generated_at']}

## ğŸ“‹ æ¦‚è¦

- **ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {data['overview']['total_files']}
- **ç·è¡Œæ•°**: {data['overview']['total_lines']:,}
- **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µã‚¤ã‚º**: {data['overview']['size_mb']} MB
- **æœ€çµ‚æ›´æ–°**: {data['overview']['last_modified']}

### ä½¿ç”¨è¨€èª
"""

        for lang, stats in data["overview"]["languages"].items():
            md_content += (
                f"- **{lang}**: {stats['files']}ãƒ•ã‚¡ã‚¤ãƒ« ({stats['lines']:,}è¡Œ)\n"
            )

        md_content += f"""
## ğŸ¯ å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ã‚¹ã‚³ã‚¢ |
|-----------|--------|
| ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ | {data['quality_metrics']['test_coverage']:0.1f}% |
| ã‚³ãƒ¼ãƒ‰å“è³ª | {data['quality_metrics']['code_quality_score']}/100 |
| ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ | {data['quality_metrics']['documentation_coverage']:0.1f}% |
| å‹ã‚«ãƒãƒ¬ãƒƒã‚¸ | {data['quality_metrics']['type_coverage']:0.1f}% |
| ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ | {data['quality_metrics']['security_score']}/100 |
| ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ | {data['quality_metrics']['performance_score']}/100 |

## ğŸ§ª ãƒ†ã‚¹ãƒˆçµæœ

- **ç·ãƒ†ã‚¹ãƒˆæ•°**: {data['test_results']['total_tests']}
- **æˆåŠŸ**: {data['test_results']['passed']} âœ…
- **å¤±æ•—**: {data['test_results']['failed']} âŒ
- **ã‚¹ã‚­ãƒƒãƒ—**: {data['test_results']['skipped']} â­ï¸
- **å®Ÿè¡Œæ™‚é–“**: {data['test_results']['duration_seconds']:0.2f}ç§’

## ğŸ”„ PDCAçŠ¶æ³

- **å®Œäº†ã‚µã‚¤ã‚¯ãƒ«æ•°**: {data['pdca_status']['cycles_completed']}
- **æ”¹å–„ææ¡ˆæ•°**: {data['pdca_status']['improvements_proposed']}
- **å®Ÿè£…æ¸ˆã¿æ”¹å–„**: {data['pdca_status']['improvements_implemented']}
- **ç¾åœ¨ãƒ•ã‚§ãƒ¼ã‚º**: {data['pdca_status']['current_phase']}

## ğŸ›ï¸ ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰æº–æ‹ 

æº–æ‹ ã‚¹ã‚³ã‚¢: **{data['elders_compliance']['compliance_score']}%**

- TDDæœ‰åŠ¹: {'âœ…' if data['elders_compliance']['tdd_enabled'] else 'âŒ'}
- 4è³¢è€…çµ±åˆ: {'âœ…' if data['elders_compliance']['four_sages_integrated'] else 'âŒ'}
- CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: {'âœ…' if data['elders_compliance']['cicd_pipeline'] else 'âŒ'}

## ğŸ“Œ æ¨å¥¨äº‹é …
"""

        for rec in data["recommendations"]:
        # ç¹°ã‚Šè¿”ã—å‡¦ç†
            priority_emoji = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(
                rec["priority"], "âšª"
            )

            md_content += (
                f"\n### {priority_emoji} {rec['title']} ({rec['priority']})\n\n"
            )
            md_content += f"{rec['description']}\n\n"
            md_content += "**ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:**\n"
            for action in rec["actions"]:
                md_content += f"- {action}\n"

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(md_content)

    async def generate_html_report(self, data: Dict, output_path: Path):
        """HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        html_content = f"""<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å“è³ªãƒ¬ãƒãƒ¼ãƒˆ: {data['project_name']}</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
        }}
        .metric-card {{
            background: white;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .metric-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
        }}
        .metric-value {{
            font-size: 2em;
            font-weight: bold;
            color: #667eea;
        }}
        .progress-bar {{
            background: #e0e0e0;
            border-radius: 10px;
            height: 20px;
            overflow: hidden;
        }}
        .progress-fill {{
            background: #667eea;
            height: 100%;
            transition: width 0.3s;
        }}
        .recommendation {{
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin-bottom: 15px;
        }}
        .high-priority {{
            border-left-color: #dc3545;
            background: #f8d7da;
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1>"ğŸ“Š" ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå“è³ªãƒ¬ãƒãƒ¼ãƒˆ</h1>
        <h2>{data['project_name']}</h2>
        <p>ç”Ÿæˆæ—¥æ™‚: {data['generated_at']}</p>
    </div>

    <div class="metric-card">
        <h3>"ğŸ“ˆ" å“è³ªã‚¹ã‚³ã‚¢ã‚µãƒãƒªãƒ¼</h3>
        <div class="metric-grid">
            <div>
                <div class="metric-label">ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸</div>
                <div class="metric-value">{data['quality_metrics']['test_coverage']:0.1f}%</div>
                <div class="progress-bar">
                    <div class="progress-fill" style="width: {data['quality_metrics']['test_coverage']}%"></div>
                </div>
            </div>
            <div>
                <div class="metric-label">ã‚³ãƒ¼ãƒ‰å“è³ª</div>
                <div class="metric-value">{data['quality_metrics']['code_quality_score']}/100</div>
                <div class="progress-bar">
                    <div class="progress-fill" style="width: {data['quality_metrics']['code_quality_score']}%"></div>
                </div>
            </div>
            <div>
                <div class="metric-label">ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºæº–æ‹ </div>
                <div class="metric-value">{data['elders_compliance']['compliance_score']}%</div>
                <div class="progress-bar">
                    <div class="progress-fill" style="width: {data['elders_compliance']['compliance_score']}%"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="metric-card">
        <h3>ğŸ§ª ãƒ†ã‚¹ãƒˆçµæœ</h3>
        <p>ç·ãƒ†ã‚¹ãƒˆæ•°: <strong>{data['test_results']['total_tests']}</strong></p>
        <p>æˆåŠŸ: <span style="color: green">âœ… {data['test_results']['passed']}</span> |
           å¤±æ•—: <span style="color: red">âŒ {data['test_results']['failed']}</span> |
           ã‚¹ã‚­ãƒƒãƒ—: <span style="color: orange">â­ï¸ {data['test_results']['skipped']}</span></p>
    </div>

    <div class="metric-card">
        <h3>ğŸ“Œ æ¨å¥¨äº‹é …</h3>
"""

        # ç¹°ã‚Šè¿”ã—å‡¦ç†
        for rec in data["recommendations"]:
            priority_class = "high-priority" if rec["priority"] == "high" else ""
            html_content += f"""
        <div class="recommendation {priority_class}">
            <h4>{rec['title']} ({rec['priority']})</h4>
            <p>{rec['description']}</p>
            <ul>
"""
            for action in rec["actions"]:
                html_content += f"                <li>{action}</li>\n"
            html_content += """            </ul>
        </div>
"""

        html_content += """
    </div>
</body>
</html>"""

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html_content)

    async def generate_summary_report(self):
        """å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        self.console.print(
            Panel(
                "ğŸ“Š å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå“è³ªã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ",
                title="Summary Report",
                border_style="bright_blue",
            )
        )

        # å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‡ãƒ¼ã‚¿åé›†
        all_projects = []
        for project_dir in self.projects_dir.iterdir():
            if project_dir.is_dir():
                try:
                    data = await self.collect_project_data(
                        project_dir, project_dir.name
                    )
                    all_projects.append(data)
                except Exception as e:
                    self.console.print(
                        f"[yellow]è­¦å‘Š: {project_dir.name} ã®ãƒ‡ãƒ¼ã‚¿åé›†ã«å¤±æ•—: {e}[/yellow]"
                    )

        if not all_projects:
            self.console.print("[red]ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“[/red]")
            return

        # ã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«
        table = Table(title="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå“è³ªã‚µãƒãƒªãƒ¼")
        table.add_column("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ", style="cyan")
        table.add_column("ã‚«ãƒãƒ¬ãƒƒã‚¸", style="magenta")
        table.add_column("å“è³ªã‚¹ã‚³ã‚¢", style="green")
        table.add_column("ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºæº–æ‹ ", style="yellow")
        table.add_column("æ¨å¥¨äº‹é …", style="red")

        for project in all_projects:
            coverage = f"{project['quality_metrics']['test_coverage']:0.1f}%"
            quality = f"{project['quality_metrics']['code_quality_score']}/100"
            compliance = f"{project['elders_compliance']['compliance_score']}%"
            recommendations = str(len(project["recommendations"]))

            table.add_row(
                project["project_name"], coverage, quality, compliance, recommendations
            )

        self.console.print(table)

        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary_path = self.reports_dir / f"all_projects_summary_{timestamp}.json"

        summary_data = {
            "generated_at": datetime.now().isoformat(),
            "total_projects": len(all_projects),
            "projects": all_projects,
            "average_metrics": self.calculate_average_metrics(all_projects),
        }

        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)

        self.console.print(f"\nâœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {summary_path.name}")

    def calculate_average_metrics(self, projects: List[Dict]) -> Dict:
        """å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        if not projects:
            return {}

        total_coverage = sum(p["quality_metrics"]["test_coverage"] for p in projects)
        total_quality = sum(
            p["quality_metrics"]["code_quality_score"] for p in projects
        )
        total_compliance = sum(
            p["elders_compliance"]["compliance_score"] for p in projects
        )

        return {
            "average_test_coverage": total_coverage / len(projects),
            "average_quality_score": total_quality / len(projects),
            "average_compliance_score": total_compliance / len(projects),
        }

    async def submit_to_elders(self, report_data: Dict):
        """ã‚¨ãƒ«ãƒ€ãƒ¼è©•è­°ä¼šã¸ã®æå‡º"""
        if hasattr(self.summoner, "submit_quality_report"):
            await self.summoner.submit_quality_report(
                {
                    "project_name": report_data["project_name"],
                    "timestamp": report_data["generated_at"],
                    "quality_score": report_data["quality_metrics"][
                        "code_quality_score"
                    ],
                    "test_coverage": report_data["quality_metrics"]["test_coverage"],
                    "compliance_score": report_data["elders_compliance"][
                        "compliance_score"
                    ],
                    "priority_recommendations": [
                        r
                        for r in report_data["recommendations"]
                        if r["priority"] == "high"
                    ],
                }
            )


async def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    reporter = ProjectQualityReporter()

    if len(sys.argv) > 1:
        # ç‰¹å®šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¬ãƒãƒ¼ãƒˆ
        project_name = sys.argv[1]
        await reporter.generate_report(project_name)
    else:
        # å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚µãƒãƒªãƒ¼
        await reporter.generate_report()


if __name__ == "__main__":
    asyncio.run(main())
