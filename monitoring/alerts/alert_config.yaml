---
# Elder Treeéšå±¤ã‚·ã‚¹ãƒ†ãƒ  ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
# Grand Elder maruçµ±æ²»ä¸‹ã®ç•°å¸¸æ¤œçŸ¥ä½“åˆ¶

alert_system:
  name: "Elder Tree Alert System"
  version: "1.0.0"
  enabled: true
  
# ã‚¢ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«å®šç¾©
alert_levels:
  critical:
    symbol: "ğŸ”´"
    priority: 1
    description: "ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ãƒªã‚¹ã‚¯ - å³åº§ã®å¯¾å¿œãŒå¿…è¦"
    notification:
      - slack: immediate
      - email: immediate
      - pager: immediate
    auto_escalate: true
    escalation_time: 300  # 5åˆ†
    
  high:
    symbol: "ğŸŸ "
    priority: 2
    description: "æ©Ÿèƒ½éšœå®³ç™ºç”Ÿ - æ—©æ€¥ãªå¯¾å¿œãŒå¿…è¦"
    notification:
      - slack: immediate
      - email: immediate
    auto_escalate: true
    escalation_time: 900  # 15åˆ†
    
  medium:
    symbol: "ğŸŸ¡"
    priority: 3
    description: "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ– - ç›£è¦–å¼·åŒ–ãŒå¿…è¦"
    notification:
      - slack: batch
      - email: hourly
    auto_escalate: false
    
  low:
    symbol: "ğŸŸ¢"
    priority: 4
    description: "æƒ…å ±é€šçŸ¥ - è¨˜éŒ²ã®ã¿"
    notification:
      - log: only
    auto_escalate: false

# Elder Treeæ¥ç¶šç•°å¸¸æ¤œçŸ¥
elder_tree_alerts:
  grand_elder_disconnection:
    level: critical
    condition: "grand_elder.status != 'active' for 30s"
    message: "Grand Elder maruæ¥ç¶šæ–­ - ã‚·ã‚¹ãƒ†ãƒ çµ±æ²»ä¸èƒ½"
    actions:
      - notify_all_channels
      - activate_emergency_protocol
      - attempt_auto_recovery
      
  elder_council_quorum_loss:
    level: critical
    condition: "active_council_members < 5"
    message: "Elder Councilå®šè¶³æ•°ä¸è¶³ - æ„æ€æ±ºå®šä¸èƒ½"
    actions:
      - notify_admins
      - activate_backup_elders
      
  elder_tree_communication_delay:
    level: high
    condition: "avg_response_time > 1000ms for 60s"
    message: "Elder Treeé€šä¿¡é…å»¶æ¤œçŸ¥"
    actions:
      - increase_monitoring_frequency
      - analyze_network_status

# ãƒ¯ãƒ¼ã‚«ãƒ¼ç•°å¸¸æ¤œçŸ¥
worker_alerts:
  worker_mass_failure:
    level: critical
    condition: "failed_workers > 16"  # 50%ä»¥ä¸Š
    message: "ãƒ¯ãƒ¼ã‚«ãƒ¼å¤§é‡éšœå®³ - å‡¦ç†èƒ½åŠ›50%ä»¥ä¸‹"
    actions:
      - emergency_worker_restart
      - scale_up_backup_workers
      
  worker_high_failure_rate:
    level: high
    condition: "failed_workers > 8"  # 25%ä»¥ä¸Š
    message: "ãƒ¯ãƒ¼ã‚«ãƒ¼éšœå®³ç‡ä¸Šæ˜‡"
    actions:
      - restart_failed_workers
      - investigate_failure_cause
      
  worker_resource_exhaustion:
    level: high
    condition: "avg_worker_cpu > 80% or avg_worker_memory > 85%"
    message: "ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡è­¦å‘Š"
    actions:
      - optimize_task_distribution
      - consider_scaling
      
  worker_idle_excess:
    level: medium
    condition: "idle_workers > 24 for 300s"  # 75%ä»¥ä¸ŠãŒ5åˆ†é–“ã‚¢ã‚¤ãƒ‰ãƒ«
    message: "ãƒ¯ãƒ¼ã‚«ãƒ¼ç¨¼åƒç‡ä½ä¸‹"
    actions:
      - analyze_task_queue
      - consider_scale_down

# Four Sagesç•°å¸¸æ¤œçŸ¥
four_sages_alerts:
  sage_unresponsive:
    level: critical
    condition: "any_sage.status != 'active' for 60s"
    message: "Sageå¿œç­”ãªã— - {sage_name}"
    actions:
      - restart_sage_service
      - activate_backup_sage
      
  incident_sage_overload:
    level: high
    condition: "incident_sage.active_incidents > 50"
    message: "Incident Sageéè² è· - ç•°å¸¸å‡¦ç†é…å»¶ãƒªã‚¹ã‚¯"
    actions:
      - prioritize_critical_incidents
      - scale_incident_workers
      
  knowledge_sage_storage_full:
    level: high
    condition: "knowledge_sage.storage_used > 90%"
    message: "Knowledge Sageã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸é€¼è¿«"
    actions:
      - archive_old_knowledge
      - expand_storage_capacity
      
  task_sage_queue_overflow:
    level: high
    condition: "task_sage.queue_length > 1000"
    message: "Task Sageã‚­ãƒ¥ãƒ¼æº¢ã‚Œè­¦å‘Š"
    actions:
      - increase_worker_allocation
      - optimize_task_scheduling
      
  rag_sage_slow_response:
    level: medium
    condition: "rag_sage.avg_response_time > 500ms"
    message: "RAG Sageå¿œç­”é…å»¶"
    actions:
      - optimize_indexes
      - cache_frequent_queries

# ãƒªã‚½ãƒ¼ã‚¹é–¾å€¤ã‚¢ãƒ©ãƒ¼ãƒˆ
resource_alerts:
  cpu_critical:
    level: critical
    condition: "system_cpu > 95% for 120s"
    message: "CPUä½¿ç”¨ç‡å±é™ºåŸŸ"
    actions:
      - identify_cpu_consumers
      - throttle_non_critical_tasks
      
  memory_critical:
    level: critical
    condition: "system_memory > 95%"
    message: "ãƒ¡ãƒ¢ãƒªæ¯æ¸‡è­¦å‘Š"
    actions:
      - trigger_garbage_collection
      - restart_memory_intensive_services
      
  disk_space_low:
    level: high
    condition: "disk_free < 10%"
    message: "ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³"
    actions:
      - cleanup_old_logs
      - archive_historical_data
      
  network_saturation:
    level: medium
    condition: "network_utilization > 80%"
    message: "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¸¯åŸŸé€¼è¿«"
    actions:
      - analyze_traffic_patterns
      - optimize_data_transfers

# ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œçŸ¥
anomaly_detection:
  patterns:
    - name: "cascade_failure"
      description: "é€£é–çš„éšœå®³ãƒ‘ã‚¿ãƒ¼ãƒ³"
      conditions:
        - "worker_failures increasing exponentially"
        - "multiple_sages reporting errors"
      level: critical
      
    - name: "performance_degradation"
      description: "æ®µéšçš„æ€§èƒ½åŠ£åŒ–"
      conditions:
        - "response_times increasing linearly"
        - "throughput decreasing"
      level: high
      
    - name: "unusual_activity"
      description: "ç•°å¸¸ãªæ´»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³"
      conditions:
        - "task_patterns != historical_baseline"
        - "access_patterns anomalous"
      level: medium

# ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥è¨­å®š
notifications:
  channels:
    slack:
      webhook_url: "${SLACK_WEBHOOK_URL}"
      critical_channel: "#alerts-critical"
      high_channel: "#alerts-high"
      default_channel: "#alerts-general"
      
    email:
      smtp_server: "smtp.gmail.com"
      smtp_port: 587
      from_address: "elder-tree-alerts@aicompany.com"
      admin_list:
        - "admin@aicompany.com"
        - "oncall@aicompany.com"
        
    pager:
      service: "pagerduty"
      api_key: "${PAGERDUTY_API_KEY}"
      
  templates:
    critical:
      subject: "ğŸ”´ CRITICAL: {alert_name}"
      body: |
        Elder Tree Critical Alert
        
        Alert: {alert_name}
        Level: CRITICAL
        Time: {timestamp}
        Message: {message}
        
        Affected Components:
        {components}
        
        Automated Actions Taken:
        {actions}
        
        Manual Intervention Required: YES
        
    high:
      subject: "ğŸŸ  HIGH: {alert_name}"
      body: |
        Elder Tree High Priority Alert
        
        Alert: {alert_name}
        Level: HIGH
        Time: {timestamp}
        Message: {message}
        
        Current Status:
        {status}
        
        Recommended Actions:
        {recommendations}

# ã‚¢ãƒ©ãƒ¼ãƒˆæŠ‘åˆ¶ãƒ«ãƒ¼ãƒ«
suppression_rules:
  - name: "maintenance_window"
    condition: "during_scheduled_maintenance"
    suppress: ["low", "medium"]
    
  - name: "known_issue"
    condition: "alert_id in known_issues_list"
    suppress: ["all"]
    duration: 3600  # 1æ™‚é–“
    
  - name: "duplicate_prevention"
    condition: "same_alert within 300s"
    suppress: ["duplicate"]
    
  - name: "storm_control"
    condition: "alert_rate > 100 per minute"
    action: "batch_and_summarize"

# è‡ªå‹•å›å¾©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
auto_recovery:
  enabled: true
  max_attempts: 3
  actions:
    restart_worker:
      command: "systemctl restart aicompany-worker@{worker_id}"
      timeout: 30
      
    restart_sage:
      command: "systemctl restart aicompany-sage-{sage_name}"
      timeout: 60
      
    clear_cache:
      command: "redis-cli FLUSHDB"
      timeout: 10
      
    scale_workers:
      command: "kubectl scale deployment workers --replicas={count}"
      timeout: 120

# ã‚¢ãƒ©ãƒ¼ãƒˆçµ±è¨ˆã¨ãƒ¬ãƒãƒ¼ãƒˆ
reporting:
  enabled: true
  intervals:
    - daily: "00:00"
    - weekly: "monday 00:00"
    - monthly: "1st 00:00"
    
  metrics:
    - total_alerts_by_level
    - mttr_by_component  # Mean Time To Recovery
    - false_positive_rate
    - auto_recovery_success_rate
    - top_10_recurring_alerts
    
  destinations:
    - email: "reports@aicompany.com"
    - slack: "#monitoring-reports"
    - dashboard: "/monitoring/reports/"