#!/usr/bin/env python3
"""
RAGè³¢è€…ãƒ—ãƒ­ã‚»ã‚¹ - æƒ…å ±æ¤œç´¢ã¨ç†è§£ã®å°‚é–€å®¶
Elder Soul - A2A Architecture
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

import sys
sys.path.append(str(Path(__file__).parent.parent))

from libs.elder_process_base import (
    ElderProcessBase,
    ElderRole,
    SageType,
    ElderMessage,
    MessageType
)


@dataclass
class SearchResult:
    """æ¤œç´¢çµæœ"""
    doc_id: str
    content: str
    score: float
    source: str
    metadata: Dict[str, Any]


@dataclass
class DocumentIndex:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
    doc_id: str
    title: str
    content: str
    embedding: Optional[List[float]] = None
    tags: List[str] = None
    created_at: datetime = None
    last_accessed: datetime = None
    access_count: int = 0


class RAGSageProcess(ElderProcessBase):
    """
    RAGè³¢è€… - æƒ…å ±æ¤œç´¢ã¨ç†è§£ã®å°‚é–€ãƒ—ãƒ­ã‚»ã‚¹

    è²¬å‹™:
    - æƒ…å ±ã®æ¤œç´¢ã¨å–å¾—
    - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ç†è§£ã¨è¦ç´„
    - é–¢é€£æƒ…å ±ã®æ¨è«–
    - æ¤œç´¢çµæœã®æœ€é©åŒ–
    """

    def __init__(self):
        super().__init__(
            elder_name="rag_sage",
            elder_role=ElderRole.SAGE,
            sage_type=SageType.RAG,
            port=5004
        )

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.document_index: Dict[str, DocumentIndex] = {}
        self.embeddings_cache: Dict[str, List[float]] = {}

        # æ¤œç´¢è¨­å®š
        self.search_config = {
            "max_results": 10,
            "min_score": 0.5,
            "use_semantic": True,
            "use_keyword": True
        }

        # ãƒ‘ã‚¹è¨­å®š
        self.data_dir = Path("data/rag_sage")
        self.data_dir.mkdir(parents=True, exist_ok=True)

    async def initialize(self):
        """åˆæœŸåŒ–å‡¦ç†"""
        self.logger.info("ğŸ” Initializing RAG Sage...")

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿
        await self._load_document_index()

        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        await self._initialize_embedding_model()

        self.logger.info(f"âœ… RAG Sage initialized with {len(self.document_index)} documents")

    async def process(self):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æœ€é©åŒ–ï¼ˆ1æ™‚é–“ã”ã¨ï¼‰
        if not hasattr(self, '_last_optimization') or \
           (datetime.now() - self._last_optimization).total_seconds() > 3600:
            await self._optimize_index()
            self._last_optimization = datetime.now()

        # æ¤œç´¢å±¥æ­´ã®åˆ†æï¼ˆ30åˆ†ã”ã¨ï¼‰
        if not hasattr(self, '_last_analysis') or \
           (datetime.now() - self._last_analysis).total_seconds() > 1800:
            await self._analyze_search_patterns()
            self._last_analysis = datetime.now()

    async def handle_message(self, message: ElderMessage):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†"""
        self.logger.info(f"Received {message.message_type.value} from {message.source_elder}")

        if message.message_type == MessageType.COMMAND:
            await self._handle_command(message)
        elif message.message_type == MessageType.QUERY:
            await self._handle_query(message)
        elif message.message_type == MessageType.REPORT:
            await self._handle_report(message)

    def register_handlers(self):
        """è¿½åŠ ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç™»éŒ²"""
        # åŸºæœ¬ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§ååˆ†
        pass

    async def on_cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†"""
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä¿å­˜
        await self._save_document_index()

        # æ¤œç´¢çµ±è¨ˆã®ä¿å­˜
        await self._save_search_statistics()

    # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰

    async def _load_document_index(self):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿"""
        index_file = self.data_dir / "document_index.json"
        if index_file.exists():
            try:
                with open(index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                    for doc_data in data.get('documents', []):
                        doc = DocumentIndex(
                            doc_id=doc_data['id'],
                            title=doc_data['title'],
                            content=doc_data['content']
                        )
                        doc.tags = doc_data.get('tags', [])
                        doc.access_count = doc_data.get('access_count', 0)

                        if doc_data.get('embedding'):
                            doc.embedding = doc_data['embedding']
                            self.embeddings_cache[doc.doc_id] = doc.embedding

                        self.document_index[doc.doc_id] = doc

                self.logger.info(f"Loaded {len(self.document_index)} documents")

            except Exception as e:
                self.logger.error(f"Failed to load document index: {e}")

    async def _initialize_embedding_model(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        # å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã®ä»£ã‚ã‚Šã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        self.logger.info("Initializing embedding model...")
        await asyncio.sleep(1)
        self.embedding_dim = 384  # ä»®ã®æ¬¡å…ƒæ•°

    async def _save_document_index(self):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä¿å­˜"""
        try:
            documents_data = []
            for doc in self.document_index.values():
                doc_data = {
                    'id': doc.doc_id,
                    'title': doc.title,
                    'content': doc.content,
                    'tags': doc.tags,
                    'access_count': doc.access_count,
                    'created_at': doc.created_at.isoformat() if doc.created_at else None
                }

                # åŸ‹ã‚è¾¼ã¿ãŒã‚ã‚Œã°ä¿å­˜
                if doc.doc_id in self.embeddings_cache:
                    doc_data['embedding'] = self.embeddings_cache[doc.doc_id]

                documents_data.append(doc_data)

            index_file = self.data_dir / "document_index.json"
            with open(index_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'documents': documents_data,
                    'last_updated': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)

            self.logger.info("Document index saved successfully")

        except Exception as e:
            self.logger.error(f"Failed to save document index: {e}")

    async def _save_search_statistics(self):
        """æ¤œç´¢çµ±è¨ˆã®ä¿å­˜"""
        stats_file = self.data_dir / "search_statistics.json"
        try:
            # çµ±è¨ˆæƒ…å ±ã®åé›†
            total_docs = len(self.document_index)
            total_accesses = sum(doc.access_count for doc in self.document_index.values())

            stats = {
                'total_documents': total_docs,
                'total_accesses': total_accesses,
                'average_access_per_doc': total_accesses / total_docs if total_docs > 0 else 0,
                'last_updated': datetime.now().isoformat()
            }

            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)

        except Exception as e:
            self.logger.error(f"Failed to save search statistics: {e}")

    async def _handle_command(self, message: ElderMessage):
        """ã‚³ãƒãƒ³ãƒ‰å‡¦ç†"""
        command = message.payload.get('command')

        if command == 'index_document':
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
            await self._index_document(message.payload)

        elif command == 'update_index':
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ›´æ–°
            await self._update_document_index(message.payload)

        elif command == 'execute_task':
            # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œï¼ˆã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼ã‹ã‚‰ï¼‰
            await self._execute_rag_task(message.payload)

    async def _handle_query(self, message: ElderMessage):
        """ã‚¯ã‚¨ãƒªå‡¦ç†"""
        query_type = message.payload.get('query_type')

        if query_type == 'search':
            # æ¤œç´¢å®Ÿè¡Œ
            results = await self._perform_search(message.payload)

            response_msg = ElderMessage(
                message_id=f"search_response_{message.message_id}",
                source_elder=self.elder_name,
                target_elder=message.source_elder,
                message_type=MessageType.REPORT,
                payload={'search_results': results},
                priority=message.priority
            )

            await self.send_message(response_msg)

        elif query_type == 'summarize':
            # è¦ç´„ç”Ÿæˆ
            summary = await self._generate_summary(message.payload)

            response_msg = ElderMessage(
                message_id=f"summary_response_{message.message_id}",
                source_elder=self.elder_name,
                target_elder=message.source_elder,
                message_type=MessageType.REPORT,
                payload={'summary': summary},
                priority=message.priority
            )

            await self.send_message(response_msg)

        elif query_type == 'availability':
            # åˆ©ç”¨å¯èƒ½æ€§ç¢ºèª
            response_msg = ElderMessage(
                message_id=f"availability_response_{message.message_id}",
                source_elder=self.elder_name,
                target_elder=message.source_elder,
                message_type=MessageType.REPORT,
                payload={
                    'available': True,
                    'capacity': 0.85,
                    'indexed_documents': len(self.document_index),
                    'cache_hit_rate': 0.75
                },
                priority=message.priority
            )

            await self.send_message(response_msg)

    async def _handle_report(self, message: ElderMessage):
        """ãƒ¬ãƒãƒ¼ãƒˆå‡¦ç†"""
        report_type = message.payload.get('type')

        if report_type == 'search_feedback':
            # æ¤œç´¢ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®å‡¦ç†
            await self._process_search_feedback(message.payload)

        elif report_type == 'document_update':
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°é€šçŸ¥
            await self._handle_document_update(message.payload)

    async def _index_document(self, payload: Dict[str, Any]):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–"""
        doc_id = f"doc_{datetime.now().timestamp()}"
        title = payload.get('title', 'Untitled')
        content = payload.get('content', '')
        tags = payload.get('tags', [])

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        doc = DocumentIndex(
            doc_id=doc_id,
            title=title,
            content=content,
            tags=tags,
            created_at=datetime.now()
        )

        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        embedding = await self._generate_embedding(content)
        doc.embedding = embedding
        self.embeddings_cache[doc_id] = embedding

        self.document_index[doc_id] = doc

        self.logger.info(f"Indexed document: {title} ({doc_id})")

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†é€šçŸ¥
        await self.report_to_superior({
            'type': 'document_indexed',
            'doc_id': doc_id,
            'title': title
        })

    async def _generate_embedding(self, text: str) -> List[float]:
        """åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        # å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«æ¨è«–ã®ä»£ã‚ã‚Šã«ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
        await asyncio.sleep(0.1)  # æ¨è«–æ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

        # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ã‚·ãƒ¼ãƒ‰å€¤ã§ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’å›ºå®š
        seed = sum(ord(c) for c in text[:100])
        np.random.seed(seed % 2**32)

        return np.random.randn(self.embedding_dim).tolist()

    async def _perform_search(self, query_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ¤œç´¢å®Ÿè¡Œ"""
        query_text = query_data.get('query', '')
        filters = query_data.get('filters', {})
        max_results = query_data.get('max_results', self.search_config['max_results'])

        self.logger.info(f"Performing search: {query_text[:50]}...")

        results = []

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
        if self.search_config['use_semantic']:
            semantic_results = await self._semantic_search(query_text, filters)
            results.extend(semantic_results)

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
        if self.search_config['use_keyword']:
            keyword_results = await self._keyword_search(query_text, filters)
            results.extend(keyword_results)

        # é‡è¤‡é™¤å»ã¨ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        unique_results = {}
        for result in results:
            doc_id = result['doc_id']
            if doc_id not in unique_results or result['score'] > unique_results[doc_id]['score']:
                unique_results[doc_id] = result

        # ä¸Šä½Nä»¶ã‚’è¿”ã™
        sorted_results = sorted(unique_results.values(), key=lambda x: x['score'], reverse=True)
        final_results = sorted_results[:max_results]

        # ã‚¢ã‚¯ã‚»ã‚¹ã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°
        for result in final_results:
            doc_id = result['doc_id']
            if doc_id in self.document_index:
                self.document_index[doc_id].access_count += 1
                self.document_index[doc_id].last_accessed = datetime.now()

        return final_results

    async def _semantic_search(self, query: str, filters: Dict) -> List[Dict[str, Any]]:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢"""
        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        query_embedding = await self._generate_embedding(query)

        results = []

        for doc_id, doc in self.document_index.items():
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒã‚§ãƒƒã‚¯
            if filters.get('tags'):
                if not any(tag in doc.tags for tag in filters['tags']):
                    continue

            # åŸ‹ã‚è¾¼ã¿ãŒã‚ã‚‹å ´åˆã®ã¿
            if doc_id in self.embeddings_cache:
                doc_embedding = self.embeddings_cache[doc_id]

                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
                similarity = cosine_similarity(
                    [query_embedding],
                    [doc_embedding]
                )[0][0]

                if similarity >= self.search_config['min_score']:
                    results.append({
                        'doc_id': doc_id,
                        'title': doc.title,
                        'content': doc.content[:200] + "...",
                        'score': float(similarity),
                        'type': 'semantic',
                        'tags': doc.tags
                    })

        return results

    async def _keyword_search(self, query: str, filters: Dict) -> List[Dict[str, Any]]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢"""
        query_terms = query.lower().split()
        results = []

        for doc_id, doc in self.document_index.items():
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒã‚§ãƒƒã‚¯
            if filters.get('tags'):
                if not any(tag in doc.tags for tag in filters['tags']):
                    continue

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
            content_lower = doc.content.lower()
            title_lower = doc.title.lower()

            score = 0.0
            for term in query_terms:
                # ã‚¿ã‚¤ãƒˆãƒ«ã§ã®å‡ºç¾ï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
                if term in title_lower:
                    score += 3.0

                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ã®å‡ºç¾
                score += content_lower.count(term) * 0.5

            if score > 0:
                # æ­£è¦åŒ–
                normalized_score = min(1.0, score / (len(query_terms) * 3))

                if normalized_score >= self.search_config['min_score']:
                    results.append({
                        'doc_id': doc_id,
                        'title': doc.title,
                        'content': doc.content[:200] + "...",
                        'score': float(normalized_score),
                        'type': 'keyword',
                        'tags': doc.tags
                    })

        return results

    async def _generate_summary(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """è¦ç´„ç”Ÿæˆ"""
        doc_ids = payload.get('doc_ids', [])
        summary_type = payload.get('type', 'extractive')

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåé›†
        documents = []
        for doc_id in doc_ids:
            if doc_id in self.document_index:
                documents.append(self.document_index[doc_id])

        if not documents:
            return {'error': 'No documents found'}

        # è¦ç´„ç”Ÿæˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        if summary_type == 'extractive':
            # æŠ½å‡ºå‹è¦ç´„
            summary = await self._extractive_summary(documents)
        else:
            # ç”Ÿæˆå‹è¦ç´„
            summary = await self._abstractive_summary(documents)

        return {
            'summary': summary,
            'document_count': len(documents),
            'type': summary_type
        }

    async def _extractive_summary(self, documents: List[DocumentIndex]) -> str:
        """æŠ½å‡ºå‹è¦ç´„ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        # å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰é‡è¦ãªæ–‡ã‚’æŠ½å‡º
        important_sentences = []

        for doc in documents[:3]:  # æœ€å¤§3ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            sentences = doc.content.split('.')[:2]  # æœ€åˆã®2æ–‡
            important_sentences.extend(sentences)

        summary = '. '.join(important_sentences)
        return summary[:500] + "..."

    async def _abstractive_summary(self, documents: List[DocumentIndex]) -> str:
        """ç”Ÿæˆå‹è¦ç´„ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        # å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ã®ä»£ã‚ã‚Šã«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
        titles = [doc.title for doc in documents[:5]]

        summary = f"Summary of {len(documents)} documents: "
        summary += f"The documents cover topics including {', '.join(titles)}. "
        summary += "Key insights include various aspects of the discussed subjects."

        return summary

    async def _optimize_index(self):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æœ€é©åŒ–"""
        self.logger.info("Optimizing document index...")

        # ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ã®ä½ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è­˜åˆ¥
        low_access_docs = [
            doc for doc in self.document_index.values()
            if doc.access_count < 5 and doc.created_at and
            (datetime.now() - doc.created_at).days > 30
        ]

        if low_access_docs:
            self.logger.info(f"Found {len(low_access_docs)} low-access documents")

            # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ãŸã‚åŸ‹ã‚è¾¼ã¿ã‚’ã‚¯ãƒªã‚¢ï¼ˆå¿…è¦æ™‚ã«å†ç”Ÿæˆï¼‰
            for doc in low_access_docs[:10]:
                if doc.doc_id in self.embeddings_cache:
                    del self.embeddings_cache[doc.doc_id]

    async def _analyze_search_patterns(self):
        """æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        self.logger.info("Analyzing search patterns...")

        # ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        popular_docs = sorted(
            self.document_index.values(),
            key=lambda x: x.access_count,
            reverse=True
        )[:10]

        if popular_docs:
            # åˆ†æçµæœã‚’ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼ã«å ±å‘Š
            analysis_report = ElderMessage(
                message_id=f"search_analysis_{datetime.now().timestamp()}",
                source_elder=self.elder_name,
                target_elder="claude_elder",
                message_type=MessageType.REPORT,
                payload={
                    'type': 'search_analysis',
                    'popular_documents': [
                        {'title': doc.title, 'access_count': doc.access_count}
                        for doc in popular_docs
                    ],
                    'total_documents': len(self.document_index),
                    'cache_size': len(self.embeddings_cache)
                },
                priority=5
            )

            await self.send_message(analysis_report)

    async def _process_search_feedback(self, feedback: Dict[str, Any]):
        """æ¤œç´¢ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®å‡¦ç†"""
        doc_id = feedback.get('doc_id')
        relevance = feedback.get('relevance', 0.5)

        if doc_id in self.document_index:
            # é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã®æ›´æ–°ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            doc = self.document_index[doc_id]

            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ã„ã¦ã‚¢ã‚¯ã‚»ã‚¹ã‚«ã‚¦ãƒ³ãƒˆã‚’èª¿æ•´
            if relevance > 0.7:
                doc.access_count += 2
            elif relevance < 0.3:
                doc.access_count = max(0, doc.access_count - 1)

            self.logger.info(f"Processed feedback for doc {doc_id}: relevance={relevance}")

    async def _handle_document_update(self, update_data: Dict[str, Any]):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°é€šçŸ¥ã®å‡¦ç†"""
        doc_id = update_data.get('doc_id')

        if doc_id in self.document_index:
            doc = self.document_index[doc_id]

            # å†…å®¹æ›´æ–°
            if 'content' in update_data:
                doc.content = update_data['content']
                # åŸ‹ã‚è¾¼ã¿å†ç”ŸæˆãŒå¿…è¦
                embedding = await self._generate_embedding(doc.content)
                doc.embedding = embedding
                self.embeddings_cache[doc_id] = embedding

            # ã‚¿ã‚°æ›´æ–°
            if 'tags' in update_data:
                doc.tags = update_data['tags']

            self.logger.info(f"Updated document: {doc_id}")

    async def _execute_rag_task(self, task_data: Dict[str, Any]):
        """RAGã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œ"""
        task_id = task_data.get('task_id')
        description = task_data.get('description', '')

        self.logger.info(f"Executing RAG task {task_id}: {description}")

        # RAGé–¢é€£ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œ
        if "search" in description.lower() or "find" in description.lower():
            result = await self._execute_search_task(task_data)
        elif "summarize" in description.lower():
            result = await self._execute_summary_task(task_data)
        else:
            # ä¸€èˆ¬çš„ãªRAGã‚¿ã‚¹ã‚¯
            result = await self._execute_general_rag_task(task_data)

        # å®Œäº†å ±å‘Š
        completion_msg = ElderMessage(
            message_id=f"task_complete_{task_id}",
            source_elder=self.elder_name,
            target_elder="claude_elder",
            message_type=MessageType.REPORT,
            payload={
                'type': 'task_complete',
                'task_id': task_id,
                'result': result
            },
            priority=6
        )

        await self.send_message(completion_msg)

    async def _execute_search_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¤œç´¢ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œ"""
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        await asyncio.sleep(2)

        return {
            'status': 'completed',
            'search_results': {
                'found': 15,
                'relevant': 8,
                'top_result': 'Relevant document found'
            }
        }

    async def _execute_summary_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¦ç´„ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œ"""
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        await asyncio.sleep(3)

        return {
            'status': 'completed',
            'summary': {
                'type': 'extractive',
                'length': 500,
                'key_points': 5
            }
        }

    async def _execute_general_rag_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸€èˆ¬çš„ãªRAGã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œ"""
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        await asyncio.sleep(1)

        return {
            'status': 'completed',
            'message': 'RAG task completed successfully'
        }


# ãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•
if __name__ == "__main__":
    from libs.elder_process_base import run_elder_process
    run_elder_process(RAGSageProcess)
