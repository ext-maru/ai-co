#!/usr/bin/env python3
"""
A2Aé€šä¿¡ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³è©³ç´°åˆ†æã‚·ã‚¹ãƒ†ãƒ 
æ¤œå‡ºã•ã‚ŒãŸ10ä»¶ã®ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©³ç´°ã«åˆ†æã—ã€å¯¾å¿œç­–ã‚’ææ¡ˆ
"""

import json
import sqlite3
import sys
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Any
from typing import Dict
from typing import List

import numpy as np

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


class A2AAnomalyAnalyzer:
    """A2Aé€šä¿¡ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã®ç¢ºèª
        self.a2a_db_path = PROJECT_ROOT / "db" / "a2a_monitoring.db"
        if not self.a2a_db_path.exists():
            self.a2a_db_path = PROJECT_ROOT / "logs" / "a2a_monitoring.db"

        self.analysis_cache = {}

    def analyze_anomaly_patterns(self) -> Dict[str, Any]:
        """ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°åˆ†æ"""
        print("ğŸ” ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³10ä»¶ã®è©³ç´°åˆ†æã‚’é–‹å§‹...")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é€šä¿¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        conn = sqlite3connect(self.a2a_db_path)
        cursor = conn.cursor()

        # å…¨é€šä¿¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        query = """
        SELECT id, timestamp, source_agent, target_agent, message_type,
               priority, payload_size, response_time, status, error_message, metadata
        FROM a2a_communications
        ORDER BY timestamp DESC
        """

        cursor.execute(query)
        communications = cursor.fetchall()
        conn.close()

        if not communications:
            return {"error": "é€šä¿¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}

        # é€šä¿¡ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
        features = self._extract_features(communications)

        # ç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè¡Œ
        anomalies = self._detect_anomalies_detailed(features, communications)

        # ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        anomaly_analysis = self._analyze_anomaly_characteristics(anomalies)

        # å¯¾å¿œç­–ã®ææ¡ˆ
        recommendations = self._generate_recommendations(anomaly_analysis)

        return {
            "timestamp": datetime.now().isoformat(),
            "total_communications": len(communications),
            "anomalies_detected": len(anomalies),
            "anomaly_details": anomalies,
            "pattern_analysis": anomaly_analysis,
            "recommendations": recommendations,
            "severity_assessment": self._assess_severity(anomalies),
        }

    def _extract_features(self, communications: List[tuple]) -> np.ndarray:
        """é€šä¿¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º"""
        features = []

        for comm in communications:
            (
                comm_id,
                timestamp,
                source,
                target,
                msg_type,
                priority,
                payload_size,
                response_time,
                status,
                error_msg,
                metadata,
            ) = comm

            # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆ20æ¬¡å…ƒï¼‰
            feature_vector = np.zeros(20)

            # æ™‚é–“çš„ç‰¹å¾´
            dt = datetime.fromisoformat(timestamp)
            feature_vector[0] = dt.hour  # æ™‚é–“
            feature_vector[1] = dt.weekday()  # æ›œæ—¥

            # é€šä¿¡ç‰¹å¾´
            feature_vector[2] = len(source) if source else 0
            feature_vector[3] = len(target) if target else 0
            feature_vector[4] = len(msg_type) if msg_type else 0
            feature_vector[5] = priority if priority else 0
            feature_vector[6] = payload_size if payload_size else 0
            feature_vector[7] = response_time if response_time else 0

            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç‰¹å¾´
            feature_vector[8] = 1 if status == "success" else 0
            feature_vector[9] = 1 if status == "error" else 0
            feature_vector[10] = len(error_msg) if error_msg else 0

            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç‰¹å¾´
            feature_vector[11] = hash(source) % 100 / 100.0 if source else 0
            feature_vector[12] = hash(target) % 100 / 100.0 if target else 0
            feature_vector[13] = hash(msg_type) % 100 / 100.0 if msg_type else 0

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´
            try:
                meta = json.loads(metadata) if metadata else {}
                feature_vector[14] = len(str(meta))
                feature_vector[15] = 1 if "error" in str(meta).lower() else 0
                feature_vector[16] = 1 if "timeout" in str(meta).lower() else 0
                feature_vector[17] = 1 if "retry" in str(meta).lower() else 0
            except:
                pass

            # è¿½åŠ çµ±è¨ˆç‰¹å¾´
            feature_vector[18] = (
                np.log1p(response_time) if response_time and response_time > 0 else 0
            )
            feature_vector[19] = 1 if payload_size and payload_size > 10000 else 0

            features.append(feature_vector)

        return np.array(features)

    def _detect_anomalies_detailed(
        self, features: np.ndarray, communications: List[tuple]
    ) -> List[Dict]:
        """è©³ç´°ãªç•°å¸¸æ¤œçŸ¥"""
        if len(features) < 10:
            return []

        try:
            from sklearn.ensemble import IsolationForest
            from sklearn.preprocessing import StandardScaler

            # ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)

            # Isolation Forestã§ç•°å¸¸æ¤œçŸ¥
            clf = IsolationForest(contamination=0.05, random_state=42)
            predictions = clf.fit_predict(features_scaled)
            anomaly_scores = clf.score_samples(features_scaled)

            # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            anomalies = []
            for i, (pred, score) in enumerate(zip(predictions, anomaly_scores)):
                if pred == -1:  # ç•°å¸¸
                    comm = communications[i]
                    anomaly = {
                        "index": i,
                        "anomaly_score": float(score),
                        "communication_id": comm[0],
                        "timestamp": comm[1],
                        "source_agent": comm[2],
                        "target_agent": comm[3],
                        "message_type": comm[4],
                        "priority": comm[5],
                        "payload_size": comm[6],
                        "response_time": comm[7],
                        "status": comm[8],
                        "error_message": comm[9],
                        "metadata": comm[10],
                        "feature_vector": features[i].tolist(),
                    }
                    anomalies.append(anomaly)

            # ç•°å¸¸ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
            anomalies.sort(key=lambda x: x["anomaly_score"])

            return anomalies[:10]  # ä¸Šä½10ä»¶

        except ImportError:
            print("âš ï¸ sklearnæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - ç°¡å˜ãªçµ±è¨ˆçš„ç•°å¸¸æ¤œçŸ¥ã‚’ä½¿ç”¨")
            return self._simple_anomaly_detection(features, communications)

    def _simple_anomaly_detection(
        self, features: np.ndarray, communications: List[tuple]
    ) -> List[Dict]:
        """ç°¡å˜ãªçµ±è¨ˆçš„ç•°å¸¸æ¤œçŸ¥"""
        anomalies = []

        # å„ç‰¹å¾´é‡ã®çµ±è¨ˆ
        means = np.mean(features, axis=0)
        stds = np.std(features, axis=0)

        for i, (feature_vec, comm) in enumerate(zip(features, communications)):
            # Z-scoreãƒ™ãƒ¼ã‚¹ã®ç•°å¸¸æ¤œçŸ¥
            z_scores = np.abs((feature_vec - means) / (stds + 1e-8))
            max_z_score = np.max(z_scores)

            if max_z_score > 3.0:  # ç•°å¸¸é–¾å€¤
                anomaly = {
                    "index": i,
                    "anomaly_score": -float(max_z_score),  # è² ã®å€¤ã§çµ±ä¸€
                    "communication_id": comm[0],
                    "timestamp": comm[1],
                    "source_agent": comm[2],
                    "target_agent": comm[3],
                    "message_type": comm[4],
                    "priority": comm[5],
                    "payload_size": comm[6],
                    "response_time": comm[7],
                    "status": comm[8],
                    "error_message": comm[9],
                    "metadata": comm[10],
                    "max_z_score": float(max_z_score),
                    "anomalous_features": [
                        j for j, z in enumerate(z_scores) if z > 3.0
                    ],
                }
                anomalies.append(anomaly)

        # ç•°å¸¸ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        anomalies.sort(key=lambda x: x["anomaly_score"])

        return anomalies[:10]  # ä¸Šä½10ä»¶

    def _analyze_anomaly_characteristics(self, anomalies: List[Dict]) -> Dict[str, Any]:
        """ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å¾´åˆ†æ"""
        if not anomalies:
            return {"error": "åˆ†æã™ã‚‹ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“"}

        analysis = {
            "temporal_patterns": self._analyze_temporal_patterns(anomalies),
            "agent_patterns": self._analyze_agent_patterns(anomalies),
            "performance_patterns": self._analyze_performance_patterns(anomalies),
            "error_patterns": self._analyze_error_patterns(anomalies),
            "severity_distribution": self._analyze_severity_distribution(anomalies),
        }

        return analysis

    def _analyze_temporal_patterns(self, anomalies: List[Dict]) -> Dict[str, Any]:
        """æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        timestamps = [datetime.fromisoformat(a["timestamp"]) for a in anomalies]

        hours = [dt.hour for dt in timestamps]
        weekdays = [dt.weekday() for dt in timestamps]

        return {
            "peak_hours": Counter(hours).most_common(3),
            "peak_weekdays": Counter(weekdays).most_common(3),
            "time_clustering": self._check_time_clustering(timestamps),
        }

    def _analyze_agent_patterns(self, anomalies: List[Dict]) -> Dict[str, Any]:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        sources = [a["source_agent"] for a in anomalies if a["source_agent"]]
        targets = [a["target_agent"] for a in anomalies if a["target_agent"]]
        message_types = [a["message_type"] for a in anomalies if a["message_type"]]

        return {
            "problematic_sources": Counter(sources).most_common(5),
            "problematic_targets": Counter(targets).most_common(5),
            "problematic_message_types": Counter(message_types).most_common(5),
            "communication_flows": Counter(
                [
                    f"{a['source_agent']} -> {a['target_agent']}"
                    for a in anomalies
                    if a["source_agent"] and a["target_agent"]
                ]
            ).most_common(5),
        }

    def _analyze_performance_patterns(self, anomalies: List[Dict]) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        response_times = [a["response_time"] for a in anomalies if a["response_time"]]
        payload_sizes = [a["payload_size"] for a in anomalies if a["payload_size"]]

        return {
            "response_time_stats": {
                "mean": np.mean(response_times) if response_times else 0,
                "median": np.median(response_times) if response_times else 0,
                "max": np.max(response_times) if response_times else 0,
                "min": np.min(response_times) if response_times else 0,
            },
            "payload_size_stats": {
                "mean": np.mean(payload_sizes) if payload_sizes else 0,
                "median": np.median(payload_sizes) if payload_sizes else 0,
                "max": np.max(payload_sizes) if payload_sizes else 0,
                "min": np.min(payload_sizes) if payload_sizes else 0,
            },
            "slow_responses": len([rt for rt in response_times if rt > 1.0]),
            "large_payloads": len([ps for ps in payload_sizes if ps > 10000]),
        }

    def _analyze_error_patterns(self, anomalies: List[Dict]) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        error_statuses = [a["status"] for a in anomalies]
        error_messages = [a["error_message"] for a in anomalies if a["error_message"]]

        return {
            "status_distribution": Counter(error_statuses),
            "common_error_keywords": self._extract_error_keywords(error_messages),
            "error_rate": (
                len([s for s in error_statuses if s == "error"]) / len(error_statuses)
                if error_statuses
                else 0
            ),
        }

    def _extract_error_keywords(self, error_messages: List[str]) -> List[tuple]:
        """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        if not error_messages:
            return []

        keywords = []
        for msg in error_messages:
            words = msg.lower().split()
            keywords.extend(words)

        return Counter(keywords).most_common(10)

    def _analyze_severity_distribution(self, anomalies: List[Dict]) -> Dict[str, Any]:
        """é‡è¦åº¦åˆ†å¸ƒã®åˆ†æ"""
        priorities = [a["priority"] for a in anomalies if a["priority"]]
        anomaly_scores = [a["anomaly_score"] for a in anomalies]

        return {
            "priority_distribution": Counter(priorities),
            "anomaly_score_stats": {
                "mean": np.mean(anomaly_scores),
                "median": np.median(anomaly_scores),
                "min": np.min(anomaly_scores),
                "max": np.max(anomaly_scores),
            },
        }

    def _check_time_clustering(self, timestamps: List[datetime]) -> bool:
        """æ™‚é–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’ãƒã‚§ãƒƒã‚¯"""
        if len(timestamps) < 2:
            return False

        # æ™‚é–“é–“éš”ã‚’è¨ˆç®—
        intervals = []
        for i in range(1, len(timestamps)):
            interval = (timestamps[i] - timestamps[i - 1]).total_seconds()
            intervals.append(interval)

        # çŸ­ã„é–“éš”ï¼ˆ1åˆ†ä»¥å†…ï¼‰ãŒå¤šã„å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        short_intervals = [i for i in intervals if i < 60]
        return len(short_intervals) > len(intervals) * 0.5

    def _assess_severity(self, anomalies: List[Dict]) -> Dict[str, Any]:
        """é‡è¦åº¦è©•ä¾¡"""
        if not anomalies:
            return {"level": "none", "score": 0}

        severity_factors = {
            "error_rate": 0,
            "response_time_issues": 0,
            "agent_concentration": 0,
            "time_clustering": 0,
        }

        # ã‚¨ãƒ©ãƒ¼ç‡
        error_count = len([a for a in anomalies if a["status"] == "error"])
        severity_factors["error_rate"] = error_count / len(anomalies)

        # å¿œç­”æ™‚é–“å•é¡Œ
        slow_responses = len(
            [a for a in anomalies if a["response_time"] and a["response_time"] > 1.0]
        )
        severity_factors["response_time_issues"] = slow_responses / len(anomalies)

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé›†ä¸­åº¦
        sources = [a["source_agent"] for a in anomalies if a["source_agent"]]
        if sources:
            most_common_source_count = Counter(sources).most_common(1)[0][1]
            severity_factors["agent_concentration"] = most_common_source_count / len(
                anomalies
            )

        # æ™‚é–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        timestamps = [datetime.fromisoformat(a["timestamp"]) for a in anomalies]
        severity_factors["time_clustering"] = (
            1.0 if self._check_time_clustering(timestamps) else 0.0
        )

        # ç·åˆã‚¹ã‚³ã‚¢
        total_score = sum(severity_factors.values()) / len(severity_factors)

        if total_score > 0.7:
            level = "critical"
        elif total_score > 0.5:
            level = "high"
        elif total_score > 0.3:
            level = "medium"
        else:
            level = "low"

        return {"level": level, "score": total_score, "factors": severity_factors}

    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """å¯¾å¿œç­–ã®ææ¡ˆ"""
        recommendations = []

        if "error" in analysis:
            return ["ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™"]

        # æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæ¨å¥¨
        temporal = analysis.get("temporal_patterns", {})
        if temporal.get("time_clustering"):
            recommendations.append(
                "ğŸ• çŸ­æ™‚é–“ã§ã®ç•°å¸¸é›†ä¸­ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ è² è·ã®åˆ†æ•£ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæ¨å¥¨
        agent = analysis.get("agent_patterns", {})
        if agent.get("problematic_sources"):
            top_source = agent["problematic_sources"][0]
            recommendations.append(
                f"ğŸ¤– ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ '{top_source[0]}' ãŒç•°å¸¸ã®{top_source[1]}ä»¶ã«é–¢ä¸ã—ã¦ã„ã¾ã™ã€‚ç›£è¦–ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„ã€‚"
            )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæ¨å¥¨
        performance = analysis.get("performance_patterns", {})
        if performance.get("slow_responses", 0) > 5:
            recommendations.append(
                "â±ï¸ å¿œç­”æ™‚é–“ã®é…å»¶ãŒå¤šç™ºã—ã¦ã„ã¾ã™ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚"
            )

        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæ¨å¥¨
        error = analysis.get("error_patterns", {})
        if error.get("error_rate", 0) > 0.5:
            recommendations.append(
                "ğŸš¨ ã‚¨ãƒ©ãƒ¼ç‡ãŒ50%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ç·Šæ€¥ã®å¯¾å¿œãŒå¿…è¦ã§ã™ã€‚"
            )

        if not recommendations:
            recommendations.append(
                "âœ… ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ¤œå‡ºã•ã‚Œã¾ã—ãŸãŒã€é‡å¤§ãªå•é¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ç¶™ç¶šçš„ãªç›£è¦–ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        return recommendations


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    analyzer = A2AAnomalyAnalyzer()

    print("ğŸš€ A2Aé€šä¿¡ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³è©³ç´°åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)

    # ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Ÿè¡Œ
    analysis_result = analyzer.analyze_anomaly_patterns()

    if "error" in analysis_result:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {analysis_result['error']}")
        return

    # çµæœè¡¨ç¤º
    print("\nğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼")
    print("-" * 40)
    print(f"ç·é€šä¿¡æ•°: {analysis_result['total_communications']:,}")
    print(f"ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º: {analysis_result['anomalies_detected']}ä»¶")
    print(f"é‡è¦åº¦ãƒ¬ãƒ™ãƒ«: {analysis_result['severity_assessment']['level'].upper()}")
    print(f"é‡è¦åº¦ã‚¹ã‚³ã‚¢: {analysis_result['severity_assessment']['score']:0.3f}")

    # ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°
    print("\nğŸ” ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³è©³ç´°")
    print("-" * 40)
    for i, anomaly in enumerate(analysis_result["anomaly_details"][:5], 1):
        print(f"{i}. {anomaly['source_agent']} -> {anomaly['target_agent']}")
        print(f"   æ™‚åˆ»: {anomaly['timestamp']}")
        print(f"   ã‚¹ã‚³ã‚¢: {anomaly['anomaly_score']:0.3f}")
        print(f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {anomaly['status']}")
        if anomaly["error_message"]:
            print(f"   ã‚¨ãƒ©ãƒ¼: {anomaly['error_message'][:50]}...")
        print()

    # æ¨å¥¨äº‹é …
    print("\nğŸ’¡ æ¨å¥¨äº‹é …")
    print("-" * 40)
    for i, rec in enumerate(analysis_result["recommendations"], 1):
        print(f"{i}. {rec}")

    # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_file = (
        PROJECT_ROOT
        / "logs"
        / f"a2a_anomaly_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(analysis_result, f, indent=2, ensure_ascii=False, default=str)

    print(f"\nğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {report_file}")


if __name__ == "__main__":
    main()
