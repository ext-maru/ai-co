#!/usr/bin/env python3
"""
A2Aé€šä¿¡ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æã‚·ã‚¹ãƒ†ãƒ 
pgvectorã‚’æ´»ç”¨ã—ã¦A2Aé€šä¿¡ãƒ­ã‚°ã‚’æ„å‘³çš„ã«åˆ†æãƒ»æ¤œç´¢
"""

import os
import sys
import json
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import sqlite3

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# A2Aåˆ†æç”¨ã«å¿…è¦æœ€å°é™ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ä½¿ç”¨
try:
    from libs.grimoire_database import GrimoireDatabase
    from libs.grimoire_vector_search import GrimoireVectorSearch
    GRIMOIRE_AVAILABLE = True
except ImportError:
    GRIMOIRE_AVAILABLE = False

class A2ASemanticAnalyzer:
    """A2Aé€šä¿¡ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æ"""
    
    def __init__(self):
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’ç¢ºèª
        self.a2a_db_path = PROJECT_ROOT / "db" / "a2a_monitoring.db"
        if not self.a2a_db_path.exists():
            self.a2a_db_path = PROJECT_ROOT / "logs" / "a2a_monitoring.db"
        self.embedding_cache = {}
        
        # Grimoireçµ±åˆãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿åˆæœŸåŒ–
        if GRIMOIRE_AVAILABLE:
            try:
                self.db = GrimoireDatabase()
                self.vector_search = GrimoireVectorSearch()
                self.grimoire_enabled = True
            except Exception:
                self.grimoire_enabled = False
        else:
            self.grimoire_enabled = False
        
    def analyze_communication_patterns(self) -> Dict[str, Any]:
        """A2Aé€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        print("ğŸ“Š A2Aé€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æä¸­...")
        
        # SQLiteã‹ã‚‰A2Aé€šä¿¡ãƒ­ã‚°ã‚’å–å¾—
        conn = sqlite3.connect(self.a2a_db_path)
        cursor = conn.cursor()
        
        # æœ€è¿‘ã®é€šä¿¡ã‚’å–å¾—
        query = """
        SELECT source_agent, target_agent, message_type, metadata, 
               response_time, timestamp
        FROM a2a_communications
        ORDER BY timestamp DESC
        LIMIT 1000
        """
        
        cursor.execute(query)
        communications = cursor.fetchall()
        conn.close()
        
        # é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        patterns = self._extract_patterns(communications)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦é¡ä¼¼æ€§åˆ†æ
        vectorized_patterns = self._vectorize_patterns(patterns)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        clusters = self._cluster_patterns(vectorized_patterns)
        
        return {
            "total_communications": len(communications),
            "unique_patterns": len(patterns),
            "clusters": clusters,
            "top_patterns": self._get_top_patterns(patterns),
            "anomalies": self._detect_anomalies(vectorized_patterns)
        }
    
    def _extract_patterns(self, communications: List[Tuple]) -> List[Dict]:
        """é€šä¿¡ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º"""
        patterns = []
        
        for comm in communications:
            source, target, msg_type, metadata, response_time, timestamp = comm
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹
            try:
                meta = json.loads(metadata) if metadata else {}
            except:
                meta = {}
            
            pattern = {
                "flow": f"{source} -> {target}",
                "type": msg_type,
                "response_time": response_time,
                "timestamp": timestamp,
                "context": meta.get("message", ""),
                "session_id": meta.get("session_id", "")
            }
            patterns.append(pattern)
        
        return patterns
    
    def _vectorize_patterns(self, patterns: List[Dict]) -> List[np.ndarray]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        print("ğŸ”„ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...")
        vectors = []
        
        for pattern in patterns:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®èª¬æ˜æ–‡ã‚’ç”Ÿæˆ
            description = f"{pattern['flow']} {pattern['type']} {pattern['context']}"
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if description in self.embedding_cache:
                vector = self.embedding_cache[description]
            else:
                # ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ (Grimoireä½¿ç”¨å¯èƒ½ãªå ´åˆã®ã¿)
                if self.grimoire_enabled:
                    try:
                        vector = self.vector_search.generate_embedding(description)
                    except (AttributeError, Exception):
                        # Grimoireã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆã¯ç°¡å˜ãªç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨
                        vector = self._generate_simple_embedding(description)
                    if vector is not None:
                        self.embedding_cache[description] = vector
                else:
                    # GrimoireãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ç°¡å˜ãªç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
                    vector = self._generate_simple_embedding(description)
                    if vector is not None:
                        self.embedding_cache[description] = vector
            
            if vector is not None:
                vectors.append(vector)
        
        return vectors
    
    def _cluster_patterns(self, vectors: List[np.ndarray]) -> List[Dict]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        if not vectors:
            return []
        
        from sklearn.cluster import DBSCAN
        
        # DBSCANã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        clustering = DBSCAN(eps=0.3, min_samples=5, metric='cosine')
        labels = clustering.fit_predict(vectors)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ã‚’é›†è¨ˆ
        clusters = {}
        for i, label in enumerate(labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(i)
        
        return [
            {
                "cluster_id": label,
                "size": len(indices),
                "is_anomaly": label == -1
            }
            for label, indices in clusters.items()
        ]
    
    def _get_top_patterns(self, patterns: List[Dict], top_n: int = 10) -> List[Dict]:
        """é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—"""
        from collections import Counter
        
        # ãƒ•ãƒ­ãƒ¼ã”ã¨ã«ã‚«ã‚¦ãƒ³ãƒˆ
        flow_counts = Counter(p["flow"] for p in patterns)
        type_counts = Counter(p["type"] for p in patterns)
        
        return {
            "top_flows": flow_counts.most_common(top_n),
            "top_types": type_counts.most_common(top_n)
        }
    
    def _detect_anomalies(self, vectors: List[np.ndarray]) -> List[Dict]:
        """ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
        if len(vectors) < 10:
            return []
        
        from sklearn.ensemble import IsolationForest
        
        # Isolation Forestã§ç•°å¸¸æ¤œçŸ¥
        clf = IsolationForest(contamination=0.1, random_state=42)
        predictions = clf.fit_predict(vectors)
        
        anomalies = []
        for i, pred in enumerate(predictions):
            if pred == -1:  # ç•°å¸¸
                anomalies.append({
                    "index": i,
                    "anomaly_score": clf.score_samples([vectors[i]])[0]
                })
        
        return sorted(anomalies, key=lambda x: x["anomaly_score"])[:10]
    
    def semantic_search_errors(self, query: str) -> List[Dict]:
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢"""
        print(f"ğŸ” ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢ä¸­: {query}")
        
        # ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        if self.grimoire_enabled:
            try:
                query_vector = self.vector_search.generate_embedding(query)
            except (AttributeError, Exception):
                query_vector = self._generate_simple_embedding(query)
        else:
            query_vector = self._generate_simple_embedding(query)
        
        if query_vector is None:
            return []
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‹ã‚‰é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        conn = sqlite3.connect(self.a2a_db_path)
        cursor = conn.cursor()
        
        # ã‚¨ãƒ©ãƒ¼ã®ã¿å–å¾—
        query_sql = """
        SELECT id, error_type, error_message, timestamp
        FROM a2a_errors
        ORDER BY timestamp DESC
        LIMIT 100
        """
        
        cursor.execute(query_sql)
        errors = cursor.fetchall()
        conn.close()
        
        # å„ã‚¨ãƒ©ãƒ¼ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        similar_errors = []
        for error in errors:
            error_id, err_type, msg, timestamp = error
            
            # ã‚¨ãƒ©ãƒ¼ã®èª¬æ˜æ–‡
            error_desc = f"{err_type}: {msg}"
            if self.grimoire_enabled:
                try:
                    error_vector = self.vector_search.generate_embedding(error_desc)
                except (AttributeError, Exception):
                    error_vector = self._generate_simple_embedding(error_desc)
            else:
                error_vector = self._generate_simple_embedding(error_desc)
            
            if error_vector is not None:
                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
                similarity = np.dot(query_vector, error_vector) / (
                    np.linalg.norm(query_vector) * np.linalg.norm(error_vector)
                )
                
                if similarity > 0.7:  # é–¾å€¤
                    similar_errors.append({
                        "id": error_id,
                        "error_type": err_type,
                        "message": msg,
                        "similarity": float(similarity),
                        "timestamp": timestamp
                    })
        
        return sorted(similar_errors, key=lambda x: x["similarity"], reverse=True)
    
    def auto_categorize_communications(self) -> Dict[str, List]:
        """é€šä¿¡ã‚’è‡ªå‹•çš„ã«ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        print("ğŸ·ï¸ é€šä¿¡ã‚’è‡ªå‹•ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ä¸­...")
        
        # äº‹å‰å®šç¾©ã‚«ãƒ†ã‚´ãƒª
        categories = {
            "knowledge_query": "çŸ¥è­˜ç…§ä¼šãƒ»å­¦ç¿’",
            "task_coordination": "ã‚¿ã‚¹ã‚¯èª¿æ•´ãƒ»å‰²ã‚Šå½“ã¦",
            "error_handling": "ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ»å¾©æ—§",
            "status_update": "çŠ¶æ…‹æ›´æ–°ãƒ»å ±å‘Š",
            "urgent_action": "ç·Šæ€¥å¯¾å¿œãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ"
        }
        
        # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ä»£è¡¨ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
        category_vectors = {}
        for cat_id, cat_desc in categories.items():
            if self.grimoire_enabled:
                try:
                    vector = self.vector_search.generate_embedding(cat_desc)
                except (AttributeError, Exception):
                    vector = self._generate_simple_embedding(cat_desc)
            else:
                vector = self._generate_simple_embedding(cat_desc)
            if vector is not None:
                category_vectors[cat_id] = vector
        
        # é€šä¿¡ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡
        categorized = {cat_id: [] for cat_id in categories}
        
        # æœ€è¿‘ã®é€šä¿¡ã‚’å–å¾—ã—ã¦åˆ†é¡
        patterns = self._get_recent_patterns(limit=500)
        
        for pattern in patterns:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            pattern_desc = f"{pattern['flow']} {pattern['type']} {pattern.get('context', '')}"
            if self.grimoire_enabled:
                try:
                    pattern_vector = self.vector_search.generate_embedding(pattern_desc)
                except (AttributeError, Exception):
                    pattern_vector = self._generate_simple_embedding(pattern_desc)
            else:
                pattern_vector = self._generate_simple_embedding(pattern_desc)
            
            if pattern_vector is not None:
                # æœ€ã‚‚è¿‘ã„ã‚«ãƒ†ã‚´ãƒªã‚’è¦‹ã¤ã‘ã‚‹
                best_category = None
                best_similarity = -1
                
                for cat_id, cat_vector in category_vectors.items():
                    similarity = np.dot(pattern_vector, cat_vector) / (
                        np.linalg.norm(pattern_vector) * np.linalg.norm(cat_vector)
                    )
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_category = cat_id
                
                if best_category and best_similarity > 0.6:
                    categorized[best_category].append({
                        "pattern": pattern,
                        "similarity": float(best_similarity)
                    })
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
        stats = {}
        for cat_id, items in categorized.items():
            stats[cat_id] = {
                "name": categories[cat_id],
                "count": len(items),
                "percentage": len(items) / len(patterns) * 100 if patterns else 0
            }
        
        return {
            "categories": categorized,
            "statistics": stats
        }
    
    def _get_recent_patterns(self, limit: int = 100) -> List[Dict]:
        """æœ€è¿‘ã®é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—"""
        conn = sqlite3.connect(self.a2a_db_path)
        cursor = conn.cursor()
        
        query = """
        SELECT source_agent, target_agent, message_type, metadata
        FROM a2a_communications
        ORDER BY timestamp DESC
        LIMIT ?
        """
        
        cursor.execute(query, (limit,))
        results = cursor.fetchall()
        conn.close()
        
        patterns = []
        for row in results:
            source, target, msg_type, metadata = row
            try:
                meta = json.loads(metadata) if metadata else {}
            except:
                meta = {}
            
            patterns.append({
                "flow": f"{source} -> {target}",
                "type": msg_type,
                "context": meta.get("message", "")
            })
        
        return patterns
    
    def _generate_simple_embedding(self, text: str) -> Optional[np.ndarray]:
        """ç°¡å˜ãªç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆï¼ˆGrimoireåˆ©ç”¨ä¸å¯æ™‚ï¼‰"""
        if not text:
            return None
        
        # ç°¡å˜ãªç‰¹å¾´æŠ½å‡º
        words = text.lower().split()
        
        # åŸºæœ¬çš„ãªç‰¹å¾´é‡ï¼ˆæ¬¡å…ƒæ•°: 50ï¼‰
        features = np.zeros(50)
        
        # å˜èªã®ç‰¹å¾´
        features[0] = len(words)  # å˜èªæ•°
        features[1] = len(text)   # æ–‡å­—æ•°
        features[2] = text.count('error')  # ã‚¨ãƒ©ãƒ¼é–¢é€£
        features[3] = text.count('success')  # æˆåŠŸé–¢é€£
        features[4] = text.count('timeout')  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆé–¢é€£
        features[5] = text.count('connection')  # æ¥ç¶šé–¢é€£
        features[6] = text.count('request')  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–¢é€£
        features[7] = text.count('response')  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹é–¢é€£
        features[8] = text.count('sage')  # è³¢è€…é–¢é€£
        features[9] = text.count('elder')  # ã‚¨ãƒ«ãƒ€ãƒ¼é–¢é€£
        
        # çŸ¢å°ã®æ–¹å‘ï¼ˆé€šä¿¡ãƒ•ãƒ­ãƒ¼ï¼‰
        features[10] = text.count('->')  # å‰å‘ãé€šä¿¡
        features[11] = text.count('<-')  # å¾Œå‘ãé€šä¿¡
        
        # é€šä¿¡ã‚¿ã‚¤ãƒ—
        features[12] = text.count('knowledge')  # çŸ¥è­˜é–¢é€£
        features[13] = text.count('task')  # ã‚¿ã‚¹ã‚¯é–¢é€£
        features[14] = text.count('incident')  # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆé–¢é€£
        features[15] = text.count('rag')  # RAGé–¢é€£
        
        # å˜èªã®ãƒãƒƒã‚·ãƒ¥ç‰¹å¾´ï¼ˆæ®‹ã‚Š35æ¬¡å…ƒï¼‰
        for i, word in enumerate(words[:35]):
            features[15 + i] = hash(word) % 100 / 100.0
        
        # æ­£è¦åŒ–
        norm = np.linalg.norm(features)
        if norm > 0:
            features = features / norm
        
        return features
    
    def generate_insights_report(self) -> Dict:
        """ç·åˆçš„ãªæ´å¯Ÿãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("ğŸ“Š A2Aé€šä¿¡ã®ç·åˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        
        # å„ç¨®åˆ†æã‚’å®Ÿè¡Œ
        pattern_analysis = self.analyze_communication_patterns()
        categories = self.auto_categorize_communications()
        
        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        error_patterns = self.semantic_search_errors("connection error timeout")
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_communications": pattern_analysis["total_communications"],
                "unique_patterns": pattern_analysis["unique_patterns"],
                "anomaly_count": len(pattern_analysis["anomalies"]),
                "error_patterns_found": len(error_patterns)
            },
            "pattern_analysis": pattern_analysis,
            "categorization": categories["statistics"],
            "top_error_patterns": error_patterns[:5],
            "recommendations": self._generate_recommendations(
                pattern_analysis, categories, error_patterns
            )
        }
        
        return report
    
    def _generate_recommendations(self, patterns: Dict, categories: Dict, errors: List) -> List[str]:
        """åˆ†æçµæœã‹ã‚‰æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        # ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¤šã„å ´åˆ
        if len(patterns["anomalies"]) > 5:
            recommendations.append(
                "âš ï¸ ç•°å¸¸ãªé€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„ã€‚"
            )
        
        # ç‰¹å®šã‚«ãƒ†ã‚´ãƒªã«åã‚ŠãŒã‚ã‚‹å ´åˆ
        stats = categories["statistics"]
        for cat_id, stat in stats.items():
            if stat["percentage"] > 40:
                recommendations.append(
                    f"ğŸ“Š {stat['name']}ã®é€šä¿¡ãŒ{stat['percentage']:.1f}%ã‚’å ã‚ã¦ã„ã¾ã™ã€‚"
                    f"è² è·åˆ†æ•£ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
                )
        
        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¤šã„å ´åˆ
        if len(errors) > 10:
            recommendations.append(
                "ğŸ”§ é¡ä¼¼ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¤šæ•°æ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ ¹æœ¬åŸå› ã®åˆ†æãŒå¿…è¦ã§ã™ã€‚"
            )
        
        return recommendations

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    analyzer = A2ASemanticAnalyzer()
    
    print("ğŸš€ A2Aé€šä¿¡ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    
    # ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = analyzer.generate_insights_report()
    
    # ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
    print("\nğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼")
    print("-" * 40)
    print(f"ç·é€šä¿¡æ•°: {report['summary']['total_communications']:,}")
    print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³: {report['summary']['unique_patterns']}")
    print(f"ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³: {report['summary']['anomaly_count']}")
    print(f"ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³: {report['summary']['error_patterns_found']}")
    
    print("\nğŸ“ˆ é€šä¿¡ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ")
    print("-" * 40)
    for cat_id, stat in report['categorization'].items():
        print(f"{stat['name']}: {stat['count']}ä»¶ ({stat['percentage']:.1f}%)")
    
    print("\nğŸ’¡ æ¨å¥¨äº‹é …")
    print("-" * 40)
    for i, rec in enumerate(report['recommendations'], 1):
        print(f"{i}. {rec}")
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_file = PROJECT_ROOT / "logs" / f"a2a_semantic_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {report_file}")
    
    # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰
    print("\nğŸ” ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ï¼ˆ'quit'ã§çµ‚äº†ï¼‰")
    while True:
        query = input("\næ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ› > ")
        if query.lower() == 'quit':
            break
        
        results = analyzer.semantic_search_errors(query)
        if results:
            print(f"\nè¦‹ã¤ã‹ã£ãŸé¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³: {len(results)}ä»¶")
            for i, result in enumerate(results[:5], 1):
                print(f"{i}. {result['flow']} - {result['error_type']} (é¡ä¼¼åº¦: {result['similarity']:.2f})")
        else:
            print("é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

if __name__ == "__main__":
    main()