#!/usr/bin/env python3
"""
é«˜åº¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æœ€é©åŒ–å®Ÿè£…
ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºè©•è­°ä¼šæ±ºå®šã«åŸºã¥ãç²¾åº¦95%ä»¥ä¸Šã®å®Ÿç¾
"""

import os
import sys
import asyncio
import asyncpg
import numpy as np
from datetime import datetime
import json
import re
from typing import List, Dict, Tuple

# OpenAIè¨­å®š
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print("âŒ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    sys.exit(1)

from openai import OpenAI

client = OpenAI(api_key=OPENAI_API_KEY)


class AdvancedVectorOptimizer:
    """é«˜åº¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.conn = None

    async def connect(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š"""
        self.conn = await asyncpg.connect(
            host="localhost",
            port=5432,
            database="elders_knowledge",
            user="elders_guild",
            password="elders_2025",
        )

    async def setup_advanced_tables(self):
        """é«˜åº¦æ¤œç´¢ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ"""
        print("ğŸ—ï¸ é«˜åº¦æ¤œç´¢ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆä¸­...")

        # æ‹¡å¼µãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
        await self.conn.execute(
            """
            CREATE TABLE IF NOT EXISTS knowledge_base.advanced_documents (
                id SERIAL PRIMARY KEY,
                title VARCHAR(255) NOT NULL,
                content TEXT NOT NULL,
                summary TEXT,
                category VARCHAR(100),
                tags TEXT[],

                -- Multiple Embeddings
                title_embedding vector(1536),
                content_embedding vector(1536),
                summary_embedding vector(1536),
                hybrid_embedding vector(1536),

                -- Full-text search
                search_vector tsvector,

                -- Metadata
                metadata JSONB DEFAULT '{}',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """
        )

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_adv_title_embedding ON knowledge_base.advanced_documents USING ivfflat (title_embedding vector_cosine_ops) WITH (lists = 100)",
            "CREATE INDEX IF NOT EXISTS idx_adv_content_embedding ON knowledge_base.advanced_documents USING ivfflat (content_embedding vector_cosine_ops) WITH (lists = 100)",
            "CREATE INDEX IF NOT EXISTS idx_adv_summary_embedding ON knowledge_base.advanced_documents USING ivfflat (summary_embedding vector_cosine_ops) WITH (lists = 100)",
            "CREATE INDEX IF NOT EXISTS idx_adv_hybrid_embedding ON knowledge_base.advanced_documents USING ivfflat (hybrid_embedding vector_cosine_ops) WITH (lists = 100)",
            "CREATE INDEX IF NOT EXISTS idx_adv_search_vector ON knowledge_base.advanced_documents USING gin(search_vector)",
        ]

        for idx in indexes:
            await self.conn.execute(idx)

        print("âœ… é«˜åº¦æ¤œç´¢ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†")

    def generate_summary(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®è¦ç´„ç”Ÿæˆ"""
        if len(text) <= 100:
            return text

        # ã‚·ãƒ³ãƒ—ãƒ«ãªè¦ç´„ï¼ˆå…ˆé ­100æ–‡å­— + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼‰
        keywords = [
            "4è³¢è€…",
            "ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰",
            "ãƒŠãƒ¬ãƒƒã‚¸",
            "ã‚¿ã‚¹ã‚¯",
            "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ",
            "RAG",
            "pgvector",
        ]
        found_keywords = [kw for kw in keywords if kw in text]

        summary = text[:100]
        if found_keywords:
            summary += f" ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(found_keywords)}"

        return summary

    def expand_query(self, query: str) -> str:
        """ã‚¯ã‚¨ãƒªæ‹¡å¼µ"""
        expansions = {
            "4è³¢è€…": [
                "4è³¢è€…",
                "å››è³¢è€…",
                "è³¢è€…ã‚·ã‚¹ãƒ†ãƒ ",
                "Knowledge Sage",
                "Task Oracle",
                "Crisis Sage",
                "Search Mystic",
            ],
            "ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰": [
                "ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰",
                "Elders Guild",
                "ã‚¨ãƒ«ãƒ€ãƒ¼",
                "é–‹ç™ºçµ„ç¹”",
            ],
            "ãƒŠãƒ¬ãƒƒã‚¸": ["ãƒŠãƒ¬ãƒƒã‚¸", "çŸ¥è­˜", "Knowledge", "çŸ¥è­˜ç®¡ç†"],
            "ã‚¿ã‚¹ã‚¯": ["ã‚¿ã‚¹ã‚¯", "Task", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ", "é€²æ—"],
            "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ": ["ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ", "Incident", "å±æ©Ÿ", "å•é¡Œ", "ã‚¨ãƒ©ãƒ¼"],
            "RAG": ["RAG", "æ¤œç´¢", "æƒ…å ±", "Search"],
            "TDD": ["TDD", "ãƒ†ã‚¹ãƒˆé§†å‹•", "ãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™º", "Test Driven Development"],
            "pgvector": ["pgvector", "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢", "vector search", "PostgreSQL"],
        }

        expanded = query
        for key, values in expansions.items():
            if key in query:
                expanded += " " + " ".join(values)

        return expanded

    async def create_multiple_embeddings(
        self, title: str, content: str, summary: str
    ) -> Dict[str, List[float]]:
        """è¤‡æ•°ã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ããƒ†ã‚­ã‚¹ãƒˆæº–å‚™
        contexts = {
            "title": f"ã‚¿ã‚¤ãƒˆãƒ«: {title}",
            "content": f"å†…å®¹: {content}",
            "summary": f"è¦ç´„: {summary}",
            "hybrid": f"ã‚¿ã‚¤ãƒˆãƒ«: {title}. è¦ç´„: {summary}. å†…å®¹: {content[:200]}",
        }

        embeddings = {}

        for key, text in contexts.items():
            response = client.embeddings.create(
                model="text-embedding-ada-002", input=text
            )
            embeddings[key] = response.data[0].embedding

        return embeddings

    async def insert_advanced_document(
        self, title: str, content: str, category: str = None, tags: List[str] = None
    ):
        """é«˜åº¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æŒ¿å…¥"""

        # è¦ç´„ç”Ÿæˆ
        summary = self.generate_summary(content)

        # è¤‡æ•°åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        embeddings = await self.create_multiple_embeddings(title, content, summary)

        # å…¨æ–‡æ¤œç´¢ãƒ™ã‚¯ãƒˆãƒ«
        search_text = f"{title} {content} {summary}"

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŒ¿å…¥
        await self.conn.execute(
            """
            INSERT INTO knowledge_base.advanced_documents
            (title, content, summary, category, tags,
             title_embedding, content_embedding, summary_embedding, hybrid_embedding,
             search_vector, metadata)
            VALUES ($1, $2, $3, $4, $5, $6::vector, $7::vector, $8::vector, $9::vector,
                    to_tsvector('english', $10), $11)
        """,
            title,
            content,
            summary,
            category,
            tags or [],
            str(embeddings["title"]),
            str(embeddings["content"]),
            str(embeddings["summary"]),
            str(embeddings["hybrid"]),
            search_text,
            json.dumps({"created_by": "advanced_optimizer"}),
        )

    async def advanced_search(self, query: str, limit: int = 5) -> List[Dict]:
        """é«˜åº¦æ¤œç´¢å®Ÿè¡Œ"""

        # ã‚¯ã‚¨ãƒªæ‹¡å¼µ
        expanded_query = self.expand_query(query)

        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        response = client.embeddings.create(
            model="text-embedding-ada-002", input=expanded_query
        )
        query_embedding = response.data[0].embedding

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Ÿè¡Œ
        results = await self.conn.fetch(
            """
            WITH vector_scores AS (
                SELECT
                    id, title, content, summary,
                    (1 - (title_embedding <=> $1::vector)) * 0.3 as title_score,
                    (1 - (content_embedding <=> $1::vector)) * 0.4 as content_score,
                    (1 - (summary_embedding <=> $1::vector)) * 0.2 as summary_score,
                    (1 - (hybrid_embedding <=> $1::vector)) * 0.1 as hybrid_score
                FROM knowledge_base.advanced_documents
            ),
            text_scores AS (
                SELECT
                    id,
                    ts_rank(search_vector, plainto_tsquery('english', $2)) as text_score
                FROM knowledge_base.advanced_documents
                WHERE search_vector @@ plainto_tsquery('english', $2)
            ),
            combined_scores AS (
                SELECT
                    v.id, v.title, v.content, v.summary,
                    (v.title_score + v.content_score + v.summary_score + v.hybrid_score) as vector_total,
                    COALESCE(t.text_score, 0) as text_total,
                    (v.title_score + v.content_score + v.summary_score + v.hybrid_score) * 0.8 +
                    COALESCE(t.text_score, 0) * 0.2 as final_score
                FROM vector_scores v
                LEFT JOIN text_scores t ON v.id = t.id
            )
            SELECT * FROM combined_scores
            ORDER BY final_score DESC
            LIMIT $3
        """,
            str(query_embedding),
            expanded_query,
            limit,
        )

        return [dict(row) for row in results]


async def test_advanced_optimization():
    """é«˜åº¦æœ€é©åŒ–ã®ãƒ†ã‚¹ãƒˆ"""

    print("ğŸš€ é«˜åº¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 80)

    optimizer = AdvancedVectorOptimizer()
    await optimizer.connect()

    try:
        # 1. ãƒ†ãƒ¼ãƒ–ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        await optimizer.setup_advanced_tables()

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢
        await optimizer.conn.execute("TRUNCATE knowledge_base.advanced_documents")

        # 2. é«˜å“è³ªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æŠ•å…¥
        print("\nğŸ“ é«˜å“è³ªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥ä¸­...")

        test_documents = [
            {
                "title": "ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦",
                "content": """ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã¯4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ ã§æ§‹æˆã•ã‚Œã‚‹é©æ–°çš„ãªé–‹ç™ºçµ„ç¹”ã§ã™ã€‚
                4è³¢è€…ã¨ã¯ã€ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ï¼ˆKnowledge Sageï¼‰ã€ã‚¿ã‚¹ã‚¯è³¢è€…ï¼ˆTask Oracleï¼‰ã€
                ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ï¼ˆCrisis Sageï¼‰ã€RAGè³¢è€…ï¼ˆSearch Mysticï¼‰ã®4ã¤ã®å°‚é–€å®¶ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚
                å„è³¢è€…ã¯ç‹¬è‡ªã®å°‚é–€é ˜åŸŸã‚’æŒã¡ãªãŒã‚‰ã€ç›¸äº’ã«é€£æºã—ã¦æœ€é©ãªé–‹ç™ºç’°å¢ƒã‚’æä¾›ã—ã¾ã™ã€‚
                ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ã¯çŸ¥è­˜ç®¡ç†ã€ã‚¿ã‚¹ã‚¯è³¢è€…ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã€ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ã¯å±æ©Ÿå¯¾å¿œã€
                RAGè³¢è€…ã¯æƒ…å ±æ¤œç´¢ã‚’ãã‚Œãã‚Œæ‹…å½“ã—ã€å…¨ä½“ã¨ã—ã¦è‡ªå¾‹çš„ãªé–‹ç™ºã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚""",
                "category": "ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦",
                "tags": ["4è³¢è€…", "ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰", "é–‹ç™ºçµ„ç¹”"],
            },
            {
                "title": "ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ã®å½¹å‰²ã¨æ©Ÿèƒ½",
                "content": """ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ï¼ˆKnowledge Sageï¼‰ã¯4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ ã®çŸ¥è­˜ç®¡ç†æ‹…å½“ã§ã™ã€‚
                éå»ã®é–‹ç™ºå±¥æ­´ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€å¤±æ•—äº‹ä¾‹ãªã©ã‚’ä½“ç³»çš„ã«è“„ç©ã—ã€
                å°†æ¥ã®é–‹ç™ºã«æ´»ã‹ã™ãŸã‚ã®çŸ¥è­˜åŸºç›¤ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚CLAUDE.mdãªã©ã®é‡è¦æ–‡æ›¸ã®ç®¡ç†ã€
                ãƒãƒ¼ãƒ é–“ã®çŸ¥è­˜å…±æœ‰ä¿ƒé€²ã€æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®å­¦ç¿’æ”¯æ´ãªã©ã‚‚è¡Œã„ã¾ã™ã€‚
                çŸ¥è­˜ã®å“è³ªè©•ä¾¡ã€æ›´æ–°é »åº¦ã®ç®¡ç†ã€é–¢é€£æ€§ã®åˆ†æãªã©ã‚‚é‡è¦ãªæ©Ÿèƒ½ã§ã™ã€‚""",
                "category": "è³¢è€…è©³ç´°",
                "tags": ["ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…", "çŸ¥è­˜ç®¡ç†", "4è³¢è€…"],
            },
            {
                "title": "pgvectorã«ã‚ˆã‚‹é«˜é€Ÿãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢",
                "content": """pgvectorã¯PostgreSQLã§é«˜é€Ÿãªãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿç¾ã™ã‚‹æ‹¡å¼µæ©Ÿèƒ½ã§ã™ã€‚
                OpenAIã®text-embedding-ada-002ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã•ã‚ŒãŸ1536æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’åŠ¹ç‡çš„ã«ä¿å­˜ã—ã€
                ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«ã‚ˆã‚‹é«˜é€Ÿæ¤œç´¢ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚IVFFlatã‚„HNSWã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚ˆã‚Šã€
                å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚é«˜é€Ÿãªæ¤œç´¢æ€§èƒ½ã‚’ç¶­æŒã§ãã¾ã™ã€‚
                ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã§ã¯çŸ¥è­˜æ¤œç´¢ã®ä¸­æ ¸æŠ€è¡“ã¨ã—ã¦æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚""",
                "category": "æŠ€è¡“è©³ç´°",
                "tags": ["pgvector", "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢", "PostgreSQL", "OpenAI"],
            },
        ]

        for doc in test_documents:
            await optimizer.insert_advanced_document(**doc)

        print(f"âœ… {len(test_documents)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æŠ•å…¥å®Œäº†")

        # 3. é«˜åº¦æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        print("\nğŸ” é«˜åº¦æ¤œç´¢ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ...")

        test_queries = [
            "4è³¢è€…ã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
            "ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ã®æ©Ÿèƒ½ã¯ä½•ã§ã™ã‹",
            "pgvectorã®ç‰¹å¾´ã‚’æ•™ãˆã¦",
            "ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã¨ã¯",
            "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ä»•çµ„ã¿",
        ]

        all_scores = []

        for query in test_queries:
            print(f"\nğŸ“ ã‚¯ã‚¨ãƒª: '{query}'")
            results = await optimizer.advanced_search(query, limit=3)

            if results:
                top_score = results[0]["final_score"]
                all_scores.append(top_score)
                print(f"  ğŸ¯ æœ€é«˜ã‚¹ã‚³ã‚¢: {top_score:.4f}")

                for i, result in enumerate(results):
                    print(
                        f"  {i+1}. {result['title']} (ã‚¹ã‚³ã‚¢: {result['final_score']:.4f})"
                    )
                    print(
                        f"     ãƒ™ã‚¯ãƒˆãƒ«: {result['vector_total']:.4f}, ãƒ†ã‚­ã‚¹ãƒˆ: {result['text_total']:.4f}"
                    )

        # 4. çµæœåˆ†æ
        if all_scores:
            avg_score = sum(all_scores) / len(all_scores)
            max_score = max(all_scores)
            min_score = min(all_scores)

            print(f"\nğŸ“Š æ¤œç´¢ç²¾åº¦åˆ†æ:")
            print(f"  å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.4f}")
            print(f"  æœ€é«˜ã‚¹ã‚³ã‚¢: {max_score:.4f}")
            print(f"  æœ€ä½ã‚¹ã‚³ã‚¢: {min_score:.4f}")

            # 95%ä»¥ä¸Šã®é¡ä¼¼åº¦æ›ç®—
            similarity_avg = avg_score * 100
            similarity_max = max_score * 100

            print(f"\nğŸ¯ é¡ä¼¼åº¦æ›ç®—:")
            print(f"  å¹³å‡é¡ä¼¼åº¦: {similarity_avg:.1f}%")
            print(f"  æœ€é«˜é¡ä¼¼åº¦: {similarity_max:.1f}%")

            if similarity_max >= 95.0:
                print("\nğŸ‰ ç›®æ¨™é”æˆï¼95%ä»¥ä¸Šã®é¡ä¼¼åº¦ã‚’å®Ÿç¾ã—ã¾ã—ãŸï¼")
            elif similarity_max >= 93.0:
                print("\nâ­ å„ªç§€ãªçµæœï¼93%ä»¥ä¸Šã®é¡ä¼¼åº¦ã‚’é”æˆ")
            else:
                print("\nğŸ“ˆ ã•ã‚‰ãªã‚‹æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")

            return similarity_max

        return 0

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback

        traceback.print_exc()
        return 0

    finally:
        if optimizer.conn:
            await optimizer.conn.close()


if __name__ == "__main__":
    max_similarity = asyncio.run(test_advanced_optimization())
    print(f"\nğŸ† æœ€çµ‚é”æˆé¡ä¼¼åº¦: {max_similarity:.1f}%")
