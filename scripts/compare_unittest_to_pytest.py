#!/usr/bin/env python3
"""
unittest to pytestç§»è¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Issue #93: OSSç§»è¡Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
"""
import json
import statistics
import subprocess
import sys
import time
from pathlib import Path
from typing import Any, Dict, List


class TestMigrationComparator:
    """ãƒ†ã‚¹ãƒˆç§»è¡Œæ¯”è¼ƒãƒ„ãƒ¼ãƒ«"""

    def __init__(self):
        self.results = {"unittest": [], "pytest": []}

    def run_unittest(self, test_file: str, iterations: int = 3) -> Dict[str, Any]:
        """unittestãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"ğŸ”§ unittestå®Ÿè¡Œä¸­: {test_file}")
        durations = []
        test_counts = []

        for i in range(iterations):
            start_time = time.time()

            try:
                result = subprocess.run(
                    ["python3", "-m", "unittest", test_file, "-v"],
                    capture_output=True,
                    text=True,
                    timeout=60,
                )
                duration = time.time() - start_time
                durations.append(duration)

                # ãƒ†ã‚¹ãƒˆæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                output_lines = result.stderr.split("\n")
                test_count = len(
                    [line for line in output_lines if line.strip().endswith("ok")]
                )
                test_counts.append(test_count)

                print(f"  å®Ÿè¡Œ {i+1}/{iterations}: {duration:0.2f}ç§’, {test_count}ãƒ†ã‚¹ãƒˆ")
            except Exception as e:
                print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        return {
            "framework": "unittest",
            "test_file": test_file,
            "durations": durations,
            "avg_duration": statistics.mean(durations) if durations else 0,
            "min_duration": min(durations) if durations else 0,
            "max_duration": max(durations) if durations else 0,
            "test_count": max(test_counts) if test_counts else 0,
            "success": len(durations) == iterations,
        }

    def run_pytest(self, test_file: str, iterations: int = 3) -> Dict[str, Any]:
        """pytestãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"ğŸš€ pytestå®Ÿè¡Œä¸­: {test_file}")
        durations = []
        test_counts = []

        for i in range(iterations):
            start_time = time.time()

            try:
                # pytestå®Ÿè¡Œï¼ˆã‚«ãƒãƒ¬ãƒƒã‚¸ãªã—ï¼‰
                result = subprocess.run(
                    ["python3", "-m", "pytest", test_file, "-v", "--tb=short"],
                    capture_output=True,
                    text=True,
                    timeout=60,
                    env={
                        **subprocess.os.environ,
                        "PYTEST_DISABLE_PLUGIN_AUTOLOAD": "1",
                    },
                )
                duration = time.time() - start_time
                durations.append(duration)

                # ãƒ†ã‚¹ãƒˆæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                output_lines = result.stdout.split("\n")
                test_count = len([line for line in output_lines if " PASSED" in line])
                test_counts.append(test_count)

                print(f"  å®Ÿè¡Œ {i+1}/{iterations}: {duration:0.2f}ç§’, {test_count}ãƒ†ã‚¹ãƒˆ")
            except Exception as e:
                print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        return {
            "framework": "pytest",
            "test_file": test_file,
            "durations": durations,
            "avg_duration": statistics.mean(durations) if durations else 0,
            "min_duration": min(durations) if durations else 0,
            "max_duration": max(durations) if durations else 0,
            "test_count": max(test_counts) if test_counts else 0,
            "success": len(durations) == iterations,
        }

    def analyze_code_metrics(
        self, unittest_file: str, pytest_file: str
    ) -> Dict[str, Any]:
        """ã‚³ãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æ"""
        metrics = {}

        # unittestãƒ•ã‚¡ã‚¤ãƒ«
        if Path(unittest_file).exists():
            with open(unittest_file) as f:
                unittest_lines = f.readlines()
            metrics["unittest"] = {
                "lines_of_code": len(unittest_lines),
                "test_methods": len(
                    [l for l in unittest_lines if l.strip().startswith("def test_")]
                ),
                "class_count": len(
                    [l for l in unittest_lines if l.strip().startswith("class Test")]
                ),
            }

        # pytestãƒ•ã‚¡ã‚¤ãƒ«
        if Path(pytest_file).exists():
            with open(pytest_file) as f:
                pytest_lines = f.readlines()
            metrics["pytest"] = {
                "lines_of_code": len(pytest_lines),
                "test_functions": len(
                    [l for l in pytest_lines if l.strip().startswith("def test_")]
                ),
                "fixture_count": len(
                    [l for l in pytest_lines if "@pytest.fixture" in l]
                ),
            }

        # å‰Šæ¸›ç‡è¨ˆç®—
        if "unittest" in metrics and "pytest" in metrics:
            reduction = (
                1
                - metrics["pytest"]["lines_of_code"]
                / metrics["unittest"]["lines_of_code"]
            ) * 100
            metrics["code_reduction_percentage"] = reduction

        return metrics

    def generate_report(
        self, unittest_result: Dict, pytest_result: Dict, code_metrics: Dict
    ) -> str:
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""# ğŸ”„ unittest â†’ pytest ç§»è¡Œæ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ
**Issue #93: OSSç§»è¡Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**
**ç”Ÿæˆæ—¥**: {time.strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š å®Ÿè¡Œæ™‚é–“æ¯”è¼ƒ

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | unittest | pytest | æ”¹å–„ç‡ |
|------------|----------|---------|--------|
| å¹³å‡å®Ÿè¡Œæ™‚é–“ | {unittest_result['avg_duration']:0.3f}ç§’ | {pytest_result[ \
    'avg_duration']:0.3f}ç§’ | {((unittest_result['avg_duration'] - \
        pytest_result['avg_duration']) / unittest_result['avg_duration'] * 100):0.1f}% |
| æœ€å°å®Ÿè¡Œæ™‚é–“ | {unittest_result['min_duration']:0.3f}ç§’ | {pytest_result['min_duration']:0.3f}ç§’ | - |
| æœ€å¤§å®Ÿè¡Œæ™‚é–“ | {unittest_result['max_duration']:0.3f}ç§’ | {pytest_result['max_duration']:0.3f}ç§’ | - |
| ãƒ†ã‚¹ãƒˆæ•° | {unittest_result['test_count']} | {pytest_result['test_count']} | - |

## ğŸ“ˆ ã‚³ãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | unittest | pytest | å‰Šæ¸›ç‡ |
|------------|----------|---------|--------|
| ã‚³ãƒ¼ãƒ‰è¡Œæ•° | {
    code_metrics.get('unittest',
    {}).get('lines_of_code',
    'N/A')} | {code_metrics.get('pytest',
    {}).get('lines_of_code',
    'N/A')} | {code_metrics.get('code_reduction_percentage',
    0):0.1f
}% |
| ãƒ†ã‚¹ãƒˆæ•° | {code_metrics.get('unittest', { \
    }).get('test_methods', 'N/A')} | {code_metrics.get('pytest', {}).get('test_functions', 'N/A')} | - |
| ã‚¯ãƒ©ã‚¹/ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ | {code_metrics.get('unittest', { \
    }).get('class_count', 'N/A')} | {code_metrics.get('pytest', {}).get('fixture_count', 'N/A')} | - |

## ğŸ¯ ç§»è¡Œã®ãƒ¡ãƒªãƒƒãƒˆ

1.0 **ã‚³ãƒ¼ãƒ‰å‰Šæ¸›**: ã‚ˆã‚Šç°¡æ½”ã§èª­ã¿ã‚„ã™ã„ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
2.0 **ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£**: å†åˆ©ç”¨å¯èƒ½ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚³ãƒ¼ãƒ‰
3.0 **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–**: åŒã˜ãƒ†ã‚¹ãƒˆãƒ­ã‚¸ãƒƒã‚¯ã®åŠ¹ç‡çš„ãªå†åˆ©ç”¨
4.0 **ã‚ˆã‚Šè‰¯ã„ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³**: ã‚·ãƒ³ãƒ—ãƒ«ãªassertæ–‡ã§ã®è©³ç´°ãªã‚¨ãƒ©ãƒ¼å‡ºåŠ›
5.0 **è±Šå¯Œãªãƒ—ãƒ©ã‚°ã‚¤ãƒ³**: ä¸¦åˆ—å®Ÿè¡Œã€ã‚«ãƒãƒ¬ãƒƒã‚¸ã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãªã©

## ğŸ“ æ¨å¥¨äº‹é …

- æ®µéšçš„ç§»è¡Œ: æ–°è¦ãƒ†ã‚¹ãƒˆã‹ã‚‰pytestæ¡ç”¨
- æ—¢å­˜ãƒ†ã‚¹ãƒˆã¯å‹•ä½œç¢ºèªå¾Œã«ç§»è¡Œ
- CI/CDçµ±åˆã§pytest-xdistã«ã‚ˆã‚‹ä¸¦åˆ—å®Ÿè¡Œã‚’æ´»ç”¨
"""

        return report


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    if len(sys.argv) < 3:
        print(
            "ä½¿ç”¨æ–¹æ³•: python3 compare_unittest_to_pytest.py <unittest_file> <pytest_file>"
        )
        sys.exit(1)

    unittest_file = sys.argv[1]
    pytest_file = sys.argv[2]

    comparator = TestMigrationComparator()

    print("ğŸ” ãƒ†ã‚¹ãƒˆç§»è¡Œæ¯”è¼ƒã‚’é–‹å§‹ã—ã¾ã™...")

    # unittestå®Ÿè¡Œ
    unittest_result = comparator.run_unittest(unittest_file)

    # pytestå®Ÿè¡Œ
    pytest_result = comparator.run_pytest(pytest_file)

    # ã‚³ãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æ
    code_metrics = comparator.analyze_code_metrics(unittest_file, pytest_file)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = comparator.generate_report(unittest_result, pytest_result, code_metrics)

    print("\n" + "=" * 60)
    print(report)
    print("=" * 60)

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_path = Path("test_reports/migration_comparison.md")
    report_path.parent.mkdir(exist_ok=True)
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"\nâœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å…ˆ: {report_path}")


if __name__ == "__main__":
    main()
