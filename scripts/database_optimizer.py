#!/usr/bin/env python3
"""
Database Optimizer - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«
ã‚¨ãƒ«ãƒ€ãƒ¼ã‚ºã‚®ãƒ«ãƒ‰ã®SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

ğŸ—„ï¸ æœ€é©åŒ–é …ç›®:
- VACUUMå®Ÿè¡Œï¼ˆæ–­ç‰‡åŒ–è§£æ¶ˆï¼‰
- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰
- å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
- çµ±è¨ˆæƒ…å ±æ›´æ–°
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºæœ€é©åŒ–
"""

import os
import sys
import sqlite3
import shutil
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any

class DatabaseOptimizer:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_dir: str = "/home/aicompany/ai_co"):
        self.project_dir = Path(project_dir)
        self.logs_dir = self.project_dir / "logs"
        
        # ãƒ­ã‚°è¨­å®š
        self.setup_logging()
        
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.logs_dir / 'database_optimizer.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def find_databases(self) -> List[Path]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡º"""
        self.logger.info("ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºé–‹å§‹")
        
        db_patterns = ["*.db", "*.sqlite", "*.sqlite3"]
        databases = []
        
        for pattern in db_patterns:
            databases.extend(self.project_dir.rglob(pattern))
        
        # é‡è¤‡é™¤å»ã¨ã‚½ãƒ¼ãƒˆ
        databases = sorted(list(set(databases)))
        
        self.logger.info(f"ğŸ” æ¤œå‡ºå®Œäº†: {len(databases)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹")
        for db in databases:
            size_kb = db.stat().st_size / 1024
            self.logger.debug(f"  {db.name}: {size_kb:.1f} KB")
        
        return databases
    
    def analyze_database(self, db_path: Path) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æ"""
        self.logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æ: {db_path.name}")
        
        analysis = {
            "path": str(db_path),
            "size_kb": db_path.stat().st_size / 1024,
            "tables": [],
            "indexes": [],
            "total_rows": 0,
            "page_count": 0,
            "page_size": 0,
            "fragmentation_percent": 0,
            "error": None
        }
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±å–å¾—
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            table_names = [row[0] for row in cursor.fetchall()]
            
            for table_name in table_names:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    row_count = cursor.fetchone()[0]
                    analysis["tables"].append({
                        "name": table_name,
                        "rows": row_count
                    })
                    analysis["total_rows"] += row_count
                except Exception as e:
                    self.logger.warning(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±å–å¾—
            cursor.execute("SELECT name FROM sqlite_master WHERE type='index'")
            index_names = [row[0] for row in cursor.fetchall() if row[0]]
            analysis["indexes"] = index_names
            
            # ãƒšãƒ¼ã‚¸æƒ…å ±å–å¾—
            try:
                cursor.execute("PRAGMA page_count")
                analysis["page_count"] = cursor.fetchone()[0]
                
                cursor.execute("PRAGMA page_size")
                analysis["page_size"] = cursor.fetchone()[0]
                
                cursor.execute("PRAGMA freelist_count")
                free_pages = cursor.fetchone()[0]
                
                if analysis["page_count"] > 0:
                    analysis["fragmentation_percent"] = (free_pages / analysis["page_count"]) * 100
            except Exception as e:
                self.logger.debug(f"ãƒšãƒ¼ã‚¸æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            conn.close()
            
        except Exception as e:
            analysis["error"] = str(e)
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼ {db_path}: {e}")
        
        self.logger.info(f"ğŸ“Š åˆ†æå®Œäº†: {analysis['total_rows']}è¡Œ, {len(analysis['tables'])}ãƒ†ãƒ¼ãƒ–ãƒ«")
        return analysis
    
    def optimize_database(self, db_path: Path) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–å®Ÿè¡Œ"""
        self.logger.info(f"âš¡ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–: {db_path.name}")
        
        result = {
            "database": str(db_path),
            "size_before_kb": db_path.stat().st_size / 1024,
            "size_after_kb": 0,
            "space_saved_kb": 0,
            "vacuum_executed": False,
            "reindex_executed": False,
            "analyze_executed": False,
            "backup_created": False,
            "error": None
        }
        
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            backup_path = db_path.with_suffix(f".backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db")
            shutil.copy2(db_path, backup_path)
            result["backup_created"] = True
            self.logger.info(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path.name}")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # VACUUMå®Ÿè¡Œï¼ˆæ–­ç‰‡åŒ–è§£æ¶ˆï¼‰
            self.logger.info("ğŸ§¹ VACUUMå®Ÿè¡Œä¸­...")
            cursor.execute("VACUUM")
            result["vacuum_executed"] = True
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰
            self.logger.info("ğŸ“‡ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ä¸­...")
            cursor.execute("REINDEX")
            result["reindex_executed"] = True
            
            # çµ±è¨ˆæƒ…å ±æ›´æ–°
            self.logger.info("ğŸ“ˆ çµ±è¨ˆæƒ…å ±æ›´æ–°ä¸­...")
            cursor.execute("ANALYZE")
            result["analyze_executed"] = True
            
            # ã‚³ãƒŸãƒƒãƒˆã¨ã‚¯ãƒ­ãƒ¼ã‚º
            conn.commit()
            conn.close()
            
            # æœ€é©åŒ–å¾Œã®ã‚µã‚¤ã‚º
            result["size_after_kb"] = db_path.stat().st_size / 1024
            result["space_saved_kb"] = result["size_before_kb"] - result["size_after_kb"]
            
            self.logger.info(f"âœ… æœ€é©åŒ–å®Œäº†: {result['space_saved_kb']:.1f}KBå‰Šæ¸›")
            
        except Exception as e:
            result["error"] = str(e)
            self.logger.error(f"æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼ {db_path}: {e}")
        
        return result
    
    def archive_old_data(self, db_path: Path, days_to_keep: int = 90) -> Dict[str, Any]:
        """å¤ã„ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–"""
        self.logger.info(f"ğŸ“¦ å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {db_path.name} ({days_to_keep}æ—¥ã‚ˆã‚Šå¤ã„)")
        
        result = {
            "database": str(db_path),
            "archived_tables": [],
            "total_archived_rows": 0,
            "error": None
        }
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾—
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)
            cutoff_timestamp = cutoff_date.isoformat()
            
            for table in tables:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚«ãƒ©ãƒ ã®æ¤œå‡º
                cursor.execute(f"PRAGMA table_info({table})")
                columns = cursor.fetchall()
                
                timestamp_columns = []
                for col in columns:
                    col_name = col[1].lower()
                    if any(keyword in col_name for keyword in ['timestamp', 'created', 'updated', 'date']):
                        timestamp_columns.append(col[1])
                
                if timestamp_columns:
                    timestamp_col = timestamp_columns[0]  # æœ€åˆã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨
                    
                    try:
                        # å¤ã„ãƒ‡ãƒ¼ã‚¿æ•°ç¢ºèª
                        cursor.execute(f"SELECT COUNT(*) FROM {table} WHERE {timestamp_col} < ?", (cutoff_timestamp,))
                        old_count = cursor.fetchone()[0]
                        
                        if old_count > 0:
                            # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
                            archive_table = f"{table}_archive"
                            cursor.execute(f"CREATE TABLE IF NOT EXISTS {archive_table} AS SELECT * FROM {table} WHERE 1=0")
                            
                            # å¤ã„ãƒ‡ãƒ¼ã‚¿ç§»å‹•
                            cursor.execute(f"INSERT INTO {archive_table} SELECT * FROM {table} WHERE {timestamp_col} < ?", (cutoff_timestamp,))
                            cursor.execute(f"DELETE FROM {table} WHERE {timestamp_col} < ?", (cutoff_timestamp,))
                            
                            result["archived_tables"].append({
                                "table": table,
                                "archived_rows": old_count
                            })
                            result["total_archived_rows"] += old_count
                            
                            self.logger.info(f"ğŸ“¦ {table}: {old_count}è¡Œã‚¢ãƒ¼ã‚«ã‚¤ãƒ–")
                    
                    except Exception as e:
                        self.logger.warning(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table} ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼: {e}")
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            result["error"] = str(e)
            self.logger.error(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼ {db_path}: {e}")
        
        return result
    
    def check_database_integrity(self, db_path: Path) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        self.logger.info(f"ğŸ” æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: {db_path.name}")
        
        result = {
            "database": str(db_path),
            "integrity_ok": False,
            "quick_check_ok": False,
            "foreign_key_check_ok": False,
            "issues": [],
            "error": None
        }
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            cursor.execute("PRAGMA integrity_check")
            integrity_result = cursor.fetchall()
            if len(integrity_result) == 1 and integrity_result[0][0] == "ok":
                result["integrity_ok"] = True
            else:
                result["issues"].extend([row[0] for row in integrity_result])
            
            # ã‚¯ã‚¤ãƒƒã‚¯ãƒã‚§ãƒƒã‚¯
            cursor.execute("PRAGMA quick_check")
            quick_result = cursor.fetchall()
            if len(quick_result) == 1 and quick_result[0][0] == "ok":
                result["quick_check_ok"] = True
            else:
                result["issues"].extend([row[0] for row in quick_result])
            
            # å¤–éƒ¨ã‚­ãƒ¼ãƒã‚§ãƒƒã‚¯
            cursor.execute("PRAGMA foreign_key_check")
            fk_result = cursor.fetchall()
            if len(fk_result) == 0:
                result["foreign_key_check_ok"] = True
            else:
                result["issues"].extend([f"FK violation: {row}" for row in fk_result])
            
            conn.close()
            
            if result["integrity_ok"] and result["quick_check_ok"] and result["foreign_key_check_ok"]:
                self.logger.info("âœ… æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: å•é¡Œãªã—")
            else:
                self.logger.warning(f"âš ï¸ æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: {len(result['issues'])}å€‹ã®å•é¡Œ")
        
        except Exception as e:
            result["error"] = str(e)
            self.logger.error(f"æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ {db_path}: {e}")
        
        return result
    
    def optimize_all_databases(self, include_archive: bool = False, 
                             archive_days: int = 90) -> Dict[str, Any]:
        """å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–"""
        self.logger.info("ğŸš€ å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–é–‹å§‹")
        
        results = {
            "start_time": datetime.now().isoformat(),
            "databases_found": 0,
            "databases_optimized": 0,
            "total_space_saved_kb": 0,
            "optimizations": [],
            "integrity_checks": [],
            "archives": [],
            "errors": [],
            "end_time": None
        }
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
        databases = self.find_databases()
        results["databases_found"] = len(databases)
        
        for db_path in databases:
            try:
                # åˆ†æ
                analysis = self.analyze_database(db_path)
                
                # æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                integrity = self.check_database_integrity(db_path)
                results["integrity_checks"].append(integrity)
                
                if not integrity["integrity_ok"]:
                    self.logger.warning(f"âš ï¸ æ•´åˆæ€§å•é¡Œã®ãŸã‚æœ€é©åŒ–ã‚¹ã‚­ãƒƒãƒ—: {db_path.name}")
                    continue
                
                # å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                if include_archive and analysis["total_rows"] > 1000:
                    archive_result = self.archive_old_data(db_path, archive_days)
                    results["archives"].append(archive_result)
                
                # æœ€é©åŒ–å®Ÿè¡Œ
                optimization = self.optimize_database(db_path)
                results["optimizations"].append(optimization)
                
                if optimization["error"]:
                    results["errors"].append(optimization["error"])
                else:
                    results["databases_optimized"] += 1
                    results["total_space_saved_kb"] += optimization["space_saved_kb"]
                
            except Exception as e:
                error_msg = f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‡¦ç†ã‚¨ãƒ©ãƒ¼ {db_path}: {e}"
                results["errors"].append(error_msg)
                self.logger.error(error_msg)
        
        results["end_time"] = datetime.now().isoformat()
        self.logger.info(f"âœ… å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–å®Œäº†: {results['total_space_saved_kb']:.1f}KBå‰Šæ¸›")
        
        return results
    
    def print_summary(self, results: Dict[str, Any]):
        """æœ€é©åŒ–çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ—„ï¸ Elders Guild Database Optimization Report")
        print("="*60)
        
        print(f"\nğŸ“Š Summary:")
        print(f"  Databases found: {results['databases_found']}")
        print(f"  Databases optimized: {results['databases_optimized']}")
        print(f"  Total space saved: {results['total_space_saved_kb']:.1f} KB")
        
        if results["optimizations"]:
            print(f"\nâš¡ Optimizations:")
            for opt in results["optimizations"]:
                if not opt["error"]:
                    db_name = Path(opt["database"]).name
                    saved = opt["space_saved_kb"]
                    print(f"  {db_name}: {saved:.1f}KB saved")
        
        if results["integrity_checks"]:
            good_dbs = sum(1 for check in results["integrity_checks"] 
                          if check["integrity_ok"] and check["quick_check_ok"])
            print(f"\nğŸ” Integrity Checks:")
            print(f"  Healthy databases: {good_dbs}/{len(results['integrity_checks'])}")
            
            for check in results["integrity_checks"]:
                if not (check["integrity_ok"] and check["quick_check_ok"]):
                    db_name = Path(check["database"]).name
                    print(f"  âš ï¸ Issues in {db_name}: {len(check['issues'])} problems")
        
        if results["archives"]:
            total_archived = sum(archive["total_archived_rows"] for archive in results["archives"])
            print(f"\nğŸ“¦ Data Archiving:")
            print(f"  Total rows archived: {total_archived}")
        
        if results["errors"]:
            print(f"\nâŒ Errors ({len(results['errors'])}):")
            for error in results["errors"][:5]:  # æœ€åˆã®5ã‚¨ãƒ©ãƒ¼ã®ã¿è¡¨ç¤º
                print(f"  {error}")
        
        print("\n" + "="*60)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Database Optimizer")
    parser.add_argument("--optimize-all", action="store_true", help="å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–")
    parser.add_argument("--analyze-only", action="store_true", help="åˆ†æã®ã¿å®Ÿè¡Œ")
    parser.add_argument("--integrity-check", action="store_true", help="æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã®ã¿")
    parser.add_argument("--include-archive", action="store_true", help="å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’å«ã‚€")
    parser.add_argument("--archive-days", type=int, default=90, help="ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¯¾è±¡æ—¥æ•°")
    parser.add_argument("--save", action="store_true", help="çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜")
    
    args = parser.parse_args()
    
    optimizer = DatabaseOptimizer()
    
    if args.analyze_only:
        databases = optimizer.find_databases()
        for db_path in databases[:5]:  # æœ€åˆã®5å€‹ã®ã¿
            analysis = optimizer.analyze_database(db_path)
            print(f"\nğŸ“Š {db_path.name}:")
            print(f"  Size: {analysis['size_kb']:.1f} KB")
            print(f"  Tables: {len(analysis['tables'])}")
            print(f"  Total rows: {analysis['total_rows']}")
            print(f"  Fragmentation: {analysis['fragmentation_percent']:.1f}%")
    elif args.integrity_check:
        databases = optimizer.find_databases()
        for db_path in databases[:5]:  # æœ€åˆã®5å€‹ã®ã¿
            integrity = optimizer.check_database_integrity(db_path)
            status = "âœ… OK" if integrity["integrity_ok"] and integrity["quick_check_ok"] else "âš ï¸ Issues"
            print(f"{db_path.name}: {status}")
    elif args.optimize_all:
        results = optimizer.optimize_all_databases(
            include_archive=args.include_archive,
            archive_days=args.archive_days
        )
        optimizer.print_summary(results)
        
        if args.save:
            import json
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = optimizer.logs_dir / f"database_optimization_{timestamp}.json"
            with open(report_file, 'w') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“„ Report saved: {report_file}")
    else:
        print("ğŸ—„ï¸ Elders Guild Database Optimizer")
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  --optimize-all    : å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–")
        print("  --analyze-only    : åˆ†æã®ã¿å®Ÿè¡Œ")
        print("  --integrity-check : æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã®ã¿")
        print("  --include-archive : å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–")
        print("  --save            : çµæœä¿å­˜")

if __name__ == "__main__":
    main()