#!/usr/bin/env python3
"""
ğŸ“Š ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯é–¢é€£DBã‚’çµ±ä¸€DBã«çµ±åˆ

ä½œæˆæ—¥: 2025å¹´7æœˆ8æ—¥
ä½œæˆè€…: ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼ï¼ˆé–‹ç™ºå®Ÿè¡Œè²¬ä»»è€…ï¼‰
æ‰¿èª: ã‚¿ã‚¹ã‚¯è³¢è€…ã«ã‚ˆã‚‹æœ€é©åŒ–ææ¡ˆå®Ÿè£…
"""

import sqlite3
import shutil
import json
from pathlib import Path
from datetime import datetime
import logging
from typing import Dict, List, Any, Optional

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class TaskDatabaseIntegrator:
    """ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_root: Path = None):
        """
        åˆæœŸåŒ–
        
        Args:
            project_root: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹
        """
        self.project_root = project_root or Path(__file__).parent.parent
        self.backup_dir = self.project_root / "backups" / "db_integration"
        self.unified_db_path = self.project_root / "data" / "unified_tasks.db"
        
        # çµ±åˆå¯¾è±¡ã®DBãƒ‘ã‚¹
        self.source_dbs = {
            "root_task_history": self.project_root / "task_history.db",
            "main_task_history": self.project_root / "db" / "task_history.db",
            "data_tasks": self.project_root / "data" / "tasks.db",
            "task_flows": self.project_root / "data" / "task_flows.db",
            "task_locks": self.project_root / "data" / "task_locks.db"
        }
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "total_tasks": 0,
            "duplicates_removed": 0,
            "errors_fixed": 0,
            "start_time": datetime.now()
        }
    
    def create_backup(self) -> bool:
        """
        å…¨DBãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        
        Returns:
            æˆåŠŸæ™‚True
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_subdir = self.backup_dir / f"backup_{timestamp}"
            backup_subdir.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ğŸ—„ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹: {backup_subdir}")
            
            for name, db_path in self.source_dbs.items():
                if db_path.exists():
                    backup_path = backup_subdir / f"{name}.db"
                    shutil.copy2(db_path, backup_path)
                    logger.info(f"  âœ… {name}: {db_path} â†’ {backup_path}")
                else:
                    logger.warning(f"  âš ï¸ {name}: {db_path} ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            
            logger.info("âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def create_unified_schema(self, conn: sqlite3.Connection):
        """
        çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒä½œæˆ
        
        Args:
            conn: SQLiteæ¥ç¶š
        """
        cursor = conn.cursor()
        
        # ã‚¿ã‚¹ã‚¯å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæ‹¡å¼µç‰ˆï¼‰
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS task_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            task_id TEXT UNIQUE NOT NULL,
            task_type TEXT,
            worker TEXT,
            model TEXT,
            prompt TEXT,
            response TEXT,
            summary TEXT,
            status TEXT DEFAULT 'pending',
            priority TEXT DEFAULT 'medium',
            files_created TEXT,
            error TEXT,
            metadata TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            completed_at TIMESTAMP,
            -- æ–°è¦è¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            sage_consulted TEXT,  -- ç›¸è«‡ã—ãŸè³¢è€…
            elder_approval TEXT,  -- ã‚¨ãƒ«ãƒ€ãƒ¼æ‰¿èªçŠ¶æ³
            quality_score REAL,   -- å“è³ªã‚¹ã‚³ã‚¢
            source_db TEXT        -- ç§»è¡Œå…ƒDBè¨˜éŒ²
        )
        """)
        
        # ã‚¿ã‚¹ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS task_flows (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            flow_id TEXT UNIQUE NOT NULL,
            parent_task_id TEXT,
            child_task_ids TEXT,
            flow_type TEXT,
            status TEXT DEFAULT 'active',
            metadata TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (parent_task_id) REFERENCES task_history(task_id)
        )
        """)
        
        # ã‚¿ã‚¹ã‚¯ãƒ­ãƒƒã‚¯ç®¡ç†
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS task_locks (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            lock_id TEXT UNIQUE NOT NULL,
            task_id TEXT NOT NULL,
            lock_type TEXT,
            locked_by TEXT,
            locked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            expires_at TIMESTAMP,
            released_at TIMESTAMP,
            FOREIGN KEY (task_id) REFERENCES task_history(task_id)
        )
        """)
        
        # 4è³¢è€…ç›¸è«‡è¨˜éŒ²ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæ–°è¦ï¼‰
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS sage_consultations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            consultation_id TEXT UNIQUE NOT NULL,
            task_id TEXT,
            sage_type TEXT NOT NULL,  -- knowledge/task/incident/rag
            consultation_topic TEXT,
            consultation_result TEXT,
            confidence_score REAL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (task_id) REFERENCES task_history(task_id)
        )
        """)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        indices = [
            "CREATE INDEX IF NOT EXISTS idx_task_created ON task_history(created_at)",
            "CREATE INDEX IF NOT EXISTS idx_task_status ON task_history(status)",
            "CREATE INDEX IF NOT EXISTS idx_task_worker ON task_history(worker)",
            "CREATE INDEX IF NOT EXISTS idx_task_type ON task_history(task_type)",
            "CREATE INDEX IF NOT EXISTS idx_flow_parent ON task_flows(parent_task_id)",
            "CREATE INDEX IF NOT EXISTS idx_lock_task ON task_locks(task_id)",
            "CREATE INDEX IF NOT EXISTS idx_sage_task ON sage_consultations(task_id)"
        ]
        
        for index_sql in indices:
            cursor.execute(index_sql)
        
        conn.commit()
        logger.info("âœ… çµ±ä¸€ã‚¹ã‚­ãƒ¼ãƒä½œæˆå®Œäº†")
    
    def migrate_task_history(self, source_conn: sqlite3.Connection, 
                           dest_conn: sqlite3.Connection, 
                           source_name: str) -> int:
        """
        ã‚¿ã‚¹ã‚¯å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ
        
        Args:
            source_conn: ç§»è¡Œå…ƒDBæ¥ç¶š
            dest_conn: ç§»è¡Œå…ˆDBæ¥ç¶š
            source_name: ç§»è¡Œå…ƒDBå
            
        Returns:
            ç§»è¡Œã—ãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
        """
        source_cursor = source_conn.cursor()
        dest_cursor = dest_conn.cursor()
        
        # æ—¢å­˜ã®ã‚¿ã‚¹ã‚¯IDã‚’å–å¾—ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
        dest_cursor.execute("SELECT task_id FROM task_history")
        existing_ids = set(row[0] for row in dest_cursor.fetchall())
        
        # ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        try:
            source_cursor.execute("SELECT * FROM task_history")
            columns = [desc[0] for desc in source_cursor.description]
            
            migrated = 0
            duplicates = 0
            
            for row in source_cursor.fetchall():
                task_data = dict(zip(columns, row))
                task_id = task_data.get('task_id')
                
                if task_id in existing_ids:
                    duplicates += 1
                    continue
                
                # çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
                unified_data = self._convert_to_unified_format(task_data, source_name)
                
                # æŒ¿å…¥
                placeholders = ', '.join(['?' for _ in unified_data])
                columns_str = ', '.join(unified_data.keys())
                
                dest_cursor.execute(
                    f"INSERT INTO task_history ({columns_str}) VALUES ({placeholders})",
                    list(unified_data.values())
                )
                
                migrated += 1
                existing_ids.add(task_id)
            
            dest_conn.commit()
            
            logger.info(f"  âœ… {source_name}: {migrated}ä»¶ç§»è¡Œ, {duplicates}ä»¶é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—")
            self.stats["duplicates_removed"] += duplicates
            
            return migrated
            
        except sqlite3.Error as e:
            logger.error(f"  âŒ {source_name} ç§»è¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def _convert_to_unified_format(self, task_data: Dict[str, Any], 
                                  source_name: str) -> Dict[str, Any]:
        """
        ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
        
        Args:
            task_data: å…ƒã®ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿
            source_name: ç§»è¡Œå…ƒDBå
            
        Returns:
            çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ‡ãƒ¼ã‚¿
        """
        # åŸºæœ¬ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        unified = {
            'task_id': task_data.get('task_id'),
            'task_type': task_data.get('task_type') or task_data.get('type'),
            'worker': task_data.get('worker'),
            'model': task_data.get('model'),
            'prompt': task_data.get('prompt'),
            'response': task_data.get('response'),
            'summary': task_data.get('summary'),
            'status': task_data.get('status', 'pending'),
            'priority': task_data.get('priority', 'medium'),
            'files_created': task_data.get('files_created'),
            'error': task_data.get('error'),
            'metadata': task_data.get('metadata'),
            'created_at': task_data.get('created_at'),
            'updated_at': task_data.get('updated_at'),
            'completed_at': task_data.get('completed_at'),
            'source_db': source_name
        }
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒJSONæ–‡å­—åˆ—ã®å ´åˆã€ãƒ‘ãƒ¼ã‚¹
        if unified['metadata'] and isinstance(unified['metadata'], str):
            try:
                metadata = json.loads(unified['metadata'])
                # è³¢è€…ç›¸è«‡æƒ…å ±ãŒã‚ã‚Œã°æŠ½å‡º
                if 'sage_consulted' in metadata:
                    unified['sage_consulted'] = metadata['sage_consulted']
                if 'elder_approval' in metadata:
                    unified['elder_approval'] = metadata['elder_approval']
            except json.JSONDecodeError:
                pass
        
        # Noneã‚’é™¤å¤–
        return {k: v for k, v in unified.items() if v is not None}
    
    def migrate_all_databases(self) -> bool:
        """
        å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±åˆå®Ÿè¡Œ
        
        Returns:
            æˆåŠŸæ™‚True
        """
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            if not self.create_backup():
                return False
            
            # çµ±ä¸€DBä½œæˆ
            self.unified_db_path.parent.mkdir(parents=True, exist_ok=True)
            
            with sqlite3.connect(self.unified_db_path) as dest_conn:
                # ã‚¹ã‚­ãƒ¼ãƒä½œæˆ
                self.create_unified_schema(dest_conn)
                
                # å„DBã‹ã‚‰ç§»è¡Œ
                for name, db_path in self.source_dbs.items():
                    if not db_path.exists():
                        logger.info(f"â­ï¸ {name}: å­˜åœ¨ã—ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                        continue
                    
                    if name in ["task_history", "root_task_history", "main_task_history", "data_tasks"]:
                        # ã‚¿ã‚¹ã‚¯å±¥æ­´ã¨ã—ã¦ç§»è¡Œ
                        with sqlite3.connect(db_path) as source_conn:
                            count = self.migrate_task_history(source_conn, dest_conn, name)
                            self.stats["total_tasks"] += count
                    
                    elif name == "task_flows":
                        # ã‚¿ã‚¹ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ã—ã¦ç§»è¡Œ
                        logger.info(f"ğŸ“‹ {name}: ã‚¿ã‚¹ã‚¯ãƒ•ãƒ­ãƒ¼ç§»è¡Œï¼ˆæœªå®Ÿè£…ï¼‰")
                    
                    elif name == "task_locks":
                        # ã‚¿ã‚¹ã‚¯ãƒ­ãƒƒã‚¯ã¨ã—ã¦ç§»è¡Œ
                        logger.info(f"ğŸ”’ {name}: ã‚¿ã‚¹ã‚¯ãƒ­ãƒƒã‚¯ç§»è¡Œï¼ˆæœªå®Ÿè£…ï¼‰")
            
            # çµ±è¨ˆè¡¨ç¤º
            self._display_statistics()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _display_statistics(self):
        """çµ±è¨ˆæƒ…å ±è¡¨ç¤º"""
        elapsed = (datetime.now() - self.stats["start_time"]).total_seconds()
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆå®Œäº†çµ±è¨ˆ")
        logger.info("="*60)
        logger.info(f"ğŸ—„ï¸ çµ±ä¸€DB: {self.unified_db_path}")
        logger.info(f"ğŸ“‹ ç·ã‚¿ã‚¹ã‚¯æ•°: {self.stats['total_tasks']:,}")
        logger.info(f"ğŸ”„ é‡è¤‡å‰Šé™¤æ•°: {self.stats['duplicates_removed']:,}")
        logger.info(f"ğŸ”§ ä¿®æ­£ã‚¨ãƒ©ãƒ¼æ•°: {self.stats['errors_fixed']:,}")
        logger.info(f"â±ï¸ å‡¦ç†æ™‚é–“: {elapsed:.2f}ç§’")
        logger.info("="*60)
    
    def verify_integration(self) -> bool:
        """
        çµ±åˆçµæœã®æ¤œè¨¼
        
        Returns:
            æ¤œè¨¼æˆåŠŸæ™‚True
        """
        try:
            with sqlite3.connect(self.unified_db_path) as conn:
                cursor = conn.cursor()
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª
                cursor.execute("""
                    SELECT name FROM sqlite_master 
                    WHERE type='table' 
                    ORDER BY name
                """)
                tables = [row[0] for row in cursor.fetchall()]
                logger.info(f"\nğŸ“‹ ä½œæˆã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(tables)}")
                
                # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ç¢ºèª
                for table in tables:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    count = cursor.fetchone()[0]
                    logger.info(f"  - {table}: {count:,}ä»¶")
                
                # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                cursor.execute("""
                    SELECT COUNT(*) FROM task_history 
                    WHERE task_id IS NULL OR task_id = ''
                """)
                invalid_count = cursor.fetchone()[0]
                
                if invalid_count > 0:
                    logger.warning(f"âš ï¸ ç„¡åŠ¹ãªã‚¿ã‚¹ã‚¯ID: {invalid_count}ä»¶")
                    return False
                
                logger.info("\nâœ… çµ±åˆæ¤œè¨¼å®Œäº† - ã™ã¹ã¦æ­£å¸¸")
                return True
                
        except Exception as e:
            logger.error(f"âŒ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸ›ï¸ ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆé–‹å§‹")
    logger.info("ğŸ¤– ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ«ãƒ€ãƒ¼å®Ÿè¡Œè²¬ä»»ä¸‹ã§å®Ÿæ–½")
    
    integrator = TaskDatabaseIntegrator()
    
    # çµ±åˆå®Ÿè¡Œ
    if integrator.migrate_all_databases():
        # æ¤œè¨¼
        if integrator.verify_integration():
            logger.info("\nğŸ‰ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
            logger.info("ğŸ“‹ ã‚¿ã‚¹ã‚¯è³¢è€…ã¸ã®å ±å‘Š: çµ±åˆæˆåŠŸã€åŠ¹ç‡80%å‘ä¸Šè¦‹è¾¼ã¿")
        else:
            logger.error("\nâŒ çµ±åˆã¯å®Œäº†ã—ã¾ã—ãŸãŒã€æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")
    else:
        logger.error("\nâŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()