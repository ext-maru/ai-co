#!/usr/bin/env python3
"""
A2Aé€šä¿¡ãƒ‡ãƒ¼ã‚¿ã®pgvectorã¸ã®ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¢å­˜ã®JSONãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLã«ç§»è¡Œã—ã€OpenAI APIã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–
"""

import json
import logging
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any
from typing import Dict
from typing import List
from typing import Optional

import psycopg2
from psycopg2.extras import execute_batch

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# OpenAI import
try:
    import openai

    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸  OpenAI library not available. Install with: pip install openai")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class A2APgVectorMigration:
    """A2Aãƒ‡ãƒ¼ã‚¿ã®pgvectorç§»è¡Œã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or self._load_default_config()
        self.connection = None
        self.cursor = None
        self.openai_client = None

        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "total_communications": 0,
            "migrated_communications": 0,
            "total_anomalies": 0,
            "migrated_anomalies": 0,
            "embeddings_generated": 0,
            "errors": [],
        }

        # ãƒãƒƒãƒã‚µã‚¤ã‚º
        self.batch_size = 100
        self.embedding_batch_size = 20

    def _load_default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®èª­ã¿è¾¼ã¿"""
        return {
            "database": {
                "host": os.getenv("PGHOST", "localhost"),
                "port": int(os.getenv("PGPORT", 5432)),
                "database": os.getenv("PGDATABASE", "ai_company_db"),
                "user": os.getenv("PGUSER", "aicompany"),
                "password": os.getenv("PGPASSWORD", ""),
            },
            "openai": {
                "api_key": os.getenv("OPENAI_API_KEY", ""),
                "model": "text-embedding-3-small",
                "dimension": 1536,
            },
            "data_sources": {
                "communications": str(
                    PROJECT_ROOT / "analysis_results" / "a2a_communications_*.json"
                ),
                "anomalies": str(
                    PROJECT_ROOT / "analysis_results" / "anomaly_patterns_*.json"
                ),
                "semantic_analysis": str(
                    PROJECT_ROOT / "analysis_results" / "semantic_analysis_*.json"
                ),
            },
        }

    def connect_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š"""
        try:
            self.connection = psycopg2.connect(**self.config["database"])
            self.cursor = self.connection.cursor()
            logger.info("Connected to PostgreSQL database")
        except Exception as e:
            logger.error(f"Failed to connect to database: {e}")
            raise

    def setup_openai(self):
        """OpenAI APIã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if not OPENAI_AVAILABLE:
            logger.warning("OpenAI library not available")
            return False

        api_key = self.config["openai"]["api_key"]
        if not api_key:
            logger.warning("OpenAI API key not configured")
            return False

        try:
            self.openai_client = openai.OpenAI(api_key=api_key)
            # ãƒ†ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            test_response = self.openai_client.embeddings.create(
                model=self.config["openai"]["model"],
                input="test",
                dimensions=self.config["openai"]["dimension"],
            )
            logger.info("OpenAI API configured successfully")
            return True
        except Exception as e:
            logger.error(f"Failed to setup OpenAI API: {e}")
            return False

    def load_json_data(self, pattern: str) -> List[Dict[str, Any]]:
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        data = []
        files = list(Path(pattern).parent.glob(Path(pattern).name))

        for file_path in files:
            if file_path.exists():
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        file_data = json.load(f)
                        if isinstance(file_data, list):
                            data.extend(file_data)
                        elif isinstance(file_data, dict):
                            # è¾æ›¸ã®å ´åˆã¯å€¤ã‚’æŠ½å‡º
                            for key, value in file_data.items():
                                if isinstance(value, dict):
                                    value["_key"] = key
                                    data.append(value)
                                elif isinstance(value, list):
                                    data.extend(value)
                except Exception as e:
                    logger.error(f"Failed to load {file_path}: {e}")
                    self.stats["errors"].append(f"Load error: {file_path}")

        logger.info(f"Loaded {len(data)} records from {len(files)} files")
        return data

    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ"""
        if not self.openai_client:
            return None

        try:
            response = self.openai_client.embeddings.create(
                model=self.config["openai"]["model"],
                input=text,
                dimensions=self.config["openai"]["dimension"],
            )
            embedding = response.data[0].embedding
            self.stats["embeddings_generated"] += 1
            return embedding
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            self.stats["errors"].append(f"Embedding error: {str(e)[:100]}")
            return None

    def generate_embeddings_batch(
        self, texts: List[str]
    ) -> List[Optional[List[float]]]:
        """ãƒãƒƒãƒã§ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ"""
        if not self.openai_client:
            return [None] * len(texts)

        embeddings = []

        # ãƒãƒƒãƒå‡¦ç†
        for i in range(0, len(texts), self.embedding_batch_size):
            batch = texts[i : i + self.embedding_batch_size]

            try:
                response = self.openai_client.embeddings.create(
                    model=self.config["openai"]["model"],
                    input=batch,
                    dimensions=self.config["openai"]["dimension"],
                )

                batch_embeddings = [data.embedding for data in response.data]
                embeddings.extend(batch_embeddings)
                self.stats["embeddings_generated"] += len(batch_embeddings)

                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                time.sleep(0.1)

            except Exception as e:
                logger.error(f"Failed to generate batch embeddings: {e}")
                self.stats["errors"].append(f"Batch embedding error: {str(e)[:100]}")
                embeddings.extend([None] * len(batch))

        return embeddings

    def migrate_communications(self):
        """A2Aé€šä¿¡ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ"""
        logger.info("Migrating A2A communications...")

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        pattern = self.config["data_sources"]["communications"]
        communications = self.load_json_data(pattern)
        self.stats["total_communications"] = len(communications)

        if not communications:
            logger.warning("No communications data found")
            return

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæƒ…å ±ã®æŠ½å‡ºã¨ç™»éŒ²
        agents = set()
        for comm in communications:
            if "sender" in comm:
                agents.add(comm["sender"])
            if "receiver" in comm:
                agents.add(comm["receiver"])

        self._migrate_agents(list(agents))

        # ãƒãƒƒãƒå‡¦ç†ã§é€šä¿¡ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œ
        for i in range(0, len(communications), self.batch_size):
            batch = communications[i : i + self.batch_size]

            # ãƒ†ã‚­ã‚¹ãƒˆæº–å‚™ï¼ˆåŸ‹ã‚è¾¼ã¿ç”¨ï¼‰
            texts = []
            for comm in batch:
                text_parts = []
                if "sender" in comm:
                    text_parts.append(f"Sender: {comm['sender']}")
                if "receiver" in comm:
                    text_parts.append(f"Receiver: {comm['receiver']}")
                if "type" in comm:
                    text_parts.append(f"Type: {comm['type']}")
                if "content" in comm:
                    content = comm["content"]
                    if isinstance(content, dict):
                        content = json.dumps(content)
                    text_parts.append(f"Content: {content}")

                texts.append(" | ".join(text_parts))

            # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            embeddings = self.generate_embeddings_batch(texts)

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æŒ¿å…¥
            insert_data = []
            for j, comm in enumerate(batch):
                try:
                    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å‡¦ç†
                    timestamp = comm.get("timestamp", datetime.now())
                    if isinstance(timestamp, str):
                        timestamp = datetime.fromisoformat(
                            timestamp.replace("Z", "+00:00")
                        )

                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æº–å‚™
                    metadata = {}
                    for key in ["priority", "status", "error", "retry_count"]:
                        if key in comm:
                            metadata[key] = comm[key]

                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†
                    content = comm.get("content", "")
                    if isinstance(content, dict):
                        content = json.dumps(content)

                    insert_data.append(
                        (
                            timestamp,
                            comm.get("sender", "unknown"),
                            comm.get("receiver", "unknown"),
                            comm.get("type", "unknown"),
                            str(content),
                            json.dumps(metadata) if metadata else None,
                            embeddings[j] if embeddings[j] else None,
                        )
                    )

                except Exception as e:
                    logger.error(f"Failed to process communication: {e}")
                    self.stats["errors"].append(
                        f"Communication processing error: {str(e)[:100]}"
                    )

            # ãƒãƒƒãƒæŒ¿å…¥
            if insert_data:
                try:
                    execute_batch(
                        self.cursor,
                        """
                        INSERT INTO a2a.communications
                        (timestamp, sender, receiver, message_type, content, metadata, embedding)
                        VALUES (%s, %s, %s, %s, %s, %s, %s)
                        ON CONFLICT DO NOTHING
                        """,
                        insert_data,
                        page_size=100,
                    )
                    self.connection.commit()
                    self.stats["migrated_communications"] += len(insert_data)
                    logger.info(
                        f"Migrated {len(insert_data)} communications (total: {self.stats['migrated_communications']})"
                    )

                except Exception as e:
                    logger.error(f"Failed to insert communications batch: {e}")
                    self.connection.rollback()
                    self.stats["errors"].append(f"Insert error: {str(e)[:100]}")

    def _migrate_agents(self, agents: List[str]):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæƒ…å ±ã®ç§»è¡Œ"""
        logger.info(f"Migrating {len(agents)} agents...")

        insert_data = []
        for agent in agents:
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã®æ¨å®š
            agent_type = "worker"
            if "system" in agent.lower():
                agent_type = "system"
            elif "elder" in agent.lower():
                agent_type = "elder"
            elif "sage" in agent.lower():
                agent_type = "sage"

            insert_data.append(
                (
                    agent,
                    agent_type,
                    "active",
                    json.dumps({"auto_detected": True}),
                    json.dumps({"message_count": 0}),
                )
            )

        try:
            execute_batch(
                self.cursor,
                """
                INSERT INTO a2a.agents (agent_name, agent_type, status, capabilities, performance_metrics)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT (agent_name) DO NOTHING
                """,
                insert_data,
                page_size=100,
            )
            self.connection.commit()
            logger.info(f"Migrated {len(insert_data)} agents")

        except Exception as e:
            logger.error(f"Failed to migrate agents: {e}")
            self.connection.rollback()

    def migrate_anomaly_patterns(self):
        """ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ"""
        logger.info("Migrating anomaly patterns...")

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        pattern = self.config["data_sources"]["anomalies"]
        anomalies = self.load_json_data(pattern)
        self.stats["total_anomalies"] = len(anomalies)

        if not anomalies:
            logger.warning("No anomaly patterns found")
            return

        # é‡è¤‡æ’é™¤ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³åã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰
        unique_patterns = {}
        for anomaly in anomalies:
            pattern_name = anomaly.get("pattern", anomaly.get("type", "unknown"))
            if pattern_name not in unique_patterns:
                unique_patterns[pattern_name] = anomaly
            else:
                # æ—¢å­˜ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç™ºç”Ÿå›æ•°ã‚’æ›´æ–°
                if "count" in anomaly:
                    unique_patterns[pattern_name]["count"] = (
                        unique_patterns[pattern_name].get("count", 0) + anomaly["count"]
                    )

        # ãƒãƒƒãƒå‡¦ç†ã§ç§»è¡Œ
        patterns_list = list(unique_patterns.values())

        # ãƒ†ã‚­ã‚¹ãƒˆæº–å‚™ï¼ˆåŸ‹ã‚è¾¼ã¿ç”¨ï¼‰
        texts = []
        for pattern in patterns_list:
            text_parts = []
            text_parts.append(
                f"Pattern: {pattern.get('pattern', pattern.get('type', 'unknown'))}"
            )
            if "description" in pattern:
                text_parts.append(f"Description: {pattern['description']}")
            if "severity" in pattern:
                text_parts.append(f"Severity: {pattern['severity']}")

            texts.append(" | ".join(text_parts))

        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        embeddings = self.generate_embeddings_batch(texts)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æŒ¿å…¥
        insert_data = []
        for i, pattern in enumerate(patterns_list):
            try:
                pattern_name = pattern.get("pattern", pattern.get("type", "unknown"))

                # æ¤œå‡ºãƒ«ãƒ¼ãƒ«ã®æ§‹ç¯‰
                detection_rules = {}
                for key in ["agents", "keywords", "threshold", "time_window"]:
                    if key in pattern:
                        detection_rules[key] = pattern[key]

                # æœ€çµ‚æ¤œå‡ºæ™‚åˆ»
                last_detected = pattern.get("last_detected", pattern.get("timestamp"))
                if isinstance(last_detected, str):
                    last_detected = datetime.fromisoformat(
                        last_detected.replace("Z", "+00:00")
                    )
                elif not isinstance(last_detected, datetime):
                    last_detected = None

                insert_data.append(
                    (
                        pattern_name,
                        pattern.get("category", "general"),
                        pattern.get("severity", "medium"),
                        pattern.get("description", ""),
                        json.dumps(detection_rules) if detection_rules else None,
                        embeddings[i] if embeddings[i] else None,
                        pattern.get("count", 1),
                        last_detected,
                    )
                )

            except Exception as e:
                logger.error(f"Failed to process anomaly pattern: {e}")
                self.stats["errors"].append(f"Anomaly processing error: {str(e)[:100]}")

        # ãƒãƒƒãƒæŒ¿å…¥
        if insert_data:
            try:
                execute_batch(
                    self.cursor,
                    """
                    INSERT INTO a2a.anomaly_patterns
                    (pattern_name, pattern_type, severity, description, detection_rules,
                     embedding, occurrence_count, last_detected)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT DO NOTHING
                    """,
                    insert_data,
                    page_size=100,
                )
                self.connection.commit()
                self.stats["migrated_anomalies"] = len(insert_data)
                logger.info(f"Migrated {len(insert_data)} anomaly patterns")

            except Exception as e:
                logger.error(f"Failed to insert anomaly patterns: {e}")
                self.connection.rollback()
                self.stats["errors"].append(f"Anomaly insert error: {str(e)[:100]}")

    def create_sample_queries(self):
        """ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã®ä½œæˆ"""
        sample_queries = [
            {
                "name": "Find similar communications",
                "description": "ç‰¹å®šã®é€šä¿¡ã«é¡ä¼¼ã—ãŸé€šä¿¡ã‚’æ¤œç´¢",
                "query": """
                    -- æœ€æ–°ã®é€šä¿¡ã‹ã‚‰é¡ä¼¼æ¤œç´¢
                    WITH latest_comm AS (
                        SELECT embedding
                        FROM a2a.communications
                        WHERE embedding IS NOT NULL
                        ORDER BY timestamp DESC
                        LIMIT 1
                    )
                    SELECT * FROM a2a.find_similar_communications(
                        (SELECT embedding FROM latest_comm),
                        10
                    );
                """,
            },
            {
                "name": "Anomaly pattern search",
                "description": "ç‰¹å®šã®ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã«é¡ä¼¼ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢",
                "query": """
                    -- é‡è¦åº¦ã®é«˜ã„ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é¡ä¼¼æ¤œç´¢
                    SELECT
                        a1.pattern_name,
                        a1.severity,
                        a1.occurrence_count,
                        1 - (a1.embedding <=> a2.embedding) as similarity
                    FROM a2a.anomaly_patterns a1
                    CROSS JOIN a2a.anomaly_patterns a2
                    WHERE a2.pattern_name = 'system-overload'
                      AND a1.pattern_name != a2.pattern_name
                      AND a1.embedding IS NOT NULL
                      AND a2.embedding IS NOT NULL
                    ORDER BY similarity DESC
                    LIMIT 5;
                """,
            },
            {
                "name": "Agent communication patterns",
                "description": "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ",
                "query": """
                    -- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥é€šä¿¡çµ±è¨ˆ
                    SELECT
                        sender,
                        receiver,
                        message_type,
                        COUNT(*) as message_count,
                        MAX(timestamp) as last_communication
                    FROM a2a.communications
                    GROUP BY sender, receiver, message_type
                    ORDER BY message_count DESC
                    LIMIT 20;
                """,
            },
        ]

        # ã‚¯ã‚¨ãƒªã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        query_file = PROJECT_ROOT / "scripts" / "pgvector_sample_queries.sql"
        with open(query_file, "w", encoding="utf-8") as f:
            f.write("-- pgvector Sample Queries for A2A Communication Analysis\n")
            f.write(f"-- Generated: {datetime.now().isoformat()}\n\n")

            for query in sample_queries:
                f.write(f"-- {query['name']}\n")
                f.write(f"-- {query['description']}\n")
                f.write(query["query"])
                f.write("\n\n")

        logger.info(f"Sample queries saved to: {query_file}")

    def generate_migration_report(self) -> Dict[str, Any]:
        """ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "statistics": self.stats,
            "database_status": {},
            "recommendations": [],
        }

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ
        try:
            # é€šä¿¡æ•°
            self.cursor.execute("SELECT COUNT(*) FROM a2a.communications;")
            report["database_status"]["total_communications"] = self.cursor.fetchone()[
                0
            ]

            # åŸ‹ã‚è¾¼ã¿ã‚ã‚Šã®é€šä¿¡æ•°
            self.cursor.execute(
                "SELECT COUNT(*) FROM a2a.communications WHERE embedding IS NOT NULL;"
            )
            report["database_status"][
                "communications_with_embeddings"
            ] = self.cursor.fetchone()[0]

            # ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°
            self.cursor.execute("SELECT COUNT(*) FROM a2a.anomaly_patterns;")
            report["database_status"][
                "total_anomaly_patterns"
            ] = self.cursor.fetchone()[0]

            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°
            self.cursor.execute("SELECT COUNT(*) FROM a2a.agents;")
            report["database_status"]["total_agents"] = self.cursor.fetchone()[0]

        except Exception as e:
            logger.error(f"Failed to get database statistics: {e}")

        # æ¨å¥¨äº‹é …
        if report["database_status"].get("communications_with_embeddings", 0) == 0:
            report["recommendations"].append(
                "OpenAI APIè¨­å®šã‚’ç¢ºèªã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„"
            )

        if self.stats["errors"]:
            report["recommendations"].append(
                f"{len(self.stats['errors'])}å€‹ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
            )

        if self.stats["migrated_communications"] < self.stats["total_communications"]:
            report["recommendations"].append(
                "ä¸€éƒ¨ã®é€šä¿¡ãƒ‡ãƒ¼ã‚¿ãŒç§»è¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
            )

        return report

    def close(self):
        """æ¥ç¶šã®ã‚¯ãƒ­ãƒ¼ã‚º"""
        if self.cursor:
            self.cursor.close()
        if self.connection:
            self.connection.close()

    def execute_migration(self) -> Dict[str, Any]:
        """å®Œå…¨ãªç§»è¡Œã®å®Ÿè¡Œ"""
        logger.info("Starting A2A to pgvector migration...")

        try:
            # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
            self.connect_database()

            # 2. OpenAIè¨­å®š
            openai_ready = self.setup_openai()
            if not openai_ready:
                logger.warning("Proceeding without OpenAI embeddings")

            # 3. é€šä¿¡ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ
            self.migrate_communications()

            # 4. ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç§»è¡Œ
            self.migrate_anomaly_patterns()

            # 5. ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã®ä½œæˆ
            self.create_sample_queries()

            # 6. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = self.generate_migration_report()

            logger.info("Migration completed successfully!")

            return report

        except Exception as e:
            logger.error(f"Migration failed: {e}")
            raise

        finally:
            self.close()


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ A2A to pgvector Migration")
    print("=" * 60)

    # OpenAI API ã‚­ãƒ¼ã®ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  Warning: OPENAI_API_KEY not set")
        print("   Embeddings will not be generated")
        response = input("   Continue without embeddings? (y/N): ")
        if response.lower() != "y":
            print("Migration cancelled")
            return

    try:
        migration = A2APgVectorMigration()
        report = migration.execute_migration()

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = (
            PROJECT_ROOT
            / "logs"
            / f"pgvector_migration_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nğŸ’¾ Migration report saved to: {report_file}")

        # çµæœè¡¨ç¤º
        print("\nğŸ“Š Migration Summary")
        print("-" * 40)
        print(
            f"Communications migrated: {report['statistics']['migrated_communications']}/{report['statistics']['total_communications']}"
        )
        print(
            f"Anomaly patterns migrated: {report['statistics']['migrated_anomalies']}/{report['statistics']['total_anomalies']}"
        )
        print(f"Embeddings generated: {report['statistics']['embeddings_generated']}")
        print(f"Errors: {len(report['statistics']['errors'])}")

        if report.get("recommendations"):
            print("\nğŸ’¡ Recommendations")
            print("-" * 40)
            for rec in report["recommendations"]:
                print(f"- {rec}")

    except Exception as e:
        print(f"\nâŒ Migration failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
