#!/usr/bin/env python3
"""
pgvectorçµ±åˆA2Aã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æã‚·ã‚¹ãƒ†ãƒ 
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§A2Aé€šä¿¡ã‚’åˆ†æã—ã€pgvectorã‚’ä½¿ç”¨ã—ã¦ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œ
"""

import logging
import os
import sys
from dataclasses import dataclass
from datetime import datetime
from datetime import timedelta
from enum import Enum
from pathlib import Path
from typing import Any
from typing import Dict
from typing import List
from typing import Optional

import numpy as np
import psycopg2
from psycopg2.extras import RealDictCursor

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# OpenAI import
try:
    import openai

    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AnalysisType(Enum):
    """åˆ†æã‚¿ã‚¤ãƒ—"""

    SIMILARITY_SEARCH = "similarity_search"
    ANOMALY_DETECTION = "anomaly_detection"
    PATTERN_MATCHING = "pattern_matching"
    AGENT_BEHAVIOR = "agent_behavior"
    TEMPORAL_ANALYSIS = "temporal_analysis"


@dataclass
class SemanticQuery:
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¯ã‚¨ãƒª"""

    query_text: str
    query_type: AnalysisType
    limit: int = 10
    threshold: float = 0.7
    time_window: Optional[timedelta] = None
    filters: Optional[Dict[str, Any]] = None


@dataclass
class AnalysisResult:
    """åˆ†æçµæœ"""

    query: SemanticQuery
    results: List[Dict[str, Any]]
    insights: Dict[str, Any]
    performance_metrics: Dict[str, float]
    timestamp: datetime


class PgVectorA2AAnalyzer:
    """pgvectorã‚’ä½¿ç”¨ã—ãŸA2Aåˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or self._load_default_config()
        self.connection = None
        self.cursor = None
        self.openai_client = None

        # åˆ†æã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.cache = {}
        self.cache_ttl = 300  # 5åˆ†

    def _load_default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®èª­ã¿è¾¼ã¿"""
        return {
            "database": {
                "host": os.getenv("PGHOST", "localhost"),
                "port": int(os.getenv("PGPORT", 5432)),
                "database": os.getenv("PGDATABASE", "ai_company_db"),
                "user": os.getenv("PGUSER", "aicompany"),
                "password": os.getenv("PGPASSWORD", ""),
            },
            "openai": {
                "api_key": os.getenv("OPENAI_API_KEY", ""),
                "model": "text-embedding-3-small",
                "dimension": 1536,
            },
            "analysis": {"default_limit": 20, "similarity_threshold": 0.7, "anomaly_threshold": 0.3, "batch_size": 100},
        }

    def connect(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š"""
        try:
            self.connection = psycopg2.connect(**self.config["database"], cursor_factory=RealDictCursor)
            self.cursor = self.connection.cursor()

            # pgvectoræ‹¡å¼µã®ç¢ºèª
            self.cursor.execute("SELECT 1 FROM pg_extension WHERE extname = 'vector';")
            if not self.cursor.fetchone():
                raise Exception("pgvector extension not installed")

            logger.info("Connected to pgvector database")

        except Exception as e:
            logger.error(f"Failed to connect: {e}")
            raise

    def setup_openai(self):
        """OpenAI APIã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if not OPENAI_AVAILABLE:
            logger.warning("OpenAI library not available")
            return False

        api_key = self.config["openai"]["api_key"]
        if not api_key:
            logger.warning("OpenAI API key not configured")
            return False

        try:
            self.openai_client = openai.OpenAI(api_key=api_key)
            logger.info("OpenAI API configured")
            return True
        except Exception as e:
            logger.error(f"Failed to setup OpenAI: {e}")
            return False

    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ"""
        if not self.openai_client:
            return None

        try:
            response = self.openai_client.embeddings.create(
                model=self.config["openai"]["model"], input=text, dimensions=self.config["openai"]["dimension"]
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            return None

    def semantic_search(self, query: SemanticQuery) -> List[Dict[str, Any]]:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®å®Ÿè¡Œ"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = f"search_{query.query_text}_{query.limit}"
        if cache_key in self.cache:
            cached_result, cached_time = self.cache[cache_key]
            if datetime.now() - cached_time < timedelta(seconds=self.cache_ttl):
                logger.info("Returning cached result")
                return cached_result

        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        query_embedding = self.generate_embedding(query.query_text)
        if not query_embedding:
            logger.error("Failed to generate query embedding")
            return []

        # SQLã‚¯ã‚¨ãƒªæ§‹ç¯‰
        sql = """
            SELECT
                c.id,
                c.timestamp,
                c.sender,
                c.receiver,
                c.message_type,
                c.content,
                c.metadata,
                1 - (c.embedding <=> %s::vector) as similarity
            FROM a2a.communications c
            WHERE c.embedding IS NOT NULL
        """

        params = [query_embedding]

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¿½åŠ 
        if query.filters:
            if "sender" in query.filters:
                sql += " AND c.sender = %s"
                params.append(query.filters["sender"])
            if "receiver" in query.filters:
                sql += " AND c.receiver = %s"
                params.append(query.filters["receiver"])
            if "message_type" in query.filters:
                sql += " AND c.message_type = %s"
                params.append(query.filters["message_type"])

        # æ™‚é–“çª“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        if query.time_window:
            sql += " AND c.timestamp >= %s"
            params.append(datetime.now() - query.time_window)

        # é¡ä¼¼åº¦é–¾å€¤
        sql += " AND 1 - (c.embedding <=> %s::vector) >= %s"
        params.extend([query_embedding, query.threshold])

        # ã‚½ãƒ¼ãƒˆã¨åˆ¶é™
        sql += " ORDER BY similarity DESC LIMIT %s"
        params.append(query.limit)

        try:
            self.cursor.execute(sql, params)
            results = self.cursor.fetchall()

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            self.cache[cache_key] = (results, datetime.now())

            return results

        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            return []

    def detect_anomalies(self, query: SemanticQuery) -> List[Dict[str, Any]]:
        """ç•°å¸¸æ¤œå‡ºã®å®Ÿè¡Œ"""
        # æœ€è¿‘ã®é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—
        recent_window = query.time_window or timedelta(hours=1)

        sql = """
            WITH recent_communications AS (
                SELECT
                    c.*,
                    ap.pattern_name,
                    ap.severity,
                    1 - (c.embedding <=> ap.embedding) as anomaly_similarity
                FROM a2a.communications c
                CROSS JOIN a2a.anomaly_patterns ap
                WHERE c.timestamp >= %s
                  AND c.embedding IS NOT NULL
                  AND ap.embedding IS NOT NULL
            )
            SELECT
                rc.*
            FROM recent_communications rc
            WHERE rc.anomaly_similarity >= %s
            ORDER BY rc.timestamp DESC, rc.anomaly_similarity DESC
            LIMIT %s
        """

        try:
            self.cursor.execute(sql, [datetime.now() - recent_window, query.threshold, query.limit])

            anomalies = self.cursor.fetchall()

            # ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            grouped_anomalies = {}
            for anomaly in anomalies:
                pattern = anomaly["pattern_name"]
                if pattern not in grouped_anomalies:
                    grouped_anomalies[pattern] = []
                grouped_anomalies[pattern].append(anomaly)

            return anomalies

        except Exception as e:
            logger.error(f"Anomaly detection failed: {e}")
            return []

    def analyze_agent_behavior(self, agent_name: str, time_window: Optional[timedelta] = None) -> Dict[str, Any]:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•åˆ†æ"""
        time_window = time_window or timedelta(days=1)

        analysis = {
            "agent": agent_name,
            "time_window": str(time_window),
            "communication_stats": {},
            "interaction_patterns": {},
            "anomalies": [],
            "embeddings_cluster": [],
        }

        try:
            # é€šä¿¡çµ±è¨ˆ
            stats_sql = """
                SELECT
                    COUNT(*) as total_messages,
                    COUNT(DISTINCT CASE WHEN sender = %s THEN receiver ELSE sender END) as unique_interactions,
                    COUNT(CASE WHEN sender = %s THEN 1 END) as sent_messages,
                    COUNT(CASE WHEN receiver = %s THEN 1 END) as received_messages,
                    COUNT(DISTINCT message_type) as message_types,
                    MAX(timestamp) as last_activity
                FROM a2a.communications
                WHERE (sender = %s OR receiver = %s)
                  AND timestamp >= %s
            """

            self.cursor.execute(
                stats_sql, [agent_name, agent_name, agent_name, agent_name, agent_name, datetime.now() - time_window]
            )

            stats = self.cursor.fetchone()
            analysis["communication_stats"] = dict(stats) if stats else {}

            # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³
            pattern_sql = """
                SELECT
                    CASE
                        WHEN sender = %s THEN receiver
                        ELSE sender
                    END as counterpart,
                    message_type,
                    COUNT(*) as message_count,
                    AVG(CASE WHEN metadata->>'priority' IS NOT NULL
                        THEN (metadata->>'priority')::int ELSE 0 END) as avg_priority
                FROM a2a.communications
                WHERE (sender = %s OR receiver = %s)
                  AND timestamp >= %s
                GROUP BY counterpart, message_type
                ORDER BY message_count DESC
                LIMIT 20
            """

            self.cursor.execute(pattern_sql, [agent_name, agent_name, agent_name, datetime.now() - time_window])

            patterns = self.cursor.fetchall()
            analysis["interaction_patterns"] = [dict(p) for p in patterns]

            # ç•°å¸¸æ¤œå‡º
            anomaly_query = SemanticQuery(
                query_text=f"Agent {agent_name} unusual behavior",
                query_type=AnalysisType.ANOMALY_DETECTION,
                limit=5,
                time_window=time_window,
                filters={"sender": agent_name},
            )

            analysis["anomalies"] = self.detect_anomalies(anomaly_query)

            return analysis

        except Exception as e:
            logger.error(f"Agent behavior analysis failed: {e}")
            return analysis

    def find_communication_patterns(self, pattern_description: str, limit: int = 10) -> List[Dict[str, Any]]:
        """é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œç´¢"""
        # ãƒ‘ã‚¿ãƒ¼ãƒ³èª¬æ˜ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
        pattern_embedding = self.generate_embedding(pattern_description)
        if not pattern_embedding:
            return []

        sql = """
            WITH pattern_matches AS (
                SELECT
                    sender,
                    receiver,
                    message_type,
                    COUNT(*) as occurrence_count,
                    AVG(1 - (embedding <=> %s::vector)) as avg_similarity,
                    ARRAY_AGG(id ORDER BY timestamp DESC LIMIT 5) as sample_ids
                FROM a2a.communications
                WHERE embedding IS NOT NULL
                  AND 1 - (embedding <=> %s::vector) >= %s
                GROUP BY sender, receiver, message_type
                HAVING COUNT(*) >= 3
            )
            SELECT
                pm.*,
                (
                    SELECT json_agg(c.*)
                    FROM a2a.communications c
                    WHERE c.id = ANY(pm.sample_ids)
                ) as sample_communications
            FROM pattern_matches pm
            ORDER BY pm.avg_similarity DESC, pm.occurrence_count DESC
            LIMIT %s
        """

        try:
            self.cursor.execute(
                sql, [pattern_embedding, pattern_embedding, self.config["analysis"]["similarity_threshold"], limit]
            )

            patterns = self.cursor.fetchall()
            return [dict(p) for p in patterns]

        except Exception as e:
            logger.error(f"Pattern search failed: {e}")
            return []

    def temporal_analysis(self, time_buckets: str = "1 hour") -> Dict[str, Any]:
        """æ™‚ç³»åˆ—åˆ†æ"""
        sql = f"""
            SELECT
                date_trunc('{time_buckets}', timestamp) as time_bucket,
                COUNT(*) as message_count,
                COUNT(DISTINCT sender) as unique_senders,
                COUNT(DISTINCT receiver) as unique_receivers,
                COUNT(DISTINCT message_type) as message_types,
                AVG(CASE WHEN metadata->>'priority' IS NOT NULL
                    THEN (metadata->>'priority')::int ELSE 0 END) as avg_priority
            FROM a2a.communications
            WHERE timestamp >= NOW() - INTERVAL '7 days'
            GROUP BY time_bucket
            ORDER BY time_bucket DESC
        """

        try:
            self.cursor.execute(sql)
            time_series = self.cursor.fetchall()

            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            if len(time_series) > 1:
                counts = [row["message_count"] for row in time_series]
                trend = "increasing" if counts[0] > counts[-1] else "decreasing"

                # ç•°å¸¸ãªã‚¹ãƒ‘ã‚¤ã‚¯ã®æ¤œå‡º
                mean_count = np.mean(counts)
                std_count = np.std(counts)
                spikes = []

                for row in time_series:
                    if abs(row["message_count"] - mean_count) > 2 * std_count:
                        spikes.append(
                            {
                                "time": row["time_bucket"],
                                "count": row["message_count"],
                                "deviation": (row["message_count"] - mean_count) / std_count,
                            }
                        )

                return {
                    "time_series": [dict(row) for row in time_series],
                    "trend": trend,
                    "spikes": spikes,
                    "statistics": {
                        "mean_messages": mean_count,
                        "std_messages": std_count,
                        "total_periods": len(time_series),
                    },
                }

            return {"time_series": [dict(row) for row in time_series]}

        except Exception as e:
            logger.error(f"Temporal analysis failed: {e}")
            return {}

    def execute_analysis(self, query: SemanticQuery) -> AnalysisResult:
        """çµ±åˆåˆ†æã®å®Ÿè¡Œ"""
        start_time = datetime.now()

        results = []
        insights = {}

        try:
            if query.query_type == AnalysisType.SIMILARITY_SEARCH:
                results = self.semantic_search(query)
                insights["similar_count"] = len(results)
                insights["avg_similarity"] = np.mean([r["similarity"] for r in results]) if results else 0

            elif query.query_type == AnalysisType.ANOMALY_DETECTION:
                results = self.detect_anomalies(query)
                insights["anomaly_count"] = len(results)
                insights["severity_distribution"] = {}
                for r in results:
                    severity = r.get("severity", "unknown")
                    insights["severity_distribution"][severity] = insights["severity_distribution"].get(severity, 0) + 1

            elif query.query_type == AnalysisType.PATTERN_MATCHING:
                results = self.find_communication_patterns(query.query_text, query.limit)
                insights["pattern_count"] = len(results)
                insights["total_occurrences"] = sum(r["occurrence_count"] for r in results)

            elif query.query_type == AnalysisType.AGENT_BEHAVIOR:
                agent_name = query.filters.get("agent") if query.filters else None
                if agent_name:
                    agent_analysis = self.analyze_agent_behavior(agent_name, query.time_window)
                    results = [agent_analysis]
                    insights = agent_analysis.get("communication_stats", {})

            elif query.query_type == AnalysisType.TEMPORAL_ANALYSIS:
                temporal_data = self.temporal_analysis()
                results = temporal_data.get("time_series", [])
                insights = temporal_data.get("statistics", {})
                insights["trend"] = temporal_data.get("trend")
                insights["spike_count"] = len(temporal_data.get("spikes", []))

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            end_time = datetime.now()
            performance_metrics = {
                "query_time_ms": (end_time - start_time).total_seconds() * 1000,
                "result_count": len(results),
                "cache_hit": cache_key in self.cache if "cache_key" in locals() else False,
            }

            return AnalysisResult(
                query=query,
                results=results,
                insights=insights,
                performance_metrics=performance_metrics,
                timestamp=end_time,
            )

        except Exception as e:
            logger.error(f"Analysis execution failed: {e}")
            return AnalysisResult(
                query=query,
                results=[],
                insights={"error": str(e)},
                performance_metrics={"error": True},
                timestamp=datetime.now(),
            )

    def close(self):
        """æ¥ç¶šã®ã‚¯ãƒ­ãƒ¼ã‚º"""
        if self.cursor:
            self.cursor.close()
        if self.connection:
            self.connection.close()


def demo_analysis():
    """ãƒ‡ãƒ¢åˆ†æã®å®Ÿè¡Œ"""
    print("ğŸ” pgvector A2A Semantic Analysis Demo")
    print("=" * 60)

    analyzer = PgVectorA2AAnalyzer()

    try:
        # æ¥ç¶š
        analyzer.connect()
        analyzer.setup_openai()

        # 1. é¡ä¼¼é€šä¿¡æ¤œç´¢
        print("\n1. Similarity Search Demo")
        print("-" * 40)

        similarity_query = SemanticQuery(
            query_text="system overload error critical",
            query_type=AnalysisType.SIMILARITY_SEARCH,
            limit=5,
            threshold=0.7,
        )

        result = analyzer.execute_analysis(similarity_query)
        print(f"Found {len(result.results)} similar communications")
        print(f"Average similarity: {result.insights.get('avg_similarity', 0):.3f}")
        print(f"Query time: {result.performance_metrics['query_time_ms']:.2f}ms")

        # 2. ç•°å¸¸æ¤œå‡º
        print("\n2. Anomaly Detection Demo")
        print("-" * 40)

        anomaly_query = SemanticQuery(
            query_text="unusual system behavior",
            query_type=AnalysisType.ANOMALY_DETECTION,
            limit=10,
            time_window=timedelta(hours=24),
        )

        result = analyzer.execute_analysis(anomaly_query)
        print(f"Detected {result.insights.get('anomaly_count', 0)} anomalies")
        print(f"Severity distribution: {result.insights.get('severity_distribution', {})}")

        # 3. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•åˆ†æ
        print("\n3. Agent Behavior Analysis Demo")
        print("-" * 40)

        # ã¾ãšã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’å–å¾—
        analyzer.cursor.execute("SELECT DISTINCT agent_name FROM a2a.agents LIMIT 1;")
        agent = analyzer.cursor.fetchone()

        if agent:
            agent_query = SemanticQuery(
                query_text="", query_type=AnalysisType.AGENT_BEHAVIOR, filters={"agent": agent["agent_name"]}
            )

            result = analyzer.execute_analysis(agent_query)
            if result.results:
                stats = result.results[0].get("communication_stats", {})
                print(f"Agent: {agent['agent_name']}")
                print(f"Total messages: {stats.get('total_messages', 0)}")
                print(f"Unique interactions: {stats.get('unique_interactions', 0)}")

        # 4. æ™‚ç³»åˆ—åˆ†æ
        print("\n4. Temporal Analysis Demo")
        print("-" * 40)

        temporal_query = SemanticQuery(query_text="", query_type=AnalysisType.TEMPORAL_ANALYSIS)

        result = analyzer.execute_analysis(temporal_query)
        print(f"Trend: {result.insights.get('trend', 'unknown')}")
        print(f"Spike count: {result.insights.get('spike_count', 0)}")
        print(f"Mean messages per period: {result.insights.get('mean_messages', 0):.2f}")

    except Exception as e:
        print(f"\nâŒ Demo failed: {e}")

    finally:
        analyzer.close()


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import argparse

    parser = argparse.ArgumentParser(description="pgvector A2A Semantic Analysis")
    parser.add_argument("--demo", action="store_true", help="Run demo analysis")
    parser.add_argument("--query", type=str, help="Semantic search query")
    parser.add_argument(
        "--type", choices=[t.value for t in AnalysisType], default="similarity_search", help="Analysis type"
    )
    parser.add_argument("--limit", type=int, default=10, help="Result limit")
    parser.add_argument("--agent", type=str, help="Agent name for behavior analysis")

    args = parser.parse_args()

    if args.demo:
        demo_analysis()
    else:
        analyzer = PgVectorA2AAnalyzer()

        try:
            analyzer.connect()
            analyzer.setup_openai()

            # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
            query = SemanticQuery(query_text=args.query or "", query_type=AnalysisType(args.type), limit=args.limit)

            if args.agent:
                query.filters = {"agent": args.agent}

            result = analyzer.execute_analysis(query)

            # çµæœè¡¨ç¤º
            print("\nğŸ“Š Analysis Results")
            print("=" * 60)
            print(f"Query type: {result.query.query_type.value}")
            print(f"Results found: {len(result.results)}")
            print(f"Query time: {result.performance_metrics['query_time_ms']:.2f}ms")

            print("\nğŸ’¡ Insights:")
            for key, value in result.insights.items():
                print(f"  {key}: {value}")

            if result.results:
                print("\nğŸ” Top Results:")
                for i, res in enumerate(result.results[:5], 1):
                    if "sender" in res:
                        print(f"\n{i}. {res.get('sender')} â†’ {res.get('receiver')}")
                        print(f"   Type: {res.get('message_type')}")
                        if "similarity" in res:
                            print(f"   Similarity: {res.get('similarity', 0):.3f}")
                    else:
                        print(f"\n{i}. {res}")

        except Exception as e:
            print(f"\nâŒ Analysis failed: {e}")

        finally:
            analyzer.close()


if __name__ == "__main__":
    main()
