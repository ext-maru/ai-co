#!/usr/bin/env python3
"""
PostgreSQLçµ±ä¸€ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ 
Elders Guild SQLite â†’ PostgreSQL Complete Migration System
"""

import asyncio
import json
import logging
import os
import sqlite3
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import asyncpg

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class PostgreSQLUnificationMigrator:
    """PostgreSQLçµ±ä¸€ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.pg_url = os.getenv(
            "GRIMOIRE_DATABASE_URL",
            "postgresql://aicompany@localhost:5432/ai_company_grimoire",
        )
        self.migration_stats = {
            "start_time": None,
            "end_time": None,
            "tasks_migrated": 0,
            "conversations_migrated": 0,
            "errors": [],
        }
        logger.info("ğŸš€ PostgreSQLçµ±ä¸€ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    async def create_unified_schemas(self):
        """çµ±åˆã‚¹ã‚­ãƒ¼ãƒä½œæˆ"""
        logger.info("ğŸ“‹ çµ±åˆPostgreSQLã‚¹ã‚­ãƒ¼ãƒä½œæˆä¸­...")

        conn = await asyncpg.connect(self.pg_url)

        # ã‚¿ã‚¹ã‚¯çµ±åˆãƒ†ãƒ¼ãƒ–ãƒ«
        await conn.execute(
            """
            CREATE TABLE IF NOT EXISTS unified_tasks (
                id SERIAL PRIMARY KEY,
                task_id VARCHAR(255) UNIQUE,
                title TEXT NOT NULL,
                description TEXT,
                status VARCHAR(50) DEFAULT 'pending',
                priority VARCHAR(20) DEFAULT 'medium',
                assigned_sage VARCHAR(50),
                created_at TIMESTAMP DEFAULT NOW(),
                updated_at TIMESTAMP DEFAULT NOW(),
                completed_at TIMESTAMP,
                metadata JSONB,
                -- AIæ©Ÿèƒ½çµ±åˆ
                task_embedding VECTOR(1536),
                complexity_score FLOAT,
                estimated_duration INTERVAL,
                -- 4è³¢è€…çµ±åˆ
                knowledge_refs TEXT[],
                incident_refs TEXT[],
                rag_context JSONB
            )
        """
        )

        # ä¼šè©±çµ±åˆãƒ†ãƒ¼ãƒ–ãƒ«
        await conn.execute(
            """
            CREATE TABLE IF NOT EXISTS unified_conversations (
                id SERIAL PRIMARY KEY,
                conversation_id VARCHAR(255),
                session_id VARCHAR(255),
                user_message TEXT,
                ai_response TEXT,
                timestamp TIMESTAMP DEFAULT NOW(),
                context JSONB,
                -- AIæ©Ÿèƒ½çµ±åˆ
                message_embedding VECTOR(1536),
                response_embedding VECTOR(1536),
                quality_score FLOAT,
                sentiment_score FLOAT,
                -- çµ±åˆæ©Ÿèƒ½
                related_tasks TEXT[],
                knowledge_used TEXT[],
                sage_consulted VARCHAR(50)
            )
        """
        )

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        logger.info("ğŸ” ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆä¸­...")

        # ã‚¿ã‚¹ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        await conn.execute(
            "CREATE INDEX IF NOT EXISTS idx_unified_tasks_status ON unified_tasks(status)"
        )
        await conn.execute(
            "CREATE INDEX IF NOT EXISTS idx_unified_tasks_sage ON unified_tasks(assigned_sage)"
        )
        await conn.execute(
            "CREATE INDEX IF NOT EXISTS idx_unified_tasks_priority ON unified_tasks(priority, created_at)"
        )
        await conn.execute(
            "CREATE INDEX IF NOT EXISTS idx_unified_tasks_metadata ON unified_tasks USING gin(metadata)"
        )

        # ä¼šè©±ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        await conn.execute(
            "CREATE INDEX IF NOT EXISTS idx_conversations_timestamp ON unified_conversations(timestamp)"
        )
        await conn.execute(
            "CREATE INDEX IF NOT EXISTS idx_conversations_session ON unified_conversations(session_id)"
        )

        # å…¨æ–‡æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        try:
            await conn.execute(
                """
                CREATE INDEX IF NOT EXISTS idx_conversations_fts ON unified_conversations
                USING gin(to_tsvector('english', COALESCE(user_message, '') || ' ' || COALESCE(ai_response, '')))
            """
            )
        except Exception as e:
            logger.warning(f"å…¨æ–‡æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆè­¦å‘Š: {e}")

        await conn.close()
        logger.info("âœ… çµ±åˆã‚¹ã‚­ãƒ¼ãƒä½œæˆå®Œäº†")

    async def migrate_task_history(self):
        """ã‚¿ã‚¹ã‚¯å±¥æ­´ç§»è¡Œ - Phase 1"""
        logger.info("ğŸ“‹ Phase 1: ã‚¿ã‚¹ã‚¯å±¥æ­´ç§»è¡Œé–‹å§‹...")

        # è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯DBã‚’ç¢ºèª
        task_databases = ["task_history.db", "db/task_history.db", "data/tasks.db"]

        for db_path in task_databases:
            if os.path.exists(db_path):
                await self._migrate_single_task_db(db_path)

    async def _migrate_single_task_db(self, db_path):
        """å˜ä¸€ã‚¿ã‚¹ã‚¯DBç§»è¡Œ"""
        logger.info(f"ğŸ“‹ {db_path} ã‹ã‚‰ã‚¿ã‚¹ã‚¯ç§»è¡Œé–‹å§‹...")

        # SQLiteã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        sqlite_conn = sqlite3.connect(db_path)
        sqlite_conn.row_factory = sqlite3.Row
        cursor = sqlite_conn.cursor()

        # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ç¢ºèª
        try:
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            logger.info(f"SQLiteãƒ†ãƒ¼ãƒ–ãƒ«: {tables}")

            if "task_history" in tables:
                table_name = "task_history"
            elif "tasks" in tables:
                table_name = "tasks"
            else:
                logger.error("ã‚¿ã‚¹ã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return

            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            cursor.execute(f"SELECT * FROM {table_name}")
            tasks = cursor.fetchall()
            logger.info(f"ç§»è¡Œå¯¾è±¡ã‚¿ã‚¹ã‚¯: {len(tasks)}ä»¶")

            # PostgreSQLã«ç§»è¡Œ
            pg_conn = await asyncpg.connect(self.pg_url)

            for task in tasks:
                try:
                    # ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
                    task_dict = dict(task)

                    # åŸºæœ¬ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
                    task_id = task_dict.get("task_id") or task_dict.get(
                        "id", f"task_{datetime.now().timestamp()}"
                    )
                    title = (
                        task_dict.get("title")
                        or task_dict.get("description", "Untitled Task")[:100]
                    )
                    description = task_dict.get("description") or task_dict.get(
                        "content", ""
                    )
                    status = task_dict.get("status", "pending")
                    priority = task_dict.get("priority", "medium")

                    # æ—¥æ™‚å¤‰æ›
                    created_at = task_dict.get("created_at") or task_dict.get(
                        "timestamp", datetime.now()
                    )
                    if isinstance(created_at, str):
                        # Deep nesting detected (depth: 5) - consider refactoring
                        try:
                            # ç°¡å˜ãªæ—¥æ™‚ãƒ‘ãƒ¼ã‚¹
                            created_at = datetime.fromisoformat(
                                created_at.replace("Z", "+00:00")
                            )
                        except:
                            created_at = datetime.now()

                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                    metadata = {
                        "migrated_from": "sqlite_task_history",
                        "migration_date": datetime.now().isoformat(),
                        "original_data": {k: str(v) for k, v in task_dict.items()},
                    }

                    await pg_conn.execute(
                        """
                        INSERT INTO unified_tasks (
                            task_id, title, description, status, priority,
                            created_at, metadata, assigned_sage
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                        ON CONFLICT (task_id) DO UPDATE SET
                            title = EXCLUDED.title,
                            description = EXCLUDED.description,
                            metadata = EXCLUDED.metadata
                    """,
                        task_id,
                        title,
                        description,
                        status,
                        priority,
                        created_at,
                        json.dumps(metadata),
                        "knowledge_sage",
                    )

                    self.migration_stats["tasks_migrated"] += 1

                except Exception as e:
                    logger.error(f"ã‚¿ã‚¹ã‚¯ç§»è¡Œã‚¨ãƒ©ãƒ¼: {e}")
                    self.migration_stats["errors"].append(f"Task migration: {str(e)}")

            await pg_conn.close()
            sqlite_conn.close()

            logger.info(f"âœ… ã‚¿ã‚¹ã‚¯ç§»è¡Œå®Œäº†: {self.migration_stats['tasks_migrated']}ä»¶")

        except Exception as e:
            logger.error(f"âŒ ã‚¿ã‚¹ã‚¯ç§»è¡Œå¤±æ•—: {e}")
            self.migration_stats["errors"].append(f"Task migration failed: {str(e)}")

    async def migrate_conversations(self):
        """ä¼šè©±å±¥æ­´ç§»è¡Œ - Phase 2"""
        logger.info("ğŸ’¬ Phase 2: ä¼šè©±å±¥æ­´ç§»è¡Œé–‹å§‹...")

        if not os.path.exists("conversations.db"):
            logger.warning("âš ï¸ conversations.db ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        # SQLiteã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        sqlite_conn = sqlite3.connect("conversations.db")
        sqlite_conn.row_factory = sqlite3.Row
        cursor = sqlite_conn.cursor()

        try:
            # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ç¢ºèª
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            logger.info(f"ä¼šè©±ãƒ†ãƒ¼ãƒ–ãƒ«: {tables}")

            conversation_table = None
            for table in ["conversations", "conversation_history", "messages"]:
                if table in tables:
                    conversation_table = table
                    break

            if not conversation_table:
                logger.warning("ä¼šè©±ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return

            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            cursor.execute(f"SELECT * FROM {conversation_table}")
            conversations = cursor.fetchall()
            logger.info(f"ç§»è¡Œå¯¾è±¡ä¼šè©±: {len(conversations)}ä»¶")

            # PostgreSQLã«ç§»è¡Œ
            pg_conn = await asyncpg.connect(self.pg_url)

            for conv in conversations:
                try:
                    conv_dict = dict(conv)

                    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
                    conversation_id = conv_dict.get("conversation_id") or conv_dict.get(
                        "id", f"conv_{datetime.now().timestamp()}"
                    )
                    user_message = conv_dict.get("user_message") or conv_dict.get(
                        "message", ""
                    )
                    ai_response = conv_dict.get("ai_response") or conv_dict.get(
                        "response", ""
                    )

                    # æ—¥æ™‚å¤‰æ›
                    timestamp = conv_dict.get("timestamp") or conv_dict.get(
                        "created_at", datetime.now()
                    )
                    if isinstance(timestamp, str):
                        # Deep nesting detected (depth: 5) - consider refactoring
                        try:
                            # ç°¡å˜ãªæ—¥æ™‚ãƒ‘ãƒ¼ã‚¹
                            timestamp = datetime.fromisoformat(
                                timestamp.replace("Z", "+00:00")
                            )
                        except:
                            timestamp = datetime.now()

                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
                    context = {
                        "migrated_from": "sqlite_conversations",
                        "migration_date": datetime.now().isoformat(),
                        "original_data": {k: str(v) for k, v in conv_dict.items()},
                    }

                    await pg_conn.execute(
                        """
                        INSERT INTO unified_conversations (
                            conversation_id, user_message, ai_response,
                            timestamp, context, sage_consulted
                        ) VALUES ($1, $2, $3, $4, $5, $6)
                    """,
                        conversation_id,
                        user_message,
                        ai_response,
                        timestamp,
                        json.dumps(context),
                        "knowledge_sage",
                    )

                    self.migration_stats["conversations_migrated"] += 1

                except Exception as e:
                    logger.error(f"ä¼šè©±ç§»è¡Œã‚¨ãƒ©ãƒ¼: {e}")
                    self.migration_stats["errors"].append(
                        f"Conversation migration: {str(e)}"
                    )

            await pg_conn.close()
            sqlite_conn.close()

            logger.info(f"âœ… ä¼šè©±ç§»è¡Œå®Œäº†: {self.migration_stats['conversations_migrated']}ä»¶")

        except Exception as e:
            logger.error(f"âŒ ä¼šè©±ç§»è¡Œå¤±æ•—: {e}")
            self.migration_stats["errors"].append(
                f"Conversation migration failed: {str(e)}"
            )

    async def verify_migration(self):
        """ç§»è¡Œæ¤œè¨¼"""
        logger.info("ğŸ” ç§»è¡Œçµæœæ¤œè¨¼ä¸­...")

        conn = await asyncpg.connect(self.pg_url)

        # ã‚¿ã‚¹ã‚¯ç¢ºèª
        task_count = await conn.fetchval("SELECT COUNT(*) FROM unified_tasks")
        logger.info(f"PostgreSQLçµ±åˆã‚¿ã‚¹ã‚¯: {task_count}ä»¶")

        # ä¼šè©±ç¢ºèª
        conv_count = await conn.fetchval("SELECT COUNT(*) FROM unified_conversations")
        logger.info(f"PostgreSQLçµ±åˆä¼šè©±: {conv_count}ä»¶")

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        sample_task = await conn.fetchrow("SELECT * FROM unified_tasks LIMIT 1")
        if sample_task:
            logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ã‚¿ã‚¹ã‚¯: {sample_task['title']}")

        sample_conv = await conn.fetchrow("SELECT * FROM unified_conversations LIMIT 1")
        if sample_conv:
            logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ä¼šè©±: {sample_conv['user_message'][:50]}...")

        await conn.close()

        return {
            "tasks_in_postgresql": task_count,
            "conversations_in_postgresql": conv_count,
            "migration_successful": task_count > 0 or conv_count > 0,
        }

    async def create_unified_views(self):
        """çµ±åˆãƒ“ãƒ¥ãƒ¼ä½œæˆ"""
        logger.info("ğŸ¯ çµ±åˆåˆ†æãƒ“ãƒ¥ãƒ¼ä½œæˆä¸­...")

        conn = await asyncpg.connect(self.pg_url)

        # 4è³¢è€…çµ±åˆãƒ“ãƒ¥ãƒ¼
        await conn.execute(
            """
            CREATE OR REPLACE VIEW four_sages_integrated_view AS
            SELECT
                'task' as data_type,
                t.task_id as entity_id,
                t.title as content,
                t.assigned_sage,
                t.created_at,
                t.metadata
            FROM unified_tasks t
            UNION ALL
            SELECT
                'conversation' as data_type,
                c.conversation_id as entity_id,
                c.user_message as content,
                c.sage_consulted as assigned_sage,
                c.timestamp as created_at,
                c.context as metadata
            FROM unified_conversations c
            UNION ALL
            SELECT
                'knowledge' as data_type,
                k.id::text as entity_id,
                k.content,
                'knowledge_sage' as assigned_sage,
                k.created_at,
                '{}'::jsonb as metadata
            FROM knowledge_grimoire k
        """
        )

        logger.info("âœ… çµ±åˆãƒ“ãƒ¥ãƒ¼ä½œæˆå®Œäº†")
        await conn.close()

    def generate_migration_report(self, verification_result):
        """ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        duration = (
            self.migration_stats["end_time"] - self.migration_stats["start_time"]
        ).total_seconds()

        report = {
            "migration_id": f"postgresql_unification_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "execution_time": duration,
            "statistics": self.migration_stats,
            "verification": verification_result,
            "success_rate": (
                (
                    self.migration_stats["tasks_migrated"]
                    + self.migration_stats["conversations_migrated"]
                )
                / max(
                    1,
                    self.migration_stats["tasks_migrated"]
                    + self.migration_stats["conversations_migrated"],
                )
                * 100
            )
            if (
                self.migration_stats["tasks_migrated"]
                + self.migration_stats["conversations_migrated"]
            )
            > 0
            else 0,
            "postgresql_integration": "complete",
        }

        report_file = f"postgresql_unification_report_{report['migration_id']}.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)

        logger.info(f"ğŸ“„ ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
        return report


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    migrator = PostgreSQLUnificationMigrator()

    try:
        migrator.migration_stats["start_time"] = datetime.now()

        # Phase 1: ã‚¹ã‚­ãƒ¼ãƒä½œæˆ
        logger.info("ğŸ—ï¸ Phase 1: çµ±åˆã‚¹ã‚­ãƒ¼ãƒä½œæˆ")
        await migrator.create_unified_schemas()

        # Phase 2: ã‚¿ã‚¹ã‚¯ç§»è¡Œ
        logger.info("ğŸ“‹ Phase 2: ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ")
        await migrator.migrate_task_history()

        # Phase 3: ä¼šè©±ç§»è¡Œ
        logger.info("ğŸ’¬ Phase 3: ä¼šè©±ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ")
        await migrator.migrate_conversations()

        # Phase 4: çµ±åˆãƒ“ãƒ¥ãƒ¼ä½œæˆ
        logger.info("ğŸ¯ Phase 4: çµ±åˆãƒ“ãƒ¥ãƒ¼ä½œæˆ")
        await migrator.create_unified_views()

        # æ¤œè¨¼
        logger.info("ğŸ” Phase 5: ç§»è¡Œæ¤œè¨¼")
        verification = await migrator.verify_migration()

        migrator.migration_stats["end_time"] = datetime.now()

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = migrator.generate_migration_report(verification)

        # çµæœè¡¨ç¤º
        print("\n" + "=" * 60)
        print("ğŸ‰ PostgreSQLçµ±ä¸€ç§»è¡Œå®Œäº†!")
        print("=" * 60)
        print(f"ç§»è¡Œã‚¿ã‚¹ã‚¯: {migrator.migration_stats['tasks_migrated']}ä»¶")
        print(f"ç§»è¡Œä¼šè©±: {migrator.migration_stats['conversations_migrated']}ä»¶")
        print(f"æˆåŠŸç‡: {report['success_rate']:.1f}%")
        print(f"å®Ÿè¡Œæ™‚é–“: {report['execution_time']:.2f}ç§’")

        if migrator.migration_stats["errors"]:
            print(f"\nâš ï¸ ã‚¨ãƒ©ãƒ¼: {len(migrator.migration_stats['errors'])}ä»¶")
            for error in migrator.migration_stats["errors"]:
                print(f"  - {error}")

        if verification["migration_successful"]:
            print("\nâœ… ç§»è¡Œæ¤œè¨¼: æˆåŠŸ")
            print("ğŸ›ï¸ Elders Guild PostgreSQLçµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒé–‹å§‹!")
        else:
            print("\nâŒ ç§»è¡Œæ¤œè¨¼: å¤±æ•—")

    except Exception as e:
        logger.error(f"âŒ ç§»è¡Œãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
