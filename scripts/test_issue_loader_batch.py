#!/usr/bin/env python3
"""
ã‚¤ã‚·ãƒ¥ãƒ¼ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ
è¤‡æ•°ã®GitHub Issueã‚’ä¸¦åˆ—å‡¦ç†
"""

import asyncio
import json
import os
import sys
import time
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import threading

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from libs.integrations.github.auto_issue_processor import AutoIssueProcessor
from github import Github

# é€²æ—è¡¨ç¤ºç”¨ã®ãƒ­ãƒƒã‚¯
print_lock = threading.Lock()

def safe_print(message):
    """ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªå‡ºåŠ›"""
    with print_lock:
        print(message)

async def evaluate_issue(processor, issue):
    """Issueã®è©•ä¾¡ã®ã¿å®Ÿè¡Œï¼ˆé«˜é€Ÿï¼‰"""
    try:
        complexity = await processor.evaluator.evaluate(issue)
        
        result = {
            'issue_number': issue.number,
            'title': issue.title,
            'labels': [label.name for label in issue.labels],
            'complexity_score': complexity.score,
            'is_processable': complexity.is_processable,
            'created_at': issue.created_at.isoformat(),
            'comments': issue.comments,
            'state': issue.state
        }
        
        safe_print(f"âœ… Issue #{issue.number}: è¤‡é›‘åº¦ {complexity.score:0.3f}")
        return result
        
    except Exception as e:
        safe_print(f"âŒ Issue #{issue.number}: ã‚¨ãƒ©ãƒ¼ {e}")
        return None

async def batch_process_issues(issues, max_concurrent=5)processor = AutoIssueProcessor()
"""è¤‡æ•°ã®Issueã‚’ä¸¦åˆ—å‡¦ç†"""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def process_with_limit(issue):
        async with semaphore:
            return await evaluate_issue(processor, issue)
    
    tasks = [process_with_limit(issue) for issue in issues]
    results = await asyncio.gather(*tasks)
    
    return [r for r in results if r is not None]

def analyze_batch_results(results)total = len(results)
"""ãƒãƒƒãƒå‡¦ç†çµæœã®åˆ†æ"""
    processable = sum(1 for r in results if r['is_processable'])
    
    # è¤‡é›‘åº¦ã®çµ±è¨ˆ
    complexities = [r['complexity_score'] for r in results]
    avg_complexity = sum(complexities) / len(complexities) if complexities else 0
    
    # ãƒ©ãƒ™ãƒ«åˆ¥çµ±è¨ˆ
    label_stats = {}
    for result in results:
        for label in result['labels']:
            label_stats[label] = label_stats.get(label, 0) + 1
    
    # å‡¦ç†å¯èƒ½ãªIssueã®è©³ç´°
    processable_issues = [r for r in results if r['is_processable']]
    
    return {
        'total_issues': total,
        'processable_count': processable,
        'processable_rate': processable / total * 100 if total > 0 else 0,
        'average_complexity': avg_complexity,
        'complexity_range': {
            'min': min(complexities) if complexities else 0,
            'max': max(complexities) if complexities else 0
        },
        'label_distribution': label_stats,
        'processable_issues': processable_issues
    }

async def main()print("="*80)
"""ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ ã‚¤ã‚·ãƒ¥ãƒ¼ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ")
    print("="*80)
    
    start_time = time.time()
    
    # GitHub APIã§Issueã‚’å–å¾—
    print("\nğŸ“‹ GitHub Issueã‚’å–å¾—ä¸­...")
    github_token = os.getenv("GITHUB_TOKEN")
    repo_owner = os.getenv("GITHUB_REPO_OWNER", "ext-maru")
    repo_name = os.getenv("GITHUB_REPO_NAME", "ai-co")
    
    github = Github(github_token)
    repo = github.get_repo(f"{repo_owner}/{repo_name}")
    
    # ã‚ªãƒ¼ãƒ—ãƒ³ãªIssueã¨ã‚¯ãƒ­ãƒ¼ã‚ºã•ã‚ŒãŸIssueã‚’å–å¾—
    print("  - ã‚ªãƒ¼ãƒ—ãƒ³ãªIssueã‚’å–å¾—ä¸­...")
    open_issues = list(repo.get_issues(state='open'))[:20]
    
    print("  - æœ€è¿‘ã‚¯ãƒ­ãƒ¼ã‚ºã•ã‚ŒãŸIssueã‚’å–å¾—ä¸­...")
    closed_issues = list(repo.get_issues(state='closed'))[:10]
    
    all_issues = open_issues + closed_issues
    
    # PRã‚’é™¤å¤–
    issues = [issue for issue in all_issues if not issue.pull_request]
    
    print(f"\nğŸ“Š å–å¾—çµæœ:")
    print(f"  - ç·Issueæ•°: {len(issues)}")
    print(f"  - ã‚ªãƒ¼ãƒ—ãƒ³: {sum(1 for i in issues if i.state }")
    print(f"  - ã‚¯ãƒ­ãƒ¼ã‚º: {sum(1 for i in issues if i.state }")
    
    # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
    print(f"\nâš¡ ãƒãƒƒãƒå‡¦ç†é–‹å§‹ï¼ˆä¸¦åˆ—åº¦: 5ï¼‰...")
    batch_start = time.time()
    
    results = await batch_process_issues(issues, max_concurrent=5)
    
    batch_time = time.time() - batch_start
    print(f"\nâœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†: {batch_time:0.2f}ç§’")
    print(f"  - å‡¦ç†é€Ÿåº¦: {len(results)/batch_time:0.1f} issues/ç§’")
    
    # çµæœåˆ†æ
    print("\nğŸ“Š å‡¦ç†çµæœåˆ†æ...")
    analysis = analyze_batch_results(results)
    
    print(f"\nğŸ“ˆ çµ±è¨ˆæƒ…å ±:")
    print(f"  - å‡¦ç†æˆåŠŸ: {len(results)}/{len(issues)} Issues")
    print(f"  - å‡¦ç†å¯èƒ½: {analysis['processable_count']} ({analysis['processable_rate']:0.1f}%)")
    print(f"  - å¹³å‡è¤‡é›‘åº¦: {analysis['average_complexity']:0.3f}")
    print(f"  - è¤‡é›‘åº¦ç¯„å›²: {analysis['complexity_range']['min']:0.3f} - {analysis['complexity_range']['max']:0.3f}")
    
    print(f"\nğŸ·ï¸ ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
    for label, count in sorted(analysis['label_distribution'].items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  - {label}: {count}ä»¶")
    
    print(f"\nâœ… å‡¦ç†å¯èƒ½ãªIssue Top 10:")
    for issue in sorted(analysis['processable_issues'], key=lambda x: x['complexity_score'])[:10]:
        print(f"  - Issue #{issue['issue_number']}: {issue['title'][:50]}... (è¤‡é›‘åº¦: {issue['complexity_score']:0.3f})")
    
    # ç·å‡¦ç†æ™‚é–“
    total_time = time.time() - start_time
    print(f"\nâ±ï¸ ç·å‡¦ç†æ™‚é–“: {total_time:0.2f}ç§’")
    
    # çµæœã‚’JSONã«ä¿å­˜
    output_dir = Path("batch_processing_results")
    output_dir.mkdir(exist_ok=True)
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'repository': f"{repo_owner}/{repo_name}",
        'total_issues_fetched': len(all_issues),
        'issues_processed': len(issues),
        'batch_processing_time': batch_time,
        'total_time': total_time,
        'analysis': analysis,
        'detailed_results': results
    }
    
    report_file = output_dir / f"batch_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    report_file.write_text(json.dumps(report, indent=2, ensure_ascii=False))
    
    print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨å®š
    print(f"\nğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨å®š:")
    issues_per_hour = (len(results) / batch_time) * 3600
    print(f"  - æ¨å®šå‡¦ç†èƒ½åŠ›: {issues_per_hour:0.0f} issues/hour")
    print(f"  - ãƒªãƒã‚¸ãƒˆãƒªå…¨Issueå‡¦ç†æ™‚é–“: {len(all_issues) / (len(results) / batch_time) / 60:0.1f}åˆ†")

if __name__ == "__main__":
    asyncio.run(main())