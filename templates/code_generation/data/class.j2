#!/usr/bin/env python3
"""
Auto-generated Data Processing implementation for Issue #{{ issue_number }}
{{ issue_title }}

Generated by Elder Flow Auto Issue Processor with Jinja2 Templates
"""

{{ imports | join('\n') }}


class {{ class_name }}:
    """
    Data processing implementation for Issue #{{ issue_number }}
    
    This class implements the requirements specified in the issue:
    {{ issue_title }}
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize data processor
        
        Args:
            config: Configuration dictionary
        """
        self.logger = logging.getLogger(__name__)
        self.config = config or {}
        
        # Data processing configuration
        self.input_path = self.config.get('input_path', 'data/input')
        self.output_path = self.config.get('output_path', 'data/output')
        self.chunk_size = self.config.get('chunk_size', 10000)
        self.encoding = self.config.get('encoding', 'utf-8')
        
        # Initialize components
        self._initialize_components()
        
        self.logger.info(f"Data processor initialized for Issue #{{ issue_number }}")
    
    def _initialize_components(self):
        """Initialize required components"""
        # Create output directory if it doesn't exist
        Path(self.output_path).mkdir(parents=True, exist_ok=True)
        
        # Data validation rules
        self.validation_rules = self.config.get('validation_rules', {})
        
        # Processing pipeline
        self.pipeline_steps = []
        
        {% if 'csv' in issue_body.lower() %}
        # CSV processing configuration
        self.csv_options = {
            'delimiter': self.config.get('csv_delimiter', ','),
            'quotechar': self.config.get('csv_quotechar', '"'),
            'escapechar': self.config.get('csv_escapechar', None),
        }
        {% endif %}
        
        {% if 'excel' in issue_body.lower() or 'xlsx' in issue_body.lower() %}
        # Excel processing configuration
        self.excel_options = {
            'engine': 'openpyxl',
            'sheet_name': self.config.get('sheet_name', None),
        }
        {% endif %}
        
        {% if 'json' in issue_body.lower() %}
        # JSON processing configuration
        self.json_options = {
            'orient': self.config.get('json_orient', 'records'),
            'lines': self.config.get('json_lines', False),
        }
        {% endif %}
    
    def execute(self, **kwargs) -> Dict[str, Any]:
        """
        Execute data processing operation
        
        Args:
            **kwargs: Operation parameters
            
        Returns:
            Dict containing operation results
        """
        try:
            self.logger.info(f"Executing data processing for Issue #{{ issue_number }}")
            
            # Validate input
            validation_result = self._validate_input(**kwargs)
            if not validation_result['valid']:
                return {
                    'success': False,
                    'error': validation_result['error'],
                    'issue_number': {{ issue_number }}
                }
            
            # Load data
            input_file = kwargs.get('input_file', None)
            data = self._load_data(input_file, **kwargs)
            
            if data is None:
                return {
                    'success': False,
                    'error': 'Failed to load data',
                    'issue_number': {{ issue_number }}
                }
            
            # Process data through pipeline
            processed_data = self._process_data(data, **kwargs)
            
            # Save results
            output_file = kwargs.get('output_file', f'processed_issue_{{ issue_number }}.csv')
            save_result = self._save_data(processed_data, output_file, **kwargs)
            
            # Generate summary statistics
            summary = self._generate_summary(processed_data)
            
            self.logger.info("Data processing completed successfully")
            return {
                'success': True,
                'result': {
                    'input_shape': data.shape if hasattr(data, 'shape') else len(data),
                    'output_shape': processed_data.shape if hasattr(processed_data, 'shape') else len(processed_data),
                    'output_file': save_result['file_path'],
                    'summary': summary
                },
                'issue_number': {{ issue_number }}
            }
            
        except Exception as e:
            self.logger.error(f"Data processing error: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'issue_number': {{ issue_number }}
            }
    
    def _validate_input(self, **kwargs) -> Dict[str, Any]:
        """Validate input parameters"""
        # Check for required parameters
        if 'input_file' not in kwargs and not kwargs.get('data'):
            return {'valid': False, 'error': 'No input file or data provided'}
        
        # Validate file exists if provided
        if 'input_file' in kwargs:
            input_path = Path(self.input_path) / kwargs['input_file']
            if not input_path.exists():
                return {'valid': False, 'error': f'Input file not found: {input_path}'}
        
        return {'valid': True}
    
    def _load_data(self, input_file: Optional[str], **kwargs) -> Optional[pd.DataFrame]:
        """Load data from various sources"""
        try:
            # If data is provided directly
            if 'data' in kwargs:
                return pd.DataFrame(kwargs['data'])
            
            if not input_file:
                return None
            
            input_path = Path(self.input_path) / input_file
            file_extension = input_path.suffix.lower()
            
            {% if 'csv' in issue_body.lower() %}
            if file_extension == '.csv':
                return pd.read_csv(
                    input_path,
                    encoding=self.encoding,
                    **self.csv_options
                )
            {% endif %}
            
            {% if 'excel' in issue_body.lower() or 'xlsx' in issue_body.lower() %}
            if file_extension in ['.xlsx', '.xls']:
                return pd.read_excel(
                    input_path,
                    **self.excel_options
                )
            {% endif %}
            
            {% if 'json' in issue_body.lower() %}
            if file_extension == '.json':
                if self.json_options.get('lines'):
                    return pd.read_json(input_path, lines=True)
                else:
                    return pd.read_json(input_path, orient=self.json_options['orient'])
            {% endif %}
            
            {% if 'parquet' in issue_body.lower() %}
            if file_extension == '.parquet':
                return pd.read_parquet(input_path)
            {% endif %}
            
            # Default: try to read as CSV
            return pd.read_csv(input_path, encoding=self.encoding)
            
        except Exception as e:
            self.logger.error(f"Error loading data: {e}")
            return None
    
    def _process_data(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """Process data through the pipeline"""
        processed = data.copy()
        
        # Apply pipeline steps
        for step in self.pipeline_steps:
            processed = step(processed)
        
        {% if 'clean' in issue_body.lower() or 'missing' in issue_body.lower() %}
        # Data cleaning
        processed = self._clean_data(processed)
        {% endif %}
        
        {% if 'transform' in issue_body.lower() %}
        # Data transformation
        processed = self._transform_data(processed)
        {% endif %}
        
        {% if 'aggregate' in issue_body.lower() or 'group' in issue_body.lower() %}
        # Data aggregation
        if 'group_by' in kwargs:
            processed = self._aggregate_data(processed, kwargs['group_by'])
        {% endif %}
        
        {% if 'filter' in issue_body.lower() %}
        # Data filtering
        if 'filter_condition' in kwargs:
            processed = self._filter_data(processed, kwargs['filter_condition'])
        {% endif %}
        
        {% if 'sort' in issue_body.lower() or 'order' in issue_body.lower() %}
        # Data sorting
        if 'sort_by' in kwargs:
            processed = processed.sort_values(by=kwargs['sort_by'])
        {% endif %}
        
        return processed
    
    {% if 'clean' in issue_body.lower() or 'missing' in issue_body.lower() %}
    def _clean_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Clean data by handling missing values and outliers"""
        cleaned = data.copy()
        
        # Handle missing values
        numeric_columns = cleaned.select_dtypes(include=[np.number]).columns
        categorical_columns = cleaned.select_dtypes(include=['object']).columns
        
        # Fill numeric missing values with median
        for col in numeric_columns:
            if cleaned[col].isnull().any():
                cleaned[col].fillna(cleaned[col].median(), inplace=True)
        
        # Fill categorical missing values with mode
        for col in categorical_columns:
            if cleaned[col].isnull().any():
                mode_value = cleaned[col].mode()
                if not mode_value.empty:
                    cleaned[col].fillna(mode_value[0], inplace=True)
                else:
                    cleaned[col].fillna('Unknown', inplace=True)
        
        # Remove duplicates
        cleaned = cleaned.drop_duplicates()
        
        self.logger.info(f"Cleaned data: removed {len(data) - len(cleaned)} duplicate rows")
        
        return cleaned
    {% endif %}
    
    {% if 'transform' in issue_body.lower() %}
    def _transform_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Transform data based on requirements"""
        transformed = data.copy()
        
        # Apply transformations
        # Example: normalize numeric columns
        numeric_columns = transformed.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if transformed[col].std() != 0:
                transformed[f'{col}_normalized'] = (
                    (transformed[col] - transformed[col].mean()) / transformed[col].std()
                )
        
        return transformed
    {% endif %}
    
    {% if 'aggregate' in issue_body.lower() or 'group' in issue_body.lower() %}
    def _aggregate_data(self, data: pd.DataFrame, group_by: List[str]) -> pd.DataFrame:
        """Aggregate data by specified columns"""
        # Define aggregation functions
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        agg_dict = {col: ['mean', 'sum', 'count'] for col in numeric_columns if col not in group_by}
        
        if agg_dict:
            aggregated = data.groupby(group_by).agg(agg_dict)
            # Flatten column names
            aggregated.columns = ['_'.join(col).strip() for col in aggregated.columns.values]
            aggregated = aggregated.reset_index()
            return aggregated
        else:
            return data
    {% endif %}
    
    {% if 'filter' in issue_body.lower() %}
    def _filter_data(self, data: pd.DataFrame, condition: Dict[str, Any]) -> pd.DataFrame:
        """Filter data based on conditions"""
        filtered = data.copy()
        
        for column, value in condition.items():
            if column in filtered.columns:
                if isinstance(value, dict):
                    # Complex condition (e.g., {'gt': 10, 'lt': 20})
                    if 'gt' in value:
                        filtered = filtered[filtered[column] > value['gt']]
                    if 'lt' in value:
                        filtered = filtered[filtered[column] < value['lt']]
                    if 'eq' in value:
                        filtered = filtered[filtered[column] == value['eq']]
                    if 'ne' in value:
                        filtered = filtered[filtered[column] != value['ne']]
                else:
                    # Simple equality condition
                    filtered = filtered[filtered[column] == value]
        
        self.logger.info(f"Filtered data: {len(data)} -> {len(filtered)} rows")
        return filtered
    {% endif %}
    
    def _save_data(self, data: pd.DataFrame, output_file: str, **kwargs) -> Dict[str, Any]:
        """Save processed data to file"""
        try:
            output_path = Path(self.output_path) / output_file
            file_extension = output_path.suffix.lower()
            
            {% if 'csv' in issue_body.lower() %}
            if file_extension == '.csv' or not file_extension:
                if not file_extension:
                    output_path = output_path.with_suffix('.csv')
                data.to_csv(output_path, index=False, encoding=self.encoding)
            {% endif %}
            
            {% if 'excel' in issue_body.lower() or 'xlsx' in issue_body.lower() %}
            elif file_extension in ['.xlsx', '.xls']:
                data.to_excel(output_path, index=False, engine='openpyxl')
            {% endif %}
            
            {% if 'json' in issue_body.lower() %}
            elif file_extension == '.json':
                data.to_json(
                    output_path,
                    orient=self.json_options['orient'],
                    lines=self.json_options.get('lines', False)
                )
            {% endif %}
            
            {% if 'parquet' in issue_body.lower() %}
            elif file_extension == '.parquet':
                data.to_parquet(output_path, index=False)
            {% endif %}
            
            else:
                # Default to CSV
                output_path = output_path.with_suffix('.csv')
                data.to_csv(output_path, index=False, encoding=self.encoding)
            
            self.logger.info(f"Data saved to: {output_path}")
            return {'success': True, 'file_path': str(output_path)}
            
        except Exception as e:
            self.logger.error(f"Error saving data: {e}")
            return {'success': False, 'error': str(e)}
    
    def _generate_summary(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Generate summary statistics"""
        summary = {
            'total_rows': len(data),
            'total_columns': len(data.columns),
            'column_types': data.dtypes.value_counts().to_dict(),
        }
        
        # Numeric summary
        numeric_data = data.select_dtypes(include=[np.number])
        if not numeric_data.empty:
            summary['numeric_summary'] = {
                col: {
                    'mean': float(numeric_data[col].mean()),
                    'std': float(numeric_data[col].std()),
                    'min': float(numeric_data[col].min()),
                    'max': float(numeric_data[col].max()),
                }
                for col in numeric_data.columns
            }
        
        # Categorical summary
        categorical_data = data.select_dtypes(include=['object'])
        if not categorical_data.empty:
            summary['categorical_summary'] = {
                col: {
                    'unique_values': int(categorical_data[col].nunique()),
                    'most_common': categorical_data[col].value_counts().head(5).to_dict()
                }
                for col in categorical_data.columns
            }
        
        return summary
    
    def add_pipeline_step(self, step_function):
        """Add a custom processing step to the pipeline"""
        self.pipeline_steps.append(step_function)
        self.logger.info(f"Added pipeline step: {step_function.__name__}")
    
    def get_status(self) -> Dict[str, Any]:
        """Get current status of the data processor"""
        return {
            'initialized': True,
            'input_path': self.input_path,
            'output_path': self.output_path,
            'issue_number': {{ issue_number }},
            'pipeline_steps': len(self.pipeline_steps),
            'configuration': {
                'chunk_size': self.chunk_size,
                'encoding': self.encoding,
            }
        }