#!/usr/bin/env python3
"""
çµ±ä¸€Auto Issue Processor æ·±å±¤ç›£æŸ»
å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰å®Ÿè£…ã®è©³ç´°æ¤œè¨¼
"""

import ast
import re
import os
import sys
from pathlib import Path
from typing import List, Dict, Tuple

sys.path.insert(0, str(Path(__file__).parent.parent.parent))


class CodeAnalyzer:
    """ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æ"""
    
    def __init__(self):
        self.issues = []
        self.base_path = Path("/home/aicompany/ai_co/libs/auto_issue_processor")
    
    def analyze_all_files(self):
        """ã™ã¹ã¦ã®Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ"""
        for py_file in self.base_path.rglob("*.py"):
            if "__pycache__" not in str(py_file):
                self.analyze_file(py_file)
        return self.issues
    
    def analyze_file(self, file_path: Path):
        """å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # 1. ASTè§£æ
        try:
            tree = ast.parse(content)
            self.analyze_ast(tree, file_path)
        except SyntaxError as e:
            self.issues.append(f"âŒ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ in {file_path}: {e}")
        
        # 2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        self.check_security_patterns(content, file_path)
        
        # 3. ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯
        self.check_code_quality(content, file_path)
        
        # 4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒã‚§ãƒƒã‚¯
        self.check_error_handling(content, file_path)
    
    def analyze_ast(self, tree: ast.AST, file_path: Path):
        """ASTè§£æã«ã‚ˆã‚‹å•é¡Œæ¤œå‡º"""
        class SecurityVisitor(ast.NodeVisitor):
            def __init__(self, analyzer):
                self.analyzer = analyzer
                self.file_path = file_path
            
            def visit_Call(self, node):
            """SecurityVisitorã‚¯ãƒ©ã‚¹"""
                # å±é™ºãªé–¢æ•°å‘¼ã³å‡ºã—
                if isinstance(node.func, ast.Name):
                    dangerous_funcs = ['eval', 'exec', '__import__']
                    if node.func.id in dangerous_funcs:
                """visit_Callãƒ¡ã‚½ãƒƒãƒ‰"""
                        self.analyzer.issues.append(
                            f"âŒ å±é™ºãªé–¢æ•° {node.func.id} in {self.file_path}:{node.lineno}"
                        )
                
                # os.systemå‘¼ã³å‡ºã—
                if isinstance(node.func, ast.Attribute):
                    if (isinstance(node.func.value, ast.Name) and 
                        node.func.value.id == 'os' and 
                        node.func.attr == 'system'):
                        self.analyzer.issues.append(
                            f"âŒ os.systemä½¿ç”¨ in {self.file_path}:{node.lineno}"
                        )
                
                self.generic_visit(node)
            
            def visit_Import(self, node):
                # å±é™ºãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
                for alias in node.names:
                    if alias.name in ['pickle', 'marshal']:
                """visit_Importãƒ¡ã‚½ãƒƒãƒ‰"""
                        self.analyzer.issues.append(
                            f"âš ï¸ å±é™ºãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« {alias.name} in {self.file_path}"
                        )
                self.generic_visit(node)
        
        visitor = SecurityVisitor(self)
        visitor.visit(tree)
    
    def check_security_patterns(self, content: str, file_path: Path):
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        patterns = [
            (r'token\s*=\s*["\'][^"\']+["\']', "ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰"),
            (r'password\s*=\s*["\'][^"\']+["\']', "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰"),
            (r'subprocess\.call\s*\([^)]*shell\s*=\s*True', "shell=Trueã®ä½¿ç”¨"),
            (r'\.format\s*\([^)]*\).*\bsql\b', "SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®å¯èƒ½æ€§"),
            (r'open\s*\([^)]*\)(?!.*\bencoding\b)', "ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æœªæŒ‡å®šã®open"),
        ]
        
        for pattern, description in patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE)
        # ç¹°ã‚Šè¿”ã—å‡¦ç†
            for match in matches:
                line_no = content[:match.start()].count('\n') + 1
                self.issues.append(f"âš ï¸ {description} in {file_path}:{line_no}")
    
    def check_code_quality(self, content: str, file_path: Path):
        """ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯"""
        lines = content.split('\n')
        
        # 1. é•·ã™ãã‚‹è¡Œ
        for i, line in enumerate(lines, 1):
            if len(line) > 120:
                self.issues.append(f"âš ï¸ è¡ŒãŒé•·ã™ãã‚‹({len(line)}æ–‡å­—) in {file_path}:{i}")
        
        # 2. TODO/FIXMEã‚³ãƒ¡ãƒ³ãƒˆ
        for i, line in enumerate(lines, 1):
            if 'TODO' in line or 'FIXME' in line:
                self.issues.append(f"âš ï¸ æœªå®Œäº†ã‚¿ã‚¹ã‚¯ in {file_path}:{i}")
        
        # 3. è¤‡é›‘åº¦ã®é«˜ã„é–¢æ•°ï¼ˆç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼‰
        function_lines = []
        in_function = False
        indent_level = 0
        
        for i, line in enumerate(lines, 1):
            if line.strip().startswith('def ') or line.strip().startswith('async def '):
                in_function = True
                function_lines = [i]
                indent_level = len(line) - len(line.lstrip())
            elif in_function and line.strip() and len(line) - len(line.lstrip()) <= indent_level:
                # é–¢æ•°çµ‚äº†
                if len(function_lines) > 50:
                    self.issues.append(
                        f"âš ï¸ é–¢æ•°ãŒé•·ã™ãã‚‹({len(function_lines)}è¡Œ) in {file_path}:{function_lines[0]}"
                    )
                in_function = False
            elif in_function:
                function_lines.append(i)
    
    def check_error_handling(self, content: str, file_path: Path):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒã‚§ãƒƒã‚¯"""
        # 1. è£¸ã®except
        if re.search(r'except\s*:', content):
            self.issues.append(f"âŒ è£¸ã®exceptç¯€ in {file_path}")
        
        # 2. ã‚¨ãƒ©ãƒ¼ã®æ¡ã‚Šã¤ã¶ã—
        if re.search(r'except.*:\s*\n\s*pass', content):
            self.issues.append(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ã®æ¡ã‚Šã¤ã¶ã— in {file_path}")
        
        # 3. assertã®ä½¿ç”¨ï¼ˆæœ¬ç•ªã‚³ãƒ¼ãƒ‰ã§ç„¡åŠ¹åŒ–ã•ã‚Œã‚‹ï¼‰
        if 'assert ' in content and 'test' not in str(file_path):
            self.issues.append(f"âš ï¸ æœ¬ç•ªã‚³ãƒ¼ãƒ‰ã§ã®assertä½¿ç”¨ in {file_path}")


class DependencyAnalyzer:
    """ä¾å­˜é–¢ä¿‚åˆ†æ"""
    
    def __init__(self):
        self.issues = []
        self.base_path = Path("/home/aicompany/ai_co/libs/auto_issue_processor")
    
    def analyze_dependencies(self):
        """ä¾å­˜é–¢ä¿‚ã®åˆ†æ"""
        # 1. å¾ªç’°å‚ç…§ãƒã‚§ãƒƒã‚¯
        imports = self.collect_imports()
        cycles = self.find_circular_dependencies(imports)
        
        for cycle in cycles:
            self.issues.append(f"âŒ å¾ªç’°å‚ç…§: {' -> '.join(cycle)}")
        
        # 2. æœªä½¿ç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        for file_path, imported_modules in imports.items():
            content = file_path.read_text()
            for module in imported_modules:
                # ç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆå®Œå…¨ã§ã¯ãªã„ï¼‰
                module_name = module.split('.')[-1]
                if module_name not in content:
                    self.issues.append(f"âš ï¸ æœªä½¿ç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å¯èƒ½æ€§: {module} in {file_path}")
        
        return self.issues
    
    def collect_imports(self) -> Dict[Path, List[str]]:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆæƒ…å ±ã‚’åé›†"""
        imports = {}
        
        for py_file in self.base_path.rglob("*.py"):
        # ç¹°ã‚Šè¿”ã—å‡¦ç†
            if "__pycache__" not in str(py_file):
                with open(py_file, 'r') as f:
                    content = f.read()
                
                try:
                    tree = ast.parse(content)
                    file_imports = []
                    
                    # ç¹°ã‚Šè¿”ã—å‡¦ç†
                    for node in ast.walk(tree):
                        if isinstance(node, ast.Import):
                            for alias in node.names:
                                file_imports.append(alias.name)
                        elif isinstance(node, ast.ImportFrom):
                            if node.module:
                                file_imports.append(node.module)
                    
                    imports[py_file] = file_imports
                except:
                    pass
        
        return imports
    
    def find_circular_dependencies(self, imports: Dict[Path, List[str]]) -> List[List[str]]:
        """å¾ªç’°å‚ç…§ã‚’æ¤œå‡º"""
        # ç°¡æ˜“å®Ÿè£…
        cycles = []
        # TODO: å®Ÿè£…
        return cycles


class ConfigurationAnalyzer:
    """è¨­å®šã®åˆ†æ"""
    
    def __init__(self):
        self.issues = []
    
    def analyze_configs(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ"""
        # 1. YAMLãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
        yaml_files = [
            "configs/auto_issue_processor.yaml",
            "configs/elder_scheduler_config.yaml"
        ]
        
        for yaml_file in yaml_files:
            if os.path.exists(yaml_file):
                self.check_yaml_security(yaml_file)
        
        # 2. ç’°å¢ƒå¤‰æ•°ã®ä½¿ç”¨çŠ¶æ³
        self.check_env_usage()
        
        return self.issues
    
    def check_yaml_security(self, file_path: str):
        """YAMLè¨­å®šã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯"""
        with open(file_path, 'r') as f:
            content = f.read()
        
        # å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³
        if '!!' in content:  # YAMLã‚¿ã‚°
            self.issues.append(f"âŒ å±é™ºãªYAMLã‚¿ã‚°ä½¿ç”¨ in {file_path}")
        
        if 'ghp_' in content or 'github_pat_' in content:
            self.issues.append(f"âŒ ãƒˆãƒ¼ã‚¯ãƒ³ãŒãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ in {file_path}")
    
    def check_env_usage(self):
        """ç’°å¢ƒå¤‰æ•°ã®ä½¿ç”¨ãƒã‚§ãƒƒã‚¯"""
        required_env_vars = [
            "GITHUB_TOKEN",
            "GITHUB_REPOSITORY"
        ]
        
        for env_var in required_env_vars:
            if not os.getenv(env_var):
                self.issues.append(f"âš ï¸ å¿…é ˆç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®š: {env_var}")


class ProcessLockAnalyzer:
    """ãƒ—ãƒ­ã‚»ã‚¹ãƒ­ãƒƒã‚¯ã®è©³ç´°åˆ†æ"""
    
    def __init__(self):
        self.issues = []
    
    def analyze_lock_implementation(self):
        """ãƒ­ãƒƒã‚¯å®Ÿè£…ã®åˆ†æ"""
        lock_file = Path("/home/aicompany/ai_co/libs/auto_issue_processor/utils/locking.py")
        
        if not lock_file.exists():
            self.issues.append("âŒ ãƒ­ãƒƒã‚¯å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„")
            return self.issues
        
        with open(lock_file, 'r') as f:
            content = f.read()
        
        # 1. ã‚¢ãƒˆãƒŸãƒƒã‚¯æ“ä½œã®ç¢ºèª
        if 'rename' not in content and 'link' not in content:
            self.issues.append("âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ãŒã‚¢ãƒˆãƒŸãƒƒã‚¯ã§ãªã„å¯èƒ½æ€§")
        
        # 2. ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯å¯¾ç­–
        if 'ttl' not in content.lower() and 'timeout' not in content.lower():
            self.issues.append("âŒ TTL/ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒå®Ÿè£…ã•ã‚Œã¦ã„ãªã„")
        
        # 3. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½
        if 'cleanup' not in content.lower():
            self.issues.append("âš ï¸ å¤ã„ãƒ­ãƒƒã‚¯ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ãŒãªã„")
        
        # 4. ãƒ—ãƒ­ã‚»ã‚¹ç”Ÿå­˜ç¢ºèª
        if 'psutil' in content or 'kill' in content:
            # ãƒ—ãƒ­ã‚»ã‚¹ç”Ÿå­˜ç¢ºèªã‚ã‚Š
            pass
        else:
            self.issues.append("âš ï¸ ãƒ—ãƒ­ã‚»ã‚¹ç”Ÿå­˜ç¢ºèªæ©Ÿèƒ½ãŒãªã„")
        
        return self.issues


def run_deep_audit():
    """æ·±å±¤ç›£æŸ»ã®å®Ÿè¡Œ"""
    print("=" * 80)
    print("çµ±ä¸€Auto Issue Processor æ·±å±¤ç›£æŸ»")
    print("=" * 80)
    print()
    
    all_issues = []
    
    # 1. ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æ
    print("ğŸ“ ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æ")
    print("-" * 40)
    code_analyzer = CodeAnalyzer()
    code_issues = code_analyzer.analyze_all_files()
    
    if code_issues:
        for issue in code_issues:
            print(f"  {issue}")
        all_issues.extend(code_issues)
    else:
        print("  âœ… ã‚³ãƒ¼ãƒ‰å“è³ªå•é¡Œãªã—")
    print()
    
    # 2. ä¾å­˜é–¢ä¿‚åˆ†æ
    print("ğŸ”— ä¾å­˜é–¢ä¿‚åˆ†æ")
    print("-" * 40)
    dep_analyzer = DependencyAnalyzer()
    dep_issues = dep_analyzer.analyze_dependencies()
    
    if dep_issues:
        for issue in dep_issues:
            print(f"  {issue}")
        all_issues.extend(dep_issues)
    else:
        print("  âœ… ä¾å­˜é–¢ä¿‚å•é¡Œãªã—")
    print()
    
    # 3. è¨­å®šåˆ†æ
    print("âš™ï¸ è¨­å®šåˆ†æ")
    print("-" * 40)
    config_analyzer = ConfigurationAnalyzer()
    config_issues = config_analyzer.analyze_configs()
    
    if config_issues:
        for issue in config_issues:
            print(f"  {issue}")
        all_issues.extend(config_issues)
    else:
        print("  âœ… è¨­å®šå•é¡Œãªã—")
    print()
    
    # 4. ãƒ­ãƒƒã‚¯å®Ÿè£…åˆ†æ
    print("ğŸ”’ ãƒ­ãƒƒã‚¯å®Ÿè£…åˆ†æ")
    print("-" * 40)
    lock_analyzer = ProcessLockAnalyzer()
    lock_issues = lock_analyzer.analyze_lock_implementation()
    
    if lock_issues:
        for issue in lock_issues:
            print(f"  {issue}")
        all_issues.extend(lock_issues)
    else:
        print("  âœ… ãƒ­ãƒƒã‚¯å®Ÿè£…å•é¡Œãªã—")
    print()
    
    # ç·åˆçµæœ
    print("=" * 80)
    print("æ·±å±¤ç›£æŸ»çµæœ")
    print("=" * 80)
    
    critical_count = len([i for i in all_issues if i.startswith("âŒ")])
    warning_count = len([i for i in all_issues if i.startswith("âš ï¸")])
    
    print(f"ğŸ”´ é‡å¤§ãªå•é¡Œ: {critical_count}ä»¶")
    print(f"ğŸŸ¡ è­¦å‘Š: {warning_count}ä»¶")
    print(f"ğŸ“‹ ç·å•é¡Œæ•°: {len(all_issues)}ä»¶")
    
    if critical_count > 0:
        print("\nâŒ æ·±å±¤ç›£æŸ»å¤±æ•—: é‡å¤§ãªå•é¡ŒãŒã‚ã‚Šã¾ã™")
        return False
    else:
        print("\nâœ… æ·±å±¤ç›£æŸ»åˆæ ¼")
        return True


if __name__ == "__main__":
    success = run_deep_audit()
    sys.exit(0 if success else 1)