#!/usr/bin/env python3
"""
ğŸš¨ RetryIssueReporter å³æ ¼ãƒ†ã‚¹ãƒˆãƒ»ç›£æŸ»ã‚·ã‚¹ãƒ†ãƒ 
Created: 2025-07-22 by Claude Elder
ç›®çš„: ãƒªãƒˆãƒ©ã‚¤ã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„å“è³ªæ¤œè¨¼
"""

import asyncio
import json
import logging
import os
import pytest
import sys
import tempfile
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List
from unittest.mock import AsyncMock, MagicMock, patch

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from libs.retry_issue_reporter import RetryIssueReporter, with_retry_reporting

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


class StrictRetryReporterAuditor:
    """å³æ ¼ãªRetryIssueReporterç›£æŸ»ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.audit_results = {}
        self.error_count = 0
        self.warning_count = 0
        self.critical_issues = []
    
    def record_audit(self, test_name: str, status: str, details: Dict[str, Any]):
        """ç›£æŸ»çµæœè¨˜éŒ²"""
        self.audit_results[test_name] = {
            "status": status,
            "details": details,
            "timestamp": datetime.now().isoformat()
        }
        
        if status == "FAIL":
            self.error_count += 1
            if details.get("severity") == "CRITICAL":
                self.critical_issues.append(test_name)
        elif status == "WARNING":
            self.warning_count += 1
    
    def generate_audit_report(self) -> Dict[str, Any]:
        """ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        total_tests = len(self.audit_results)
        passed_tests = sum(1 for r in self.audit_results.values() if r["status"] == "PASS")
        
        return {
            "audit_summary": {
                "total_tests": total_tests,
                "passed": passed_tests,
                "failed": self.error_count,
                "warnings": self.warning_count,
                "success_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0,
                "critical_issues": len(self.critical_issues)
            },
            "critical_issues": self.critical_issues,
            "detailed_results": self.audit_results,
            "audit_timestamp": datetime.now().isoformat(),
            "auditor": "Claude Elder - Strict Quality Gate"
        }


class MockGitHubRepo:
    """MockGitHubãƒªãƒã‚¸ãƒˆãƒªï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    
    def __init__(self):
        self.issues = {}
        self.comments = {}
    
    def get_issue(self, issue_number: int):
        """Issueå–å¾—"""
        if issue_number not in self.issues:
            self.issues[issue_number] = MockGitHubIssue(issue_number)
        return self.issues[issue_number]


class MockGitHubIssue:
    """MockGitHub Issue"""
    
    def __init__(self, number: int):
        self.number = number
        self.comments_list = []
    
    def create_comment(self, body: str):
        """ã‚³ãƒ¡ãƒ³ãƒˆä½œæˆ"""
        comment = {
            "body": body,
            "created_at": datetime.now(),
            "id": len(self.comments_list) + 1
        }
        self.comments_list.append(comment)
        return comment
    
    def get_comments(self):
        """ã‚³ãƒ¡ãƒ³ãƒˆä¸€è¦§å–å¾—"""
        return self.comments_list


@pytest.fixture
def mock_github():
    """GitHub Mock Fixture"""
    with patch('libs.retry_issue_reporter.Github') as mock_gh:
        mock_repo = MockGitHubRepo()
        mock_gh.return_value.get_repo.return_value = mock_repo
        yield mock_repo


@pytest.fixture
def audit_env():
    """ç›£æŸ»ç’°å¢ƒè¨­å®š"""
    return {
        "GITHUB_TOKEN": "test_token_strict_audit",
        "GITHUB_REPO_OWNER": "test_owner",
        "GITHUB_REPO_NAME": "test_repo"
    }


class TestRetryIssueReporterStrictAudit:
    """RetryIssueReporter å³æ ¼ç›£æŸ»ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    auditor = StrictRetryReporterAuditor()  # ã‚¯ãƒ©ã‚¹å¤‰æ•°ã¨ã—ã¦å…±æœ‰
    
    def setup_method(self):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        pass
    
    @pytest.mark.asyncio
    async def test_001_initialization_robustness(self, mock_github, audit_env):
        """ğŸ” Test 001: åˆæœŸåŒ–å …ç‰¢æ€§æ¤œè¨¼"""
        test_name = "initialization_robustness"
        
        try:
            # æ­£å¸¸åˆæœŸåŒ–
            with patch.dict(os.environ, audit_env):
                reporter = RetryIssueReporter()
                assert reporter.github_token == "test_token_strict_audit"
                assert reporter.repo_owner == "test_owner"
                assert reporter.repo_name == "test_repo"
            
            # ç’°å¢ƒå¤‰æ•°ãªã—ã§ã‚¨ãƒ©ãƒ¼å‡¦ç†ç¢ºèª
            with patch.dict(os.environ, {}, clear=True):
                try:
                    RetryIssueReporter()
                    assert False, "Should raise ValueError for missing token"
                except ValueError as e:
                    assert "GitHub token is required" in str(e)
            
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–
            reporter_custom = RetryIssueReporter(
                github_token="custom_token",
                repo_owner="custom_owner", 
                repo_name="custom_repo"
            )
            assert reporter_custom.github_token == "custom_token"
            
            TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "PASS", {
                "initialization_modes": ["env_vars", "custom_params", "error_handling"],
                "robustness_score": 95
            })
            
        except Exception as e:
            TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "FAIL", {
                "error": str(e),
                "severity": "CRITICAL"
            })
            raise
    
    @pytest.mark.asyncio
    async def test_002_session_management_integrity(self, mock_github, audit_env):
        """ğŸ” Test 002: ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†æ•´åˆæ€§æ¤œè¨¼"""
        test_name = "session_management_integrity"
        
        try:
            with patch.dict(os.environ, audit_env):
                reporter = RetryIssueReporter()
                
                # è¤‡æ•°ã‚»ãƒƒã‚·ãƒ§ãƒ³åŒæ™‚ç®¡ç†
                sessions = []
                for i in range(10):
                    session_id = reporter.start_retry_session(100 + i, f"operation_{i}")
                    sessions.append(session_id)
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDä¸€æ„æ€§ç¢ºèª
                assert len(set(sessions)) == 10, "Session IDs must be unique"
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±æ•´åˆæ€§ç¢ºèª
                for session_id in sessions:
                    summary = reporter.get_session_summary(session_id)
                    assert summary.get("session_id") == session_id
                    assert "issue_number" in summary
                    assert "operation" in summary
                    assert summary.get("attempt_count") == 0  # åˆæœŸçŠ¶æ…‹
                
                # ç„¡åŠ¹ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                invalid_summary = reporter.get_session_summary("invalid_session_id")
                assert "error" in invalid_summary
                
                TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "PASS", {
                    "concurrent_sessions": 10,
                    "uniqueness_verified": True,
                    "integrity_checks": ["session_id", "issue_number", "operation", "attempt_count"]
                })
                
        except Exception as e:
            TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "FAIL", {
                "error": str(e),
                "severity": "HIGH"
            })
            raise
    
    @pytest.mark.asyncio 
    async def test_003_retry_recording_accuracy(self, mock_github, audit_env):
        """ğŸ” Test 003: ãƒªãƒˆãƒ©ã‚¤è¨˜éŒ²ç²¾åº¦æ¤œè¨¼"""
        test_name = "retry_recording_accuracy"
        
        try:
            with patch.dict(os.environ, audit_env):
                reporter = RetryIssueReporter()
                session_id = reporter.start_retry_session(200, "accuracy_test")
                
                # è¤‡æ•°ãƒªãƒˆãƒ©ã‚¤è¨˜éŒ²
                test_errors = [
                    ConnectionError("Network timeout"),
                    ValueError("Invalid parameter"),
                    RuntimeError("System error")
                ]
                
                for i, error in enumerate(test_errors, 1):
                    await reporter.record_retry_attempt(
                        session_id=session_id,
                        attempt_number=i,
                        error=error,
                        recovery_action="RETRY",
                        recovery_message=f"Retry attempt {i}",
                        retry_delay=2.0 ** i,
                        context={"test_data": f"value_{i}"}
                    )
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹æ¤œè¨¼
                session = reporter.retry_sessions[session_id]
                assert len(session["attempts"]) == 3
                
                # å„è©¦è¡Œãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
                for i, attempt in enumerate(session["attempts"]):
                    assert attempt["attempt"] == i + 1
                    assert attempt["error_type"] == type(test_errors[i]).__name__
                    assert attempt["error_message"] == str(test_errors[i])
                    assert attempt["recovery_action"] == "RETRY"
                    assert attempt["retry_delay"] == 2.0 ** (i + 1)
                    assert attempt["context"]["test_data"] == f"value_{i + 1}"
                
                # æˆåŠŸè¨˜éŒ²
                result = {"status": "success", "data": "test_result"}
                await reporter.record_retry_success(session_id, result)
                
                assert session["final_status"] == "success"
                assert session["result"] == result
                
                TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "PASS", {
                    "retry_attempts_recorded": 3,
                    "data_accuracy": 100,
                    "context_preservation": True,
                    "final_status_recorded": True
                })
                
        except Exception as e:
            TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "FAIL", {
                "error": str(e),
                "severity": "HIGH"
            })
            raise
    
    @pytest.mark.asyncio
    async def test_004_github_comment_generation_quality(self, mock_github, audit_env):
        """ğŸ” Test 004: GitHubã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆå“è³ªæ¤œè¨¼"""
        test_name = "github_comment_generation_quality"
        
        try:
            with patch.dict(os.environ, audit_env):
                reporter = RetryIssueReporter()
                session_id = reporter.start_retry_session(300, "GitHub comment quality test")
                
                # ãƒªãƒˆãƒ©ã‚¤ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
                error = ConnectionError("Detailed connection error message")
                await reporter.record_retry_attempt(
                    session_id=session_id,
                    attempt_number=1,
                    error=error,
                    recovery_action="RETRY",
                    recovery_message="Network connection failed, retrying with backoff",
                    retry_delay=4.0,
                    context={
                        "branch_name": "test-branch",
                        "pr_number": 123,
                        "additional_info": "test context"
                    }
                )
                
                # ã‚³ãƒ¡ãƒ³ãƒˆå“è³ªæ¤œè¨¼
                issue = mock_github.get_issue(300)
                comments = issue.get_comments()
                
                assert len(comments) == 1, "One comment should be created"
                
                comment_body = comments[0]["body"]
                
                # å¿…é ˆè¦ç´ ã®å­˜åœ¨ç¢ºèª
                required_elements = [
                    "Auto Issue Processor ãƒªãƒˆãƒ©ã‚¤ #1",
                    "ğŸ• æ™‚åˆ»",
                    "ğŸ”§ æ“ä½œ", 
                    "âŒ ã‚¨ãƒ©ãƒ¼",
                    "ğŸ› ï¸ å›å¾©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
                    "ğŸ’¬ è©³ç´°",
                    "â° æ¬¡å›è©¦è¡Œã¾ã§",
                    "ğŸŒ¿ ãƒ–ãƒ©ãƒ³ãƒ",
                    "ğŸ“‹ é–¢é€£PR",
                    "ğŸ¤– è‡ªå‹•ç”Ÿæˆ"
                ]
                
                missing_elements = []
                for element in required_elements:
                    if element not in comment_body:
                        missing_elements.append(element)
                
                assert len(missing_elements) == 0, f"Missing required elements: {missing_elements}"
                
                # ã‚³ãƒ¡ãƒ³ãƒˆé•·ã•åˆ¶é™ç¢ºèª
                assert len(comment_body) < 2000, "Comment should be under GitHub's practical limit"
                
                # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ç¢ºèª
                assert comment_body.startswith("##"), "Should start with markdown header"
                assert "**" in comment_body, "Should contain bold formatting"
                
                TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "PASS", {
                    "required_elements_present": len(required_elements),
                    "comment_length": len(comment_body),
                    "markdown_formatted": True,
                    "context_integration": True
                })
                
        except Exception as e:
            TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "FAIL", {
                "error": str(e),
                "severity": "HIGH"
            })
            raise
    
    @pytest.mark.asyncio
    async def test_005_error_handling_resilience(self, mock_github, audit_env):
        """ğŸ” Test 005: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å›å¾©åŠ›æ¤œè¨¼"""
        test_name = "error_handling_resilience"
        
        try:
            with patch.dict(os.environ, audit_env):
                reporter = RetryIssueReporter()
                
                # GitHub APIå¤±æ•—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                with patch.object(reporter.repo, 'get_issue', side_effect=Exception("GitHub API Error")):
                    session_id = reporter.start_retry_session(400, "error_handling_test")
                    
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãªã„ã“ã¨ã‚’ç¢ºèª
                    await reporter.record_retry_attempt(
                        session_id=session_id,
                        attempt_number=1,
                        error=ValueError("Test error"),
                        recovery_action="RETRY",
                        recovery_message="Testing error resilience"
                    )
                    
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒã•ã‚Œã¦ã„ã‚‹
                    session = reporter.retry_sessions[session_id]
                    assert len(session["attempts"]) == 1
                
                # ç„¡åŠ¹ã‚»ãƒƒã‚·ãƒ§ãƒ³IDå‡¦ç†
                await reporter.record_retry_attempt(
                    session_id="invalid_session_id",
                    attempt_number=1,
                    error=RuntimeError("Test"),
                    recovery_action="RETRY",
                    recovery_message="Should not crash"
                )
                
                # ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿ï¼‰
                large_context = {"large_data": "x" * 10000}
                session_id_large = reporter.start_retry_session(401, "large_data_test")
                
                await reporter.record_retry_attempt(
                    session_id=session_id_large,
                    attempt_number=1,
                    error=MemoryError("Out of memory"),
                    recovery_action="ABORT",
                    recovery_message="Memory limit exceeded",
                    context=large_context
                )
                
                TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "PASS", {
                    "github_api_error_handled": True,
                    "invalid_session_handled": True,
                    "large_data_handled": True,
                    "no_crashes": True
                })
                
        except Exception as e:
            TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "FAIL", {
                "error": str(e),
                "severity": "CRITICAL"
            })
            raise
    
    @pytest.mark.asyncio
    async def test_006_performance_scalability(self, mock_github, audit_env):
        """ğŸ” Test 006: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ¤œè¨¼"""
        test_name = "performance_scalability"
        
        try:
            with patch.dict(os.environ, audit_env):
                reporter = RetryIssueReporter()
                
                # å¤§é‡ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
                start_time = time.time()
                session_ids = []
                
                for i in range(100):
                    session_id = reporter.start_retry_session(500 + i, f"perf_test_{i}")
                    session_ids.append(session_id)
                
                creation_time = time.time() - start_time
                
                # å¤§é‡ãƒªãƒˆãƒ©ã‚¤è¨˜éŒ²ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
                start_time = time.time()
                
                for session_id in session_ids[:10]:  # 10ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ãƒ†ã‚¹ãƒˆ
                    for attempt in range(1, 4):  # å„ã‚»ãƒƒã‚·ãƒ§ãƒ³3å›ãƒªãƒˆãƒ©ã‚¤
                        await reporter.record_retry_attempt(
                            session_id=session_id,
                            attempt_number=attempt,
                            error=RuntimeError(f"Test error {attempt}"),
                            recovery_action="RETRY",
                            recovery_message=f"Performance test attempt {attempt}"
                        )
                
                recording_time = time.time() - start_time
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
                import psutil
                process = psutil.Process()
                memory_usage = process.memory_info().rss / 1024 / 1024  # MB
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–
                assert creation_time < 5.0, f"Session creation too slow: {creation_time}s"
                assert recording_time < 10.0, f"Recording too slow: {recording_time}s"
                assert memory_usage < 100, f"Memory usage too high: {memory_usage}MB"
                
                TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "PASS", {
                    "session_creation_time": f"{creation_time:.2f}s",
                    "recording_time": f"{recording_time:.2f}s", 
                    "memory_usage": f"{memory_usage:.1f}MB",
                    "sessions_created": 100,
                    "retry_records": 30,
                    "performance_acceptable": True
                })
                
        except Exception as e:
            TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "FAIL", {
                "error": str(e),
                "severity": "MEDIUM"
            })
            raise
    
    @pytest.mark.asyncio
    async def test_007_helper_function_integration(self, mock_github, audit_env):
        """ğŸ” Test 007: ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°çµ±åˆæ¤œè¨¼"""
        test_name = "helper_function_integration"
        
        try:
            with patch.dict(os.environ, audit_env):
                
                # æˆåŠŸã™ã‚‹ãƒ†ã‚¹ãƒˆé–¢æ•°
                async def successful_function(value):
                    return {"result": value, "status": "success"}
                
                result = await with_retry_reporting(
                    successful_function,
                    issue_number=600,
                    operation="helper_success_test",
                    max_retries=3,
                    value="test_value"
                )
                
                assert result["result"] == "test_value"
                assert result["status"] == "success"
                
                # å¤±æ•—ã—ã¦ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ãƒ†ã‚¹ãƒˆé–¢æ•°
                call_count = 0
                async def failing_then_success_function():
                    nonlocal call_count
                    call_count += 1
                    if call_count <= 2:
                        raise ConnectionError(f"Simulated failure #{call_count}")
                    return {"result": "success_after_retries", "call_count": call_count}
                
                call_count = 0  # ãƒªã‚»ãƒƒãƒˆ
                result = await with_retry_reporting(
                    failing_then_success_function,
                    issue_number=601,
                    operation="helper_retry_test", 
                    max_retries=4
                )
                
                assert result["result"] == "success_after_retries"
                assert result["call_count"] == 3
                
                # æœ€çµ‚çš„ã«å¤±æ•—ã™ã‚‹ãƒ†ã‚¹ãƒˆ
                async def always_failing_function():
                    raise RuntimeError("Always fails")
                
                try:
                    await with_retry_reporting(
                        always_failing_function,
                        issue_number=602,
                        operation="helper_failure_test",
                        max_retries=2
                    )
                    assert False, "Should raise exception"
                except RuntimeError as e:
                    assert "Always fails" in str(e)
                
                TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "PASS", {
                    "success_case": True,
                    "retry_success_case": True,
                    "final_failure_case": True,
                    "helper_function_working": True
                })
                
        except Exception as e:
            TestRetryIssueReporterStrictAudit.auditor.record_audit(test_name, "FAIL", {
                "error": str(e),
                "severity": "HIGH"
            })
            raise
    
    def test_zzz_generate_comprehensive_audit_report(self):
        """ğŸ” Test ZZZ: åŒ…æ‹¬çš„ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        audit_report = TestRetryIssueReporterStrictAudit.auditor.generate_audit_report()
        
        # ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        report_file = Path(__file__).parent / "retry_issue_reporter_strict_audit_report.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(audit_report, f, indent=2, ensure_ascii=False)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        print("\n" + "="*80)
        print("ğŸš¨ RETRY ISSUE REPORTER - å³æ ¼ç›£æŸ»çµæœ")
        print("="*80)
        print(f"ğŸ“Š ç·ãƒ†ã‚¹ãƒˆæ•°: {audit_report['audit_summary']['total_tests']}")
        print(f"âœ… æˆåŠŸ: {audit_report['audit_summary']['passed']}")
        print(f"âŒ å¤±æ•—: {audit_report['audit_summary']['failed']}")
        print(f"âš ï¸ è­¦å‘Š: {audit_report['audit_summary']['warnings']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {audit_report['audit_summary']['success_rate']:.1f}%")
        print(f"ğŸš¨ é‡å¤§å•é¡Œ: {audit_report['audit_summary']['critical_issues']}")
        
        if audit_report['critical_issues']:
            print(f"\nğŸš¨ é‡å¤§å•é¡Œè©³ç´°:")
            for issue in audit_report['critical_issues']:
                print(f"  - {issue}")
        
        print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
        print("="*80)
        
        # å“è³ªã‚²ãƒ¼ãƒˆåˆ¤å®š
        success_rate = audit_report['audit_summary']['success_rate']
        critical_issues = audit_report['audit_summary']['critical_issues']
        
        if success_rate >= 95 and critical_issues == 0:
            print("ğŸ‰ å“è³ªã‚²ãƒ¼ãƒˆ: åˆæ ¼ (EXCELLENT)")
        elif success_rate >= 85 and critical_issues <= 1:
            print("âœ… å“è³ªã‚²ãƒ¼ãƒˆ: åˆæ ¼ (GOOD)")
        elif success_rate >= 70:
            print("âš ï¸ å“è³ªã‚²ãƒ¼ãƒˆ: æ¡ä»¶ä»˜ãåˆæ ¼ (ACCEPTABLE)")
        else:
            print("âŒ å“è³ªã‚²ãƒ¼ãƒˆ: ä¸åˆæ ¼ (REQUIRES_IMPROVEMENT)")
            raise AssertionError("Quality gate failed - requires improvement")


if __name__ == "__main__":
    # pytestå®Ÿè¡Œ
    pytest_args = [
        __file__,
        "-v",
        "--tb=short",
        "-x",  # æœ€åˆã®ã‚¨ãƒ©ãƒ¼ã§åœæ­¢
        "--asyncio-mode=auto"
    ]
    
    exit_code = pytest.main(pytest_args)
    sys.exit(exit_code)