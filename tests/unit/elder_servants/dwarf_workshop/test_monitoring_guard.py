"""
MonitoringGuard (D15) ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹

TDDæ–¹å¼ã§ç›£è¦–è­¦å‚™å“¡ã®å„æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚
24/7ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†æ©Ÿèƒ½ã®å“è³ªã‚’ä¿è¨¼ã€‚
"""

import asyncio
import pytest
from unittest.mock import Mock, patch, AsyncMock
import sys
import os

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(os.path.join(os.path.dirname(__file__), "../../.."))

from libs.elder_servants.dwarf_workshop.monitoring_guard import (
    MonitoringGuard,
    MonitoringGuardRequest,
    MonitoringGuardResponse,
    MonitoringConfig,
    MetricDefinition,
    AlertRule,
    Dashboard,
    SystemHealth,
    AlertSeverity,
    MetricType,
)


class TestMonitoringGuard:
    """MonitoringGuard (D15) ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ"""

    @pytest.fixture
    def monitoring_guard(self):
        """MonitoringGuardã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ"""
        return MonitoringGuard()

    @pytest.fixture
    def basic_request(self):
        """åŸºæœ¬çš„ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ"""
        return MonitoringGuardRequest(
            action="setup_monitoring",
            target_system="web_application",
            config={
                "environment": "production",
                "web_servers": [{"host": "web1.example.com", "port": 80}],
                "databases": [{"host": "db1.example.com", "type": "postgresql"}],
                "applications": [{"host": "app1.example.com", "name": "web_app"}],
                "infrastructure": {"containers": True},
            }
        )

    def test_monitoring_guard_initialization(self, monitoring_guard):
        """ğŸ”´ MonitoringGuardã®åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        assert monitoring_guard.servant_id == "D15"
        assert monitoring_guard.name == "MonitoringGuard"
        assert monitoring_guard.monitoring_tools is not None
        assert "prometheus" in monitoring_guard.monitoring_tools
        assert "grafana" in monitoring_guard.monitoring_tools
        assert "elk" in monitoring_guard.monitoring_tools

    def test_monitoring_guard_capabilities(self, monitoring_guard):
        """ğŸ”´ MonitoringGuardã®èƒ½åŠ›ä¸€è¦§ãƒ†ã‚¹ãƒˆ"""
        capabilities = monitoring_guard.get_capabilities()
        
        assert "ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–è¨­å®š" in capabilities
        assert "ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†" in capabilities
        assert "ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†" in capabilities
        assert "ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ" in capabilities
        assert "24/7ç›£è¦–ä½“åˆ¶" in capabilities
        assert len(capabilities) >= 8

    @pytest.mark.asyncio
    async def test_validate_request_valid(self, monitoring_guard, basic_request):
        """ğŸ”´ æœ‰åŠ¹ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        is_valid = await monitoring_guard.validate_request(basic_request)
        assert is_valid is True

    @pytest.mark.asyncio
    async def test_validate_request_invalid_action(self, monitoring_guard):
        """ğŸ”´ ç„¡åŠ¹ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        invalid_request = MonitoringGuardRequest(action="invalid_action")
        is_valid = await monitoring_guard.validate_request(invalid_request)
        assert is_valid is False

    @pytest.mark.asyncio
    async def test_setup_monitoring_success(self, monitoring_guard, basic_request):
        """ğŸŸ¢ ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æˆåŠŸãƒ†ã‚¹ãƒˆ"""
        response = await monitoring_guard.process_request(basic_request)
        
        assert response.success is True
        assert response.monitoring_config is not None
        assert len(response.monitoring_config.targets) > 0
        assert len(response.monitoring_config.metrics) > 0
        assert len(response.monitoring_config.alerts) > 0
        assert response.recommendations is not None

    @pytest.mark.asyncio
    async def test_monitoring_targets_definition(self, monitoring_guard, basic_request):
        """ğŸŸ¢ ç›£è¦–ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå®šç¾©ãƒ†ã‚¹ãƒˆ"""
        response = await monitoring_guard.process_request(basic_request)
        
        assert response.success is True
        
        # ç›£è¦–ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ç¢ºèª
        targets = response.monitoring_config.targets
        target_jobs = [target["job"] for target in targets]
        
        assert "web-server" in target_jobs
        assert "database-postgresql" in target_jobs
        assert "application-web_app" in target_jobs
        assert "node-exporter" in target_jobs

    @pytest.mark.asyncio
    async def test_metrics_definition(self, monitoring_guard, basic_request):
        """ğŸŸ¢ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©ãƒ†ã‚¹ãƒˆ"""
        response = await monitoring_guard.process_request(basic_request)
        
        assert response.success is True
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©ã®ç¢ºèª
        metrics = response.monitoring_config.metrics
        metric_names = [metric.name for metric in metrics]
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        assert "cpu_usage_percent" in metric_names
        assert "memory_usage_bytes" in metric_names
        assert "disk_usage_percent" in metric_names
        
        # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        assert "http_requests_total" in metric_names
        assert "application_errors_total" in metric_names

    @pytest.mark.asyncio
    async def test_alert_rules_definition(self, monitoring_guard, basic_request):
        """ğŸŸ¢ ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«å®šç¾©ãƒ†ã‚¹ãƒˆ"""
        response = await monitoring_guard.process_request(basic_request)
        
        assert response.success is True
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ã®ç¢ºèª
        alerts = response.monitoring_config.alerts
        alert_names = [alert.name for alert in alerts]
        
        # ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆ
        assert "HighCPUUsage" in alert_names
        assert "HighMemoryUsage" in alert_names
        assert "DiskSpaceLow" in alert_names
        
        # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ©ãƒ¼ãƒˆ
        assert "HighErrorRate" in alert_names
        assert "ServiceDown" in alert_names

    @pytest.mark.asyncio
    async def test_dashboard_definition(self, monitoring_guard, basic_request):
        """ğŸŸ¢ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å®šç¾©ãƒ†ã‚¹ãƒˆ"""
        response = await monitoring_guard.process_request(basic_request)
        
        assert response.success is True
        
        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å®šç¾©ã®ç¢ºèª
        dashboards = response.monitoring_config.dashboards
        dashboard_names = [dashboard.name for dashboard in dashboards]
        
        assert "system_overview" in dashboard_names
        assert "application_monitoring" in dashboard_names
        assert "database_monitoring" in dashboard_names

    @pytest.mark.asyncio
    async def test_notification_channels_setup(self, monitoring_guard):
        """ğŸŸ¢ é€šçŸ¥ãƒãƒ£ãƒ³ãƒãƒ«è¨­å®šãƒ†ã‚¹ãƒˆ"""
        request = MonitoringGuardRequest(
            action="setup_monitoring",
            config={
                "slack": {"webhook_url": "https://hooks.slack.com/...", "channel": "#alerts"},
                "email": {"addresses": ["admin@example.com"]},
                "discord": {"webhook_url": "https://discord.com/api/webhooks/..."},
            }
        )
        
        response = await monitoring_guard.process_request(request)
        
        assert response.success is True
        
        # é€šçŸ¥ãƒãƒ£ãƒ³ãƒãƒ«ã®ç¢ºèª
        channels = response.monitoring_config.notification_channels
        channel_names = [channel["name"] for channel in channels]
        
        assert "slack" in channel_names
        assert "email" in channel_names
        assert "discord" in channel_names

    @pytest.mark.asyncio
    async def test_retention_policies_definition(self, monitoring_guard, basic_request):
        """ğŸŸ¢ ä¿æŒãƒãƒªã‚·ãƒ¼å®šç¾©ãƒ†ã‚¹ãƒˆ"""
        response = await monitoring_guard.process_request(basic_request)
        
        assert response.success is True
        
        # ä¿æŒãƒãƒªã‚·ãƒ¼ã®ç¢ºèª
        retention_policies = response.monitoring_config.retention_policies
        
        assert "metrics_retention" in retention_policies
        assert "logs_retention" in retention_policies
        assert "alerts_history_retention" in retention_policies
        assert retention_policies["metrics_retention"] == "30d"

    @pytest.mark.asyncio
    async def test_global_settings_definition(self, monitoring_guard, basic_request):
        """ğŸŸ¢ ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šå®šç¾©ãƒ†ã‚¹ãƒˆ"""
        response = await monitoring_guard.process_request(basic_request)
        
        assert response.success is True
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šã®ç¢ºèª
        global_settings = response.monitoring_config.global_settings
        
        assert "scrape_interval" in global_settings
        assert "evaluation_interval" in global_settings
        assert "query_timeout" in global_settings

    @pytest.mark.asyncio
    async def test_prometheus_config(self, monitoring_guard):
        """ğŸŸ¢ Prometheusè¨­å®šãƒ†ã‚¹ãƒˆ"""
        prometheus_config = monitoring_guard._get_prometheus_config()
        
        assert prometheus_config["name"] == "prometheus"
        assert "version" in prometheus_config
        assert "port" in prometheus_config
        assert prometheus_config["port"] == 9090

    @pytest.mark.asyncio
    async def test_grafana_config(self, monitoring_guard):
        """ğŸŸ¢ Grafanaè¨­å®šãƒ†ã‚¹ãƒˆ"""
        grafana_config = monitoring_guard._get_grafana_config()
        
        assert grafana_config["name"] == "grafana"
        assert "version" in grafana_config
        assert "port" in grafana_config
        assert grafana_config["port"] == 3000

    @pytest.mark.asyncio
    async def test_elk_config(self, monitoring_guard):
        """ğŸŸ¢ ELK Stackè¨­å®šãƒ†ã‚¹ãƒˆ"""
        elk_config = monitoring_guard._get_elk_config()
        
        assert elk_config["name"] == "elk"
        assert "elasticsearch" in elk_config
        assert "logstash" in elk_config
        assert "kibana" in elk_config

    @pytest.mark.asyncio
    async def test_metric_definition_creation(self):
        """ğŸŸ¢ MetricDefinitionãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ãƒ†ã‚¹ãƒˆ"""
        metric = MetricDefinition(
            name="test_metric",
            type=MetricType.GAUGE,
            description="Test metric",
            labels={"instance": "test"},
            unit="bytes"
        )
        
        assert metric.name == "test_metric"
        assert metric.type == MetricType.GAUGE
        assert metric.description == "Test metric"
        assert metric.labels["instance"] == "test"
        assert metric.unit == "bytes"

    @pytest.mark.asyncio
    async def test_alert_rule_creation(self):
        """ğŸŸ¢ AlertRuleãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ãƒ†ã‚¹ãƒˆ"""
        alert = AlertRule(
            name="TestAlert",
            expression="cpu_usage > 80",
            severity=AlertSeverity.HIGH,
            description="High CPU usage",
            threshold=80,
            duration="5m",
            labels={"team": "ops"}
        )
        
        assert alert.name == "TestAlert"
        assert alert.expression == "cpu_usage > 80"
        assert alert.severity == AlertSeverity.HIGH
        assert alert.threshold == 80
        assert alert.duration == "5m"
        assert alert.labels["team"] == "ops"

    @pytest.mark.asyncio
    async def test_dashboard_creation(self):
        """ğŸŸ¢ Dashboardãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ãƒ†ã‚¹ãƒˆ"""
        dashboard = Dashboard(
            name="test_dashboard",
            title="Test Dashboard",
            description="Test dashboard description",
            panels=[{"title": "CPU Usage", "type": "stat"}],
            tags=["system", "monitoring"]
        )
        
        assert dashboard.name == "test_dashboard"
        assert dashboard.title == "Test Dashboard"
        assert len(dashboard.panels) == 1
        assert "system" in dashboard.tags
        assert dashboard.refresh_interval == "30s"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    @pytest.mark.asyncio
    async def test_predefined_metrics_loading(self, monitoring_guard):
        """ğŸŸ¢ äº‹å‰å®šç¾©ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
        predefined_metrics = monitoring_guard._load_predefined_metrics()
        
        assert len(predefined_metrics) > 0
        metric_names = [metric.name for metric in predefined_metrics]
        assert "system_uptime_seconds" in metric_names
        assert "process_count" in metric_names

    @pytest.mark.asyncio
    async def test_predefined_alerts_loading(self, monitoring_guard):
        """ğŸŸ¢ äº‹å‰å®šç¾©ã‚¢ãƒ©ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
        predefined_alerts = monitoring_guard._load_predefined_alerts()
        
        assert len(predefined_alerts) > 0
        alert_names = [alert.name for alert in predefined_alerts]
        assert "SystemDown" in alert_names

    @pytest.mark.asyncio
    async def test_dashboard_templates_loading(self, monitoring_guard):
        """ğŸŸ¢ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
        dashboard_templates = monitoring_guard._load_dashboard_templates()
        
        assert "basic_system" in dashboard_templates
        basic_template = dashboard_templates["basic_system"]
        assert basic_template.name == "basic_system"

    @pytest.mark.asyncio
    async def test_notification_channels_loading(self, monitoring_guard):
        """ğŸŸ¢ é€šçŸ¥ãƒãƒ£ãƒ³ãƒãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
        notification_channels = monitoring_guard._load_notification_channels()
        
        assert len(notification_channels) > 0
        channel_names = [channel["name"] for channel in notification_channels]
        assert "default_email" in channel_names

    @pytest.mark.asyncio
    async def test_custom_metrics_support(self, monitoring_guard):
        """ğŸŸ¢ ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚µãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ"""
        request = MonitoringGuardRequest(
            action="setup_monitoring",
            config={
                "custom_metrics": [
                    {
                        "name": "custom_business_metric",
                        "type": "gauge",
                        "description": "Custom business metric",
                        "unit": "count",
                        "labels": {"department": "sales"}
                    }
                ]
            }
        )
        
        response = await monitoring_guard.process_request(request)
        
        assert response.success is True
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå«ã¾ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        metrics = response.monitoring_config.metrics
        metric_names = [metric.name for metric in metrics]
        assert "custom_business_metric" in metric_names

    @pytest.mark.asyncio
    async def test_custom_alerts_support(self, monitoring_guard):
        """ğŸŸ¢ ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆã‚µãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ"""
        request = MonitoringGuardRequest(
            action="setup_monitoring",
            config={
                "custom_alerts": [
                    {
                        "name": "CustomBusinessAlert",
                        "expression": "business_metric > 100",
                        "severity": "high",
                        "description": "Business metric threshold exceeded",
                        "threshold": 100,
                        "duration": "2m"
                    }
                ]
            }
        )
        
        response = await monitoring_guard.process_request(request)
        
        assert response.success is True
        
        # ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆãŒå«ã¾ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        alerts = response.monitoring_config.alerts
        alert_names = [alert.name for alert in alerts]
        assert "CustomBusinessAlert" in alert_names

    @pytest.mark.asyncio
    async def test_unsupported_action(self, monitoring_guard):
        """ğŸ”´ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        request = MonitoringGuardRequest(action="unsupported_action")
        response = await monitoring_guard.process_request(request)
        
        assert response.success is False
        assert "ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³" in response.error_message

    @pytest.mark.asyncio
    async def test_metrics_retrieval(self, monitoring_guard):
        """ğŸŸ¢ å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ãƒ†ã‚¹ãƒˆ"""
        metrics = await monitoring_guard.get_metrics()
        
        assert isinstance(metrics, dict)
        assert "root_cause_resolution" in metrics
        assert "test_coverage" in metrics
        assert "security_score" in metrics
        
        # Iron WillåŸºæº–ã®ç¢ºèª
        assert metrics["root_cause_resolution"] >= 95.0
        assert metrics["test_coverage"] >= 95.0
        assert metrics["security_score"] >= 90.0

    @pytest.mark.asyncio
    async def test_error_handling(self, monitoring_guard):
        """ğŸ”´ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        # ä¸æ­£ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        with patch.object(monitoring_guard, '_setup_monitoring', side_effect=Exception("Test error")):
            request = MonitoringGuardRequest(action="setup_monitoring")
            response = await monitoring_guard.process_request(request)
            
            assert response.success is False
            assert "å‡¦ç†ã‚¨ãƒ©ãƒ¼" in response.error_message

    @pytest.mark.asyncio
    async def test_system_health_monitoring(self, monitoring_guard):
        """ğŸŸ¢ ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ç›£è¦–ãƒ†ã‚¹ãƒˆ"""
        # ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã¯å®Ÿè£…ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€åŸºæœ¬ãƒ†ã‚¹ãƒˆã®ã¿
        request = MonitoringGuardRequest(action="check_system_health")
        response = await monitoring_guard.process_request(request)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå®Ÿè£…ã§ã¯æˆåŠŸã‚’è¿”ã™
        assert response.success is True

    @pytest.mark.asyncio
    async def test_alert_severity_levels(self):
        """ğŸŸ¢ ã‚¢ãƒ©ãƒ¼ãƒˆé‡è¦åº¦ãƒ¬ãƒ™ãƒ«ãƒ†ã‚¹ãƒˆ"""
        # ã™ã¹ã¦ã®é‡è¦åº¦ãƒ¬ãƒ™ãƒ«ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        severities = list(AlertSeverity)
        severity_values = [s.value for s in severities]
        
        assert "critical" in severity_values
        assert "high" in severity_values
        assert "medium" in severity_values
        assert "low" in severity_values
        assert "info" in severity_values

    @pytest.mark.asyncio
    async def test_metric_types(self):
        """ğŸŸ¢ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¿ã‚¤ãƒ—ãƒ†ã‚¹ãƒˆ"""
        # ã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¿ã‚¤ãƒ—ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        metric_types = list(MetricType)
        type_values = [mt.value for mt in metric_types]
        
        assert "counter" in type_values
        assert "gauge" in type_values
        assert "histogram" in type_values
        assert "summary" in type_values

    @pytest.mark.asyncio
    async def test_monitoring_config_validation(self, monitoring_guard, basic_request):
        """ğŸŸ¢ ç›£è¦–è¨­å®šæ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        response = await monitoring_guard.process_request(basic_request)
        
        assert response.success is True
        
        # è¨­å®šæ¤œè¨¼çµæœã®ç¢ºèª
        config = response.monitoring_config
        
        # å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒå«ã¾ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        assert len(config.targets) > 0
        assert len(config.metrics) > 0
        assert len(config.alerts) > 0
        assert len(config.dashboards) > 0

    @pytest.mark.asyncio
    async def test_cache_functionality(self, monitoring_guard):
        """ğŸŸ¢ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã®åŸºæœ¬ãƒ†ã‚¹ãƒˆ
        assert monitoring_guard.cache_ttl == 300  # 5åˆ†
        assert isinstance(monitoring_guard.health_cache, dict)

    @pytest.mark.asyncio
    async def test_concurrent_monitoring_setup(self, monitoring_guard):
        """ğŸ”µ ä¸¦è¡Œç›£è¦–è¨­å®šãƒ†ã‚¹ãƒˆï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å“è³ªç¢ºèªï¼‰"""
        # è¤‡æ•°ã®ç›£è¦–è¨­å®šã‚’ä¸¦è¡Œå®Ÿè¡Œ
        requests = [
            MonitoringGuardRequest(
                action="setup_monitoring",
                target_system=f"system_{i}",
                config={"environment": "test"}
            )
            for i in range(3)
        ]
        
        # ä¸¦è¡Œå®Ÿè¡Œ
        responses = await asyncio.gather(*[
            monitoring_guard.process_request(req) for req in requests
        ])
        
        # ã™ã¹ã¦ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒæˆåŠŸã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        for response in responses:
            assert response.success is True
            assert response.monitoring_config is not None