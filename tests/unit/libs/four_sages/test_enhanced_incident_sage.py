#!/usr/bin/env python3
"""
ğŸ§ª Enhanced Incident Sage ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
Phase 26: çµ±åˆãƒ†ã‚¹ãƒˆ
Created: 2025-07-17
Author: Claude Elder
Version: 1.0.0
"""

import pytest
import asyncio
from datetime import datetime, timedelta
from unittest.mock import Mock, AsyncMock, patch
import sqlite3
from pathlib import Path
import tempfile
import os

# ãƒ†ã‚¹ãƒˆå¯¾è±¡
from libs.four_sages.incident.enhanced_incident_sage import EnhancedIncidentSage
from libs.four_sages.incident.failure_pattern_detector import FailurePattern, FailurePatternDetector
from libs.four_sages.incident.preventive_alert_system import PreventiveAlert, AlertLevel
from libs.four_sages.incident.automatic_response_system import ResponseRule, ResponseStatus
from libs.four_sages.incident.incident_predictor import PredictionResult, FeatureVector
from libs.four_sages.incident.incident_sage import (
    IncidentEntry, IncidentSeverity, IncidentCategory, IncidentStatus
)


class TestEnhancedIncidentSage:
    """Enhanced Incident Sageçµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    async def enhanced_sage(self):
        """ãƒ†ã‚¹ãƒˆç”¨Enhanced Incident Sageã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹"""
        with tempfile.TemporaryDirectory() as tmpdir:
            tracking_db = os.path.join(tmpdir, "test_tracking.db")
            sage = EnhancedIncidentSage(tracking_db_path=tracking_db)
            await sage.initialize_integration()
            yield sage
            await sage.shutdown()
    
    @pytest.fixture
    def sample_incident(self):
        """ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ"""
        return {
            "title": "Test Service Failure",
            "description": "Service timeout error detected",
            "category": "system_failure",
            "severity": "high",
            "affected_systems": ["api-service", "database"],
            "tags": ["timeout", "critical"]
        }
    
    @pytest.fixture
    def sample_metrics(self):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
        return {
            "error_rate": 0.15,
            "response_time": 8.5,
            "cpu_usage": 85.0,
            "memory_usage": 78.0,
            "failure_rate": 0.12,
            "quality_score": 0.82
        }
    
    @pytest.mark.asyncio
    async def test_initialization(self, enhanced_sage):
        """åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        assert enhanced_sage is not None
        assert enhanced_sage.failure_detector is not None
        assert enhanced_sage.alert_system is not None
        assert enhanced_sage.response_system is not None
        assert enhanced_sage.predictor is not None
        assert len(enhanced_sage.background_tasks) > 0
    
    @pytest.mark.asyncio
    async def test_create_incident_with_auto_response(self, enhanced_sage, sample_incident):
        """è‡ªå‹•å¯¾å¿œä»˜ãã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆãƒ†ã‚¹ãƒˆ"""
        # è‡ªå‹•å¯¾å¿œã‚’æœ‰åŠ¹åŒ–
        enhanced_sage.integration_config["auto_response_enabled"] = True
        
        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆ
        response = await enhanced_sage.process_request({
            "type": "create_incident",
            **sample_incident
        })
        
        assert response["success"] is True
        assert "incident_id" in response
        
        # è‡ªå‹•å¯¾å¿œãŒå®Ÿè¡Œã•ã‚ŒãŸã‹ç¢ºèª
        if "auto_response" in response:
            assert response["auto_response"]["handled"] is True
            assert enhanced_sage.integration_metrics["auto_responses"] > 0
    
    @pytest.mark.asyncio
    async def test_pattern_analysis(self, enhanced_sage):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãƒ†ã‚¹ãƒˆ"""
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Ÿè¡Œ
        response = await enhanced_sage.process_request({
            "type": "analyze_patterns",
            "days_back": 7
        })
        
        assert response["success"] is True
        assert "patterns_found" in response
        assert enhanced_sage.integration_metrics["patterns_detected"] >= 0
    
    @pytest.mark.asyncio
    async def test_risk_prediction(self, enhanced_sage):
        """ãƒªã‚¹ã‚¯äºˆæ¸¬ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆçŠ¶æ…‹
        test_state = {
            "metrics": {
                "error_rate": 0.2,
                "response_time": 10.0,
                "cpu_usage": 90.0,
                "memory_usage": 85.0
            },
            "history": {
                "failures_1h": 15,
                "failures_24h": 50
            }
        }
        
        # ãƒªã‚¹ã‚¯äºˆæ¸¬å®Ÿè¡Œ
        response = await enhanced_sage.process_request({
            "type": "predict_risk",
            "state": test_state
        })
        
        assert response["success"] is True
        assert "prediction" in response
        
        prediction = response["prediction"]
        assert "risk_score" in prediction
        assert "risk_level" in prediction
        assert "recommended_actions" in prediction
    
    @pytest.mark.asyncio
    async def test_preventive_alerts(self, enhanced_sage, sample_metrics):
        """äºˆé˜²çš„ã‚¢ãƒ©ãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ"""
        # ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
        await enhanced_sage.monitor_tracking_metrics(sample_metrics)
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚ŒãŸã‹ç¢ºèª
        assert enhanced_sage.integration_metrics["preventive_alerts"] >= 0
    
    @pytest.mark.asyncio
    async def test_integration_status(self, enhanced_sage):
        """çµ±åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—ãƒ†ã‚¹ãƒˆ"""
        response = await enhanced_sage.process_request({
            "type": "get_integration_status"
        })
        
        assert response["success"] is True
        assert "integration_status" in response
        
        status = response["integration_status"]
        assert "config" in status
        assert "metrics" in status
        assert "components" in status
        assert all(comp == "active" for comp in status["components"].values())
    
    @pytest.mark.asyncio
    async def test_configure_integration(self, enhanced_sage):
        """çµ±åˆè¨­å®šå¤‰æ›´ãƒ†ã‚¹ãƒˆ"""
        new_config = {
            "pattern_analysis_interval": 7200,
            "auto_response_enabled": False
        }
        
        response = await enhanced_sage.process_request({
            "type": "configure_integration",
            "config": new_config
        })
        
        assert response["success"] is True
        assert enhanced_sage.integration_config["pattern_analysis_interval"] == 7200
        assert enhanced_sage.integration_config["auto_response_enabled"] is False
    
    @pytest.mark.asyncio
    async def test_high_risk_prediction_handling(self, enhanced_sage):
        """é«˜ãƒªã‚¹ã‚¯äºˆæ¸¬å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        # é«˜ãƒªã‚¹ã‚¯äºˆæ¸¬ã‚’ä½œæˆ
        high_risk_prediction = PredictionResult(
            risk_score=0.85,
            risk_level="critical",
            incident_probability={"system_failure": 0.8, "none": 0.2},
            severity_probability={"critical": 0.7, "high": 0.3},
            contributing_factors=[("High CPU usage", 0.9)],
            recommended_actions=["Scale up resources"],
            confidence=0.85
        )
        
        # å‡¦ç†å®Ÿè¡Œ
        await enhanced_sage._handle_high_risk_prediction(high_risk_prediction)
        
        # äºˆé˜²çš„ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãŒä½œæˆã•ã‚ŒãŸã‹ç¢ºèª
        assert enhanced_sage.integration_metrics["incidents_prevented"] > 0


class TestFailurePatternDetector:
    """å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def detector(self):
        """ãƒ†ã‚¹ãƒˆç”¨æ¤œå‡ºå™¨"""
        with tempfile.TemporaryDirectory() as tmpdir:
            tracking_db = os.path.join(tmpdir, "test_tracking.db")
            incident_db = os.path.join(tmpdir, "test_incident.db")
            return FailurePatternDetector(tracking_db, incident_db)
    
    @pytest.fixture
    def sample_failures(self):
        """ã‚µãƒ³ãƒ—ãƒ«å¤±æ•—ãƒ‡ãƒ¼ã‚¿"""
        return [
            {
                "task_id": "task1",
                "task_description": "API call",
                "detail_type": "command",
                "stderr": "Connection timeout error",
                "exit_code": 1,
                "execution_time": 30.5,
                "timestamp": datetime.now().isoformat()
            },
            {
                "task_id": "task2",
                "task_description": "Database query",
                "detail_type": "query",
                "stderr": "Out of memory error",
                "exit_code": 137,
                "execution_time": 5.2,
                "timestamp": datetime.now().isoformat()
            }
        ]
    
    @pytest.mark.asyncio
    async def test_extract_error_patterns(self, detector, sample_failures):
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºãƒ†ã‚¹ãƒˆ"""
        patterns = await detector._extract_error_patterns(sample_failures)
        
        assert isinstance(patterns, list)
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ¤œå‡ºã•ã‚Œã‚‹ã¯ãš
        pattern_names = [p.get("pattern_name") for p in patterns]
        assert any("timeout" in name for name in pattern_names if name)
    
    @pytest.mark.asyncio
    async def test_pattern_classification(self, detector):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡ãƒ†ã‚¹ãƒˆ"""
        test_pattern = {
            "pattern_type": "error_message",
            "pattern_name": "timeout"
        }
        
        category = await detector._classify_pattern(test_pattern)
        assert category == IncidentCategory.PERFORMANCE_ISSUE
    
    @pytest.mark.asyncio
    async def test_pattern_severity_evaluation(self, detector):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é‡è¦åº¦è©•ä¾¡ãƒ†ã‚¹ãƒˆ"""
        test_pattern = {
            "occurrences": 25,
            "pattern_type": "error_message"
        }
        
        severity = await detector._evaluate_pattern_severity(test_pattern)
        assert severity == IncidentSeverity.HIGH


class TestPreventiveAlertSystem:
    """äºˆé˜²çš„ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def alert_system(self):
        """ãƒ†ã‚¹ãƒˆç”¨ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ """
        mock_sage = Mock()
        mock_sage.process_request = AsyncMock(return_value={"success": True, "alert_id": "test123"})
        return PreventiveAlertSystem(mock_sage)
    
    @pytest.mark.asyncio
    async def test_threshold_alert(self, alert_system):
        """é–¾å€¤ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ"""
        # é«˜ã„ã‚¨ãƒ©ãƒ¼ç‡ã§ãƒ†ã‚¹ãƒˆ
        metrics = {
            "error_rate": 0.3,  # 30%ã‚¨ãƒ©ãƒ¼ç‡
            "quality_score": 0.65  # å“è³ªã‚¹ã‚³ã‚¢ä½ä¸‹
        }
        
        result = await alert_system.monitor_metrics(metrics)
        
        assert result["success"] is True
        assert result["alerts_generated"] > 0
        assert len(result["alerts"]) > 0
    
    @pytest.mark.asyncio
    async def test_trend_analysis(self, alert_system):
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ†ã‚¹ãƒˆ"""
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        for i in range(10):
            metrics = {"cpu_usage": 50 + i * 5}  # å¢—åŠ ãƒˆãƒ¬ãƒ³ãƒ‰
            await alert_system.monitor_metrics(metrics)
            await asyncio.sleep(0.1)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¢ãƒ©ãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã‚‹ã‹ç¢ºèª
        assert alert_system.alert_stats["total_alerts"] >= 0
    
    @pytest.mark.asyncio
    async def test_alert_suppression(self, alert_system):
        """é‡è¤‡ã‚¢ãƒ©ãƒ¼ãƒˆæŠ‘åˆ¶ãƒ†ã‚¹ãƒˆ"""
        # åŒã˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§è¤‡æ•°å›ã‚¢ãƒ©ãƒ¼ãƒˆ
        metrics = {"failure_rate": 0.5}
        
        result1 = await alert_system.monitor_metrics(metrics)
        result2 = await alert_system.monitor_metrics(metrics)
        
        # 2å›ç›®ã¯æŠ‘åˆ¶ã•ã‚Œã‚‹ã¯ãš
        assert result1["alerts_generated"] > 0
        assert result2["alerts_generated"] == 0  # é‡è¤‡ã¯æŠ‘åˆ¶


class TestAutomaticResponseSystem:
    """è‡ªå‹•å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def response_system(self):
        """ãƒ†ã‚¹ãƒˆç”¨å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ """
        mock_sage = Mock()
        mock_sage.process_request = AsyncMock(return_value={"success": True})
        return AutomaticResponseSystem(mock_sage)
    
    @pytest.fixture
    def test_incident(self):
        """ãƒ†ã‚¹ãƒˆç”¨ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ"""
        return IncidentEntry(
            id="test123",
            title="Service Down",
            description="Service health check failed timeout error",
            category=IncidentCategory.SYSTEM_FAILURE,
            severity=IncidentSeverity.HIGH,
            status=IncidentStatus.OPEN,
            affected_systems=["api-service"]
        )
    
    @pytest.mark.asyncio
    async def test_rule_matching(self, response_system, test_incident):
        """ãƒ«ãƒ¼ãƒ«ãƒãƒƒãƒãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        matched_rules = await response_system._match_rules(test_incident)
        
        assert len(matched_rules) > 0
        # ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•ãƒ«ãƒ¼ãƒ«ãŒãƒãƒƒãƒã™ã‚‹ã¯ãš
        rule_ids = [r.rule_id for r in matched_rules]
        assert "service_restart_rule" in rule_ids
    
    @pytest.mark.asyncio
    async def test_response_execution(self, response_system, test_incident):
        """å¯¾å¿œå®Ÿè¡Œãƒ†ã‚¹ãƒˆ"""
        # ãƒ¢ãƒƒã‚¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        async def mock_action(incident):
            return {"success": True, "message": "Action executed"}
        
        response_system.action_registry["test_action"] = mock_action
        
        # ãƒ†ã‚¹ãƒˆãƒ«ãƒ¼ãƒ«
        test_rule = ResponseRule(
            rule_id="test_rule",
            rule_name="Test Rule",
            conditions=["test_condition"],
            actions=["test_action"],
            cooldown=0
        )
        
        execution = await response_system._execute_rule(test_rule, test_incident)
        
        assert execution.status == ResponseStatus.SUCCESS
        assert len(execution.actions_executed) == 1


class TestIncidentPredictor:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆæ¸¬å™¨ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def predictor(self):
        """ãƒ†ã‚¹ãƒˆç”¨äºˆæ¸¬å™¨"""
        mock_detector = Mock()
        mock_detector.process_request = AsyncMock(return_value={"success": True, "patterns": []})
        return IncidentPredictor(mock_detector)
    
    @pytest.fixture
    def training_data(self):
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿"""
        return [
            {
                "state": {
                    "metrics": {
                        "error_rate": 0.05,
                        "cpu_usage": 50.0,
                        "memory_usage": 60.0
                    },
                    "history": {"failures_1h": 2}
                },
                "incident": None  # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãªã—
            },
            {
                "state": {
                    "metrics": {
                        "error_rate": 0.3,
                        "cpu_usage": 95.0,
                        "memory_usage": 90.0
                    },
                    "history": {"failures_1h": 20}
                },
                "incident": {
                    "category": "system_failure",
                    "severity": "critical"
                }
            }
        ]
    
    @pytest.mark.asyncio
    async def test_feature_extraction(self, predictor):
        """ç‰¹å¾´é‡æŠ½å‡ºãƒ†ã‚¹ãƒˆ"""
        test_state = {
            "metrics": {
                "error_rate": 0.1,
                "response_time": 2.5,
                "cpu_usage": 70.0
            }
        }
        
        feature = predictor.feature_extractor.extract(test_state)
        
        assert isinstance(feature, FeatureVector)
        assert feature.error_rate == 0.1
        assert feature.cpu_usage == 70.0
    
    @pytest.mark.asyncio
    async def test_model_training(self, predictor, training_data):
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ†ã‚¹ãƒˆ"""
        # ååˆ†ãªè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        extended_data = training_data * 50  # 100ã‚µãƒ³ãƒ—ãƒ«
        
        result = await predictor.train_model(extended_data)
        
        assert result["success"] is True
        assert predictor.prediction_model is not None
        assert predictor.severity_model is not None
    
    @pytest.mark.asyncio
    async def test_risk_calculation(self, predictor):
        """ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
        incident_prob = {"system_failure": 0.8, "none": 0.2}
        severity_prob = {"critical": 0.6, "high": 0.3, "medium": 0.1}
        
        risk_score = predictor._calculate_risk_score(incident_prob, severity_prob)
        
        assert 0.0 <= risk_score <= 1.0
        assert risk_score > 0.5  # é«˜ãƒªã‚¹ã‚¯


@pytest.mark.asyncio
async def test_end_to_end_flow():
    """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰çµ±åˆãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ"""
    with tempfile.TemporaryDirectory() as tmpdir:
        # Enhanced Incident Sageä½œæˆ
        tracking_db = os.path.join(tmpdir, "test_tracking.db")
        sage = EnhancedIncidentSage(tracking_db_path=tracking_db)
        await sage.initialize_integration()
        
        try:
            # 1. é«˜ãƒªã‚¹ã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ•å…¥
            high_risk_metrics = {
                "error_rate": 0.25,
                "response_time": 12.0,
                "cpu_usage": 92.0,
                "memory_usage": 88.0,
                "failure_rate": 0.3
            }
            
            # 2. ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ï¼ˆã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆï¼‰
            await sage.monitor_tracking_metrics(high_risk_metrics)
            
            # 3. ãƒªã‚¹ã‚¯äºˆæ¸¬å®Ÿè¡Œ
            prediction_response = await sage.process_request({
                "type": "predict_risk",
                "state": {
                    "metrics": high_risk_metrics,
                    "history": {"failures_1h": 25}
                }
            })
            
            assert prediction_response["success"] is True
            
            # 4. ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆä½œæˆï¼ˆè‡ªå‹•å¯¾å¿œä»˜ãï¼‰
            incident_response = await sage.process_request({
                "type": "create_incident",
                "title": "High System Load",
                "description": "CPU and memory usage critical",
                "category": "performance_issue",
                "severity": "high"
            })
            
            assert incident_response["success"] is True
            
            # 5. çµ±åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
            status_response = await sage.process_request({
                "type": "get_integration_status"
            })
            
            assert status_response["success"] is True
            assert status_response["integration_status"]["metrics"]["predictions_made"] > 0
            
        finally:
            await sage.shutdown()


if __name__ == "__main__":
    pytest.main(["-v", __file__])