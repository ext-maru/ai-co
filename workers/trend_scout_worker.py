#!/usr/bin/env python3
"""
Trend Scout Worker v1.0
æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰è‡ªå‹•åé›†ãƒ¯ãƒ¼ã‚«ãƒ¼

ğŸ”® nWo Prophetic Development Matrix - Trend Scouting System
Think it, Rule it, Own it - æœªæ¥äºˆæ¸¬é–‹ç™ºã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import aiohttp
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import logging
import re


class TrendSource(Enum):
    """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚½ãƒ¼ã‚¹"""
    GITHUB = "github"
    HACKERNEWS = "hackernews"
    REDDIT = "reddit"
    STACKOVERFLOW = "stackoverflow"
    DEVTO = "devto"
    PRODUCTHUNT = "producthunt"


class TrendType(Enum):
    """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¿ã‚¤ãƒ—"""
    TECHNOLOGY = "technology"
    FRAMEWORK = "framework"
    LIBRARY = "library"
    TOOL = "tool"
    LANGUAGE = "language"
    CONCEPT = "concept"
    NEWS = "news"


@dataclass
class Trend:
    """ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±"""
    title: str
    description: str
    url: str
    source: TrendSource
    trend_type: TrendType
    score: float
    tags: List[str]
    github_stars: Optional[int] = None
    discussions: Optional[int] = None
    created_at: str = ""
    discovered_at: str = ""


@dataclass
class TrendAnalysis:
    """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æçµæœ"""
    period: str
    total_trends: int
    top_technologies: List[str]
    emerging_frameworks: List[str]
    hot_topics: List[str]
    growth_rate: Dict[str, float]
    predictions: List[str]
    analysis_time: str


class TrendScoutWorker:
    """Trend Scout Worker - æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰è‡ªå‹•åé›†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._default_config()
        self.logger = self._setup_logger()
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.trends_db = Path("data/trends.json")
        self.trends_db.parent.mkdir(parents=True, exist_ok=True)
        
        # åé›†ã—ãŸãƒˆãƒ¬ãƒ³ãƒ‰
        self.discovered_trends: List[Trend] = []
        self.trend_history: List[TrendAnalysis] = []
        
        # APIè¨­å®šï¼ˆå®Ÿéš›ã®APIã‚­ãƒ¼ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ï¼‰
        self.api_keys = {
            "github_token": None,  # GitHub API Token
            "reddit_client_id": None,  # Reddit API
        }
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸
        self.tech_keywords = self._load_tech_keywords()
        
        self.logger.info("ğŸ”® Trend Scout Worker v1.0 initialized")
    
    def _default_config(self) -> Dict:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š"""
        return {
            "scan_interval": 3600,  # 1æ™‚é–“
            "max_trends_per_source": 50,
            "min_score_threshold": 0.3,
            "enabled_sources": [
                TrendSource.GITHUB,
                TrendSource.HACKERNEWS,
                TrendSource.REDDIT
            ],
            "keywords": [
                "python", "javascript", "react", "vue", "angular",
                "ai", "ml", "machine learning", "deep learning",
                "blockchain", "web3", "cloud", "kubernetes",
                "microservices", "serverless", "devops"
            ]
        }
    
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼è¨­å®š"""
        logger = logging.getLogger("trend_scout_worker")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s - Trend Scout - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _load_tech_keywords(self) -> Dict[str, List[str]]:
        """æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸"""
        return {
            "frameworks": [
                "react", "vue", "angular", "svelte", "nextjs", "nuxt",
                "express", "fastapi", "django", "flask", "spring",
                "laravel", "rails", "nestjs"
            ],
            "languages": [
                "python", "javascript", "typescript", "rust", "go",
                "java", "csharp", "kotlin", "swift", "dart"
            ],
            "ai_ml": [
                "tensorflow", "pytorch", "scikit-learn", "keras",
                "huggingface", "openai", "llm", "gpt", "bert",
                "transformer", "neural network"
            ],
            "cloud": [
                "aws", "gcp", "azure", "kubernetes", "docker",
                "terraform", "serverless", "microservices"
            ],
            "tools": [
                "vscode", "github", "gitlab", "jenkins", "docker",
                "webpack", "vite", "eslint", "prettier"
            ]
        }
    
    async def scout_github_trends(self) -> List[Trend]:
        """GitHub ãƒˆãƒ¬ãƒ³ãƒ‰åé›†"""
        self.logger.info("ğŸ” Scouting GitHub trends...")
        trends = []
        
        try:
            # GitHub Trending APIï¼ˆç°¡æ˜“ç‰ˆï¼‰
            base_url = "https://api.github.com/search/repositories"
            
            # äººæ°—ã®ã‚¯ã‚¨ãƒª
            queries = [
                "created:>2024-01-01 stars:>100",
                "language:python pushed:>2024-07-01",
                "topic:ai stars:>50",
                "topic:web3 created:>2024-06-01"
            ]
            
            async with aiohttp.ClientSession() as session:
                for query in queries:
                    params = {
                        "q": query,
                        "sort": "stars",
                        "order": "desc",
                        "per_page": 20
                    }
                    
                    headers = {}
                    if self.api_keys.get("github_token"):
                        headers["Authorization"] = f"token {self.api_keys['github_token']}"
                    
                    try:
                        async with session.get(base_url, params=params, headers=headers) as response:
                            if response.status == 200:
                                data = await response.json()
                                
                                for repo in data.get("items", []):
                                    trend = self._parse_github_repo(repo)
                                    if trend:
                                        trends.append(trend)
                            else:
                                self.logger.warning(f"GitHub API error: {response.status}")
                    
                    except Exception as e:
                        self.logger.error(f"GitHub query error: {e}")
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                    await asyncio.sleep(1)
        
        except Exception as e:
            self.logger.error(f"GitHub trends error: {e}")
        
        self.logger.info(f"ğŸ“Š Found {len(trends)} GitHub trends")
        return trends[:self.config["max_trends_per_source"]]
    
    def _parse_github_repo(self, repo: Dict) -> Optional[Trend]:
        """GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒˆãƒ¬ãƒ³ãƒ‰ã«å¤‰æ›"""
        try:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
            repo_text = f"{repo['name']} {repo['description'] or ''} {' '.join(repo.get('topics', []))}"
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¿ã‚¤ãƒ—åˆ¤å®š
            trend_type = self._classify_trend_type(repo_text)
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            score = self._calculate_github_score(repo)
            
            if score < self.config["min_score_threshold"]:
                return None
            
            # ã‚¿ã‚°æŠ½å‡º
            tags = repo.get("topics", [])
            tags.extend(self._extract_tech_tags(repo_text))
            
            return Trend(
                title=repo["name"],
                description=repo["description"] or "No description",
                url=repo["html_url"],
                source=TrendSource.GITHUB,
                trend_type=trend_type,
                score=score,
                tags=list(set(tags)),
                github_stars=repo["stargazers_count"],
                created_at=repo["created_at"],
                discovered_at=datetime.now().isoformat()
            )
        
        except Exception as e:
            self.logger.error(f"GitHub repo parsing error: {e}")
            return None
    
    def _calculate_github_score(self, repo: Dict) -> float:
        """GitHubãƒªãƒã‚¸ãƒˆãƒªã®ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        stars = repo.get("stargazers_count", 0)
        forks = repo.get("forks_count", 0)
        watchers = repo.get("watchers_count", 0)
        
        # ä½œæˆæ—¥ã‹ã‚‰ã®çµŒéæ™‚é–“
        created_at = datetime.fromisoformat(repo["created_at"].replace("Z", "+00:00"))
        days_old = (datetime.now(created_at.tzinfo) - created_at).days
        
        # æœ€è¿‘ã®æ›´æ–°
        updated_at = datetime.fromisoformat(repo["updated_at"].replace("Z", "+00:00"))
        days_since_update = (datetime.now(updated_at.tzinfo) - updated_at).days
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        score = 0.0
        
        # ã‚¹ã‚¿ãƒ¼æ•°ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        if stars > 0:
            score += min(1.0, (stars / 1000) ** 0.5)
        
        # æ´»ç™ºåº¦
        if days_since_update < 30:
            score += 0.3
        elif days_since_update < 90:
            score += 0.1
        
        # æ–°ã—ã•
        if days_old < 90:
            score += 0.2
        elif days_old < 365:
            score += 0.1
        
        # ãƒ•ã‚©ãƒ¼ã‚¯æ•°
        if forks > 10:
            score += 0.1
        
        return min(1.0, score)
    
    async def analyze_hackernews(self) -> TrendAnalysis:
        """Hacker News åˆ†æ"""
        self.logger.info("ğŸ“° Analyzing Hacker News...")
        
        trends = []
        hot_topics = []
        
        try:
            # Hacker News API
            async with aiohttp.ClientSession() as session:
                # ãƒˆãƒƒãƒ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å–å¾—
                async with session.get("https://hacker-news.firebaseio.com/v0/topstories.json") as response:
                    if response.status == 200:
                        story_ids = await response.json()
                        
                        # ä¸Šä½50ä»¶ã‚’åˆ†æ
                        for story_id in story_ids[:50]:
                            try:
                                async with session.get(f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json") as story_response:
                                    if story_response.status == 200:
                                        story = await story_response.json()
                                        
                                        if story and story.get("title"):
                                            # æŠ€è¡“é–¢é€£ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æŠ½å‡º
                                            if self._is_tech_related(story["title"]):
                                                trend = self._parse_hn_story(story)
                                                if trend:
                                                    trends.append(trend)
                                                    hot_topics.append(story["title"])
                                
                                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                                await asyncio.sleep(0.1)
                            
                            except Exception as e:
                                self.logger.error(f"HN story error: {e}")
        
        except Exception as e:
            self.logger.error(f"Hacker News analysis error: {e}")
        
        return TrendAnalysis(
            period="daily",
            total_trends=len(trends),
            top_technologies=self._extract_top_technologies(trends),
            emerging_frameworks=[],
            hot_topics=hot_topics[:10],
            growth_rate={},
            predictions=[],
            analysis_time=datetime.now().isoformat()
        )
    
    def _parse_hn_story(self, story: Dict) -> Optional[Trend]:
        """HN ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ãƒˆãƒ¬ãƒ³ãƒ‰ã«å¤‰æ›"""
        try:
            title = story["title"]
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            score = story.get("score", 0) / 500.0  # æ­£è¦åŒ–
            score = min(1.0, score)
            
            if score < self.config["min_score_threshold"]:
                return None
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¿ã‚¤ãƒ—åˆ¤å®š
            trend_type = self._classify_trend_type(title)
            
            # ã‚¿ã‚°æŠ½å‡º
            tags = self._extract_tech_tags(title)
            
            return Trend(
                title=title,
                description=story.get("text", "")[:200],
                url=story.get("url", f"https://news.ycombinator.com/item?id={story['id']}"),
                source=TrendSource.HACKERNEWS,
                trend_type=trend_type,
                score=score,
                tags=tags,
                discussions=story.get("descendants", 0),
                discovered_at=datetime.now().isoformat()
            )
        
        except Exception as e:
            self.logger.error(f"HN story parsing error: {e}")
            return None
    
    async def scan_reddit_programming(self) -> List[Trend]:
        """Reddit ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°é–¢é€£ã‚¹ã‚­ãƒ£ãƒ³"""
        self.logger.info("ğŸ” Scanning Reddit programming...")
        trends = []
        
        try:
            # Reddit APIï¼ˆJSON endpointsï¼‰
            subreddits = ["programming", "webdev", "MachineLearning", "artificial"]
            
            async with aiohttp.ClientSession() as session:
                for subreddit in subreddits:
                    try:
                        url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=25"
                        headers = {"User-Agent": "TrendScout/1.0"}
                        
                        async with session.get(url, headers=headers) as response:
                            if response.status == 200:
                                data = await response.json()
                                
                                for post in data["data"]["children"]:
                                    post_data = post["data"]
                                    
                                    if self._is_tech_related(post_data["title"]):
                                        trend = self._parse_reddit_post(post_data)
                                        if trend:
                                            trends.append(trend)
                        
                        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                        await asyncio.sleep(2)
                    
                    except Exception as e:
                        self.logger.error(f"Reddit {subreddit} error: {e}")
        
        except Exception as e:
            self.logger.error(f"Reddit scan error: {e}")
        
        self.logger.info(f"ğŸ“Š Found {len(trends)} Reddit trends")
        return trends[:self.config["max_trends_per_source"]]
    
    def _parse_reddit_post(self, post: Dict) -> Optional[Trend]:
        """Reddit æŠ•ç¨¿ã‚’ãƒˆãƒ¬ãƒ³ãƒ‰ã«å¤‰æ›"""
        try:
            title = post["title"]
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            score = min(1.0, post.get("score", 0) / 1000.0)
            
            if score < self.config["min_score_threshold"]:
                return None
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¿ã‚¤ãƒ—åˆ¤å®š
            trend_type = self._classify_trend_type(title)
            
            # ã‚¿ã‚°æŠ½å‡º
            tags = self._extract_tech_tags(title)
            tags.append(post["subreddit"])
            
            return Trend(
                title=title,
                description=post.get("selftext", "")[:200],
                url=f"https://reddit.com{post['permalink']}",
                source=TrendSource.REDDIT,
                trend_type=trend_type,
                score=score,
                tags=tags,
                discussions=post.get("num_comments", 0),
                discovered_at=datetime.now().isoformat()
            )
        
        except Exception as e:
            self.logger.error(f"Reddit post parsing error: {e}")
            return None
    
    def _classify_trend_type(self, text: str) -> TrendType:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
        text_lower = text.lower()
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
        framework_keywords = ["framework", "library", "react", "vue", "angular", "django", "flask"]
        if any(keyword in text_lower for keyword in framework_keywords):
            return TrendType.FRAMEWORK
        
        # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª
        language_keywords = ["python", "javascript", "rust", "go", "java", "language"]
        if any(keyword in text_lower for keyword in language_keywords):
            return TrendType.LANGUAGE
        
        # ãƒ„ãƒ¼ãƒ«
        tool_keywords = ["tool", "cli", "editor", "ide", "deploy"]
        if any(keyword in text_lower for keyword in tool_keywords):
            return TrendType.TOOL
        
        # AI/ML
        ai_keywords = ["ai", "ml", "machine learning", "neural", "model"]
        if any(keyword in text_lower for keyword in ai_keywords):
            return TrendType.TECHNOLOGY
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return TrendType.CONCEPT
    
    def _extract_tech_tags(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ€è¡“ã‚¿ã‚°ã‚’æŠ½å‡º"""
        tags = []
        text_lower = text.lower()
        
        for category, keywords in self.tech_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    tags.append(keyword)
        
        return list(set(tags))
    
    def _is_tech_related(self, text: str) -> bool:
        """æŠ€è¡“é–¢é€£ãƒ†ã‚­ã‚¹ãƒˆã‹ã©ã†ã‹åˆ¤å®š"""
        tech_indicators = [
            "programming", "code", "software", "development", "framework",
            "library", "api", "algorithm", "data", "web", "mobile",
            "ai", "ml", "blockchain", "cloud", "devops"
        ]
        
        text_lower = text.lower()
        return any(indicator in text_lower for indicator in tech_indicators)
    
    def _extract_top_technologies(self, trends: List[Trend]) -> List[str]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ã‹ã‚‰ä¸»è¦æŠ€è¡“ã‚’æŠ½å‡º"""
        tech_count = {}
        
        for trend in trends:
            for tag in trend.tags:
                tech_count[tag] = tech_count.get(tag, 0) + trend.score
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_tech = sorted(tech_count.items(), key=lambda x: x[1], reverse=True)
        
        return [tech for tech, _ in sorted_tech[:10]]
    
    async def generate_trend_report(self) -> Dict[str, Any]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        self.logger.info("ğŸ“Š Generating trend report...")
        
        # å…¨ã‚½ãƒ¼ã‚¹ã‚¹ã‚­ãƒ£ãƒ³
        all_trends = []
        
        if TrendSource.GITHUB in self.config["enabled_sources"]:
            github_trends = await self.scout_github_trends()
            all_trends.extend(github_trends)
        
        if TrendSource.REDDIT in self.config["enabled_sources"]:
            reddit_trends = await self.scan_reddit_programming()
            all_trends.extend(reddit_trends)
        
        if TrendSource.HACKERNEWS in self.config["enabled_sources"]:
            hn_analysis = await self.analyze_hackernews()
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        top_trends = sorted(all_trends, key=lambda t: t.score, reverse=True)[:20]
        
        # æŠ€è¡“åˆ†å¸ƒ
        tech_distribution = {}
        for trend in all_trends:
            for tag in trend.tags:
                tech_distribution[tag] = tech_distribution.get(tag, 0) + 1
        
        # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report = {
            "generated_at": datetime.now().isoformat(),
            "total_trends": len(all_trends),
            "sources_scanned": len(self.config["enabled_sources"]),
            "top_trends": [asdict(trend) for trend in top_trends],
            "technology_distribution": dict(sorted(tech_distribution.items(), 
                                                 key=lambda x: x[1], reverse=True)[:15]),
            "emerging_technologies": self._identify_emerging_tech(all_trends),
            "predictions": self._generate_predictions(all_trends),
            "summary": self._generate_summary(all_trends)
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = Path(f"data/trend_reports/report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ“„ Trend report saved: {report_path}")
        
        return report
    
    def _identify_emerging_tech(self, trends: List[Trend]) -> List[str]:
        """æ–°èˆˆæŠ€è¡“ã®ç‰¹å®š"""
        # æœ€è¿‘ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‹ã‚‰æ–°ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        recent_trends = [t for t in trends if t.discovered_at and 
                        datetime.fromisoformat(t.discovered_at) > datetime.now() - timedelta(days=7)]
        
        emerging = {}
        for trend in recent_trends:
            for tag in trend.tags:
                if tag not in self.tech_keywords.get("frameworks", []):
                    emerging[tag] = emerging.get(tag, 0) + trend.score
        
        return sorted(emerging.keys(), key=lambda k: emerging[k], reverse=True)[:5]
    
    def _generate_predictions(self, trends: List[Trend]) -> List[str]:
        """äºˆæ¸¬ç”Ÿæˆ"""
        predictions = [
            "AI/ML integration will continue accelerating in web frameworks",
            "Rust adoption in systems programming will grow significantly",
            "WebAssembly will become mainstream for performance-critical web apps",
            "Edge computing frameworks will see major development",
            "Developer experience tools will focus on AI-assistance"
        ]
        
        return predictions[:3]
    
    def _generate_summary(self, trends: List[Trend]) -> str:
        """ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        total = len(trends)
        avg_score = sum(t.score for t in trends) / total if total > 0 else 0
        top_source = max(self.config["enabled_sources"], 
                        key=lambda s: len([t for t in trends if t.source == s]))
        
        return f"Analyzed {total} trends with average score {avg_score:.2f}. " \
               f"Primary source: {top_source.value}. " \
               f"Key focus areas: AI/ML, web frameworks, and developer tools."


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
async def demo_trend_scout():
    """Trend Scout Worker ã®ãƒ‡ãƒ¢"""
    print("ğŸ”® Trend Scout Worker v1.0 Demo")
    print("=" * 50)
    
    scout = TrendScoutWorker()
    
    # GitHub ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚­ãƒ£ãƒ³
    print("\nğŸ“Š Scanning GitHub trends...")
    github_trends = await scout.scout_github_trends()
    
    print(f"Found {len(github_trends)} GitHub trends:")
    for trend in github_trends[:3]:
        print(f"  â­ {trend.title} ({trend.github_stars} stars)")
        print(f"     {trend.description[:80]}...")
        print(f"     Tags: {', '.join(trend.tags[:3])}")
    
    # Reddit ã‚¹ã‚­ãƒ£ãƒ³
    print("\nğŸ“± Scanning Reddit...")
    reddit_trends = await scout.scan_reddit_programming()
    
    print(f"Found {len(reddit_trends)} Reddit trends:")
    for trend in reddit_trends[:2]:
        print(f"  ğŸ’¬ {trend.title}")
        print(f"     Score: {trend.score:.2f}, Comments: {trend.discussions}")
    
    # å®Œå…¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\nğŸ“„ Generating comprehensive report...")
    report = await scout.generate_trend_report()
    
    print(f"ğŸ“Š Report Summary:")
    print(f"   Total Trends: {report['total_trends']}")
    print(f"   Sources: {report['sources_scanned']}")
    print(f"   Top Technologies: {', '.join(list(report['technology_distribution'].keys())[:5])}")
    print(f"   Emerging: {', '.join(report['emerging_technologies'][:3])}")


if __name__ == "__main__":
    asyncio.run(demo_trend_scout())