#!/usr/bin/env python3
"""
ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ˜ãƒ«ã‚¹ç›£è¦–ã‚µãƒ¼ãƒ“ã‚¹
çµ±åˆçš„ãªãƒ¯ãƒ¼ã‚«ãƒ¼ç›£è¦–ãƒ»ç®¡ç†ã‚’æä¾›ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹
"""
import sys
import time
import signal
import threading
from pathlib import Path
from typing import Dict, Any
from datetime import datetime, timedelta

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from core import setup_logging, EMOJI
from libs.worker_health_monitor import WorkerHealthMonitor, WorkerPerformanceAnalyzer, WorkerAutoScaler
from libs.slack_notifier import SlackNotifier


class WorkerHealthMonitorService:
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ˜ãƒ«ã‚¹ç›£è¦–ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        """ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–"""
        self.logger = setup_logging(
            name="WorkerHealthMonitorService",
            log_file=PROJECT_ROOT / "logs" / "worker_health_monitor.log"
        )
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.health_monitor = WorkerHealthMonitor()
        self.performance_analyzer = WorkerPerformanceAnalyzer()
        self.auto_scaler = WorkerAutoScaler()
        self.slack = SlackNotifier()
        
        # è¨­å®š
        self.running = True
        self.check_interval = 30  # 30ç§’é–“éš”
        self.performance_check_interval = 300  # 5åˆ†é–“éš”
        self.scaling_check_interval = 600  # 10åˆ†é–“éš”
        
        # æœ€å¾Œã®ãƒã‚§ãƒƒã‚¯æ™‚åˆ»
        self.last_performance_check = datetime.now()
        self.last_scaling_check = datetime.now()
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´
        self.metrics_history = []
        self.max_history_length = 1000
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ã‚¹ãƒ«ãƒ¼ãƒ—"""
        self.logger.info(f"{EMOJI['start']} Worker Health Monitor Service started")
        
        # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©è¨­å®š
        signal.signal(signal.SIGTERM, self._handle_signal)
        signal.signal(signal.SIGINT, self._handle_signal)
        
        try:
            while self.running:
                self._perform_health_checks()
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
                if self._should_perform_performance_check():
                    self._perform_performance_analysis()
                    self.last_performance_check = datetime.now()
                
                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒã‚§ãƒƒã‚¯
                if self._should_perform_scaling_check():
                    self._perform_scaling_analysis()
                    self.last_scaling_check = datetime.now()
                
                time.sleep(self.check_interval)
                
        except Exception as e:
            self.logger.error(f"{EMOJI['error']} Service error: {str(e)}")
            self._send_critical_alert("Service error", str(e))
        finally:
            self.logger.info(f"{EMOJI['stop']} Worker Health Monitor Service stopped")
    
    def _handle_signal(self, signum, frame):
        """ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©"""
        self.logger.info(f"{EMOJI['stop']} Received signal {signum}, shutting down...")
        self.running = False
    
    def _perform_health_checks(self):
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        try:
            # åŒ…æ‹¬çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
            metrics = self.health_monitor.collect_comprehensive_metrics()
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ã«è¿½åŠ 
            self._add_to_history(metrics)
            
            # å¥å…¨æ€§è©•ä¾¡
            overall_healthy = metrics['system_health']['overall_healthy']
            
            if not overall_healthy:
                self.logger.warning(f"{EMOJI['warning']} System health degraded")
                self._handle_unhealthy_system(metrics)
            else:
                self.logger.debug(f"{EMOJI['success']} System health OK")
            
        except Exception as e:
            self.logger.error(f"{EMOJI['error']} Health check failed: {str(e)}")
    
    def _perform_performance_analysis(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æå®Ÿè¡Œ"""
        try:
            self.logger.info(f"{EMOJI['monitor']} Performing performance analysis")
            
            # æœ€è¿‘ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
            recent_metrics = self._get_recent_metrics(minutes=30)
            
            if not recent_metrics:
                return
            
            # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º
            worker_metrics = self._extract_worker_metrics(recent_metrics)
            bottlenecks = self.performance_analyzer.detect_bottlenecks(worker_metrics)
            
            if bottlenecks:
                self.logger.warning(f"{EMOJI['warning']} Performance bottlenecks detected: {list(bottlenecks.keys())}")
                self._handle_performance_issues(bottlenecks)
            
        except Exception as e:
            self.logger.error(f"{EMOJI['error']} Performance analysis failed: {str(e)}")
    
    def _perform_scaling_analysis(self):
        """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åˆ†æå®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼è€æ€§å¼·åŒ–ï¼‰"""
        try:
            self.logger.info(f"{EMOJI['scaling']} Performing scaling analysis")
            
            # WorkerHealthMonitor ãŒä¸å®Œå…¨ãªå ´åˆã®å¯¾ç­–
            if not hasattr(self.health_monitor, 'collect_comprehensive_metrics'):
                self.logger.warning("Scaling analysis skipped - collect_comprehensive_metrics not implemented")
                return
                
            if not hasattr(self.health_monitor, 'get_scaling_recommendations'):
                self.logger.warning("Scaling analysis skipped - get_scaling_recommendations not implemented")
                return
            
            # ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            current_metrics = self.health_monitor.collect_comprehensive_metrics()
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¨å¥¨å–å¾—
            system_metrics = {
                'queue_lengths': {'task_queue': 50, 'pm_queue': 10},  # å®Ÿéš›ã«ã¯ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—
                'worker_counts': {'task_worker': 2, 'pm_worker': 1},
                'avg_processing_times': {'task_worker': 2000, 'pm_worker': 1500},
                'system_load': 0.6
            }
            
            recommendations = self.health_monitor.get_scaling_recommendations(system_metrics)
            
            if recommendations:
                self.logger.info(f"{EMOJI['scaling']} Scaling recommendations: {recommendations}")
                self._handle_scaling_recommendations(recommendations)
                
        except AttributeError as e:
            self.logger.warning(f"{EMOJI['warning']} Scaling feature not available: {e}")
        except Exception as e:
            self.logger.error(f"{EMOJI['error']} Scaling analysis failed: {str(e)}")
    
    def _handle_unhealthy_system(self, metrics: Dict[str, Any]):
        """ä¸å¥å…¨ã‚·ã‚¹ãƒ†ãƒ ã®å¯¾å‡¦"""
        unhealthy_workers = []
        
        for worker_name, worker_metrics in metrics.get('workers', {}).items():
            if worker_metrics.get('status') != 'running':
                unhealthy_workers.append(worker_name)
        
        if unhealthy_workers:
            self.logger.warning(f"{EMOJI['warning']} Unhealthy workers: {unhealthy_workers}")
            
            # è‡ªå‹•å†èµ·å‹•è©¦è¡Œ
            restart_results = self.health_monitor.restart_unhealthy_workers()
            
            # çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
            for result in restart_results:
                if result.get('success'):
                    self.logger.info(f"{EMOJI['success']} Worker restarted successfully")
                else:
                    self.logger.error(f"{EMOJI['error']} Worker restart failed: {result.get('error')}")
                    
                    # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
                    self._send_critical_alert(
                        f"Worker restart failed",
                        f"Failed to restart worker: {result}"
                    )
    
    def _handle_performance_issues(self, bottlenecks: Dict[str, Any]):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã®å¯¾å‡¦"""
        for worker_name, issue in bottlenecks.items():
            issue_type = issue.get('type')
            
            if issue_type == 'queue_overload':
                self._send_performance_alert(worker_name, "Queue overload detected")
            elif issue_type == 'slow_processing':
                self._send_performance_alert(worker_name, "Slow processing detected")
            elif issue_type == 'high_error_rate':
                self._send_critical_alert(worker_name, "High error rate detected")
    
    def _handle_scaling_recommendations(self, recommendations: Dict[str, Any]):
        """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¨å¥¨ã®å¯¾å‡¦"""
        for worker_name, rec in recommendations.items():
            action = rec.get('action')
            current_count = rec.get('current_count', 1)
            recommended_count = rec.get('recommended_count', 1)
            
            if action == 'scale_up':
                self.logger.info(f"{EMOJI['scaling']} Recommending scale up for {worker_name}: {current_count} â†’ {recommended_count}")
                self._send_scaling_alert(worker_name, f"Scale up recommended: {current_count} â†’ {recommended_count}")
            elif action == 'scale_down':
                self.logger.info(f"{EMOJI['scaling']} Recommending scale down for {worker_name}: {current_count} â†’ {recommended_count}")
    
    def _should_perform_performance_check(self) -> bool:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°åˆ¤å®š"""
        return (datetime.now() - self.last_performance_check).total_seconds() >= self.performance_check_interval
    
    def _should_perform_scaling_check(self) -> bool:
        """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°åˆ¤å®š"""
        return (datetime.now() - self.last_scaling_check).total_seconds() >= self.scaling_check_interval
    
    def _add_to_history(self, metrics: Dict[str, Any]):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ã«è¿½åŠ """
        self.metrics_history.append(metrics)
        
        # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.metrics_history) > self.max_history_length:
            self.metrics_history = self.metrics_history[-self.max_history_length:]
    
    def _get_recent_metrics(self, minutes: int = 30) -> list:
        """æœ€è¿‘ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        cutoff_time = datetime.now() - timedelta(minutes=minutes)
        
        recent_metrics = []
        for metric in reversed(self.metrics_history):
            timestamp_str = metric.get('timestamp')
            if timestamp_str:
                try:
                    timestamp = datetime.fromisoformat(timestamp_str)
                    if timestamp >= cutoff_time:
                        recent_metrics.append(metric)
                except ValueError:
                    continue
        
        return recent_metrics
    
    def _extract_worker_metrics(self, metrics_list: list) -> Dict[str, Any]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º"""
        worker_metrics = {}
        
        for metrics in metrics_list:
            workers = metrics.get('workers', {})
            for worker_name, worker_data in workers.items():
                if worker_name not in worker_metrics:
                    worker_metrics[worker_name] = {
                        'queue_length': 50,  # å®Ÿéš›ã«ã¯ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—
                        'processing_time': 2000,  # å®Ÿéš›ã«ã¯è¨ˆæ¸¬
                        'error_rate': 0.05  # å®Ÿéš›ã«ã¯ãƒ­ã‚°ã‹ã‚‰è¨ˆç®—
                    }
        
        return worker_metrics
    
    def _send_critical_alert(self, title: str, message: str):
        """ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        try:
            alert_message = f"ğŸš¨ CRITICAL: {title}\n{message}\nTimestamp: {datetime.now().isoformat()}"
            self.slack.send_message(alert_message)
        except Exception as e:
            self.logger.error(f"Failed to send critical alert: {e}")
    
    def _send_performance_alert(self, worker_name: str, message: str):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        try:
            alert_message = f"âš ï¸ PERFORMANCE: {worker_name}\n{message}\nTimestamp: {datetime.now().isoformat()}"
            self.slack.send_message(alert_message)
        except Exception as e:
            self.logger.error(f"Failed to send performance alert: {e}")
    
    def _send_scaling_alert(self, worker_name: str, message: str):
        """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        try:
            alert_message = f"ğŸ“ˆ SCALING: {worker_name}\n{message}\nTimestamp: {datetime.now().isoformat()}"
            self.slack.send_message(alert_message)
        except Exception as e:
            self.logger.error(f"Failed to send scaling alert: {e}")


if __name__ == "__main__":
    service = WorkerHealthMonitorService()
    service.run()